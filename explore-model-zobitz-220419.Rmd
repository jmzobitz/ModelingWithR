--- 
title: "Exploring Modeling with Data and Differential Equations Using R"
author: "John M. Zobitz"
date: "Version 3.0.0"
description: "A textbook used for MAT 369 at Augsburg University."
#geometry: "left=0.5in,right=0.5in,top=0.75in,bottom=0.5in"
site: bookdown::bookdown_site
documentclass: krantz
#classoption: crcpaper
#documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
colorlinks: yes
lot: yes
lof: yes
github-repo: openscapes/series


output:
  bookdown::gitbook:
    toc_depth: 2
    split_by: part
    number_sections: true
    split_bib: true
    config:
      toc:
        scroll_highlight: yes
        collapse: section
        before: |
          <li><a href="./"><strong>Modeling with Data and Differential Equations in R</strong><br>by John Zobitz</a></li>
        after: |
          <li><a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a></li>
          <li><a href="https://bookdown.org" target="_blank">Built with Bookdown + RStudio</a></li>
      view: https://jmzobitz.github.io/ModelingWithR/%s
      search: yes
      sharing:
        twitter: yes
        facebook: no
        github: yes
        all: no

  bookdown::markdown_document2:
    default
---

```{r include=FALSE}
# Define the packages that we need to load up:
library(tidyverse)
library(demodelr)
library(lubridate)
library(bookdown)
library(knitr)
library(kableExtra)
library(gridExtra)
library(ggthemes)
```

# Welcome {-} 
This book is written for you, the student learning about modeling and differential equations. Perhaps you first encountered models, differential equations, and better yet, building plausible models from data in your calculus course.

This book sits "at the intersection" of several different mathematics courses: differential equations, linear algebra, statistics, calculus, data science - as well as the partner disciplines of biology, chemistry, physics, business, and economics.  An important idea is one of *transference* where a differential equation model applied in one context can also be applied (perhaps with different variable names) in a separate context.

I intentionally emphasize models from biology and the environmental sciences, but throughout the text you can find examples from the other disciplines. In some cases I've created homework exercises based on sources that I have found useful for teaching (denoted with "Inspired by ... "). I hope you see the connections of this content to your own intended major.

This book is divided into 4 parts:

1. Models with differential equations
2. Parameterizing models with data
3. Stability analysis for differential equations
4. Stochastic differential equations

You may notice the interwoven structure for this book: models are introduced first, followed by data analysis and parameter estimation, returning back to analyzing models, and ending with simulating random (stochastic) models. 

Unsure what about all these topics mean? Do not worry! The topics are presented with a "modeling first" paradigm that first introduces models, and equally important, explains how data are used to inform a model. This "conversation" between models and data are important to help build plausibility and confidence in a model.  Stability analysis helps to solidify the connection between models and parameters (which may change the underlying dynamical stability).  Finally the notion of *randomness* is extended with the introduction of stochastic differential equations.

Unifying all of these approaches is the idea of developing workflows for analysis, visualization results, and interpreting any results in the context of the problem.

## Computational code {-}
This book makes heavy use of the `R` programming language, and unabashedly develops programming principles using the `tidyverse` syntax and programming approach.  This is intentional to facilitate direct connections to courses in introductory data science or data visualization. Throughout my years learning (and teaching) different programming languages, I have found `R` to be the most versatile and adaptable. The `tidyverse` syntax, in my opinion, has transformed my own thinking about sustainable computation and modeling processes - and I hope it does for you as well.

There is a companion `R` package available called `demodelr` to run programs and functions in the text [@R-demodelr].  Instructions to install this package are given in Chapter \@ref(r-intro-02). The minimum version of `R` used was Version 4.0.2 (2020-06-22) [@R-base] and `RStudio` is Version 1.4.1717 [@rstudio_team_rstudio_2020].

The `demodelr` package uses the following `R` packages:

- `tidyverse` (and the associated packages) (Version 1.3.1) [@tidyverse2019]
- `GGally` (Version 2.1.2) [@R-GGally]
- `formula.tools` (Version 1.7.1) [@R-formula.tools]
- `expm` (Version 0.999-6) [@R-expm]


## Questions? Comments? Issues? {-}
Any errors or omissions are of my own accord, so please contact me at `zobitz@augsburg.edu`. Feel free to file an issue with the `demodelr` package to my [github.](https://github.com/jmzobitz/ModelingWithR/issues)

## About the cover {-}
The photo on the back cover was taken by Shannon Zobitz during a hike at [Orinoro Gorge](https://visitleppavirta.fi/en/service/orinoro-gorge) in Finland. The photo is indicative of several things: (1) the journey ahead as you commence learning about modeling, differential equations, and `R`, (2) the occasional roots in the path that may cause you to stumble (such as coding errors). Everyone makes them, so you are in good company. (3) the yellow markings on the trees indicate the way forward. The vector field image on the cover is an example of a spiral node, indicating my hope that the knowledge contained here spirals out and informs your future endeavors. May this textbook be the guide for you as you progress over the hill and onward. Let's get started!

## Acknowledgments {-}
This book has been developed over the course of several years in a variety of places: two continents, between meetings, in the early mornings, at coffee shops, or while waiting for practices to end. Special thanks are to the following: 

- **Augsburg University:** You have been my professional home for over a decade and given me the space and support to be intellectually creative in my teaching and scholarship. Special thanks to my Mathematics, Statistics, and Computer Science Department colleagues - it is a joy to work with all of you.

- **Augsburg University students:** Thank you for your interest and engagement in this topic, allowing me to test ideas in an upper division course titled (wait for it ...) *Modeling and Differential Equations in the Biological and Natural Sciences*. While the course title is a mouthful, you provided concise, honest, and insightful feedback, shaping this text. I am forever indebted to you. Kiitos to students in the Fall 2019 and 2021 courses.

- **My family:** Shannon, Colin, Grant, and Phoebe for humoring me (and my occasional grumpiness) while this project has been completed.

- **Taylor & Francis:** Thank you for your confidence in me with this project, and to my editor Lara Spieker for shepherding the project and Robin Lloyd Starkes and her team for their careful copyediting.



```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown','GGally','expm',
  'formula.tools','rstudioapi'
), 'packages.bib')
```

\mainmatter


<!--chapter:end:index.Rmd-->

# (PART) Models with Differential Equations {.unnumbered}

# Models of Rates with Data  {#intro-01}

## Rates of change in the world: a model is born

This book focuses on understanding _rates of change_ and their application to modeling real-world phenomena with contexts from the natural sciences. Additionally, this book emphasizes _using_ equations with data, building both competence and confidence to construct and evaluate a mathematical model with data. Perhaps these emphases are different from when you analyzed rates of change in a calculus course; consider the following types of questions:

- If $y = xe^{-x}$, what is the derivative function $f'(x)$?
- What is the equation of the tangent line to $y=x^{3}-x$ at $a=1$?
- Where is the graph of $\sin(x)$ increasing at an increasing rate?
- If you release a ball from the top of a skyscraper 500 meters above the ground, what is its speed when it impacts the ground?
- What is the largest area that can be enclosed in a chicken coop with 100 feet of fencing, with one side being along a wall?

The first three questions do not appear to be connected in a real-world context in their framing - but the last two questions *do* have some context from real-world situations. The given context may reveal underlying assumptions or physical principles, which are the starting point to build a mathematical model. For the chicken coop problem, perhaps the next step is to use the assumed geometry (rectangle) with the 100 feet of fencing to develop a function for the area.

Maybe the context includes observational data and several different (perhaps conflicting) assumptions about the context at hand. For example, how does air resistance affect the ball's velocity? Would a circular chicken coop maximize the area more than a rectangular coop? For both of these cases, which model is the best one to approximate any observational data?  The short answer: it depends. To understand why, let's take a look at another problem in context.


## Modeling in context: the spread of a disease
Consider the data in Figure \@ref(fig:sierra-leone-01), which come from an [Ebola outbreak](https://www.cdc.gov/vhf/ebola/history/2014-2016-outbreak/index.html) in Sierra Leone in 2014. (Data provided from @matthes_bisc204biomodeling_2021.) The vertical axis in Figure \@ref(fig:sierra-leone-01) represents Ebola *infections* over 2 years from initial monitoring in March 2014.

```{r sierra-leone-01,fig.cap='Infections from a 2014 Ebola outbreak in Sierra Leone, with the initial monitoring in March 2014.',echo=FALSE,message=FALSE}
ebola <- read_csv("data/ebola.csv") %>%
  mutate(date = mdy(`WHO report date`), `WHO report date` = NULL) %>%
  select(-c(2,4,6)) %>%
  gather(key=nation, value=cases,
         1:3) %>%
  mutate(monitor_days = as.numeric(date) - min(as.numeric(date)) + 1)



ebola %>%
  filter(nation == "Cases Sierra Leone") %>%
  ggplot() +
  geom_point(aes(x = monitor_days, y = cases), size = 1) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  labs(x = "Monitoring Days", y = "Ebola Infections") +
    scale_colour_colorblind() +
    coord_cartesian(ylim = c(0,15000))
```

Constructing a model from disease dynamics is part of the field of [mathematical epidemiology.](https://en.wikipedia.org/wiki/Mathematical_modelling_of_infectious_disease)\index{epidemiology} Here we focus on person to person or population spread of Ebola. Other types of models could focus on the immune response within a single person - perhaps with a goal to design effective types of treatments to reduce the severity of infection. How we construct a mathematical model for this outbreak largely depends on the assumptions underlying the biological dynamics of disease transmission (which we will call the *infection rate*\index{infection rate}). Three plausible assumptions for the infection rate are the following:

1. The infection rate is proportional to the number of people infected.
2. The infection rate is proportional to the number of people **not** infected.
3. The infection rate is proportional to the number of infected people coming into contact with those not infected.

Now let's explore how to translate these assumptions into a mathematical model. Since we are discussing *rates* of infection, this means we will need a *rate of change* or derivative. Let's use the letter $I$ to represent the number of people that are infected.

### Model 1: Infection rate proportional to number infected
The first assumption states that the infection rate is proportional to the number of people infected. Translated into an equation this would be the following:

\begin{equation}
\frac{dI}{dt} = kI (\#eq:00infected)
\end{equation}

Equation \@ref(eq:00infected) is an example of a *differential equation*\index{differential equation}, which is just a mathematical equation with rates of change. In Equation \@ref(eq:00infected) $k$ is a proportionality constant or parameter\index{parameter!differential equation}, with units of time$^{-1}$ for consistency.

The *solution* to a differential equation is a function $I(t)$. When we "solve" a differential equation we determine the family of functions consistent with our rate equation.^[You may be used to working with *algebraic equations* (e.g. solve $x^{2}-4=0$ for $x$) rather than differential equations. For algebraic equations the solution can be points (for our example, the solution to $x^2-4=0$ is $x=\pm2$).] There are a lot of techniques to solve a differential equation; we will explore some in Chapter \@ref(exact-solns-07).

The proportionality constant or *parameter* $k$ is important to understand the solution to Equation \@ref(eq:00infected). Even though no numerical value for $k$ is specified, you can always solve an equation without specifying the parameter. In some situations we may not be as concerned with the particular _value_ of the parameter but rather its influence on the long-term behavior of the system (this is a key aspect of bifurcation theory\index{bifurcation} described in Chapter \@ref(bifurcation-20)). Otherwise we can use the collected data shown above with the given model to determine the value for $k$. This combination of a mathematical model with data is called *data assimilation* or *model-data fusion* (see Chapters \@ref(linear-regression-08)-\@ref(information-criteria-14)).


How plausible is this first model? The first model assumes the rate of change (Equation \@ref(eq:00infected)) gets larger as the number of infected people $I$ increases. This reasoning certainly seems plausible: when there are so many people infected it can be hard to stay healthy! At some point the number of people who are *not* sick will reach zero, making the rate of infection zero (or no increase). In the case of Ebola or any other infectious disease, stringent public health measures would be enacted if the number of people infected became too large.^[The COVID-19 pandemic that began in 2020 is an example of the heroic efforts of public health officials.] Following public health measures we would expect that the rate of infection would decrease and the number of infections to slow. So perhaps another model this can capture this "slowing down" of the infection rate is more plausible. 

### Model 2:  Infection rate proportional to number NOT infected
The second model considers the interaction between people who are sick (which we have denoted as $I$) and people who are *not* sick, which we will call $S$, or susceptible.\index{susceptible} Equation \@ref(eq:01notinfected) is an example of a differential equation that models this interation:

\begin{equation} 
\frac{dI}{dt} = kS  (\#eq:01notinfected)
\end{equation}

As with Equation \@ref(eq:00infected) the parameter $k$ represents an infection rate. We would expect that both $I$ and $S$ change in time as the infection occurs; for a finite population as more people get sick ($I$), that would mean that $S$ would decrease. In effect, Model 2 should have *two* rates of change: one for $I$ and one for $S$. Figure \@ref(fig:initial-si) shows a schematic of this process of infection.

```{tikz, initial-si,fig.cap="Schematic diagram for Model 2, showing that the rate of infection is proportional to the number of susceptible people $S$. Assuming a constant population size $N$, the differential equation for Model 2 is given by Equation \\ref{eq:single-02}.",echo=FALSE,message=FALSE}
\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=2cm]

\node [vspecies] (S) {$S$} ;
\node [vspecies, right of = S] (I) {$I$} ;
\draw [->,thick] (S) --  node {\small{$k$}} (I) ;

\end{tikzpicture}
```

There are three reasons why I like to use diagrams like Figure \@ref(fig:initial-si):

(1) Diagrams build a bridge between biological processes and mathematical models.
(2) Diagrams signal which rates (if any) can be conserved (more on this below).
(3) Diagrams help to identify assumed parameters (i.e. $k$ in Figure \@ref(fig:initial-si)).
(4) Diagrams suggest how to construct differential equations for this mathematical model. Figure \@ref(fig:initial-si) suggests a flow between the suspectible state $S$ to the infected state $I$. So then the rate of change equation for $S$ is $\displaystyle \frac{dS}{dt} = -kS$ (the parameter listed above the arrow in Figure \@ref(fig:initial-si)). Equation \@ref(eq:02coupled) combines all this thinking and Equation \@ref(eq:01notinfected) into the following coupled system of differential equations in Equation \@ref(eq:02coupled):

\begin{equation}
\begin{split}
\frac{dS}{dt} &= -kS \\ 
\frac{dI}{dt} &= kS
\end{split}
(\#eq:02coupled)
\end{equation}


The solution to Equation \@ref(eq:02coupled) is functions $S(t)$ and $I(t)$ that evolve over time. We don't have the tools to determine the exact solutions for Equation \@ref(eq:02coupled) yet (we will study systems like these in Chapters \@ref(linearsystems-15)-\@ref(bifurcation-20)). However something interesting occurs with Equation \@ref(eq:02coupled) when we add the rates $\displaystyle \frac{dS}{dt}$ and $\displaystyle \frac{dI}{dt}$ together (Equation \@ref(eq:single-02)):

\begin{equation} 
\frac{dS}{dt} +  \frac{dI}{dt} = \frac{d}{dt}(S+I) = 0 (\#eq:single-02)
\end{equation}

If a rate of change equals zero then the corresponding function is constant. In effect, Equation \@ref(eq:single-02) means that the combined variable $S+I$ is constant, so we could say that $S+I=N$, where $N$ is the total population size. The expression $S+I=N$ is an example of a conservation law\index{conservation law} for our system.^[A finite population (meaning nobody can exit or enter the population) usually should have some type of conservation law.] Figure \@ref(fig:initial-si) also suggests a conservation law because there are no additional arrows going into or from the variables $S$ or $I$. Since $S=N-I$, Equation \@ref(eq:02coupled) can be re-written with a single equation (Equation \@ref(eq:02single)):

\begin{equation}
\frac{dI}{dt} = k(N-I) (\#eq:02single)
\end{equation}


Equation \@ref(eq:02single) also indicates limiting behavior for Model 2. As the number of infected people reaches $N$ (the total population size), the values of $\displaystyle \frac{dI}{dt}$ approaches zero, meaning $I$ doesn't change. Biologically this would suggest that eventually everyone in the population would get sick with the disease (assuming no one has any natural immunity). Equation \@ref(eq:02single) also has one caveat: if there are no infected people around ($I=0$) *the disease can still be transmitted*, which might not make good biological sense. The next model (Model 3) tries to amend that shortcoming.

### Model 3: Infection rate proportional to infected meeting not infected
Now consider a third model that rectifies some of the shortcomings of the second model (the second model rectified the shortcomings of the first model). The third model states that the rate of infection is due to those who are sick infecting those who are not sick. This scenario would also make some sense, as it focuses on the *transmission* of the disease between susceptibles and infected people. So if nobody is sick ($I=0$) then the disease is not spread. Likewise if there are no susceptibles ($S=0$), the disease is not spread as well.

In this case the diagram outlining the third model looks something like this:

```{tikz, logistic-scheme,echo=FALSE,message=FALSE,fig.cap = "Schematic diagram for Model 3, showing that the rate of infection is proportional to the number of susceptible people $S$ encountering an infected person $I$. Assuming a constant population size $N$, the differential equation for Model 3 is given by Equation \\ref{eq:logistic-01}."}
\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=2cm]

\node [vspecies] (S) {$S$} ;
\node [vspecies, right of = S] (I) {$I$} ;
\draw [->,thick] (S) --  node {\small{$kS$}} (I) ;

\end{tikzpicture}
```

Notice how in Figure \@ref(fig:logistic-scheme) there is an additional variable $S$ associated with $k$ to show how the rate of infection depends on $S$. Equation \@ref(eq:logistic-sys-01) contains the differential equations that describe the scenario outlined in Figure \@ref(fig:logistic-scheme):

\begin{equation}
\begin{split}
\frac{dS}{dt} &= -kSI \\
\frac{dI}{dt} &= kSI
\end{split} (\#eq:logistic-sys-01)
\end{equation}

Similar to Model 2 we can combine the two equations to yield a single differential equation (Equation \@ref(eq:logistic-01)):

\begin{equation}
\frac{dI}{dt} = k\cdot I \cdot (N-I) (\#eq:logistic-01)
\end{equation}

Equation \@ref(eq:logistic-01) appears similar to Equation \@ref(eq:02single), doesn't it?  However in Equation \@ref(eq:logistic-01) notice the variable $I$ outside the expression $(N-I)$. If $I=0$, then there is no increase in infection (the rate is zero). If $I=N$ (the total population size) then there is no increase in the infection (the rate is zero as well). Model 3 seems to be more consistent with the biological reasoning for the spread of infection.


Let's compare all the rates for all three models together in Figure \@ref(fig:threeRates-01). Figure \@ref(fig:threeRates-01) has a lot to unpack, but we can use some of our understanding of rates of change in calculus to compare the three models. Notice how the sign of $\displaystyle \frac{dI}{dt}$ is always positive for Model 1, indicating that the solution ($I$) is always increasing. For Models 2 and 3, $\displaystyle \frac{dI}{dt}$ equals zero when $I=10$, which also is the value for $N$  After that case, $\displaystyle \frac{dI}{dt}$ turns negative, meaning that $I$ is decreasing.

```{r threeRates-01,fig.cap='Comparing the rates of change for three models (Equation \\@ref(eq:00infected), Equation \\@ref(eq:02single), and Equation \\@ref(eq:logistic-01), with $k=1$ and $N=10$).',echo=FALSE,message=FALSE}
x <- seq(0, 15, length.out = 500)
n <- 10
k <- 1

data.frame(x, m1 = k*x, m2 = k*(n - x), m3 = k*x * (n - x)) %>%
  gather(key = model, value = rate, m1, m2, m3) %>%
  ggplot(aes(x = x, y = rate, color = model)) +
  geom_line(size = 1) +
  xlab("I") +
  ylab("dI/dt") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  geom_hline(yintercept = 0) +
  coord_cartesian(ylim = c(-20,30)) +
  scale_colour_colorblind(labels = c("Model 1", "Model 2", "Model 3"), name = NULL)

```


In summary, examining the graphs of the rates can tell a lot about the *qualitative behavior*\index{differential equation!qualitative behavior} of a solution to a differential equation even without the solution.

## Model solutions
Let's return back to possible solutions (in this case formulas for $I(t)$) for our models. Usually a differential equation also has a starting or an initial value (typically at $t=0$) that actualizes the solution. When we state a differential equation with a starting value we have an **initial value problem**.\index{differential equation!initial value problem}  We will represent that initial value as $I(0)=I_{0}$.

With that assumption, we can (and will solve later!) the following solutions for these models:

\begin{equation}
\begin{split}
\mbox{ Model 1 (Exponential): } & I(t) = I_{0}e^{kt} \\
\mbox{Model 2 (Saturating): } & I(t) = N-(N-I_{0})e^{-kt} \\
\mbox{Model 3 (Logistic): } & I(t) = \frac{N \cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}}
\end{split} (\#eq:all3-01)
\end{equation}

Notice how I assigned the names to each model (Exponential, Saturating, and Logistic). That may not mean much at the moment, but Figure \@ref(fig:three-soln) plots the three functions $I(t)$ together when $I_{0}=250$, $k=0.023$, and $N=13600$.

```{r three-soln,message=FALSE,echo=FALSE,warning=FALSE,fig.cap='Three models (Exponential, Saturating, and Logistic; Equation \\@ref(eq:all3-01)) compared.'}
x <- seq(0, 600, length.out = 100)
n <- 13600
k <- 0.023
i0 <- 250

data.frame(x, m1 = i0 * exp(k * x), m2 = n - (n - i0) * exp(-k * x), m3 = n * i0 / (i0 + (n - i0) * exp(-k * x))) %>%
  gather(key = model, value = rate, m1, m2, m3) %>%
  ggplot(aes(x = x, y = rate, color = model)) +
  geom_line(size = 1) +
  xlab("t (days)") +
  ylab("Infected people") +
  theme_bw() +
  scale_color_colorblind(labels = c("Model 1", "Model 2", "Model 3"), name = NULL) +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  coord_cartesian(ylim = c(0,15000))

```

Notice how in Figure \@ref(fig:three-soln) Model 1 increases quickly - it actually grows without bound off the chart!  Model 2 and Model 3 have saturating behavior, but it looks like Model 3 might be the one that actually captures the trend of the data. 



## Which model is best?
All three of these scenarios describe different modeling scenarios. With the saturating and logistic models (Models 2 and 3) we have some limiting behavior, the possibility that the rate of infection slows. Of the two models, which one is the _best_ one?  Here could be some possible criteria we could evaluate:

- Do the model outputs match the data?
- For timeseries data are the trends accurately represented?
- Can the model be coded easily into the computer?
- How will model outputs compare with newly collected measurements?
- Regarding model complexity - how many equations do we have?
- Are the number of model parameters too few or too many?


We will address several of these critera later on in this textbook when we discuss *model selection* (Chapter \@ref(information-criteria-14)).\index{model!selection}  Model selection is one key part of the modeling hypothesis - where we investigate the implications of a particular model analyzed. If we don't do this, we don't have an opportunity to test out what is plausible for our models.


## Start here
In summary, it turns out that even with some initial assumptions we can very quickly build up a mathematical model to explain data. Even with these first steps we have a lot more to uncover:

- How would you determine the parameters $k$ and $N$ with the collected data?
- Are there other more complicated models?
- What exact techniques are used to determine the solution $I(t)$?
- Are there other numerical techniques to approximate the solution $I(t)$?
- What happens to our solutions when the parameters $k$ and $N$ change?
- What happens to our solutions when the number of infected people changes randomly for some reason?

This text  will study answers to these questions and more. Let's get started!


## Exercises

```{exercise, label="plot-soln"} 
Solutions to an outbreak model of the flu are the following:
  
  \begin{equation}
\begin{split}
\mbox{Saturating model: } & I(t) = 3000-2990e^{-.1t} \\
\mbox{Logistic model: } &  I(t) = \frac{30000 }{10+2990e^{-.15t}},
\end{split}
\end{equation}

where $t$ is in days. Use these two functions to answer the following questions:
  


a. Plot the saturating and logistic models when $0 \leq t \leq 100$. 
b. For both models, how would you describe the growth of the outbreak as $t$ increases?  How many people will be infected overall?  
c. Finally, for both models evaluate $\lim_{t \rightarrow \infty} I(t)$. How do these results compare to values found on your graph?

```



```{r, label="liberia-01",fig.cap='Infections from a 2014 Ebola outbreak in Liberia, with the initial monitoring in March 2014.',echo=FALSE,message=FALSE}
# ebola <- read_csv("data/ebola.csv") %>%
#   mutate(date = mdy(`WHO report date`), `WHO report date` = NULL) %>%
#   select(-c(1, 3, 5)) %>%
#   gather(
#     key = nation, value = deaths,
#     1:3
#   ) %>%
#   mutate(monitor_days = as.numeric(date) - min(as.numeric(date)) + 1)

ebola %>%
  filter(nation == "Cases Liberia") %>%
  ggplot() +
  geom_point(aes(x = monitor_days, y = cases), size = 1) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  labs(x = "Monitoring Days", y = "Ebola infections") +
  scale_colour_colorblind()

```


```{exercise} 
Figure \@ref(fig:liberia-01) shows the Ebola outbreak for the country of Liberia in 2014.
If we were to apply the logistic model (Model 3) based on this graphic what would be your estimate for $N$?
```



```{exercise}
The general solutions for the saturating and the logistic models are:
    \begin{equation}
\begin{split}
\mbox{Saturating model: } & I(t) = N-(N-I_{0})e^{-kt} \\
\mbox{Logistic model: } &  I(t) = \frac{N \cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}},
\end{split}
\end{equation}
where $I_{0}$ is the initial number of people infected and $N$ is the overall population size. Using the functions from Exercise \@ref(exr:plot-soln) for both models, what are the values for $N$ and $I_{0}$?
```


```{exercise}
The general solutions for the saturating and the logistic models are:
    \begin{equation}
\begin{split}
\mbox{Saturating model: } & I(t) = N-(N-I_{0})e^{-kt} \\
\mbox{Logistic model: } &  I(t) = \frac{N \cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}},
\end{split}
\end{equation}
where $I_{0}$ is the initial number of people infected and $N$ is the overall population size. For both models carefully evaluate the limits to show $\lim_{t \rightarrow \infty} I(t)=N$. How do these limiting values compare to the steady-state values you found for Models 2 and 3 in Figure \@ref(fig:three-soln), where $N=13600$?
```



```{tikz, exercise-imm,fig.cap="Schematic diagram for Exercise \\ref{exr:simple-mig-01}.",echo=FALSE,message=FALSE}
\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=2cm]

\node [vspecies] (S) {$S$} ;
\node [vspecies, right of = S] (I) {$I$} ;
\node [left of = S] (beta) {} ;
\draw [->,thick] (S) --  node {\small{$k$}} (I) ;
\draw [->,thick] (beta) --  node {\small{$\beta$}} (S) ;


\end{tikzpicture}
```

```{exercise simple-mig-01}
Figure \@ref(fig:exercise-imm) shows a schematic diagram which is a variation on Figure \@ref(fig:initial-si). In this case people are entering the the susceptible population $S$ at a rate $\beta$, so the population is not conserved. What is the coupled system of differential equations for this model?
```

 <!-- Sethi model for advertising -->
```{exercise}
A model that describes the growth of sales of a product in response to advertising is the following:
  
  \begin{equation}
  \frac{dS}{dt} = .55\sqrt{1-S}-S,
\end{equation}
  where $S$ is the product's share of the market (scaled between 0 and 1) [@sethi_deterministic_1983]. Use this information to answer the following questions:
  

a. Make a plot of the function $f(S)=.55\sqrt{1-S}-S$. for $0 \leq S \leq 1$.
b. Interpret your plot to predict when the market share will be increasing and decreasing. At what value is $\displaystyle \frac{dS}{dt}=0$?  (This is called the *steady state* value.) \index{steady state}
c. A second campaign has the following differential equation:

\begin{equation}
  \frac{dS}{dt} = .2\sqrt{1-S}-S
\end{equation}
  
What is the steady state value and how does it compare to the previous one?

```


```{exercise}
A more general form of the advertising model is

\begin{equation}
\frac{dS}{dt} = r\sqrt{1-S}-S,
\end{equation}

where $S$ is the product's share of the market (scaled between 0 and 1). The parameter $r$ is related to the effectiveness of the advertising (between 0 and 1).
  

a. Solve $\displaystyle \frac{dS}{dt} = r\sqrt{1-S}-S$ for the steady state value (where $\displaystyle \frac{dS}{dt}=0$). Your final answer should be expressed as a function $S(r)$ - for which you will need to use the quadratic formula.
b. Make a plot of the steady state value as a function of $r$, where $0 \leq r \leq 1$.
c. Based on your plot, what can you conclude about the steady state value as the effectiveness of the advertising increases?

  
```


```{exercise}
A common saying is "you are what you eat." An equation that relates an organism's nutrient content (denoted as $y$) to the nutrient content of food or resource (denoted as $x$) is given by:

\begin{equation}
 y = c x^{1/\theta},
\end{equation}

where $\theta$ and $c$ are both constants. Units on $x$ and $y$ are expressed as a proportion of a given nutrient (such as nitrogen or carbon). For example, when $c=1$ and $\theta = 1$ the function is $y=x$. In this case the point $(0.05,0.05)$ would say that nutrient composition for the organism and resource would be the same.


a. Now assume that $c=1$. How does the nutrient content of the organism compare to the resource when $\theta=2$?  Draw a sample curve and interpret it, contrasting it to when $\theta = 1$.
b. Now assume that $c=1$. How does the nutrient content of the organism compare to the resource when $\theta=5$?  Draw a sample curve and interpret it, contrasting this curve to the previous two.
c. What do you think will happen when $\theta \rightarrow \infty$? Draw some sample curves to help illustrate your findings.

```


```{exercise}
A model for the outbreak of a cold virus assumes that the rate people get infected is proportional to infected people contacting susceptible people, as with Model 3 (the Logistic model). However people who are infected can also recover and become susceptible again with rate $\alpha$. Construct a diagram similar to Figure \@ref(fig:logistic-scheme) for this scenario and also write down what you think the system of differential equations would be.
```



```{exercise flu-quarantine-01}
A model for the outbreak of the flu assumes that the rate people get infected is proportional to infected people contacting susceptible people, as in Model 3. However people also recover from the flu, denoted with the variable $R$. Assume that the rate of recovery is proportional to the number of infected people with parameter $\beta$. Construct a diagram similar to Figure \@ref(fig:logistic-scheme) for this scenario and also write down what you think the system of differential equations would be.
```

 <!-- Van den Berg page 19 -->
```{exercise}
(Inspired by @berg_mathematical_2011) Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of $S$ in the blood is the following:

  \begin{equation}
\frac{dS}{dt} = I + p \cdot (W - S),
\end{equation}
where the parameter $I$ represents the active uptake of salt, $p$ is the permeability of the skin, and $W$ is the salinity in the water. Use this information to answer the following questions:
  

a. What is that value of $S$ at *steady state*, or when $\displaystyle \frac{dS}{dt} = 0$?  Your final answer should be a function $S(I,p,W)$.
b. With the steady state solution, what parameters ($I$, $p$, or $W$) cause the steady state value $S$ to increase?

```



<!-- From LW -->
```{exercise}
(Inspired by @logan_mathematical_2009) The immigration rate of bird species (species per time) from a mainland to an offshore island is $I_{m} \cdot (1-S/P)$, where $I_{m}$ is the maximum immigration rate, $P$ is the size of the source pool of species on the mainland, and $S$ is the number of species already occupying the island. Additionally the extinction rate is $E \cdot S / P$, where $E$ is the maximum extinction rate. The growth rate of the number of species on the island is the immigration rate minus the extinction rate.


a. Make representative plots of the immigration and the extinction rates as a function of $S$. You may set $I_{m}$, $P$, and $E$ all equal to 1.
b. Determine the number of species for which the net growth rate is zero, or the number of species is in equilibrium. Express your answer as $S$ as a function of $I_{m}$, $P$, and $E$.
c. Suppose that two islands of the same size are at different distances from the mainland. Birds arrive from the source pool and they have the same extinction rate on each island. However the maximum immigration rate is larger for the island farther away. Which of the two islands will have the larger number of species at equilibrium?

```

<!-- Based off LW pg 4 -->
```{exercise}
(Inspired by @logan_mathematical_2009) Assume that an animal assimilates nutrients at a rate $R$ proportional to its surface area. Also assume that it uses nutrients at a rate proportional to its volume. You may assume that the size of the animal is implicitly a function of the nutrient intake and usage, so $R = k_{A} A - k_{V} V$, where $k_{A}$ and $k_{V}$ are constants, $A$ is the surface area, and $V$ the volume. Determine expressions for the size of the animal if its intake and use rates were in balance (meaning $R$ is set to zero), assuming the animal is the following shapes:


a. A sphere (assume size is measured with radius $r$) *Note:* first determine the geometric formulas for surface area and volume.
b. A cube (assume size is measured with length $l$)

```




<!--chapter:end:01-intro.Rmd-->

# Introduction to R {#r-intro-02}

The primary tools we will use to analyze models for this book are `R` [@R-base] and `RStudio` [@rstudio_team_rstudio_2020].[^r-posit] These programs are powerful ones to learn! Admittedly learning a new software may be challenging; however I think it is worth it. With `R` you will have enormous flexibility to efficiently utilize data, design effective visualizations, and process statistical models. Let's get started!

[^r-posit]: In July 2022 `RStudio` announced it was changing its name to `posit` (<https://www.rstudio.com/blog/rstudio-is-becoming-posit/>). At this time of this writing `posit` was not available, although I expect its look and feel will be very similar to `RStudio`.

## `R` and RStudio

First let's talk terminology. The program `RStudio` is called an *Integrated Development Environment* for the statistical software language `R`.

To get both `R` and `RStudio` requires two separate downloads and files, which can be found here:

-   `R`: <https://cran.r-project.org/mirrors.html> (You need to select a location to download from; choose any one that is geographically close to you.)
-   `RStudio`: <https://www.rstudio.com/products/rstudio/download/>

### Why do we have two programs?

Think of `R` as your basic program - this is the engine that does the computation. `RStudio` is a program where you can see everything you are working on in one place. Figure \@ref(fig:r-studio-pane) shows an example of a typical `RStudio` workspace:

```{r label="r-studio-pane", echo=FALSE,out.width = "85%",fig.align="center",fig.cap="A sample \\texttt{RStudio} workspace from one of my projects."}
knitr::include_graphics("figures/02-intro/rStudio-pane.png")
```

There are 4 key panels that I work with, clockwise from the top:

-   The **source** window is in the upper left - notice how those have different tabs associated with them. You can have multiple source files that you can toggle between. For the moment think of these as commands that you will want to send to R.
-   The **environment** and **history** pane - these tables allow you to see what variables are stored locally in your environment, or the history of commands.
-   The **files** or **plots** pane (a simple plot I was working on is shown currently), but you can toggle between the tabs. The files tab shows the files in the current `Rstudio` project directory.
-   Finally, the **console** pane is the place where `R` works and runs commands. You can type in there directly; otherwise we will also just "send" commands from the source down to the console.

Now we are ready to work with `R` and `RStudio`!

## First steps: getting acquainted with `R`

Open up `RStudio`. The first task is to create a project file. A project is a central place to organize your materials. If you have previous experience with `R` you may be familiar with how the program is picky about its working directory - or the location on the computer where computations, files, and data are currently saved. Creating a project file is an easy way to avoid some of that fussiness. Here are the steps to accomplish this:

1. In `RStudio` select "File" then "New Project".
2. Next select the first option "New Directory" in the window - this will create a new folder on your computer.
3. At the next window choose "New Directory" or "Existing Directory". Depending on the option you choose, you will have some choice as to where you want to place this project.
4. Name the project as you like.
5. Click the "Create Project" button.

It might be helpful to think of a project file as a physical folder where you store papers that have something in common (such as class notes). When you want to work on the project, you open up your folder (and similarly close your project when you are done). At the point of return, you can re-open your folder and pick up where you left off. An `RStudio` project is similar in that regard as well.

### Working with `R`

Our next step: how does `R` compute something? For example if we wanted to compute `4+9` we could type this command in the `R` console (lower left) window.^[I know you know the result is 13, but this is an illustrative example.] Try this out now:

1. In the console type `4+9`
2. Then hit enter (or return)
3. Is the result 13?

Success! While this workflow is helpful for a single expression, making use of script files (`.R` file) can help run multiple steps of code at once. Script files are located in the upper left hand corner of your `RStudio` window - or the source window. (You may not have anything there when you start working on a project, so let's create one.)

1. In `RStudio` select "File" then "New File"
2. Next select the first option "New Script"
3. A new window called "UntitledX" should appear, where X is a number. You are set to go![^rintro-1]

[^rintro-1]: **Pro tip:** There are shortcuts to creating a new file: Ctrl+Shift+N (Windows and Linux) or Command+Shift+N (Mac).

Source files allow you to type in `R` code and then evaluate, which is sometimes called "sending a command to the console" - or moving an `R` statement from the source window to the console. Working with a script file allows you to fix any coding errors more quickly and then re-run your code rather than re-type everything. Let's practice this.Type `4+9` in the script file. To evaluate this statement you have several options:

1. Copying and pasting the command to the window. Shortcuts are Ctrl+C / Command+C for copying and Ctrl+V / Command+C for Windows / Mac.
2. Run one line at a time. This means that your cursor is at the line in your source file; then click the "Run" button in the upper right hand side of the source window. Shortcuts are Ctrl+Enter / Command+Enter.
3. You can also source the entire script file, which means running all the lines from top to bottom. You do this by clicking the source button, or with shortcuts Ctrl+Shift+Enter / Cmd+Shift+Enter (Windows / Mac). Understandably for several lines of code this makes things easier.

Sometimes source files contain comments, which are helpful notes to you, or future you, or anyone else you want to share your work with. Comments in `R` are used with the hashtag (`#`), which appear as green text in `RStudio`.

### Saving your work

The neat part about a source file is that it allows you to save the file (Ctrl+S / Cmd+S). The first time you do this you may need to give this a name. The location where this file will be saved is in the same directory as your `.Rproj` project file. Now you have a file that you can come back to! In general I try to use descriptive names of files so I can refer back to them later.


## Increasing functionality with packages

Packages are one way that `R` gets some awesome versatility.\index{package} Packages are contributed, specialized code produced by users (just like you!), and shared with the world. Packages are similar to apps on your phone, which rather than obtaining them from the app store can be found in two different places:

-   [CRAN](https://cran.r-project.org/), which stands for **C**omprehensive **R** **A**rchive **N**etwork. This is the clearing house for many contributed packages - and allows for easy cross-platform functionality.
-   Github. This is another place where people can share code and packages (including myself!). The code here has not been vetted through CRAN for compatibility, but if you trust the person sharing the code, it should work.

Let's now start to download some useful packages. The first package is [`tidyverse`](https://www.tidyverse.org/), which is actually a collection of packages. If you take an introductory data science course you will most likely be learning more about this package, but to install this at the command line you type the following:

```{r eval=FALSE}
install.packages("tidyverse")
```

Typing this line will connect to the CRAN download mirrors and install this set of packages locally to your computer. It make take some time, but be patient. Sometimes when you are installing packages you may be prompted to install additional packages. In this case just say yes.

For this textbook I have written a collection of functions and data that we will use. This package name is called `demodelr` (**D**ifferential **E**quations and **Model**s in **R**; @R-demodelr).^[You can also find `demodelr` on github as well: <https://github.com/jmzobitz/demodelr>.]\index{demodelr} To install this package you will run the following line:

```{r eval=FALSE}
install.packages("demodelr")
```

Here is the good news: *you only need to install a package once before using it!* To load the package up into your workspace you use the command `library`:

```{r eval=FALSE}
# Purpose: compute the growth in weight of a dog over time.
# Author: JMZ
# Last modified: 02-17-2022

library(tidyverse)
library(demodelr)
```

You need to load these libraries *each time you restart your R session*. This is part of the benefit of a script file - at the start I always declare the libraries that I will need. In addition, the first few lines of the script file contains comments (prefaced with `#`) to denote the basic purpose of the file, who wrote it, and the date it was last revised. This type of information is good programming practice. If you are a newbie to programming with `R`, building these habits will become second nature as you progress in your abilities.

## Working with `R`: variables, data frames, and datasets

### Creating variables

The next thing we will want to do is to define variables that are stored locally, which is easy to do:

```{r}
my_result <- 4 + 9
```

The symbol `<-` is assignment (you can use equals (=), but it is good coding practice to use the arrow for assignment). Notice how I named the variable called `my_result`. Generally I prefer using *descriptive* names for variables for the context at hand. (In other words, the variable `x` would be an odd choice - too ambiguous.) I also used snake case to string together multiple words. In practice you can use snake case, or alphabetic cases (`myResult`) or even `my.result` (although that may not be preferred practice in the long run). However, if you name variables as `my-result` it looks like subtraction between variables `my` and `result`. I try to follow the [tidyverse style guide](https://style.tidyverse.org/) whenever possible.

Once we have defined a variable, we can compute with it. For example `10*my_result` should yield 130. Cool, no?

Sequences defined as vectors are another useful construction. In `R` this is done with the `seq` function along with additional information such as the starting value, ending value, and step size. As an example, let's define a sequence, spaced from 0 to 5 with spacing of 0.05 and then store this sequence as variable called `my_sequence`:

```{r eval=FALSE}
my_sequence <- seq(from = 0, to = 5, by = 0.05)
```

The format for the function `seq` is `seq(from=start,to=end,by=step_size)`. The `seq` command is a pretty flexible - there are alternative ways you can generate a sequence by specifying the starting and the end values along with the number of points. If you want to know more about `seq` you can always use `?` followed by the command - that will bring up the help values:

```{r eval=FALSE}
?seq
```

Once you get more comfortable with syntax in `R`, you will see that `seq(0,5,0.5)` gives the same result as `seq(from=0,to=5,by=0.05)`, but it is helpful to write your code *so that you can understand what it does*.[^rintro-2]

[^rintro-2]: I believe that code should be built for humans, not computers; see @wilson_best_2014 for more information.

### Data frames

A key structure in `R` is that of a data frame, which allows different types of data to be collected together.\index{data frame} A data frame is like a spreadsheet where each column is a value and each row a value (much like you would find in a spreadsheet). As an example, a data frame may list values for solutions to a differential equation, like we did with our three infection models in Chapter \@ref(intro-01) (Table \@ref(tab:model-table)).

```{r model-table, echo = FALSE, message=FALSE, results='asis'}
time <- seq(0, 600, by=1)
n <- 13600
k <- 0.03
i0 <- 250

model_out <- tibble(time, model_1 = i0 * exp(k * time), model_2 = n - (n - i0) * exp(-k * time), model_3 = n * i0 / (i0 + (n - i0) * exp(-k * time))) %>% round()

model_out[1:5, ] %>%
  kbl(caption = "Sample model solutions for an exponential, saturating, or logistic differential equation") %>%
  kable_paper(full_width = FALSE)
```

Data frames are an example of *tidy* data, where each row is an observation, each column a variable (which can be quantitative or categorical).\index{tidy data} There are several different ways to define a data frame in `R`. I am going to rely on the approach utilized by the `tidyverse`, which defines data frames as [`tibbles`](https://tibble.tidyverse.org/).\index{tidy data!tibbles}

As an example, the following code defines a data frame that computes the quadratic function $y=3x^2-2x$ for $-5 \leq x \leq 2$.

```{r eval=FALSE}

x <- seq(from = -5, to = 2, by = 0.05)
y <- 3 * x^2 - 2 * x

my_data <- tibble(
  x = x,
  y = y
) # Notice how x and y are specifically defined
```

Notice that the data frame `my_data` uses the column (variable) names of `x` and `y`. You could have also used `tibble(x,y)`, but it is helpful to name the columns in the way that you would like them to be named.

In addition to defining a data frame, `R` also contains several datasets in memory. In fact to see all the datasets, type `data()` at the console. Packages may also have datasets bundled with them. If you want to see the datasets for the `demodelr` package, you would type `data(package = "demodelr")` at the console.

### Reading in datasets

Another `R` skill is importing data into `R`. Data come in several different types of formats, but one of the more versatile ones is a csv (**c**omma **s**eparated **v**alues) file. A csv file is a simplified version of an Excel or Google spreadsheet.[^rintro-3] To read in the file you will use the command `read_csv` (part of the `readr` package in the `tidyverse`). The `read_csv` command which has the following structure, where FILENAME refers to the location of the file on your computer:

[^rintro-3]: While the following steps focus on csv files, `R` can read in Excel files with the `readxl` package (<https://readxl.tidyverse.org/>) or Google sheets with the `googlesheets4` package (<https://googlesheets4.tidyverse.org/>).

```{r eval=FALSE}
in_data <- read_csv(FILENAME)
```

For example the following code would read in a csv file of Ebola data located in the project directory:

```{r eval=FALSE}
ebola <- read_csv("ebola.csv")
```

Notice the quotes around the FILENAME.[^rintro-4] The command `read_csv` is part of the `tidyverse`, but the function `read.csv` uses base `R`. They operate a little differently, but this book will use the `read_csv` command.

[^rintro-4]: **Pro tips:** It is helpful to make a subfolder of your `R` project called data, where all csv files are stored. Then if you have the data files in the data folder, in RStudio you can type "data" and it may start to autocomplete - this is handy.

## Visualization with `R`

Now we are ready to begin visualizing data frames. Two types of plots that we will need to make will be a scatter plot and a line plot.\index{plot!scatter}\index{plot!line} We are going to consider both of these separately, with examples that you should be able to customize.

### Making a scatterplot

One dataset we have is the weight of a dog over time, adapted from [this referenced website.](http://bscheng.com/2014/05/07/modeling-logistic-growth-data-in-r/) The data frame we will use is called `wilson` and is part of the `demodelr` library. You can also explore the documentation for this dataset by typing `?wilson` at the console. The `wilson` dataset has two variables here: $D=$ the age of the dog in days and $W=$ the weight of the dog in pounds. I have the data loaded into the `demodelr` package, which you can investigate by typing the following at the command line:

```{r eval=FALSE}
glimpse(wilson)
```

Notice that this data frame has two variables: `days` and `weight`. To make a scatter plot of these data we are going to use the command `ggplot` in Figure \@ref(fig:wilson-weight-02):

```{r eval = FALSE}
ggplot(data = wilson) +
  geom_point(aes(x = days, y = weight)) +
  labs(
    x = "Days since birth",
    y = "Weight (pounds)"
  )
```

```{r wilson-weight-02,echo=FALSE,fig.cap="Measured weight of the dog Wilson over time."}
ggplot(data = wilson) +
  geom_point(aes(x = days, y = weight)) +
  labs(
    x = "Days since birth",
    y = "Weight (pounds)"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```


Wow! The code to produce Figure \@ref(fig:wilson-weight-02) looks complicated. Let's break this down step by step:

-   `ggplot(data = wilson) +` sets up the graphics structure and identifies the name of the data frame we are including.
-   `geom_point(aes(x = days, y = weight))` defines the type of plot we are going to be making.
-   `geom_point()` defines the type of plot geometry (or *geom*) we are using here - in this case, a point plot.
-   `aes(x = days, y = weight)` maps the *aesthetics* of the plot. On the $x$ axis is the `days` variable; on the $y$ axis is the `weight` variable. You may also write this as `mapping = aes(x = days, y = weight)`.
-   The statement beginning with `labs(x=...)` defines the labels on the $x$ and $y$ axes.

I know this seems like a lot of code to make a visualization, but this structure is actually used for some more advanced data visualization. Think of the `+` structure at the end of each line as the connector between `ggplot` and the plot `geom`. Trust me - learning how to make informative plots can be a useful skill!

### Making a line plot

Using the same `wilson` data, later on we will discover that the function $\displaystyle W =f(D)= \frac{70}{1+e^{2.46-0.017D}}$. represents these data. In order to make a graph of this function we need to first build a data frame (Figure \@ref(fig:wilson-model-02)):

```{r eval = FALSE}

# Choose spacing that is "smooth enough"
days <- seq(from = 0, to = 1500, by = 1) 
weight <- 70 / (1 + exp(2.46 - 0.017 * days))

wilson_model <- tibble(
  days = days,
  weight = weight
)

ggplot(data = wilson_model) +
  geom_line(aes(x = days, y = weight)) +
  labs(
    x = "Days since birth",
    y = "Weight (pounds)"
  )
```

```{r wilson-model-02,fig.cap="Logistic model equation to describe the weight of the dog Wilson over time.",echo=FALSE}

# Choose spacing that is "smooth enough"
days <- seq(from = 0, to = 1500, by = 1) 
weight <- 70 / (1 + exp(2.46 - 0.017 * days))

wilson_model <- tibble(
  days = days,
  weight = weight
)

ggplot(data = wilson_model) +
  geom_line(aes(x = days, y = weight)) +
  labs(
    x = "Days since birth",
    y = "Weight (pounds)"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```


Notice that once we have the data frame set up, the structure is very similar to the scatter plot - but this time we are using `geom_line()` rather than `geom_point`.

### Changing options

Curious about using a different color in your plot or a thicker line? That is fairly easy to do. For example if we wanted to make either our points or line a different color, we adjust the `ggplot` to the following code (not evaluated here, but try it out on your own):

```{r eval=FALSE}
ggplot(data = wilson) +
  geom_point(aes(x = days, y = weight), color = "red", size = 2)
labs(
  x = "Days since birth",
  y = "Weight (pounds)"
)
```

Notice how the command `color='red'` was applied *outside* of the aes - which means it gets mapped to each of the points in the data frame. `size=2` refers to the size (in millimeters) of the points. I've linked more options about the colors and sizes you can use here:

-   **Named colors in R:** [gallery of `R` colors.](https://www.r-graph-gallery.com/42-colors-names.html) Scroll down to "Picking one color in R" - you can see the list of options!
-   **More colors:** [colors in `ggplot`.](http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/). More information about working with colors.
-   **Using hexadecimal colors:** [hexadecimal colors.](http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#hexadecimal-color-code-chart) (You specify these by the code so `"#FF3300"` is a red color.)
-   **Changing sizes of lines and points:** [modifying a `ggplot`.](https://ggplot2.tidyverse.org/articles/ggplot2-specs.html)

### Combining scatter and line plots.

Combining the data (Figure \@ref(fig:wilson-weight-02)) with the model (Figure \@ref(fig:wilson-model-02)) in the same plot can be done by combining the `geom_point` with the `geom_line`, as shown in the following code (try it out on your own):

```{r wilson-model-data-02,fig.cap="Comparison of the logistic model equation with the measured weight of the dog Wilson over time.",eval=FALSE}
ggplot(data = wilson) +
  geom_point(aes(x = days, y = weight), color = "red") +
  geom_line(data = wilson_model, aes(x = days, y = weight)) +
  labs(
    x = "Days since birth",
    y = "Weight (pounds)"
  )
```

Notice in the above code a subtle difference when I added in the dataset `wilson_model` with `geom_line`: you need to name the `data` bringing in a new data frame to a plot geom. While it may be useful to have a [plot legend](http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/), for this textbook the context will be apparent without having a legend.

## Defining functions

We will study lots of other built-in functions for this course, but you may also be wondering how you define your own function (let's say $y=x^{3}$). We need the following construct for our code:

```{r eval=FALSE}

function_name <- function(inputs) {

  # Code

  return(outputs)
}
```

Here `function_name` serves as what you call the function, inputs are what you need in order to run the function, and outputs are what gets returned. So if we are doing $y=x^{3}$ then we will call that function `cubic`:

```{r}
cubic <- function(x) {
  y <- x^3
  return(y)
}
```

So now if we want to evaluate $y(2)=2^{3}$ at the console we type `cubic(2)`. Neat! The following code will make a plot of the function $y=x^{3}$ using `cubic` (try this out on your own):

```{r cubic-one-02, eval=FALSE,fig.cap="Plot of the cubic function $y=x^{3}$."}
x <- seq(from = 0, to = 2, by = 0.05)
y <- cubic(x)

my_data <- tibble(x = x, y = y)

ggplot(data = my_data) +
  geom_line(aes(x = x, y = y)) +
  labs(
    x = "x",
    y = "y"
  )
```

### Functions with multiple inputs

Sometimes you may want to define a function with different input parameters, so for example the function $y=x^{3}+c$. To define that, we can modify the function to have input variables:

```{r}
cubic_revised <- function(x, c) {
  y <- x^3 + c
  return(y)
}
```

To create and plot several examples of the function `cubic` for different values of $c$ is shown in the following code and Figure \@ref(fig:cubic-all-02).

```{r eval = FALSE}
x <- seq(from = 0, to = 2, by = 0.05)


my_data_revised <- tibble(
  x = x,
  c_zero = cubic_revised(x, 0),
  c_pos1 = cubic_revised(x, 1),
  c_pos2 = cubic_revised(x, 2),
  c_neg1 = cubic_revised(x, -1)
)

ggplot(data = my_data_revised) +
  geom_line(aes(x = x, y = c_zero)) +
  geom_line(aes(x = x, y = c_pos1)) +
  geom_line(aes(x = x, y = c_pos2)) +
  geom_line(aes(x = x, y = c_neg1)) +
  labs(
    x = "x",
    y = "y"
  )
```


```{r cubic-all-02,fig.cap="Plot of several cubic functions $y=x^{3}+c$ when $c=-1,0,1,2$.",echo=FALSE}
x <- seq(from = 0, to = 2, by = 0.05)


my_data_revised <- tibble(
  x = x,
  c_zero = cubic_revised(x, 0),
  c_pos1 = cubic_revised(x, 1),
  c_pos2 = cubic_revised(x, 2),
  c_neg1 = cubic_revised(x, -1)
)

ggplot(data = my_data_revised) +
  geom_line(aes(x = x, y = c_zero)) +
  geom_line(aes(x = x, y = c_pos1)) +
  geom_line(aes(x = x, y = c_pos2)) +
  geom_line(aes(x = x, y = c_neg1)) +
  labs(
    x = "x",
    y = "y"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```


Notice how I defined multiple columns of the data frame `my_data_revised` in the `tibble` command, and then used mutiple `geom_line` commands to plot the data. Since we had combined the different values of `c` in a single data frame we didn't need to define the `data` with each instance of `geom_line`.

## Concluding thoughts

This is not meant to be a self-contained chapter in `R` but rather one so that you can quickly compute to code. Curious to learn more? Thankfully there are several good resources. Here are few of my favorites that I turn to:

-   [**R Graphics**.](http://www.cookbook-r.com/) This is a go-to resource for making graphics. (I also use Google a lot too.)
-   [**The Pirates Guide to R**.](https://bookdown.org/ndphillips/YaRrr/) This book promises to build your R knowledge from the ground up.
-   [**R for Reproducible Scientific Analysis**.](http://swcarpentry.github.io/r-novice-gapminder/) This set of guided tutorials can help you build your programming skills in R.
-   [**R for Data Science**.](https://r4ds.had.co.nz/) This is a useful book to take your R knowledge to the next level.

The best piece of advice: DON'T PANIC! Patience and persistence are your friend. Reach out for help, and recognize that like with any new endeavor, practice makes progress.


## Exercises

```{exercise}
Create a folder on your computer and a project file where you will store all your `R` work.
```

```{exercise}
Install the packages `devtools`, `tidyverse` to your `R` installation. Once that is done, then install the package `demodelr`.
```

```{exercise}
What are the variables listed in the dataset `phosphorous` in the `demodelr` library?  (Hint: try the command `?phosphorous`.)
```

```{exercise phos-scatter-02}
Make a scatterplot (`geom_point()`) of the dataset `phosphorous` in the `demodelr` library. Be sure to label the axes with descriptive titles.
```

```{exercise}
Change Figure \@ref(fig:wilson-model-02) so the line is blue and the size is 4 mm.
```

```{exercise}
Change the color of the points in Figure \@ref(fig:wilson-weight-02) to either a hexadecimal color or a named color of your choice.
```

```{exercise}
For this exercise you will do some plotting:

a. Define a sequence (call this sequence $x$) that ranges between $-12$ to $12$ with spacing of $.05$.
b. Also define the variable $y$ such that $y=\sin(x)$.
c. Make a scatter plot to graph $y=\sin(x)$. Set the points to be red.
d. Make a line plot to graph $y=\sin(x)$. Label the x-axis with your favorite book title. Label the y-axis with your favorite food to eat.

```

```{exercise nutrient-02}
An equation that relates a consumer's nutrient content (denoted as $y$) to the nutrient content of food (denoted as $x$) is given by: $\displaystyle y = c x^{1/\theta}$, where $\theta \geq 1$ and $c>0$ are both constants. Let's just assume that $c=1$ and the $0 \leq x \leq 1$.

a. Construct a function called `nutrient` that will make a sequence of `y` values for an input `x` and `theta` ($\theta$).
b. Use your `nutrient` function to create a line plot (`geom_line()`) for five different values of $\theta>1$, appropriately labeling all axes.

```

```{exercise}
The dataset `phosphorous` in the `demodelr` library contains measurements of the phosphorous content of *Daphnia* and its primary food source algae. 

Researchers believe that *Daphnia* has strict homeostatic regulation of the phosphorous contained in algae, and want to determine the value of $\theta$ in the equation $y= \displaystyle y = c x^{1/\theta}$. They have already determined that the value of $c=1.737$.

a. Complete Exercise \@ref(exr:phos-scatter-02). Be sure to label the axes correctly.
b. Use your function `nutrient` from Exercise \@ref(exr:nutrient-02) to make an initial guess for `theta` ($\theta$) that is consistent with the data. You can evaluate your guess by plotting (with `geom_line()`) against the data.
c. Use guess and check to refine the value of $\theta$ that seems to work best.
d. Report your value of $\theta$.

```

```{exercise}
For this exercise you will investigate some built-in functions. Remember you can learn more about a function by typing `?FUNCTION`, where `FUNCTION` is the name.

a. Explain (using your own words) what the function `runif(1,100,1000)` does.
b. Explain (using your own words) what the function `ceiling()` does, showing an example of its use.

```

```{exercise}
The Ebola outbreak in Africa in 2014 severely affected the country of Sierra Leone. A model for the number of Ebola infections $I$ is given by the following equation:
$$ I(t) = \frac{K \cdot I_{0} }{I_{0} + (K-I_{0})  \exp(-rt)}, $$
where $K = 13580$, $I_{0}=251$ and $r = 0.0227$. The variable $t$ is in days. Use `geom_line()` to visualize this curve from $0 \leq t \leq 700$.
```

```{exercise}
Consider the following piecewise function:

  \begin{equation}
y =
\begin{cases}
x^2 & \text{ for } 0 \leq x < 1,\\
2-x &\text{ for } 1 \leq x \leq  2 \\
\end{cases}
\end{equation}

a. Define a function in `R` that computes $y$ for $0 \leq x \leq 2$.
b. Use `geom_line()` to generate a graph of $y(x)$ over the interval $0 \leq x \leq 2$.

```

```{exercise}
An insect's development rate $r$ depends on temperature $T$ (degrees Celsius) according to the following equation:

\begin{equation}
r =
\begin{cases}
0.1 & \text{ for } 17 \leq T < 27,\\
0 &\text{ otherwise.}
\end{cases}
\end{equation}


a. Define a function in `R` that computes $r$ for $0 \leq T \leq 30$.
b. Use `geom_line()` to generate a graph of $r(T)$ over the interval $0 \leq T \leq 30$.


```

<!--chapter:end:02-Rintro.Rmd-->

# Modeling with Rates of Change {#modeling-rates-03}
Chapter \@ref(intro-01) provided examples for modeling with rates of change, and Chapter \@ref(r-intro-02) introduced the computational and visualization software `R` and `RStudio`, and how we can translate equations with rates of change to understand phenomena. The focus for this chapter will be on taking a contextual description and starting to develop differential equation models for them.

Oftentimes when we construct differential equations from a contextual description we bring our own understanding and knowledge to this situation. How _you_ may write down the differential equation may be different from someone else - _do not worry!_  This is the fun part of modeling: models can be considered testable hypotheses that can be refined when confronted with data. Let's get started


## Competing plant species and equilibrium solutions
Consider the following context to develop a mathematical model:


> A newly introduced plant species is introduced to a region. It competes with another established species for nutrients (and is a better competitor). However, the growth rate of the new species is proportional to the difference between the current number of established species and the number of new species. You may assume that the number of established species is a constant _E_.


<!-- LW pg 5 -->


For this problem we will start by naming our variables. Let $N$ represent number of new species and $E$ the number of established species. We will break this down accordingly:

- *"the growth rate of the new species"* describes the rate of change, or derivative, expressed as $\displaystyle \frac{dN}{dt}$.

- *"is proportional to the difference between the current number of established species and the number of new species"* means $\displaystyle \alpha \cdot (E-N)$, where $\alpha$ is the proportionality constant. Including this parameter helps to avoid assuming we have a 1:1 correspondence between the growth rate of the new species and the population difference.

- *"and is a better competitor"* helps to explain why the term is $\displaystyle \alpha \cdot (E-N)$ instead of $\displaystyle \alpha \cdot (N-E)$. We know that the newly established species will start out in much smaller numbers than $N$. But since it is a better competitor, we would expect its rate to increase initially. So $\displaystyle \frac{dN}{dt}$ should be *positive* rather than negative.

Taking all these assumptions together, Equation \@ref(eq:compete-03) shows the differential equation to model this context:

\begin{equation}
\frac{dN}{dt} = \alpha \cdot (E-N) (\#eq:compete-03)
\end{equation}

You may recognize that Equation \@ref(eq:compete-03) is similar to Equation \@ref(eq:single-02) in Chapter \@ref(intro-01) for the spread of Ebola. It is not surprising to have similar differential equations appear in different contexts. We will see throughout this book that it is more advantageous to learn techniques to analyze models qualitatively rather than memorize several different types of models and not see the connections between them.

An interesting solution to a differential equation is the _steady state_ or _equilibrium solution_.\index{steady state}\index{equilibrium solution}  Equilibrium solutions occur where the rates of change are zero. For Equation \@ref(eq:compete-03), this means that we are solving $\displaystyle \frac{dE}{dt} = \alpha \cdot (E-N)  = 0$. Granted, the expression $\alpha \cdot (E-N)$ may look like alphabet soup, but it is helpful to remember that $\alpha$ and $E$ are both parameters; the steady state occurs when the expression $E-N$ equals zero, or when $N = E$. We may consider the new species $N$ to be established when it reaches the same population level as $E$. Identifying steady states in a model aids in understanding the behavior of any solutions for a differential equation. Chapters \@ref(phase-05) and \@ref(coupled-06) dig deeper into steady states and their calculation.

## The Law of Mass Action
Our next example focuses on how to generate a model that borrows concepts from modeling chemical reactions. For example let's say you have a substrate *A* that reacts with enzyme *B* to form a product *S*. One common way to represent this process is with a reaction equation (Equation \@ref(eq:reaction-03)):\index{reaction equation}

\begin{equation}
A+B \rightarrow S  (\#eq:reaction-03)
\end{equation}

Figure \@ref(fig:mass-action) is a schematic diagram of Equation \@ref(eq:reaction-03):

```{tikz, mass-action,warning=FALSE,message=FALSE,echo=FALSE,fig.align="center",fig.cap="Schematic diagram of a substrate-enzyme reaction."}

\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=2cm]

\node [vspecies] (A) {$A$} ;
\node [vspecies, right of = A] (S) {$P$} ;
\draw [->] ([yshift=0pt]A.east) --  node[above] {\small{$kB$}} ([yshift=0pt]S.west) ;
\end{tikzpicture}


```

One key quantity is the rate of formation for the product $P$, which we express by Equation \@ref(eq:mass-action):

\begin{equation}
\frac{dP}{dt}= kAB, (\#eq:mass-action)
\end{equation}

where $k$ is the proportionality constant or the rate constant associated with the reaction. Notice how we express the interaction between $A$ and $B$ as a product - if either the substrate $A$ or enzyme $B$ is not present (i.e. $A$ or $B$ equals zero), then product $P$ is not formed. Equation \@ref(eq:mass-action) is an example of the law of mass action.\index{mass action}

Modeling interactions (whether between susceptible and infected individuals, enzymes and substrates, or predators and prey) with the law of mass action is always a good first assumption to understand the system, which can be subsequently refined. For example, if we consider that the substrate might decay, we can revise Figure \@ref(fig:mass-action) to Figure \@ref(fig:mass-action-revised):

```{tikz, mass-action-revised,warning=FALSE,message=FALSE,echo=FALSE,fig.align="center",fig.cap="Revised schematic diagram of substrate-enzyme reaction with decay of the product $P$."}


\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=2cm]

\node [vspecies] (A) {$A$} ;
\node [vspecies, right of = A] (S) {$P$} ;
\draw [->] ([yshift=0pt]A.east) --  node[above] {\small{$kB$}} ([yshift=0pt]S.west) ;
\draw [->] (S.east) --  node[above] {\small{$d$}} +(1cm,0pt) ;
\end{tikzpicture}


```

In this instance the rate of change of $P$ would then include a term $dP$ (Equation \@ref(eq:mass-action-decay):

\begin{equation}
\frac{dP}{dt}= kAB - dP (\#eq:mass-action-decay)
\end{equation}




<!-- LW pg 73 has some good examples, derived in handlingh patches. -->
<!-- For determine rate constants: -->
<!-- https://kids.frontiersin.org/articles/10.3389/frym.2021.651131 -->
<!-- https://jmahaffy.sdsu.edu/courses/f09/math636/lectures/lotka/qualde2.html -->

## Coupled differential equations: lynx and hares
Another example is a *system of differential equations*.\index{differential equation!system of equations}  The context is between the snowshoe hare and the Canadian lynx, shown in Figure \@ref(fig:lynx-hare). Figure \@ref(fig:lynx-hare-time) also displays a timeseries of the two populations overlaid. Notice how in Figure \@ref(fig:lynx-hare-time) both populations show regular periodic fluctuations. One plausible reason is that the lynx prey on the snowshoe hares, which causes the population to initially decline. Once the snowshoe hare population declines, then there is less food for the lynx to survive, so their population declines. The decline in the lynx population causes the hare population to increase, and the cycle repeats.^[There is a lot more nuance for reasons behind periodic fluctuations in these two populations, which includes more complicated food web interactions and climate variation. @maclulich_fluctuations_1937, @stenseth_population_1997, and @king_geometry_2001 are good places to dig into the complexity of this fascinating biological system. &nbsp; Image sources for Figure \@ref(fig:lynx-hare): @usa_canada_2012 and @preserve_snowshoe_2011. Image source for Figure \@ref(fig:lynx-hare-time): @openstax_notitle_2016]

```{r lynx-hare, echo=FALSE, fig.cap = "Examples of lynx and hare - aren't they beautiful?",out.width="70%",fig.align='center'}

knitr::include_graphics("figures/03-systems/lynx-hare.png")

```




```{r,label="lynx-hare-time", echo=FALSE, fig.cap="Timeseries of the combined lynx and hare populations. Notice how the populations are coupled with each other.",out.width="70%",fig.align='center'}

#[Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Figure_45_06_01.jpg)
knitr::include_graphics("figures/03-systems/Figure_45_06_01.jpg")

```




In summary it is safe to say that the two populations are *coupled* to one another, yielding a coupled system of equations.\index{differential equation!coupled system}  But in order to understand how they are coupled together, first let's consider the two populations *separately*.


To develop the mathematical model we will make some simplifying assumptions. The hares grow much more quickly than then lynx - in fact some hares have been known to reproduce several times a year. A reasonable assumption for large hare populations is that rate of change of the hares is proportional to the hare population. Based on this assumption Equation \@ref(eq:hareOnly) describes the rate of change of the hare population, with $H$ as the population of the hares:

\begin{equation}
\frac{dH}{dt} = r H (\#eq:hareOnly)
\end{equation}

Since the growth rate $r$ is positive, so then the rate of change ($H'$) will be positive as well, and $H$ will be increasing. A representative value for $r$ is 0.5 year$^{-1}$ [@mahaffy_lotka-volterra_2010; @brady_circle_2021]. You may be thinking that the units on $r$ seem odd - (year$^{-1}$), but that unit on $r$ makes the term $rH$ dimensionally consistent to be a rate of change.

Let's consider the lynx now. An approach is to assume their population declines exponentially, or changes at the rate proportional to the current population. Let's consider $L$ to be the lynx population, with the following differential equation (Equation \@ref(eq:lynxOnly)):

\begin{equation}
\frac{dL}{dt} = -dL (\#eq:lynxOnly)
\end{equation}

We assume the death rate $d$ in Equation \@ref(eq:lynxOnly) is positive, leading to a negative rate of change for the Lynx population (and a decreasing value for $L$). A typical value of $d$ is 0.9 yr$^{-1}$ [@mahaffy_lotka-volterra_2010; @brady_circle_2021].

The next part to consider is how the lynx and hare interact. Since the hares are prey for the lynx, when the lynx hunt, the hare population decreases. We can represent the process of hunting with the following adjustment to our hare equation:

\begin{equation}
\frac{dH}{dt} = r H - b HL
\end{equation}

So the parameter $b$ represents the hunting rate. Notice how we have the term $HL$ for this interaction. This term injects a sense of realism: if the lynx are not present ($L=0$), then the hare population can't decrease due to hunting. We model the *interaction* between the hares and the lynx with multiplication between the $H$ and $L$. A typical value for $b$ is .024 lynx$^{-1}$ year$^{-1}$. It is okay if that unit seems a little odd to you - it should be! As before, if we multiply out the units on $bHL$ we would get units of hares per year.

How does hunting affect the lynx population?  One possibility is that it increases the lynx population:

\begin{equation}
\frac{dL}{dt} =bHL -dL
\end{equation}

Notice the symmetry between the rate of change for the hares and the lynx equations. In many cases this makes sense - if you subtract a rate from one population, then that rate should be added to the receiving population. You could also argue that there is some efficiency loss in converting the hares to lynx - not all of the hare is converted into lynx biomass. In this situation we sometimes like to adjust the hunting term for the lynx equation with another parameter $e$, representing the efficiency that hares are converted into lynx:

\begin{equation}
\frac{dL}{dt} =e \, bHL -dL
\end{equation}

(sometimes people just make a new parameter $c=e \, b$, but for now we will just leave it as is and set $e=0.2$). Equation \@ref(eq:lynx-hare-combined) shows the coupled system of differential equations:

\begin{equation}
\begin{split}
\frac{dH}{dt} &= r H - b HL \\
\frac{dL}{dt} &=e\,bHL -dL
\end{split}
(\#eq:lynx-hare-combined)
\end{equation}

The schematic diagram representing these interactions is shown in Figure \@ref(fig:lynxhare-schematic):

```{tikz, lynxhare-schematic,warning=FALSE,message=FALSE,echo=FALSE,fig.align="center",fig.cap="Schematic diagram Lynx-Hare system."}


\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=2cm]

\node [vspecies] (H) {$H$} ;
\node [vspecies, right of = H] (L) {$L$} ;
\draw [->] ([yshift=3pt]H.east) --  node[above] {\small{$ebH$}} ([yshift=3pt]L.west) ;
% \draw [->] ([yshift=-3pt]L.west) --  node[below] {\small{$ebL$}} ([yshift=-3pt]H.east) ;
\draw [->] (L.east) --  node[above] {\small{$d$}} +(1cm,0pt) ;
\draw [<-] (H.west) --  node[above] {\small{$r$}} +(-1cm,0pt) ;

\end{tikzpicture}


```


Equation \@ref(eq:lynx-hare-combined) is a classical model in mathematical biology and differential equations - it is called the *predator-prey* model, also known as the *Lotka-Volterra model* [@lotka_analytical_1920; @lotka_elements_1926; @volterra_fluctuations_1926].\index{model!predator-prey}\index{model!Lotka-Volterra}

## Functional responses 
In several examples we have seen a rate of change proportional to the current population, as, for example, the rate of growth of the hare population is $rH$. This is one example of what we would call a [functional response](https://en.wikipedia.org/wiki/Functional_response).\index{functional response}  Another type of functional response assumes that the rate reaches a limiting value proportional to the population size, so $\displaystyle \frac{dH}{dt} = \frac{rH}{1+arH}$. This is an example of a **type II functional response**.\index{functional response!type II} Finally, the type II response has also been generalized (a **type III functional response**) $\displaystyle \frac{dH}{dt} = \frac{rH^{2}}{1+arH^{2}}$.\index{functional response!type III} Figure \@ref(fig:function-response) shows all three functional responses together:

```{r function-response,echo=FALSE,fig.cap="Comparison between examples of Type I - Type III functional responses. For a Type I functional response the rate grows proportional to population size *H*, whereas for Types II and III the rate reaches a saturating value."}
x <- seq(0,2,by=.01)


type1 <-  .5*x
type2 <- 2*x/(1+2*x)
type3 <- 4*x^2/(1+4*x^2)

my_data <- data.frame(x,type1,type2,type3) %>%
  gather(key=type,value=value,-x)
ggplot() +
  geom_line(data=my_data,aes(x=x,y=value,color=type,linetype=type),size=1.0) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  labs(x='H',y = 'Rate (dH/dt)') +
  scale_color_colorblind(name="Functional \n Response",
                       labels=c("Type I", "Type II","Type III")) +
  scale_linetype_discrete(name="Functional \n Response",
                         labels=c("Type I", "Type II","Type III"))  +
  coord_cartesian(ylim = c(0,1.5))



```

Notice the limiting behavior in the Type II and Type III functional responses. These responses are commonly used in ecology and predator-prey dynamics and in problems of how animals search for food.



## Exercises
```{exercise}
Consider the following types of functional responses:
  
\begin{equation}
\begin{split}
\mbox{ Type I: } \frac{dP}{dt} &= 0.1 P \\
\mbox{ Type II: } \frac{dP}{dt} &= \frac{0.1P}{1+.03P} \\
\mbox{ Type III: } \frac{dP}{dt} &= \frac{0.1P^{2}}{1+.05P^{2}}
\end{split}
\end{equation}
  
For each of the functional responses evaluate $\displaystyle \lim_{P \rightarrow \infty} \frac{dP}{dt}$. Since these functional responses represent a rate of change of a population, what are some examples (hypothetical or actual) in which each of these responses would be appropriate?

```


```{exercise}
A population grows according to the equation:
  
\begin{equation}
\frac{dP}{dt} = \frac{P}{1+.05P} -.1P = f(P) - g(P)
\end{equation}


a. On the same axis, plot the equations $f(P)$ and $g(P)$. What are the two positive values of $P$ where $f(P)$ and $g(P)$ intersect?
  
b. Next algebraically determine the two steady state values of $P$, that is solve $\displaystyle \frac{dP}{dt}=0$ for $P$. (*Hint:* factor a $P$ out of the expression $\displaystyle f(P)-g(P)$.)

c. Does your algebraic solution match your graphical solutions?

```


```{exercise}
A population grows according to the equation:

\begin{equation}
\frac{dP}{dt} = 2P - \frac{4P^{2}}{1+P^{2}} = r(P)-d(P)
\end{equation}

a. On the same axis, plot the equations $r(P)$ and $d(P)$. What are the two positive values of $P$ where $r(P)$ and $d(P)$ intersect?
b. Next algebraically determine the two steady state values of $P$, that is solve $\displaystyle \frac{dP}{dt}=0$ for $P$. (*Hint:* factor a $P$ out of the expression $r(P)-d(P)$.)
c. Does your algebraic solution match your graphical solutions?

```



```{exercise}
A population grows according to the equation:
  
\begin{equation}
\frac{dP}{dt} = \frac{aP}{1+abP} - dP,
\end{equation}

where $a$, $b$, and $d$ are all positive parameters. Determine the two steady state values of $P$, that is solve $\displaystyle \frac{dP}{dt}=0$ for $P$.
```


```{exercise}
A chemical reaction takes two chemicals $X$ and $Y$ to form a substrate $Z$ through the law of mass action. However the substrate can also disassociate. The reaction schematic is the following:
  
\begin{equation}
X + Y \rightleftharpoons Z,
\end{equation}

where you may define the proportionality constant $k_+$ as associated with the formation of the substrate $Z$ and $k_-$ the disassociation ($Z$ decays back to $X$ and $Y$).

&nbsp;

Write down a differential equation that represents the rate of reaction $\displaystyle \frac{dZ}{dt}$.

```  



<!-- Thornley and Johnson. Logan and Wollesensky pg 4 -->
```{exercise}
(Inspired from @thornley_plant_1990 and @logan_mathematical_2009) For each of the following exercises consider the following contextual situations modeling rates of change. For each problem you will need to:

- Name and describe all variables and parameters;
- Determine a differential equation representing the context;
- Write a brief one-two sentence explanation of why your differential equation models the situation at hand;
- Hand sketch a rough graph of what you think the solution is as a function of time, consistent with the context given.

a. The rate of change of an animal's body temperature is proportional to the difference in temperature between the environment and the current body temperature of the animal.
b. A plant grows proportional to its current length $L$. Assume this proportionality constant is $\mu$, whose rate also decreases proportional to its current value. You will need to write down a system of two equations with variables $L$ and $\mu$.
c. A patient undergoing chemotherapy receives an injection at rate $I$. This injection decreases the rate that a tumor accumulates mass. Independent of the injection, the tumor accumulates mass at a rate proportional to the mass of the tumor.
d. A cell with radius $r$ assimilates nutrients at a rate proportional to its surface area, but uses nutrients proportional to its volume. Determine an equation that represents the rate of change of the radius. 
e. The rate that a cancer cell divides (increases in amount) is proportional to the number of healthy cells in its surrounding environment. You may assume that a healthy cell has mortality $\delta_{H}$ and a cancer cell has mortality $\delta_{C}$. Be sure to write down a system of differential equations for the population of cancer cells $C$ and healthy cells $H$.
f. The rate that a virus is spread to the population is proportional to the probability that a person is sick (out of $N$ total sick and healthy individuals).

```



```{tikz,pesticide-ch3,engine='tikz',warning=FALSE,message=FALSE,echo=FALSE,fig.cap="Modeled reaction schemes representing the potential effect of a pesticide on water quality."}


\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=1.5cm]

%% Model 1  Burnham Anderson
\node [vspecies] (A) {$W$} ;
\node [vspecies, right of = A] (B) {$P$} ;
\node [vspecies, below of = A] (C) {$F$} ;
\node [vspecies, above of = A,node distance = 1cm] (model) {\textbf{Model 1}};
\draw [->] ([yshift=3pt]A.east) --  node[above] {\small{$k_1$}} ([yshift=3pt]B.west) ;
\draw [->] ([yshift=-3pt]B.west) --  node[below] {\small{$k_2$}} ([yshift=-3pt]A.east) ;

\draw [->] ([xshift=0]A.south) --  node[left] {\small{$k_3$}} ([xshift=0pt]C.north) ;

%%% Model 2a  Burnham Anderson
\node [vspecies, right of = B, node distance = 1.75 cm] (A2) {$W$} ;
\node [vspecies, right of = A2] (B2) {$P$} ;
\node [vspecies, below of = A2] (C2) {$F$} ;
\node [vspecies, above of = A2,node distance = 1cm] (model2) {\textbf{Model 2a}};
\draw [->] ([yshift=3pt]A2.east) --  node[above] {\small{$k_1$}} ([yshift=3pt]B2.west) ;
\draw [->] ([yshift=-3pt]B2.west) --  node[below] {\small{$k_2$}} ([yshift=-3pt]A2.east) ;

\draw [->] ([xshift=-2pt]A2.south) --  node[left] {\small{$k_3$}} ([xshift=-2pt]C2.north) ;
\draw [<-] ([xshift=2pt]A2.south) --  node[right] {\small{$k_4$}} ([xshift=2pt]C2.north) ;


%%% Model 2b Burnham Anderson
\node [vspecies, right of = B2, node distance = 1.75 cm] (A3) {$W$} ;
\node [vspecies, right of = A3] (B3) {$P$} ;
\node [vspecies, below of = A3] (C3) {$F$} ;
\node [vspecies, below of = C3] (A3pr) {$W'$} ;
\node [vspecies, above of = A3,node distance = 1cm] (model3) {\textbf{Model 2b}};
\draw [->] ([yshift=3pt]A3.east) --  node[above] {\small{$k_1$}} ([yshift=3pt]B3.west) ;
\draw [->] ([yshift=-3pt]B3.west) --  node[below] {\small{$k_2$}} ([yshift=-3pt]A3.east) ;

\draw [->] ([xshift=0]A3.south) --  node[left] {\small{$k_3$}} ([xshift=0pt]C3.north) ;
\draw [->] ([xshift=0]C3.south) --  node[left] {\small{$k_4$}} ([xshift=0pt]A3pr.north) ;


%%% Model 3a Burnham Anderson
\node [vspecies, below of = C, node distance = 3 cm] (A4) {$W$} ;
\node [vspecies, right of = A4] (B4) {$P$} ;
\node [vspecies, below of = A4] (C4) {$F$} ;
\node [vspecies, below of = C4] (A4pr) {$W'$} ;
\node [vspecies, right of = A4pr] (B4pr) {$P'$} ;


\node [vspecies, above of = A4,node distance = 1cm] (model4) {\textbf{Model 3a}};
\draw [->] ([yshift=3pt]A4.east) --  node[above] {\small{$k_1$}} ([yshift=3pt]B4.west) ;
\draw [->] ([yshift=-3pt]B4.west) --  node[below] {\small{$k_2$}} ([yshift=-3pt]A4.east) ;

\draw [->] ([xshift=0]A4.south) --  node[left] {\small{$k_3$}} ([xshift=0pt]C4.north) ;

\draw [->] ([xshift=0]C4.south) --  node[left] {\small{$k_4$}} ([xshift=0pt]A4pr.north) ;

\draw [->] ([yshift=0pt]A4pr.east) --  node[above] {\small{$k_5$}} ([yshift=0pt]B4pr.west) ;




%%% Model 3b Burnham Anderson
\node [vspecies, right of = B4, node distance = 1.75 cm] (A5) {$W$} ;
\node [vspecies, right of = A5] (B5) {$P$} ;
\node [vspecies, below of = A5] (C5) {$F$} ;
\node [vspecies, below of = C5] (A5pr) {$W'$} ;
\node [vspecies, right of = A5pr] (B5pr) {$P'$} ;


\node [vspecies, above of = A5,node distance = 1cm] (model5) {\textbf{Model 3b}};
\draw [->] ([yshift=3pt]A5.east) --  node[above] {\small{$k_1$}} ([yshift=3pt]B5.west) ;
\draw [->] ([yshift=-3pt]B5.west) --  node[below] {\small{$k_2$}} ([yshift=-3pt]A5.east) ;

\draw [->] ([xshift=0]A5.south) --  node[left] {\small{$k_3$}} ([xshift=0pt]C5.north) ;

\draw [->] ([yshift=-3pt]B5pr.west) --  node[below] {\small{$k_6$}} ([yshift=-3pt]A5pr.east) ;
\draw [<-] ([yshift=3pt]B5pr.west) --  node[above] {\small{$k_5$}} ([yshift=3pt]A5pr.east) ;


\draw [->] ([xshift=0]C5.south) --  node[left] {\small{$k_4$}} ([xshift=0pt]A5pr.north) ;


%%% Model 4a Burnham Anderson
\node [vspecies, below of = A4pr, node distance = 2.25 cm] (A6) {$W$} ;
\node [vspecies, right of = A6] (B6) {$P$} ;
\node [vspecies, below of = A6] (C6) {$F$} ;
\node [vspecies, right of = C6] (C6pr) {$F'$} ;
\node [vspecies, below of = C6] (A6pr) {$W'$} ;
\node [vspecies, right of = A6pr] (B6pr) {$P'$} ;


\node [vspecies, above of = A6,node distance = 1cm] (model6) {\textbf{Model 4a}};


\draw [->] ([yshift=3pt]A6.east) --  node[above] {\small{$k_1$}} ([yshift=3pt]B6.west) ;
\draw [->] ([yshift=-3pt]B6.west) --  node[below] {\small{$k_2$}} ([yshift=-3pt]A6.east) ;


\draw [->] ([yshift=3pt]C6.east) --  node[above] {\small{$k_8$}} ([yshift=3pt]C6pr.west) ;
\draw [->] ([yshift=-3pt]C6pr.west) --  node[below] {\small{$k_9$}} ([yshift=-3pt]C6.east) ;

\draw [->] ([yshift=0pt]A6pr.east) --  node[above] {\small{$k_5$}} ([yshift=0pt]B6pr.west) ;

\draw [->] ([yshift=0pt]C6.south) --  node[left] {\small{$k_4$}} ([yshift=0pt]A6pr.north) ;


\draw [->] ([xshift=0]A6.south) --  node[left] {\small{$k_3$}} ([xshift=0pt]C6.north) ;

%%% Model 4b Burnham Anderson
\node [vspecies, right of = B6, node distance = 1.75 cm] (A7) {$W$} ;
\node [vspecies, right of = A7] (B7) {$P$} ;
\node [vspecies, below of = A7] (C7) {$F$} ;

\node [vspecies, below of = C7] (A7pr) {$W'$} ;
\node [vspecies, right of = A7pr] (B7pr) {$P'$} ;

\node [vspecies, above of = A7,node distance = 1cm] (model7) {\textbf{Model 4b}};
\draw [->] ([yshift=3pt]A7.east) --  node[above] {\small{$k_1$}} ([yshift=3pt]B7.west) ;
\draw [->] ([yshift=-3pt]B7.west) --  node[below] {\small{$k_2$}} ([yshift=-3pt]A7.east) ;

\draw [->] ([xshift=0]A7.south) --  node[left] {\small{$k_3$}} ([xshift=0pt]C7.north) ;

\draw [->] ([xshift=-3pt]C7.south) --  node[left] {\small{$k_4$}} ([xshift=-3pt]A7pr.north) ;
\draw [<-] ([xshift=3pt]C7.south) --  node[right] {\small{$k_7$}} ([xshift=3pt]A7pr.north) ;

\draw [->] ([xshift=0]A7pr.east) --  node[above] {\small{$k_5$}} ([xshift=0pt]B7pr.west) ;

\end{tikzpicture}

```


<!-- From Burnham and Anderson pg 135 of pdf -->
```{exercise}
(Inspired by @burnham_model_2002) You are tasked with the job of investigating the effect of a pesticide on water quality, in terms of its effects on the health of the plants and fish in the ecosystem. Different models can be created that investigate the effect of the pesticide. Different types of reaction schemes for this system are shown in Figure \@ref(fig:pesticide-ch3), where $F$ represents the amount of pesticide in the fish, $W$ the amount of pesticide in the water, and $S$ the amount of pesticide in the soil. The prime (e.g. $F'$, $W'$, and $S'$ represent other bound forms of the respective state). In all seven different models can be derived. For each of the model schematics, apply the Law of Mass Action to write down a system of differential equations.
```


<!--chapter:end:03-modelingWithRates.Rmd-->

# Euler's Method {#euler-04}
Chapter \@ref(modeling-rates-03) examined modeling with rates of change. Once a differential equation model is defined one possible next step is to determine the solution to the differential equation. While in some cases an exact solution can be found (Chapter \@ref(exact-solns-07)), in many instances we will rely on numerical methods.

The focus of this chapter is on *approximation* of solutions to a differential equation via a numerical method. Typically a first numerical method you might learn is *Euler's method*, popularized in the movie [Hidden Figures.](https://www.youtube.com/watch?v=v-pbGAts_Fg) \index{Euler's method} This chapter will develop Euler's method from tangent line equations or locally linear approximations from calculus.\index{tangent! line}\index{local linearization} Let's get started!

## The flu and locally linear approximation 
Consider Equation \@ref(eq:flu-model-02), which is one way to model the rate of change of the flu through a population:

\begin{equation}
\frac{dI}{dt} = 3e^{-.025t} (\#eq:flu-model-02)
\end{equation}

In Equation \@ref(eq:flu-model-02) the variable $I$ represents the number of people infected at day $t$. One question we could address using Equation \@ref(eq:flu-model-02) is to predict the value of $I$ after 1 day, assuming that $I(0)=10$. To do that we will build a locally linear approximation at $t=0$ and use the approximation to forecast and estimate $I(1)$.

In order to solve this problem, the formula for the locally linear approximation to $I(t)$ at $t=0$ is $L(t) = I(0) + I'(0) \cdot (t-0)$. Here, $I(0)=10$ and $I'(0)=3$ (found by evaluating Equation \@ref(eq:flu-model-02) at $t=0$). Using $L(t) \approx I(t)$, the formula for the locally linear approximation is given by Equation \@ref(eq:flu-linear-04). To define Equation \@ref(eq:flu-linear-04) we used two pieces of information: the (given) value of the function at $t=0$ and the estimate of the derivative from Equation \@ref(eq:flu-model-02).

\begin{equation}
L(t)=10 + 3t (\#eq:flu-linear-04)
\end{equation}


At $t=1$ we can make a prediction with Equation \@ref(eq:flu-linear-04) to estimate that there will be 13 people sick. To evaluate this approximation it is helpful to compare our prediction from $L(1)$ (Equation \@ref(eq:flu-linear-04)) to the actual value from the solution to the differential equation given in Equation \@ref(eq:flu-exact-04):

\begin{equation}
I(t) = 130-120e^{-.025t} (\#eq:flu-exact-04)
\end{equation}

Table \@ref(tab:compare-04) compares the values of the linear approximation (Equation \@ref(eq:flu-linear-04)) to Equation \@ref(eq:flu-exact-04):

Table: (\#tab:compare-04) Comparison of the exact solution $I(t)$ (Equation \@ref(eq:flu-exact-04)) to the linear approximation $L(t)$ (Equation \@ref(eq:flu-linear-04)) at $t=0$ and $t=1$. 

$t$ | Linear approximation $L(t)$ | Exact solution $I(t)$
------------- | ------------- | -------------
0 | 10 | 10
1 | 13 | 12.96


Table \@ref(tab:compare-04) shows that $L(1)$ is an *overestimate* compared to $I(1)$. Let's expand Equation \@ref(eq:flu-linear-04) even more by constructing _another_ linear approximation using the differential equation. We will denote this linear approximation as $L_{1}(t)$ to distinguish it from $L(t)$ from Equation \@ref(eq:flu-linear-04). First we evaluate Equation \@ref(eq:flu-model-02), which yields $I'(1)=2.92$. The formula for the linear approximation at $t=1$ is $L_{1}(t) = I(1) + I'(1) \cdot (t-1)$. Here we will use $I(1) = 13$, recognizing that this value is a pretty close estimate for the number infected ($I$) at $t=1$. This assumption yields $L_{1}(t) = 13 +2.92(t-1)$. 

We can continue to build out the solution in a similar manner to develop a locally linear approximation at $t=2$, shown graphically in Figure \@ref(fig:eulers-ver1). The approximation and the exact solution in Figure \@ref(fig:eulers-ver1) appear very close to each other, suggesting that approximation using local linearization could work for other types of differential equations.


```{r, label="eulers-ver1",fig.cap='Approximation of a solution to Equation \\@ref(eq:flu-model-02) using local linearity.',echo=FALSE}
tibble(
  t = c(0, 1, 2),
  i_approx = c(10, 13, 15.92),
  i_soln = c(10, 12.96, 15.85)
) %>%
  pivot_longer(cols = c("i_approx", "i_soln"), names_to = "solution", values_to = "value") %>%
  ggplot(aes(x = t, y = value, color = solution, shape = solution)) +
  geom_point(size = 1) +
  geom_line() +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  labs(x = "Time (days)", y = "Infected people") +
  theme(legend.title = element_blank(),legend.position = "bottom") +
  scale_shape_discrete(labels = c("Linear approximation", "Exact solution")) +
  scale_color_colorblind(labels = c("Linear approximation", "Exact solution")) 

```

By eye, the approximate and exact solutions in Figure \@ref(fig:eulers-ver1) appear indistinguishable from each other. Encouraged by these results, let's develop the approach with linear approximations even more.

## A workflow for approximation
The previous chapter alludes to a possible workflow to numerically approximate a solution to a differential equation:

* Determine the locally linear approximation at a given point.
* Forecast out to another time value.
* Repeat the locally linear approximation.

The results of continuing this workflow (approximate $\rightarrow$ forecast $\rightarrow$ repeat) several times is shown in Table \@ref(tab:compare-later-04). 

```{r warning=FALSE,message=FALSE,echo=FALSE}

dt <- 1
t <- seq(0, 100, by = dt)
i_soln <- 130 - 120 * exp(-.025 * t)
didt <- 3 * exp(-0.025 * t) # We are multiplying by dt
i_approx <- array(0, dim = length(t))
i_approx[1] <- 10
for (i in 2:length(t)) {
  i_approx[i] <- i_approx[i - 1] + didt[i - 1] * dt
}
```

Comparison of the exact solution $I(t)$ (Equation \@ref(eq:flu-exact-04)) to the linear approximation $L(t)$ (Equation \@ref(eq:flu-linear-04)) at $t=0$ and $t=1$. 

Table: (\#tab:compare-later-04) Comparison of the exact solution $I(t)$ (Equation \@ref(eq:flu-exact-04)) to forecasting with linear approximations at $t=90$ and $t=95$. 

$t$ | Approximate solution | Exact solution $I(t)$
------------- | ------------- | -------------
90 | `r round(i_approx[90],1)` | `r round(i_soln[90],1)`
95 |  `r round(i_approx[95],1)` | `r round(i_soln[95],1)`


Table \@ref(tab:compare-later-04) suggests that the accuracy of our solution decreases as time increases. A potential fix would be to approximate the solution not at every day, but every half day. The length of time that we forecast out our solution is called the step size, denoted as $\Delta t$.\index{step size} While approximating our solution every half day ($\Delta t = 0.5$) would require more computation (or more iterations) of the locally linear approximation, perhaps a smaller $\Delta t$ would lead to more accurate solutions.  Let's start out smaller with the first few timesteps (Table \@ref(tab:calculate-forecast-04)):


Table: (\#tab:calculate-forecast-04) Calculation of the solution $I(t)$ for Equation \@ref(eq:flu-model-02) using the linear approximations at each timestep with $\Delta t = 0.5$. 

$t$ | $I$ | $\displaystyle \frac{dI}{dt}$ | $\displaystyle \frac{dI}{dt} \cdot \Delta t$
------------- | ------------- | ------------- | -------------
0 | 10 | 3 | 1.5
0.5 | = 10 + 1.5 = 11.5 | 2.96 | 1.48
1 | = 11.5 + 1.48 = 12.98 | 2.92 | 1.46
1.5 | = 12.92 + 1.46 = 14.38 | 2.88 | 1.44
2 | = 14.38 + 1.44 = 15.82 |  | 


Notice how Table \@ref(tab:calculate-forecast-04) organizes a way to compute the solution $I$ with linear approximations. Each row is a "step" of the method, computing the solution based on our step size $\Delta t$. The third column computes the value of the derivative for a particular time (Equation \@ref(eq:flu-model-02)), and then the fourth column represents the forecasted change in the solution by the next timestep.^[When you have a *rate of change* multiplied by a time increment this will give you an approximation of the net change in a function.]


This idea of *approximate, forecast, repeat* is at the heart of many [numerical methods](https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations) that approximate solutions to differential equations. The particular method that we have developed here is called _Euler's method_. We display the results from additional steps in Figure \@ref(fig:eulers-ver2). Based on the trend of the solution in Figure \@ref(fig:eulers-ver2), it appears that the number of infections might start to level off at $I=130$, which is the steady state value in Equation \@ref(eq:flu-exact-04) when evaluating $\displaystyle \lim_{t \rightarrow \infty} I(t)$.

```{r,label="eulers-ver2",fig.cap='Longer-term approximation of a solution to Equation \\@ref(eq:flu-model-02). Notice how the solution seems to level off to a steady state at $I=130$ (dashed line).',echo=FALSE}

# Let's clean this up somewhat
t_end <- 200
# Solution
t_soln <- seq(0, t_end, by = 0.05)
i_soln <- 130 - 120 * exp(-.025 * t_soln)

# First timesteps
dt <- 1
t_1 <- seq(0, t_end, by = dt)

didt <- 3 * exp(-0.025 * t_1) # We are multiplying by dt
i_approx <- array(0, dim = length(t_1))
i_approx[1] <- 10
for (i in 2:length(t_1)) {
  i_approx[i] <- i_approx[i - 1] + didt[i - 1] * dt
}


euler_1 <- tibble(t_1, i_approx)



ggplot() +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  geom_line(data = euler_1, aes(x = t_1, y = i_approx)) +
  labs(x = "Time (days)", y = "Infected people")+
  geom_hline(yintercept = 130,size=0.25,linetype="dashed")

```


## Building an iterative method
Now that we have worked on an example, let's carefully formulate Euler's method with another example. In Chapter \@ref(intro-01) we discussed the spread of Ebola through a population. Equation \@ref(eq:flu-de-04) is the differential equation for the logistic model (see Equation \@ref(eq:all3-01) and Figure \@ref(fig:three-soln)), modeled with Equation \@ref(eq:flu-de-pre-04):

\begin{equation}
\frac{dI}{dt} = 0.023 I \cdot (13600-I) (\#eq:flu-de-pre-04),
\end{equation}


where the variable $I$ represents the proportion of people that are infected. The *carrying capacity*, or the place where the solution levels off in Equation \@ref(eq:flu-de-pre-04) is at $I=13600$ (notice that when $I=13600$, $\displaystyle \frac{dI}{dt}=0$). Numerical methods such as Euler's method can become unstable for large values of the independent variable, because the rates are so large. To account for this, we will re-define Equation \@ref(eq:flu-de-pre-04) with the variable $p = \frac{I}{13600}$, leading to the revised model:

\begin{equation}
\frac{dp}{dt} = 0.023 p \cdot (1-p) (\#eq:flu-de-04),
\end{equation}

where the variable $p$ represents the proportion of infected, So $p=1$ means that 13600 people are infected. Once we have our solution $p(t)$, we can just multiply that by $N=13600$ to return back to the total infected.

In Equation \@ref(eq:flu-de-04) we define the function $f(p) = 0.023 p\cdot (1-p)$. In order to numerically approximate the solution, we will need to recall some concepts from calculus. This first step is that we will approximate the rate of change $\displaystyle \frac{dp}{dt}$ with a difference quotient (Equation \@ref(eq:flu-dq-04)):

\begin{equation}
\frac{dp}{dt} = \lim_{\Delta t \rightarrow 0} \frac{p(t+\Delta t) - p(t)}{\Delta t} (\#eq:flu-dq-04)
\end{equation}

When the quantity $\Delta t$ in Equation \@ref(eq:flu-dq-04) is small (for example $\Delta t = 1$ day), this difference quotient provides a reasonable way to organize the problem:

\begin{equation}
\begin{split}
\frac{p(t+\Delta t) - p(t)}{\Delta t}  &= 0.023 p \cdot (1-p) \\
p(t+\Delta t) - p(t)  &= 0.023 p \cdot (1-p)  \cdot \Delta t \\
p(t+\Delta t) &= p(t) + 0.023 p \cdot (1-p)  \cdot \Delta t
\end{split} 
\end{equation}


The last expression ($p(t+\Delta t) = p(t) + 0.023 p \cdot (1-p)  \cdot \Delta t$) defines an iterative system, easily computed with a spreadsheet program, or with a `for` loop in `R`:

```{r eval=FALSE}

# Define your timestep and time vector
deltaT <- 1
t <- seq(0, 600, by = deltaT)

# Define the number of steps we take. This is equal to 10 / dt (why?)
N <- length(t)

# Define current solution state:
p_approx <- 250/13600

# Define a vector for your solution:the derivative equation
for (i in 2:N) { # We start this at 2 because the first value is 10
  dpdt <- .023 * p_approx[i - 1] * (1 - p_approx[i - 1])
  p_approx[i] <- p_approx[i - 1] + dpdt * deltaT
}


# Define your data for the solution into a tibble:
solution_data <- tibble(
  time = t,
  prop_infected = p_approx
)

# Plot your solution:
ggplot(data = solution_data) +
  geom_line(aes(x = time, y = prop_infected)) +
labs(
  x = "Time (days)",
  y = "Proportion infected"
)

```

```{r echo=FALSE,my-iterative-method,fig.cap="Results from applying an iterative method to solve Equation \\@ref(eq:flu-de-04)."}

# Define your timestep and time vector
deltaT <- 1
t <- seq(0, 600, by = deltaT)

# Define the number of steps we take. This is equal to 10 / dt (why?)
N <- length(t)

# Defines current solution state:
p_approx <- 250/13600

# Define a vector for your solution:the derivative equation
for (i in 2:N) { # We start this at 2 because the first value is 10
  dpdt <- .023 * p_approx[i - 1] * (1 - p_approx[i - 1])
  p_approx[i] <- p_approx[i - 1] + dpdt * deltaT
}


# Define your data for the solution into a tibble:
solution_data <- tibble(
  time = t,
  prop_infected = p_approx
)

# Plot your solution:
ggplot(data = solution_data) +
  geom_line(aes(x = time, y = prop_infected),size=1) +
labs(
  x = "Time (days)",
  y = "Proportion infected"
) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) 

```

Let's break the code down that produced Figure \@ref(fig:my-iterative-method) step by step:^[You may notice that when you run the code to produce Figure \@ref(fig:my-iterative-method) on your own it may not look like the output shown here. I've customized how the plots are displayed in this book using the pacakge `ggthemes`.]

- `deltaT <- 0.1` and `t <- seq(0,2,by=deltaT)` define the timesteps ($\Delta t$) and the output time vector `t`.
- The statement `N <- length(t)` defines how many steps we take.
- `p_approx<- 250/13600` defines the *proportion* of the population initially infected (assuming that $I(0)=250$). We will use this as the starting point to the solution vector.
- The `for` loop goes through this system - first computing the value of $\displaystyle \frac{dp}{dt}$ and then forecasing out the next timestep $p(t+\Delta t) = f(p) \cdot \Delta t$. We iteratively build the vector `p_approx`, adding another element at each timestep.
- The remaining code plots the data frame, like we learned in Chapter \@ref(r-intro-02).

### Euler's method in demodelr
To generate Figure \@ref(fig:my-iterative-method) we created the solution directly in `R` - but you don't want to copy and paste the code. The `demodeler` package has a function called `euler` that does the same process to generate the output solution:^[Don't forget to load up the `demodelr` library in your code at the top of your `R` code.] Try running the following code and plotting your solution:

```{r,eval=FALSE}

# Define the rate equation:
infection_eq <- c(dpdt ~ .023 * p * (1 - p))

# Define the initial condition (as a named vector):
prop_init <- c(p = 250/13600)

# Define deltaT and the time steps:
deltaT <- 1
n_steps <- 600

# Compute the solution via Euler's method:
out_solution <- euler(system_eq = infection_eq,
                      initial_condition = prop_init,
                      deltaT = deltaT, 
                      n_steps = n_steps
                      )
```

Once the vector `out_solution` is created, it has variables `t` and `p`, which can then be plotted with a `ggplot` statement. Let's talk through the steps of this code as well:

- The line `infection_eq <- c(dpdt ~ .023 * p * (13600-i))` represents the differential equation, written in formula notation. So $\displaystyle \frac{dp}{dt} \rightarrow$ `dpdt` and $f(p) \rightarrow$ `.023 * p * (1-p))`, with the variable `p`.
- The initial condition $p(0)=250/13600 = .018$ is written as a **named vector:** `prop_init <- c(p=250/13600)`. Make sure the name of the variable is consistent with your differential equation.
- As before we need to identify $\Delta t$ (`deltaT`) and the number of steps $N$ (`n_steps`). When we generated the solution in Figure \@ref(fig:my-iterative-method), in the `for` loop we defined the ending point at $t=2$ so the number of steps (`N`) was 20.^[In general if we know $\Delta t$ and the time we wish to end computing ($t_{end}$, then $N = t_{end}/\Delta t$.]

The command `euler` then computes the solution applying Euler's method, returning a data frame so we can plot the results. Note the columns of the data frame are the variables $t$ and $i$ that have been named in our equations.


### Euler's method applied to systems
Now that we have some experience with Euler's method, let's see how we can apply the function `euler` to a system of differential equations. Here is a sample code that shows the dynamics for the lynx-hare equations, as studied in Chapter \@ref(modeling-rates-03):

\begin{equation}
\begin{split} 
\frac{dH}{dt} &= r H - bHL \\  
\frac{dL}{dt} &= e  b  H L - dL
\end{split} (\#eq:lynx-hare-04)
\end{equation}

The variables $H$ and $L$ are already in thousands of animals, so we don't need to rescale anything like we did with Equation \@ref(eq:flu-de-04). We are going to use Euler's method to solve this differential equation, using the code below:

```{r,eval=FALSE}

# Define the rate equation:
lynx_hare_eq <- c(
  dHdt ~ r * H - b * H * L,
  dLdt ~ e * b * H * L - d * L
)

# Define the parameters (as a named vector):
lynx_hare_params <- c(r = 2, b = 0.5, e = 0.1, d = 1)

# Define the initial condition (as a named vector):
lynx_hare_init <- c(H = 1, L = 3)

# Define deltaT and the number of time steps:
deltaT <- 0.05
n_steps <- 200

# Compute the solution via Euler's method:
out_solution <- euler(system_eq = lynx_hare_eq,
                      parameters = lynx_hare_params,
                      initial_condition = lynx_hare_init,
                      deltaT = deltaT,
                      n_steps = n_steps
                      )

# Make a plot of the solution,
# using different colors for lynx or hares:
ggplot(data = out_solution) +
  geom_line(aes(x = t, y = H), color = "red") +
  geom_line(aes(x = t, y = L), color = "blue",linetype='dashed') +
  labs(
    x = "Time",
    y = "Lynx (red) or Hares (blue/dashed)"
  )
```

```{r,warning=FALSE,echo=FALSE,fig.cap="Euler's method solution for Lynx-Hare system (Equation \\ref{eq:lynx-hare-04})."}

# Define the rate equation:
lynx_hare_eq <- c(
  dHdt ~ r * H - b * H * L,
  dLdt ~ e * b * H * L - d * L
)

# Define the parameters (as a named vector):
lynx_hare_params <- c(r = 2, b = 0.5, e = 0.1, d = 1)

# Define the initial condition (as a named vector):
lynx_hare_init <- c(H = 1, L = 3)

# Define deltaT and the number of time steps:
deltaT <- 0.05
n_steps <- 200

# Compute the solution via Euler's method:
out_solution <- euler(system_eq = lynx_hare_eq,
                      parameters = lynx_hare_params,
                      initial_condition = lynx_hare_init,
                      deltaT = deltaT,
                      n_steps = n_steps
                      )



  
# Make a plot of the solution,
# using different colors for lynx or hares:
out_solution %>% pivot_longer(cols=c(-"t")) %>%
  ggplot() +
  geom_line(aes(x = t, y = value,linetype=name,color=name), size=1) +
  labs(
    x = "Time",
    y = "Population (thousands)"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_linetype_discrete(labels = c("Hares", "Lynx"),name=NULL) +
  scale_color_colorblind(labels = c("Hares", "Lynx"),name=NULL) 
```


This example is structured similarly when we used Euler's method to solve a single variable differential equation, with some key changes (that are easy to adapt):

- The variable `lynx_hare_eq` is now a vector, with each entry one of the rate equations.
- We need to identify both variables in their initial condition.
- Most importantly, Equation \@ref(eq:lynx-hare-04) has parameters, which we define as a named vector `lynx_hare_params <- c(r = 2, b = 0.5, e = 0.1, d = 1)` that we pass through to the command `euler` with the option `parameters`. If your equation does not have any parameters you do not need to worry about specifying this input.
- We plot both solutions together at the end, or you can make two separate plots. Remember that you can choose the color in your plot. I included the additional option `linetype=dashed` for the hares population for ease of viewing.


## Euler's method and beyond
Sometimes when working with Euler's method you encounter a differential equation that produces some nonsensible results. For example, consider a model that represents infection with quarantine (see Exercise \@ref(exr:flu-quarantine-01) in Chapter \@ref(intro-01)):

\begin{equation}
\begin{split}
\frac{dS}{dt} &= -kSI \\
\frac{dI}{dt} &= -kSI  - \beta I
\end{split} (\#eq:flu-quarantine-04)
\end{equation}

In Equation \@ref(eq:flu-quarantine-04), susceptibles become sick by encountering an infected person, but infected people are removed from the population at a rate $\beta$. The model in Figure \@ref(fig:euler-flu-bad-04) illustrates the results when this model is implemented using `euler`:

```{r euler-flu-bad-04,echo=FALSE,fig.cap="Surprising results when using Euler's method to solve Equation \\@ref(eq:flu-quarantine-04). Notice how some values for $I$ are negative."}
# Define the rate equation:
quarantine_eq <- c(
  dSdt ~ -k * S * I,
  dIdt ~ k * S * I - beta * I
)

# Define the parameters (as a named vector):
quarantine_parameters <- c(k = .05, beta = .2)

# Define the initial condition (as a named vector):
quarantine_init <- c(S = 300, I = 1)

# Define deltaT and the number of time steps:
deltaT <- .1 # timestep length
n_steps <- 10 # must be a number greater than 1

# Compute the solution via Euler's method:
out_solution <- euler(system_eq = quarantine_eq,
                      parameters = quarantine_parameters,
                      initial_condition = quarantine_init,
                      deltaT = deltaT,
                      n_steps = n_steps
                      )

# Make a plot of the solution:
out_solution %>% pivot_longer(cols=c(-"t")) %>%
  mutate(name = fct_relevel(name, 
            "S", "I")) %>%
  ggplot() +
  geom_line(aes(x = t, y = value,linetype=name,color=name), size=1) +
  labs(
    x = "Time",
    y = "People"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_linetype_discrete(labels = c("Susceptible","Infected"),name=NULL) +
  scale_color_colorblind(labels = c("Susceptible","Infected"),name=NULL) +
  geom_hline(yintercept=0,size=0.25)

```

You may notice in Figure \@ref(fig:euler-flu-bad-04) the solution for $S$ falls below $S=0$ around $t=0.75$.^[Notice in the code I've added a line for the horizontal axis with `geom_hline`.]  Negative values for $S$ are concerning because we know there can't be negative people!

At a given timestep, Euler's method constructs a locally linear approximation and forecasts the solution forward to the next timestep. Using Figure \@ref(fig:euler-flu-bad-04), at $t=0.75$ the value for $S \approx 1$ and the value for $I \approx 280$. If we let $k=0.05$ and $\beta=0.2$, this means that $\displaystyle \frac{dS}{dt}=-14$ and $\displaystyle \frac{dI}{dt}=-42$. At this point, the values of $S$ and $I$ are both decreasing. In turn, the forecast value for $S$ at $t=0.75$ is $S = 1 -14\cdot 0.1 = -0.4$. Mathematically, Euler's method is working correctly, but we know realistically that neither $S$ nor $I$ can be negative.

While Euler's method is useful, it does quite poorly in cases where the solution is changing rapidly, such as described above. A way to circumvent this is to adjust the value of $\Delta t$ to be smaller, which comes at the expense of more computational time. A second way is to use a *higher order solver* than `euler`, and one such method is called the [Runge-Kutta method](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods). (You study these methods when you take a course in numerical analysis.\index{Runge-Kutta} How we implement the Runge-Kutta method is to replace the command `euler` with `rk4`:

```{r, eval = FALSE}
# Define the rate equation:
quarantine_eq <- c(
  dSdt ~ -k * S * I,
  dIdt ~ k * S * I - beta * I
)

# Define the parameters (as a named vector):
quarantine_parameters <- c(k = .05, beta = .2)

# Define the initial condition (as a named vector):
quarantine_init <- c(S = 300, I = 1)

# Define deltaT and the number of time steps:
deltaT <- .1 # timestep length
n_steps <- 10 # must be a number greater than 1

# Compute the solution via Runge-Kutta method:
out_solution <- rk4(system_eq = quarantine_eq,
                      parameters = quarantine_parameters,
                      initial_condition = quarantine_init,
                      deltaT = deltaT,
                      n_steps = n_steps
                      )

# Make a plot of the solution:
ggplot(data = out_solution) +
  geom_line(aes(x = t, y = S), color = "red") +
  geom_line(aes(x = t, y = I), color = "blue",linetype="dashed") +
  geom_hline(yintercept=0,size=0.25) +
  labs(
    x = "Time",
    y = "Susceptible (red) or Infected (blue/dashed)"
  )
```

```{r rk4-flu-good-04,echo=FALSE,fig.cap="Runge-Kutta solution for Equation \\ref{eq:flu-quarantine-04}. Notice how the solution curve for the variable $S$ does not fall below zero as it does in Figure \\@ref(fig:euler-flu-bad-04)."}
# Define the rate equation:
quarantine_eq <- c(
  dSdt ~ -k * S * I,
  dIdt ~ k * S * I - beta * I
)

# Define the parameters (as a named vector):
quarantine_parameters <- c(k = .05, beta = .2)

# Define the initial condition (as a named vector):
quarantine_init <- c(S = 300, I = 1)

# Define deltaT and the number of time steps:
deltaT <- .1 # timestep length
n_steps <- 10 # must be a number greater than 1

# Compute the solution via Euler's method:
out_solution <- rk4(system_eq = quarantine_eq,
                      parameters = quarantine_parameters,
                      initial_condition = quarantine_init,
                      deltaT = deltaT,
                      n_steps = n_steps
                      )

# Make a plot of the solution:
out_solution %>% pivot_longer(cols=c(-"t")) %>%
  mutate(name = fct_relevel(name, 
            "S", "I")) %>%
  ggplot() +
  geom_line(aes(x = t, y = value,linetype=name,color=name), size=1) +
  labs(
    x = "Time",
    y = "People"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_linetype_discrete(labels = c("Susceptible","Infected"),name=NULL) +
  scale_color_colorblind(labels = c("Susceptible","Infected"),name=NULL) +
  geom_hline(yintercept=0,size=0.25)


```


Another benefit to the `rk4` method is the numerical error when computing the solution. The numerical error is quantified as the difference between the actual solution and the numerical solution. Euler's method has an error on the order of the stepsize $\Delta t$, whereas the Runge-Kutta method has an error of $(\Delta t)^4$. For this example, $\Delta t = 0.1$, whereas for the Runge-Kutta method the numerical error is on the order of 0.0001 ($(\Delta t)^{4} =.0001$) - noticeably different!  We can improve Euler's method by taking a smaller timestep - BUT that means we need a larger number of steps $N$ - which may take more computational time (see Exercise \@ref(exr:rk4-euler)). Does this discussion of numerical error sounds familiar?  In calculus you may have examined the numerical error when using Riemann sums (left, right, trapezoid, midpoint sums) to approximate the area underneath a curve. While the context is different, Riemann sums and numerical differential equation solvers are closely related.

In summary, the most general form of a differential equation is: 

\begin{equation}
\displaystyle \frac{d\vec{y}}{dt} = f(\vec{y},\vec{\alpha},t), (\#eq:de-general-04)
\end{equation}

where in Equation \@ref(eq:de-general-04) $\vec{y}$ is the vector of state variables you want to solve for, and $\vec{\alpha}$ is your vector of parameters. At a given initial condition, Euler's method applies locally linear approximations to forecast the solution forward $\Delta t$ time units (Equation \@ref(eq:euler-general-04)):

\begin{equation}
\vec{y}_{n+1} = \vec{y}_{n} + f(\vec{y}_{n},\vec{\alpha},t_{n}) \cdot \Delta t (\#eq:euler-general-04)
\end{equation}

Both the Euler or Runge-Kutta methods define a workflow (approximate $\rightarrow$ forecast $\rightarrow$ repeat) to generate a numerical solution to a system of differential equations. The process of defining a workflow is a powerful technique that we will revisit several times throughout this textbook, so stay tuned!



## Exercises

```{exercise}
Verify that $I(t) = 130-120e^{-0.025t}$ is a solution to the differential equation $$\displaystyle \frac{dI}{dt} = 130-0.025I $$ with $I(0)=10$.
```


```{exercise}
Apply the `rk4` solver with $\Delta t = 1$ with $N=600$ to the initial value problem $\displaystyle \frac{dp}{dt} = 0.023 p \cdot (13600-p)$, $p(0)=250/13600$. Compare your solution to Figure \@ref(fig:my-iterative-method). What differences do you observe?  Which solution method (`euler` or `rk4`) is better (and why)?
```

```{exercise}
In the model presented by Equation \@ref(eq:flu-quarantine-04), is $S+I$ constant? *Hint: add $\displaystyle \frac{dS}{dt}$ and $\displaystyle \frac{dI}{dt}$.*
```

```{exercise}
The following exercise will help you explore the relationships between stepsize, ending points, and number of steps needed. You may assume that we will start at $t=0$ in all parts.

a. If we wish to do a Euler's method solution with step size 1 second and ending at $T=5$ seconds, how many steps will we take?
b. If we wish to do a Euler's method solution with step size 0.5 seconds and ending at $T=5$ seconds, how many steps will we take? 
c. If we wish to do a Euler's method solution with step size 0.1 seconds and ending at $T=5$ seconds, how many steps will we take?
d. If we wish to do a Euler's method solution with step size $\Delta t$ and go to ending value of $T$, what is an expression that relates the number steps $N$ as a function of $\Delta t$ and $T$?
  
```

```{exercise euler-rk4}
To get a rough approximation between error and step size, let's say for a particular differential equation that we are starting at $t=0$ and going to $t=2$, with $\Delta t = 0.2$ with 10 steps. We know that the Runge-Kutta error will be on the order of $(\Delta t)^{4} =0.0016$. If we want to use Euler's method with the same order of error, we could say $\Delta t = .0016$. For that case, how many steps will we need to take?

```

```{exercise euler-solve}
For each of the following differential equations, apply Euler's method to generate a numerical solution to the differential equation and plot your solution. The stepsize ($\Delta t$) and number of iterations ($N$) are listed.


a. Differential equation: $\displaystyle \frac{dS}{dt} =3-S$. Set $\Delta t = 0.1$, $N = 50$. Initial conditions:  $S(0) = 0.5$, $S(0) = 5$.
b. Differential equation: $\displaystyle \frac{dS}{dt} =\frac{1}{1-S}$. Set $\Delta t = 0.01$, $N = 30$. Initial conditions:  $S(0) = 0.5$, $S(0) = 2$.
c. Differential equation: $\displaystyle \frac{dS}{dt} = 0.8 \cdot S \cdot (10-S)$. Set $\Delta t = 0.1$, $N = 50$. Initial conditions:  $S(0) = 3$, $S(0) = 10$.

```

```{exercise}
For each of the following differential equations, apply the Runge-Kutta method to generate a numerical solution to the differential equation and plot your solution. The stepsize ($\Delta t$) and number of iterations ($N$) are listed. Contrast your answers with Exercise \@ref(exr:euler-solve).


a. Differential equation: $\displaystyle \frac{dS}{dt} =3-S$. Set $\Delta t = 0.1$, $N = 50$. Initial conditions:  $S(0) = 0.5$, $S(0) = 5$.
b. Differential equation: $\displaystyle \frac{dS}{dt} =\frac{1}{1-S}$. Set $\Delta t = 0.01$, $N = 30$. Initial conditions:  $S(0) = 0.5$, $S(0) = 2$.
c. Differential equation: $\displaystyle \frac{dS}{dt} = 0.8 \cdot S \cdot (10-S)$. Set $\Delta t = 0.1$, $N = 50$. Initial conditions:  $S(0) = 3$, $S(0) = 10$.

```


```{exercise}
Complete the following steps:
  
a. Apply the code `euler` to generate a numerical solution to the differential equation:

- Differential equation: $\displaystyle \frac{dS}{dt} = r \cdot S \cdot (K-S)$.
- Set $r=1.2$ and $K=3$.
- Set $\Delta t = 0.1$, $N = 50$.
- Initial conditions (three different ones): $S(0) = 1$, $S(0) = 3$, $S(0) = 5$. 

b. Plot your Euler's method solutions with the three initial conditions on the same plot. What do you notice when you do plot them together?
c. Make a hypothesis regarding the long term behavior of this system. Then plot a few more solution curves to verify your guess.

```


```{exercise}
Complete the following steps:
  
a. Apply the code `euler` to generate a numerical solution to the differential equation:
  
- Differential equation: $\displaystyle \frac{dS}{dt} =K-S$.
- Set $K=2$.
- Set $\Delta t = 0.1$, $N = 50$.
- Initial conditions (three different ones): $S(0) = 0$, $S(0) = 2$, $S(0) = 5$. 

b. Plot your Euler's method solutions with the three initial conditions on the same plot. What do you notice when you do plot them together?
c. Make a hypothesis regarding the long term behavior of this system. Then plot a few more solution curves to verify your guess.



```


```{exercise}
This exercise uses the following differential equation:

\begin{equation}
\frac{dS}{dt} = 0.8 \cdot S \cdot (10-S)
\end{equation}

a. Apply Euler's method with $S(0)=15$, $\Delta t = 0.1$, $N = 10$. 
b. When you examine your solution, what is incorrect about the Euler's method solution based on your qualitative knowledge of the underlying dynamics?
c. Now calculate Euler's method for the same differential equation for the following conditions: $S(0)=15$, $\Delta t = 0.01$, $N = 100$. What has changed in your solution? 

```


```{exercise euler-ftbu}
Apply Euler's method to the differential equation $\displaystyle \frac{dS}{dt} =\frac{1}{1-S}$ with the following conditions:

- $S(0)=1.5$, $\Delta t = 0.1$, $N = 10$
- $S(0)=1.5$, $\Delta t = 0.01$, $N = 100$.

Between these two solutions, what has changed? Do you think it is numerically possible to calculate a reasonable solution for Euler's method near $S=1$?  (*note: this differential equation is an example of finite time blow up*)
```


```{exercise}
One way to model the growth rate of hares is with $\displaystyle f(H) = \frac{r H}{1+kH}$, where $r$ and $k$ are parameters. This is in constrast to exponential growth, which assumes $f(H) = rH$.

a. First evaluate $\displaystyle \lim_{H \rightarrow \infty} rH$.
b. Then $\displaystyle \lim_{H \rightarrow \infty} \frac{r H}{1+kH}$.
c. Compare your two answers. Discuss how the growth rate $\displaystyle f(H) = \frac{r H}{1+kH}$ seems to be a more realistic model.

```

```{exercise}
In the lynx-hare example we can also consider an alternative system where the growth of the hare is not exponential:
  
\begin{equation}
\begin{split}
\frac{dH}{dt} &= \frac{2 H}{1+kH} - 0.5HL \\ 
\frac{dL}{dt} &= 0.05  H L - L
\end{split}
\end{equation}

Set the number of timesteps to be 2000, $\delta t = 0.1$, with initial condition $H=1$ and $L=3$. Apply Euler's method to numerically solve this system of equations when $k=0.1$ and $k=1$ and plot your simulation results.
```


```{exercise}
Consider the differential equation $\displaystyle \frac{dS}{dt} = \frac{1}{1-S}$. Notice that at $S=1$ the rate $\displaystyle \frac{dS}{dt}$ is not defined.


a. If you applied Euler's method solution with initial condition $S(0)=0.9$, what would the values of $S$ approach as time increases?
b. If you applied Euler's method solution with initial condition $S(0)=1.1$, what would the values of $S$ approach as time increases?
c. Explain how you could come to the same conclusion as the previous two problems if you graphed $\displaystyle f(S) = \frac{1}{1-S}$.

```


```{exercise rk4-euler}
Building on Exercise \@ref(exr:euler-rk4), let's say for a particular differential equation we have $N$ steps from $0 \leq t \leq b$. An error of $\epsilon$ is desired. 

a. What is the ratio $\displaystyle \frac{N_{E}}{N_{RK4}}$, where $N_{RK4}$ represents the number of steps needed for the Runge-Kutta method, and $N_{E}$ the number of steps for Euler's method?
b. Make a plot of the ratio $\displaystyle \frac{N_{E}}{N_{RK4}}$ for $0 \leq \epsilon \leq 1$. How many more steps does Euler's method need to do to achieve the same level of error, compared to the Runge-Kutta method?

```

<!--chapter:end:04-eulersmethod.Rmd-->

# Phase Lines and Equilibrium Solutions {#phase-05}

Chapter \@ref(euler-04) explored numerical techniques to solve initial value problems. This chapter takes a step back to examine the general family of solutions to a differential equation. Will the family of solutions converge in the long run (as $t \rightarrow \infty$) to a constant value? Are these specific solutions that always remain constant (or independent of time)? Answering questions such as these address the *qualitative* behavior for a single differential equation.^[Qualitative behavior for coupled systems of differential equations is addressed in Chapter \@ref(coupled-06).] Let's get started!\index{differential equation!qualitative behavior}

## Equilibrium solutions
Chapter \@ref(modeling-rates-03) introduced the concept of an equilibrium solution, or where the rate of change for a differential equation is zero.\index{equilibrium solution} We can determine equilibrium solutions for a single-variable differential equation by setting the left hand side of $\displaystyle \frac{dy}{dt}=f(y)$ equal to zero and solving for $y$ (or whatever dependent variable describes the problem).

```{example exponential-05}
What are the equilibrium solutions to $\displaystyle \frac{dy}{dt}=- y$?
```

```{solution}
For this example we know that when the rate of change is zero, this means that $\displaystyle \frac{dy}{dt} = 0$, or when $0 = -y$. So $y=0$ is the equilibrium solution. 
```

The general solution to the differential equation $\displaystyle \frac{dy}{dt}=- y$ is $y(t)=Ce^{-t}$, where $C$ is an arbitrary constant. (We will explore techniques to determine this in Chapter \@ref(exact-solns-07).)  Figure \@ref(fig:exponential) plots different initial conditions, with the equilibrium solution shown as a horizontal line:

```{r, label="exponential", echo=FALSE,fig.cap="Solution curves to $y'=-y$ for different initial conditions (values of $C$)."}
t <- seq(0, 5, length.out = 50)
data.frame(time = t, s1 = exp(-t), s2 = 2 * exp(-t), s3 = -1 * exp(-t), s4 = -2 * exp(-t)) %>%
  gather(key = constant, value = soln, -time) %>%
  ggplot() +
  geom_line(aes(x = time, y = soln, color = constant), size = 1) +
  theme_bw() +
  theme(
    legend.position = "right",
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  labs(x = "t", y = "y") +
  geom_hline(aes(yintercept = 0), linetype = "dashed") +
  scale_color_colorblind(
    labels = c("C = 1", "C = 2", "C = -1", "C = -2"),
    name = NULL
  )
```


Notice that in Figure \@ref(fig:exponential) as $t$ increases, all solutions approach the equilibrium solution $y=0$, regardless if the initial condition is positive or negative. This observation is also confirmed by evaluating the limit $\displaystyle \lim_{t\rightarrow \infty} Ce^{-t}$, which is 0.

```{example logistic-05}
Determine equilibrium solutions to

\begin{equation}
\displaystyle \frac{dN}{dt} = N \cdot(1-N) (\#eq:logistic-05)
\end{equation}
```

```{solution}
In this case the equilibrium solutions for Equation \@ref(eq:logistic-05) occur when $N \cdot(1-N) = 0$, or when $N=0$ or $N=1$.

The general solution to Equation \@ref(eq:logistic-05) is

\begin{equation}
N(t)= \frac{C}{C +(1-C) e^{-t}}. (\#eq:logistic-soln-05)
\end{equation}

Figure \@ref(fig:logistic-soln) displays several different solution curves for Equation \@ref(eq:logistic-soln-05).
```


```{r, label="logistic-soln", echo=FALSE,fig.cap="Solution curves for Equation \\@ref(eq:logistic-05) with different initial conditions (values of $C$)."}
t <- seq(0, 5, length.out = 50)
data.frame(time = t, s1 = 2 / (2 + (1 - 2) * exp(-t)), s2 = 1.5 / (1.5 + (1 - 1.5) * exp(-t)), s3 = 0.5 / (0.5 + (1 - 0.5) * exp(-t)), s4 = 0.25 / (0.25 + (1 - 0.25) * exp(-t)), s5 = 0.1 / (0.1 + (1 - 0.1) * exp(-t))) %>%
  gather(key = constant, value = soln, -time) %>%
  ggplot() +
  geom_line(aes(x = time, y = soln, color = constant), size = 1) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  labs(x = "t", y = "N") +
  geom_hline(aes(yintercept = 0), linetype = "dashed") +
  geom_hline(aes(yintercept = 1), linetype = "dashed") +
  scale_color_colorblind(
    labels = c("C = 2", "C = 1.5", "C = 0.5", "C = 0.25", "C = 0.1"),
    name = NULL
  )
```



In Figure \@ref(fig:logistic-soln) notice how all the solutions tend towards $N=1$, but even solutions that start close to $N=0$ seem to move away from the $N=0$ equilibrium solution. Solutions in  Figure \@ref(fig:logistic-soln) exhibit the idea of the *stability* of an equilibrium solution, which we discuss next.

## Phase lines for differential equations


The **stability** of an equilibrium solution describes the long-term behavior of the family of solutions. Solutions can converge to the equilibrium solution in the long run, or they may not. More formally stated:

> An equilibrium solution $y_{*}$ to a differential equation $\displaystyle \frac{dy}{dt} = f(y)$ is considered *stable* when for a given solution $\displaystyle \lim_{t \rightarrow \infty} y(t) = y_{*}$.

You may note that the definition of stability relies on determining the solution $y(t)$. However we can circumvent determining this solution by using ideas from calculus and the rate of change:


- If $\displaystyle \frac{dy}{dt}<0$, the solution $y(t)$ is decreasing.
- If $\displaystyle \frac{dy}{dt}>0$, the solution $y(t)$ is increasing.


So to classify stability of an equilibrium solution we can investigate the behavior of the differential equation *around* the equilibrium solutions.

Let's apply this logic to our differential equation $\displaystyle \frac{dy}{dt}=- y$ from Example \@ref(exm:exponential-05). When $y=3$, $\displaystyle \frac{dy}{dt}=- 3 <0$, so we say the function is *decreasing* to $y=0$. When  $y=-2$, $\displaystyle \frac{dy}{dt}=- (-2) = 2 >0$, so we say the function is *increasing* to $y=0$. This can be represented neatly in the *phase line diagram* for Figure \@ref(fig:phaseline1).^[Sometimes arrows are used in the phase line to signify if the solutions are increasing or decreasing. I will stick to the convention presented in Figure \@ref(fig:phaseline1) because it illustrates connections between the differential equation and the solution.]


```{r,label="phaseline1",echo=FALSE,fig.cap="Phase line for the differential equation $y'=-y$."}
eq_soln <- data.frame(x = c(0), y = c(0), label = "y = 0")
data.frame(point = c(-5, -2, 3), label = c("y':", "y' > 0", "y' < 0"), tendency = c("y:", "y increasing", "y decreasing")) %>%
  ggplot() +
  geom_text(aes(point, 0.5, label = label)) +
  geom_text(aes(point, -.5, label = tendency)) +
  geom_text(data = eq_soln, aes(x = x, y = -1.2, label = label)) +
  geom_hline(yintercept = 0) +
  geom_segment(data = eq_soln, aes(x = x, xend = x, y = -1, yend = 1)) +
  geom_point(data = eq_soln, aes(x = x, y = 0), size = 3, color = "red") +
  coord_cartesian(xlim = c(-5, 5), ylim = c(-4, 4)) +
  labs(
    x = NULL,
    y = NULL
  ) +
  theme_void()
# theme(axis.line.x = element_blank(),
#        axis.ticks = element_blank(),
#        axis.text = element_blank())
```

Because the solution is *increasing* to $y=0$ when $y <0$, and *decreasing* to $y=0$ when $y >0$, we say that the equilibrium solution for the differential equation $y'=-y$ is **stable**, which is also confirmed by the solutions plotted in Figure \@ref(fig:exponential).

Now let's generalize the example $y'=-y$ to classify the stability of the equilibrium solutions to $\displaystyle \frac{dy}{dt} = r y$, where $r$ is a parameter. Fortunately the equilibrium solution is still $y=0$. We will need to consider three different cases for the stability depending on the value of $r$ ($r>0$, $r<0$, and $r=0$):

- When $r<0$, the phase line will be similar to Figure \@ref(fig:phaseline1).
- When $r>0$ the phase line will be as shown in Figure \@ref(fig:phaseline2). We say in this case that the equilibrium solution is *unstable*, as all solutions flow away from the equilibrium. Several different solutions are shown in Figure \@ref(fig:solncurve-1) .
- When $r=0$ we have the differential equation $\displaystyle \frac{dy}{dt}=0$, which has $y=C$ as a general solution. For this special case the equilibrium solution is neither stable or unstable^[Arguably when $r=0$ the resulting differential equation $y'=0$ is different than $y'=ry$; something peculiar is going on here - which is discussed more in Chapter \@ref(bifurcation-20).].


  
```{r phaseline2,echo=FALSE, fig.cap="Phase line for the differential equation $y'=ry$, with $r>0$."}
eq_soln <- data.frame(x = c(0), y = c(0), label = "y = 0")
data.frame(point = c(-5, -2, 3), label = c("y':", "y' < 0", "y' > 0"), tendency = c("y:", "y decreasing", "y increasing")) %>%
  ggplot() +
  geom_text(aes(point, 0.5, label = label)) +
  geom_text(aes(point, -.5, label = tendency)) +
  geom_text(data = eq_soln, aes(x = x, y = -1.2, label = label)) +
  geom_hline(yintercept = 0) +
  geom_segment(data = eq_soln, aes(x = x, xend = x, y = -1, yend = 1)) +
  geom_point(data = eq_soln, aes(x = x, y = 0), size = 3, color = "red") +
  coord_cartesian(xlim = c(-5, 5), ylim = c(-4, 4)) +
  labs(
    x = NULL,
    y = NULL
  ) +
  theme_void()
# theme(axis.line.x = element_blank(),
#        axis.ticks = element_blank(),
#        axis.text = element_blank())
```



```{r,label=solncurve-1, echo=FALSE,fig.cap="Solution curves for the differential equation $y'=ry$, with $r>0$ for different initial conditions (values of $C$)."}
t <- seq(0, 2, length.out = 50)
data.frame(time = t, s1 = exp(t), s2 = 2 * exp(t), s3 = -1 * exp(t), s4 = -2 * exp(t)) %>%
  gather(key = constant, value = soln, -time) %>%
  ggplot() +
  geom_line(aes(x = time, y = soln, color = constant), size = 1) +
  labs(x = "t", y = "y") +
  geom_hline(aes(yintercept = 0), linetype = "dashed") +
  theme_bw() +
  theme(
    legend.position = "right",
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(
    labels = c("C = 1", "C = 2", "C = -1", "C = -2"),
    name = NULL
  )
```


Based on the above discussion, let's return back to the differential equation $\displaystyle \frac{dN}{dt} = N \cdot(1-N)$ from Example \@ref(exm:logistic-05). We evaluate the stability of the equilibrium solutions $N=0$ and $N=1$ in Table \@ref(tab:logistic-table-05).

Table: (\#tab:logistic-table-05) Evaluation of the stability of the equilibrium solutions for Equation \@ref(eq:logistic-05).

**Test point** | **Sign of $N'$**  | **Tendency of solution**
-------------| ------------- | -------------
    $N=-1$ | Negative | Decreasing
    $N=0$ |  Zero | Equilibrium solution
    $N=0.5$ | Positive | Increasing
    $N=1$ |  Zero | Equilibrium solution
    $N=2$ |  Negative | Decreasing

Notice how the selected test points in the first column of Table \@ref(tab:logistic-table-05) were selected to the _left_ or the _right_ of the equilibrium solutions. The phase line diagram of Figure \@ref(fig:phaseline-N) also presents the same information as in Table \@ref(tab:logistic-table-05), but in contrast to Figures \@ref(fig:phaseline1) and \@ref(fig:phaseline2) we need to include _two_ equilibrium solutions. Phase line diagrams should include all the computed equilibrium solutions.

  
```{r,label="phaseline-N",echo=FALSE,fig.cap="Phase line diagram for Equation \\@ref(eq:logistic-05)."}
eq_soln <- data.frame(x = c(0, 1), y = c(0, 1), label = c("N = 0", "N = 1"))
data.frame(point = c(-1.5, -0.5, 0.5, 1.5), label = c("N':", "N' < 0", "N' > 0", "N' < 0"), tendency = c("N:", "N decreasing", "N increasing", "N decreasing")) %>%
  ggplot() +
  geom_text(aes(point, 0.5, label = label)) +
  geom_text(aes(point, -.5, label = tendency)) +
  geom_text(data = eq_soln, aes(x = x, y = -1.2, label = label)) +
  geom_hline(yintercept = 0) +
  geom_segment(data = eq_soln, aes(x = x, xend = x, y = -1, yend = 1)) +
  geom_point(data = eq_soln, aes(x = x, y = 0), size = 3, color = "red") +
  coord_cartesian(xlim = c(-1.5, 2), ylim = c(-4, 4)) +
  labs(
    x = NULL,
    y = NULL
  ) +
  theme_void()
# theme(axis.line.x = element_blank(),
#        axis.ticks = element_blank(),
#        axis.text = element_blank())
```

Table \@ref(tab:logistic-table-05) and Figure \@ref(fig:phaseline-N) confirm that solutions move _away_ from the equilibrium solution $N=0$ and move _towards_ the equilibrium solution $N=1$. These results suggest that the equilibrium solution at $N=1$ is *stable* and the equilibrium solution at $N=0$ is *unstable*.\index{equilibrium solution!unstable} Therefore, one way to define an equilibrium solution $y_{*}$ as unstable is when $\displaystyle \lim_{t \rightarrow -\infty} y(t) = y_{*}$. 

## A stability test for equilibrium solutions


Notice how when constructing the phase line diagram we relied on the behavior of solutions *around* the equilibrium solution to classify the stability. As an alternative we can also use the point at the equilibrium solution itself.

Consider the general differential equation $\displaystyle \frac{dy}{dt}=f(y)$ with an equilibrium solution at $y_{*}$. Next we apply local linearization to construct a locally linear approximation to $L(y)$ to $f(y)$ at $y=y_{*}$ (Equation \@ref(eq:local-linear-1-05)):

\begin{equation}
L(y) = f(y_{*}) + f'(y_{*}) \cdot (y-y_{*})  (\#eq:local-linear-1-05)
\end{equation}

There are two follow-on steps to simplify Equation \@ref(eq:local-linear-1-05). First, because we have an equilibrium solution, $f(y_{*}) =0$. Second, Equation \@ref(eq:local-linear-1-05) can be written with a new variable $P$, defined by variable $P = y-y_{*}$. With these two steps Equation \@ref(eq:local-linear-1-05) translates to Equation \@ref(eq:local-linear):

\begin{equation}
\frac{dP}{dt} = f'(y_{*}) \cdot P  (\#eq:local-linear)
\end{equation}

Does Equation \@ref(eq:local-linear) look familiar?  It should!  This equation is similar to the example where we classified the stability of $\displaystyle \frac{dy}{dt} = ry$ (notice that $f'(y_{*})$ is a number). Using this information, a test to classify the stability of an equilibrium solution is the following:

**Local linearization stability test for equilibrium solutions:** For a differential equation $\displaystyle \frac{dy}{dt} = f(y)$ with equilibrium solution $y_{*}$, we can classify the stability of the equilibrium solution through the following:

- If $f'(y_{*})>0$ at an equilibrium solution, the equilibrium solution $y=y_{*}$ will be _unstable_.
- If $f'(y_{*}) <0$ at an equilibrium solution, the equilibrium solution $y=y_{*}$ will be _stable_.
- If $f'(y_{*}) = 0$, we cannot conclude anything about the stability of $y=y_{*}$.

Let's return back to the differential equation $\displaystyle \frac{dN}{dt} = N \cdot(1-N)$ from Example \@ref(exm:logistic-05) and apply the local linearization stability test, $f'(N)=1-2N$. Since $f'(0)=1$, which is greater than 0, the equilibrium solution $N=0$ is unstable. Likewise, if $f'(1)=-1$, the equilibrium solution $N=1$ is stable.

Applying the local linearization test may be easier to quickly determine stability of an equilibrium solution. Guess what? This test also is a simplified form of determining stability of equilibrium solutions for systems of differential equations. We will explore this more in Chapter \@ref(stability-19).


## Exercises

```{exercise,label="eq-soln-ex"}
For the following differential equations, (1) determine any equilibrium solutions, and (2) classify the stability of the equilibrium solutions by applying the local linearization test.

a. $\displaystyle \frac{dS}{dt} = 0.3 \cdot(10-S)$
  
b. $\displaystyle \frac{dP}{dt} = P \cdot(P-1)(P-2)$


```


```{exercise}
Using your results from Exercise \@ref(exr:eq-soln-ex), construct a phase line for each of the differential equations and classify the stability of the equilibrium solutions.
```



```{exercise}
A population grows according to the equation $\displaystyle \frac{dP}{dt} = \frac{P}{1+2P} - 0.2P$.

a. Determine the equilibrium solutions for this differential equation.
b. Classify the stability of the equilibrium solutions using the local linearization stability test.


```




<!-- Based off LW pg 4 -->
```{exercise}
(Inspired by @logan_mathematical_2009) A cell with radius $r$ assimilates nutrients at a rate proportional to its surface area, but uses nutrients proportional to its volume, according to the following differential equation:

  \begin{equation}
 \frac{dr}{dt} = k_{A} 4 \pi r^{2} - k_{V} \frac{4}{3} \pi r^{3},
\end{equation}

where $k_{A}$ and $k_{V}$ are positive constants.

a. Determine the equilibrium solutions for this differential equation.
b. Construct a phase line for this differential equation to classify the stability of the equilibrium solutions.
c. Classify the stability of the equilibrium solutions using the local linearization stability test. Are your conclusions the same from the previous part?

```


<!-- Thornley and Johnson -->
```{exercise}
(Inspired by @thornley_plant_1990) The Chanter equation of growth is the following, where $W$ is the weight of an object:
  
  \begin{equation}
\frac{dW}{dt} =  W(3-W)e^{-Dt}
\end{equation}

Use this differential equation to answer the following questions.


a. What happens to the rate of growth ($\displaystyle  \frac{dW}{dt}$) as $t$ grows large?
b. What are the equilibrium solutions to this model?  Are they stable or unstable?
c. Notice how the equilbrium solutions are the same as those for the logistic model. Based on your previous work, why would this model be a more realistic model of growth than the logistic model $\displaystyle  \frac{dW}{dt} = W(3-W)$?

```


```{exercise}
Red blood cells are formed from stem cells in the bone marrow. The red blood cell density $r$ satisfies an equation of the form

\begin{equation}
\frac{dr}{dt} = \frac{br}{1+r^{n}} - c r,
\end{equation}

where $n>1$ and $b>1$ and $c>0$. Find all the equilibrium solutions $r_{*}$ to this differential equation. *Hint:* can you factor an $r$ from your equation first?
```


 <!-- Van den Berg page 19 -->
```{exercise}
(Inspired by @berg_mathematical_2011) Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of $S$ in the blood is the following:

\begin{equation}
\frac{dS}{dt} = I + p \cdot (W - S),
\end{equation}

where the parameter $I$ represents the active uptake of salt, $p$ is the permeability of the skin, and $W$ is the salinity in the water.


a. First set $I=0$. Determine the equilibrium solutions for this differential equation. Express your answer $S_{*}$ in terms of the parameters $p$ and $W$.
b. Next consider $I>0$. Determine the equilibrium solutions for this differential equation. Express your answer $S_{*}$ in terms of the parameters $p$, $W$, and $I$. Why should your new equilbrium solution be greater than the equilibrium solution from the previous problem?
c. Classify the stability of both equilibrium solutions in both cases using the local linearization stability test.


```



<!-- LW exercise 5, pg 36 -->

```{exercise}
(Inspired by @logan_mathematical_2009) The immigration rate of bird species (species per time) from a mainland to an offshore island is $I_{m} \cdot (1-S/P)$, where $I_{m}$ is the maximum immigration rate, $P$ is the size of the source pool of species on the mainland, and $S$ is the number of species already occupying the island. Further, the extinction rate is $E \cdot S / P$, where $E$ is the maximum extinction rate. The growth rate of the number of species on the island is the immigration rate minus the extinction rate, given by the following differential equation:

\begin{equation} \frac{dS}{dt} = I_{m} \left(1-\frac{S}{P} \right) - \frac{ES}{P}
\end{equation}
  

a. Determine the equilibrium solutions $S_{*}$ for this differential equation. Expression your answer in terms of $I_{M}$, $P$, and $E$.
b. Classify the stability of the equilibrium solutions using the local linearization stability test.

  
```


```{exercise}
A colony of bacteria growing in a nutrient-rich medium depletes the nutrient as they grow. As a result, the nutrient concentration $x(t)$ is steadily decreasing. The equation describing this decrease is the following:
  
\begin{equation} \frac{dx}{dt} = - \mu \frac{x \cdot (\xi- x)}{\kappa + x},
\end{equation}

where $\mu$, $\kappa$, and $\xi$ are all parameters greater than zero.


a. Determine the equilibrium solutions $x_{*}$ for this differential equation.
b. Construct a phase line for this differential equation and classify the stability of the equilibrium solutions.

```



```{exercise cross-eq}
Can a solution curve cross an equilibrium solution of a differential equation?
```

<!--chapter:end:05-phase.Rmd-->

# Coupled Systems of Equations {#coupled-06}
Chapter \@ref(phase-05) focused on qualitative analysis of a single differential equation using phase lines and slope fields. This chapter extends this idea further to systems of differential equations, where the natural extension of a phase line is a *phase plane*.\index{phase plane} Here is the good news: many of the techniques are similar to the ones introduced in Chapter \@ref(phase-05), so let's get started!

## Flu with quarantine and equilibrium solutions

In Exercise \@ref(exr:flu-quarantine-01) in Chapter \@ref(intro-01) we developed the following model for the flu as a coupled system of equations shown in Equation \@ref(eq:flu-quarantine-06):


\begin{equation}
\begin{split}
\frac{dS}{dt} &= -kSI \\
\frac{dI}{dt} &= kSI-\beta I \\
\frac{dR}{dt} &= \beta I,
\end{split} (\#eq:flu-quarantine-06)
\end{equation}

where $S$ represents susceptible people, $I$ infected people, and $R$ recovered people. Another way to represent this context is with the schematic shown in Figure \@ref(fig:quarantine):


```{tikz,quarantine,warning=FALSE,message=FALSE,echo=FALSE,fig.align="center",fig.cap="Schematic of the flu model with quarantine."}

\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=2cm]

\node [vspecies] (S) {$S$} ;
\node [vspecies, right of = S] (I) {$I$} ;
\draw [->,thick] (S) --  node {\small{$kS$}} (I) ;
\draw [->,thick] (I.south) --  node[right] {\small{$\beta$}} +(0pt,-1cm) ;
\end{tikzpicture}

```


While Equation \@ref(eq:flu-quarantine-06) is a system of three differential equations, notice that the variable $R$ is not present on the right hand sides of each equation. As a result, the variable $R$ is decoupled from this system of equations, so we can just focus on the rates of change for $S$ and $I$ (Equation \@ref(eq:flu-quarantine-small-06)):

\begin{equation}
\begin{split}
\frac{dS}{dt} &= -kSI \\
\frac{dI}{dt} &= kSI-\beta I \\
\end{split} (\#eq:flu-quarantine-small-06)
\end{equation}

With Equation \@ref(eq:flu-quarantine-small-06) we will solve for equilibrium solutions (similar to what we did in Chapters \@ref(modeling-rates-03) and \@ref(phase-05)), which we focus on next. 

## Nullclines
The process to determine equilibrium solutions for a system of differential equations starts with computing the *nullclines* for each rate in the system of equations.\index{nullclines} The nullclines are solutions in the plane where one of the rates is zero, so for example either $\displaystyle \frac{dS}{dt}$ or $\displaystyle \frac{dI}{dt}$ is zero. For coupled systems of equations, the equilibrium solutions are where the rates $\displaystyle \frac{dS}{dt}$ and $\displaystyle \frac{dI}{dt}$ in Equation \@ref(eq:flu-quarantine-small-06) are _both_ zero, found through algebraically solving the system of equations in Equation \@ref(eq:flu-ss-06):

\begin{equation}
\begin{split}
0 &= -kSI \\
0 &= kSI - \beta I
\end{split} (\#eq:flu-ss-06)
\end{equation}

Let's examine the first equation ($0 = -kSI$). Since all the terms are expressed as a product, then nullclines for $S$ occur when either $S=0$ or $I=0$. 

In a similar manner, the nullclines for $I$ occur when $0 = kSI - \beta I$. For this expression we can factor out an $I$, yielding $0 = I \cdot (kS - \beta)$. Because the last equation is factored as a product, nullclines for $I$ are either $I=0$ or by solving $kS-\beta$ for $S$ to yield $\displaystyle S = \frac{\beta}{k}$.

Nullclines are not equilibrium solutions by themselves - it is the _intersection_ of two different nullclines that determines equilibrium solutions. Figure \@ref(fig:quarantine-nullclines) shows the nullclines in the $S-I$ plane (since we have two equations), with $S$ on the horizontal axis and $I$ on the vertical axis. In Figure \@ref(fig:quarantine-nullclines) we have also assumed that $\beta=1$ and $k=1$. The $S-I$ plane shown in Figure \@ref(fig:quarantine-nullclines) is the beginning of the construction of the phase plane for Equation \@ref(eq:flu-quarantine-small-06) and also to determine the equilibrium solutions.


```{r quarantine-nullclines, echo=FALSE,message=FALSE,warning=FALSE,fig.cap="Nullclines for Equation \\@ref(eq:flu-quarantine-small-06). To generate the plot we assumed $\\beta=1$ and $k=1$."}
k <- 1
beta <- 1
ggplot(data.frame(x = c(0, 2)), aes(x)) +
  labs(x = "S", y = "I") +
  # stat_function(fun=function(x)beta/k, geom="line", aes(colour="I' = 0"),size=2) +
  stat_function(fun = function(x) 0, geom = "line", aes(colour = "S' = 0"), size = 1) +
  stat_function(fun = function(x) 0, geom = "line", aes(colour = "I' = 0"), size = 1, linetype = 2, inherit.aes = TRUE) +
  geom_segment(data = data.frame(x1 = 0, x2 = 0, y1 = 0, y2 = 2), aes(x = x1, xend = x2, y = y1, yend = y2, colour = "S' = 0"), size = 1, inherit.aes = TRUE) +
  geom_segment(data = data.frame(x1 = beta / k, x2 = beta / k, y1 = 0, y2 = 2), aes(x = x1, xend = x2, y = y1, yend = y2, colour = "I' = 0"), size = 1, inherit.aes = TRUE, linetype = "dashed") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  # geom_vline(xintercept = beta/k, aes(colour="I' = 0"),size=2) +
  scale_color_colorblind(name = "Nullclines:", breaks = c("S' = 0", "I' = 0"))
```

A key thing to note is that where two different nullclines cross is an *equilibrium solution* to the system of equations (**both** $\displaystyle \frac{dS}{dt}$ and $\displaystyle \frac{dI}{dt}$ are zero at this point). Examining Figure \@ref(fig:quarantine-nullclines) three possibilities appear:

1. There is an equilibrium solution at $S=0$ and $I=0$ (otherwise known as the origin). This equilibrium solution makes biological sense: if there is nobody susceptible or infected there are no flu cases (everyone is perfectly healthy - yay!) .
2. The entire horizontal axis is an equilibrium solution because $I=0$, which makes both $\displaystyle \frac{dS}{dt}$ and $\displaystyle \frac{dI}{dt}$ zero. There is a practical interpretation of this nullcline - whenever $I=0$, meaning there are no infected people around, infection cannot occur.
3. There is also a third possibility where the vertical line at $S=1$ crosses the horizontal axis ($S=1$, $I=0$), but that also falls under the second equilibrium solution.^[For the general system (Equation \@ref(eq:flu-quarantine-small-06)), the equilibrium solution would that be $\displaystyle S=\frac{\beta}{k}$ and $I=0$.]

Now that we have identified our nullclines and equilibrium solutions, we will add additional context with the *flow* of the solution.


## Phase planes
Next we can add more context to the Figure \@ref(fig:quarantine-nullclines) by evaluating different values of $S$ and $I$ into our system of equations and plotting the _phase plane_.\index{phase plane} How we plot the phase plane is similar to the method in Chapter \@ref(phase-05). We will test points around an equilibrium solution to determine if the solution is increasing or decreasing in $S$ or $I$ independently.

Table \@ref(tab:slope-arrows-06) evaluates the derivatives $\displaystyle \frac{dS}{dt}$ and $\displaystyle \frac{dI}{dt}$ in \@ref(eq:flu-quarantine-small-06) for different values of $S$ and $I$.


```{r slope-arrows-06, echo=FALSE,message=FALSE,warning=FALSE}
k <- 1
beta <- 1

dS <- function(S, I) {
  -k * S * I
}

dI <- function(S, I) {
  k * S * I - beta * I
}

S <- seq(0, 2, by = 1)
I <- seq(0, 2, by = 1)
model_out <- expand.grid(S, I) %>%
  rename(S = 1, I = 2) %>%
  mutate(dSdt = dS(S, I), dIdt = dI(S, I)) %>%
  round(digits = 2)
kable(t(model_out), caption = "Values of $\\displaystyle \\frac{dS}{dt}$ (as `dSdt`) and $\\displaystyle \\frac{dI}{dt}$ (as `dIdt`) for Equation \\ref{eq:flu-quarantine-small-06}.")
```

Notice in Table \@ref(tab:slope-arrows-06) the different values of $\displaystyle \frac{dS}{dt}$ and $\displaystyle \frac{dI}{dt}$ in Equation \@ref(eq:flu-quarantine-small-06) at each of the given $S$ and $I$ values. We can plot each of the coordinate pairs of $\displaystyle \left( \frac{dS}{dt}, \frac{dI}{dt} \right)$ as a vector (arrows) in the $(S,I)$ plane. To do so, associate $\displaystyle \frac{dS}{dt}$ with left-right motion, so positive values of $\displaystyle \frac{dS}{dt}$ mean the vector points to the right. Likewise, we associate  $\displaystyle \frac{dI}{dt}$ with up-down motion, so positive values $\displaystyle \frac{dI}{dt}$ mean the vector points up.

Defining the directions of the vectors in this way is also consistent when Equation \@ref(eq:flu-quarantine-small-06) is evaluated at the nullcline solutions. At the point $(S,I)=(1,1)$, we have an arrow that points directly to the west because $\displaystyle \frac{dS}{dt} < 0$ and $\displaystyle \frac{dI}{dt} =0$. Continuing on in this manner, by sequentially sampling points in the $(S,I)$ plane we get a vector field plot (Figure \@ref(fig:slope-field-06)), superimposed with the nullclines.

```{r slope-field-06, echo=FALSE, message = FALSE, warning = FALSE, fig.cap="Phase plane for Equation \\@ref(eq:flu-quarantine-small-06), with $\\beta=1$ and $k=1$."}
k <- 1
beta <- 1
s_window <- c(0, 2)
i_window <- c(0, 2)

systems_eq <- c(
  dsdt ~ -1 * S * I,
  didt ~ 1 * S * I - 1 * I
)



# We hacked out the color codes - can't do stat function on a phase plot!
phaseplane(systems_eq, "S", "I", x_window = s_window, y_window = i_window) +
  stat_function(fun = function(x) 0, geom = "line", aes(colour = "S' = 0"), size = 1) +
  stat_function(fun = function(x) 0, geom = "line", aes(colour = "I' = 0"), size = 1, linetype = 2, inherit.aes = TRUE) +
  geom_segment(data = data.frame(x1 = 0, x2 = 0, y1 = 0, y2 = 2), aes(x = x1, xend = x2, y = y1, yend = y2, colour = "S' = 0"), size = 1, inherit.aes = TRUE) +
  geom_segment(data = data.frame(x1 = beta / k, x2 = beta / k, y1 = 0, y2 = 2), aes(x = x1, xend = x2, y = y1, yend = y2, colour = "I' = 0"), size = 1, inherit.aes = TRUE, linetype = "dashed") +
  # geom_vline(xintercept = beta/k, aes(colour="I' = 0"),size=2) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name = "Nullclines:", breaks = c("S' = 0", "I' = 0"))
```



### Motion around the nullclines

We can also extend the motion around the nullclines to investigate the stability of an equilbrium solution. With a one-dimensional differential equation we used a number line to quantify values where the solution is increasing / decreasing. The problem with several differential equations is that the notion of "increasing" or "decreasing" becomes difficult to understand - there is an additional degree of freedom! Simply put, in a plane you can move left/right *or* up/down. The benefit for having nullclines is that they **isolate** the motion in one direction. When $\displaystyle \frac{dS}{dt}=0$ the only allowed motion is up and down; when $\displaystyle \frac{dI}{dt}=0$ the only allowed motion is left and right.

In general for a two-dimensional system:

- When a horizontal axis variable has a nullcline, the only allowed motion is up/down.
- When a vertical axis variable has a nullcline,  the only motion is up/down.

Applying this knowledge to Equation \@ref(eq:flu-quarantine-small-06), if we choose points where $I'=0$ then we know that the only motion is to the left and the right because $S$ can still change along that curve. If we choose points where $S'=0$ then we know that the only motion is to the up/down because $I$ can still change along that curve.


### Stability of an equilbrium solution
Figure \@ref(fig:slope-field-06) qualitatively tells us about the stability of an equilibrium point. One of the equilibrium solutions is at the origin $(S,I)=(0,0)$. As before we want to investigate if the equilibrium solution is stable or unstable. As you can see the arrows appear to be pointing into and towards the equilibrium solution. So we would classify this equilbrium solution as *stable*.


## Generating a phase plane in `R`
Let's take what we learned from the case study of the flu model with quarantine to qualitatively analyze a system of differential equations:

- We determine nullclines by setting the derivatives equal to zero.
- Equilibrium solutions occur where nullclines for the two different equations intersect.
- The arrows in the phase plane help us characterize the stability of the equilibrium solution.

The `demodelr` package has some basic functionality to generate a phase plane. Consider the following system of differential equations (Equation \@ref(eq:phase-example)):

\begin{equation}
\begin{split}
\frac{dx}{dt} &= x-y \\
\frac{dy}{dt} &= x+y
\end{split} (\#eq:phase-example)
\end{equation}


In order to generate a phase plane diagram for Equation \@ref(eq:phase-example) we need to define functions for $x'$ and $y'$, which I will annotate as $dx$ and $dy$ respectively. We are going to collect these equations in one vector called `system_eq`, using the tilde (~) as a replacement for the equals sign:

```{r}
system_eq <- c(
  dx ~ x - y,
  dy ~ x + y
)
```

Then what we do is apply the command `phaseplane`, which will generate a vector field over a domain:


```{r,eval = FALSE}
phaseplane(system_eq,
  x_var = "x",
  y_var = "y"
)
```


```{r,echo=FALSE,fig.cap="Phase plane for Equation \\@ref(eq:phase-example)."}
phaseplane(system_eq,
  x_var = "x",
  y_var = "y"
) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```


Let's discuss how the `phaseplane` function works, first with the required inputs:

- You will need a system of differential equations (which we defined as `system_eq`).
- Next you need to define which variable belongs on the horizontal axis (`x_var = 'x'`) or the vertical axis (`y_var = 'y'`). In Exercise \@ref(exr:wacky-phase) you will explore what happens if these get mixed up.

There are some additional options for `phaseplane`:

- The option `eq_soln = TRUE` will determine if there are any equilibrium solutions to be found and report them to the console. This option does not provide a definitive answer, but at least it tells you where to look. You can always confirm if a point is an equilibrium solution by evaluating the differential equation.
- You can adjust the windows that are plotted with the options `x_window` and `y_window`. Both of these need to be defined as a vector (e.g. `x_window = c(-0.1,0.1)`. The default window size is $[-4,4]$ for both axes.
- There is an option `parameters` that allows you to pass any parameters to the phase plane. Later chapters will introduce systems where you can modify the parameters - we won't worry about that now.


## Slope fields
For a one-dimensional differential equation, we call the phase plane a *slope field*.\index{slope field} For a given differential equation $y'=f(t,y)$, at each point in the $t-y$ plane the differential equation is evaluated, showing the direction of the tangent line at that particular point. The `phaseplane` function can also plot slope fields. Let's take a look at an example first and then discuss how that it works.


```{example}
A colony of bacteria growing in a nutrient-rich medium depletes the nutrient as they grow. As a result, the nutrient concentration $x(t)$ is steadily decreasing. Determine the slope field for the following differential equation:

\begin{equation}
\frac{dx}{dt} = - 0.7 \cdot \frac{x \cdot (3- x)}{1 + x} (\#eq:bacteria-colony-06)
\end{equation}


```

The `R` code shown below will generate the slope field for Equation \@ref(eq:bacteria-colony-06) (shown in Figure \@ref(fig:phaseline-06)):

```{r, eval = FALSE}

# Define the windows where we make the plots
t_window <- c(0, 3)
x_window <- c(0, 5)

# Define the differential equation
system_eq <- c(
  dt ~ 1,
  dx ~ -0.7 * x * (3 - x) / (1 + x)
)

phaseplane(system_eq,
  x_var = "t",
  y_var = "x",
  x_window = t_window,
  y_window = x_window
) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```


```{r phaseline-06,echo=FALSE,fig.cap="Slope field for Equation \\@ref(eq:bacteria-colony-06)."}

# Define the windows where we make the plots
t_window <- c(0, 3)
x_window <- c(0, 5)

# Define the differential equation
system_eq <- c(
  dt ~ 1,
  dx ~ -0.7 * x * (3 - x) / (1 + x)
)

phaseplane(system_eq,
  x_var = "t",
  y_var = "x",
  x_window = t_window,
  y_window = x_window
) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```

A few notes about the code that generated Figure \@ref(fig:phaseline-06):

- The variable on the horizontal axis (`x_var`) is $t$, and on the vertical axis (`y_var`) is $x$. Confusing, I know.
- The viewing window for the axis is also defined accordingly.
- Notice how the variable `system_eq` also contains the additional equation `dt = 1`. What we are doing is re-writing Equation \@ref(eq:bacteria-colony-06) by introducing a new variable $s$ (Equation \@ref(eq:single-phase)):

\begin{equation}
\begin{split}
\frac{dt}{ds} &= 1 \\
\frac{dx}{ds} &= - 0.7 \cdot \frac{x \cdot (3- x)}{1 + x}
\end{split} (\#eq:single-phase)
\end{equation}

The differential equation $\displaystyle \frac{dt}{ds} = 1$ has a solution $s=t$, so really Equation \@ref(eq:single-phase) is a (slightly more) complicated way to express Equation \@ref(eq:bacteria-colony-06). Hacky? Perhaps. However re-writing Equation \@ref(eq:bacteria-colony-06) was a quick and handy workaround to re-use code.

This chapter introduced a lot of useful `R` code to aid in visualization. The good news is that we will explore additional analyses of systems of differential equations starting with Chapter \@ref(linearsystems-15) - there is so much more to learn. Onward!

## Exercises

```{exercise phase-eq}
Determine equilibrium solutions for Equation \@ref(eq:phase-example).
```

```{exercise phase-eq-num}
Generate a phase plane for Equation \@ref(eq:phase-example), but set the option `eq_soln = TRUE`. Did `phaseplane` detect the equilibrium solution you found in Exercise \@ref(exr:phase-eq)? If not, repeat the phase plane, but set `x_window = c(-0.1,0.1)` and `y_window = c(-0.1,0.1)` and repeat.
```

```{exercise wacky-phase}
Generate a phase plane for Equation \@ref(eq:phase-example), but this time set `x_var = y` and `y_var = x` (swap which variable is which). Notice that the incorrect phase plane is produced. What is the corresponding differential equation that is visualized by this phase plane?
```

```{exercise}
This problem considers the following system of differential equations:
\begin{equation}
\begin{split}
\frac{dx}{dt} &= y \\ 
\frac{dy}{dt} &= -x 
\end{split}
\end{equation}


a. Determine the equations of the nullclines and equilibrium solution of this system of differential equations.
b. Modify the function `phaseplane` to generate a phase plane of this system.
c. For each point along a nullcline, determine the resulting motion (up-down or left-right).
d. Based on the work you generated, determine if the equilibrium solution is *stable*, *unstable*, or *inconclusive*.
e. Verify that the functions $x(t) = \sin(t)$ and $y=\cos(t)$ is one solution to this system of differential equations.

```
 

 
```{exercise}
Consider the following system of differential equations:

\begin{equation} 
\begin{split}
\frac{dx}{dt} &= y \\ 
\frac{dy}{dt} &= 3x^{2}-1 
\end{split}
\end{equation}

a. Determine the equations of the nullclines and equilibrium solutions for this system of differential equations.
b. For each point along a nullcline, determine the resulting motion (up-down or left-right).
c. Modify the function `phaseplane` to generate a phase plane of this system. Adjust the windows for $x$ and $y$ to be between -1 and 1.
d. Make a hypothesis to classify if the equilibrium point is *stable* or *unstable*.

```




<!-- %Thornley pg 80 -->
```{exercise}
(Inspired by @thornley_plant_1990) A plant grows proportional to its current length $L$. Assume this proportionality constant is $\mu$, whose rate also decreases proportional to its current value. The system of equations that models this plant growth is the following:

\begin{equation}
\begin{split}
\frac{dL}{dt} &= \mu L \\ 
\frac{d\mu}{dt}  &= -0.1 \mu \\
\end{split}
\end{equation}

a. Explain why $L=0$ and $\mu=0$ is an equilibrium solution to this differential equation.
b. Modify the function `phaseplane` to generate a phase plane of this system. Use the window $-0.1 \leq L \leq 0.1$ and $-0.1 \leq \mu \leq 0.1$. (For this problem negative values of $L$ and $\mu$ are not sensible, but it aids in visualizing the equilibrium solution.)
c. Is the origin a stable equilibrium solution?


```



```{exercise}
(Inspired by @logan_mathematical_2009) Red blood cells are formed from stem cells in the bone marrow. The red blood cell density $r$ satisfies an equation of the form

\begin{equation}
\frac{dr}{dt} = \frac{0.2r}{1+r^{2}} -  0.1 r
\end{equation}


a. What are the equilibrium solutions for this differential equation?
b. Modify the function `phaseplane` to generate a phase line for this differential equation for $0 \leq t \leq 5$ and $0 \leq r \leq 5$.
c. Based on the phase line, are the equilibrium solutions stable or unstable?


```


 <!-- Van den Berg page 19 -->
```{exercise}
(Inspired by @berg_mathematical_2011) Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of $S$ in the blood is the following:

$$ \frac{dS}{dt} = 1 + 0.3 \cdot (3 - S) $$

a. What are the equilibrium solutions for this differential equation?
b. Modify the function `phaseplane` to generate a phase line for this differential equation for $0 \leq t \leq 10$ and $0 \leq S \leq 10$.
c. Based on the phase line, are the equilibrium solutions stable or unstable?


```






```{exercise reparam}
Consider the differential equation $\displaystyle \frac{dx}{dt} = -3x$. Here you will examine creating a two-dimensional system of equations by re-parameterizing $s=t$.


a. Define the variable $t = s$.  For this case, what is $\displaystyle \frac{dt}{ds}$?
b. When $x = f (t (s))$ ($x$ is a composition between $t$ and $s$), one way to express the chain rule is $\displaystyle \frac{dx}{ds} = \frac{dx}{dt} \cdot \frac{dt}{ds}$. Use this fact to explain why $\displaystyle \frac{dx}{ds} = -3x$.
c. Finally use your previous work to determine the system of equations for $\displaystyle \frac{dx}{ds}$ and $\displaystyle \frac{dt}{ds}$.


```




<!-- From van den Berg, pg 59, exercise 3.13 -->
```{exercise}
(Inspired by @berg_mathematical_2011) The core body temperature ($T$) of a mammal is coupled to the heat production (scaled by heat capacity $Q$) with the following system of differential equations:

\begin{equation}
\begin{split}
\frac{dT}{dt} &= Q + 0.5 \cdot (20-T) \\ 
\frac{dQ}{dt} &= 0.1 \cdot (38-T)
\end{split}
\end{equation}

 

a. Determine the equations of the nullclines and equilibrium solution of this system of differential equations.
b. For each point along a nullcline, determine the resulting motion (up-down or left-right). You may assume that both $T>0$ and $Q>0$.
c. Make a hypothesis to classify if the equilibrium solution is *stable* or *unstable*.

 
```




```{exercise}
Consider the following system of differential equations for the lynx-hare model (Equation \@ref(eq:lynx-hare-combined) from Chapter \@ref(modeling-rates-03)):

\begin{equation}
\begin{split}
\frac{dH}{dt} &= r H - b HL \\
\frac{dL}{dt} &=ebHL -dL
\end{split}
\end{equation}


a. Determine the equilibrium solutions for this system of differential equations.
b. Determine equations for the nullclines, expressed as $L$ as a function of $H$. There should be two nullclines for each rate. 


```




```{exercise}
(Inspired by @berg_mathematical_2011) A chemostat is a tank used to study microbes and ecology, where microbes grow under controlled conditions.\index{chemostat}  Think of this like a large tank with nutrient-rich water that is continuously cycled. For example, differential equations that describe the microbial biomass $W$ and the nutrient concentration $C$ (in the culture) are the following:

\begin{equation}
\begin{split}
\frac{dW}{dt} &= \mu W - F \frac{W}{V} \\
\frac{dC}{dt} &= D \cdot (C_{R}-C) - S \mu \frac{W}{V},
\end{split}
\end{equation}

where we have the following parameters: $\mu$ is the per capita reproduction rate, $F$ is the flow rate, $V$ is the volume of the culture solution, $D$ is the dilution rate, $C_{R}$ is the concentration of nutrients entering the culture, and $S$ is a stoichiometric conversion of nutrients to biomass. 

a. Write the equations of the nullclines for this differential equation.
b. Determine the equilibrium solutions for this system of differential equations.
c. Generate a phase plane for this differential equation with the values $\mu=1$, $D=1$, $C_{R}=2$, $S=1$, and $V=1$.
d. Classify the stability of the equilbrium solutions.

```
 

 
```{exercise}
 A classical paper *Experimental Studies on the Struggle for Existence: I. Mixed Population of Two Species of Yeast* by @gause_experimental_1932 examined two different species of yeast growing in competition with each other. The differential equations given for two species in competition are:

\begin{equation}
\begin{split}
\frac{dy_{1}}{dt} &= -b_{1} y_{1} \frac{(K_{1}-(y_{1}+\alpha y_{2}) )}{K_{1}} \\
\frac{dy_{2}}{dt} &= -b_{2} y_{2} \frac{(K_{2}-(y_{2}+\beta y_{1}) )}{K_{2}}, \\
\end{split}
\end{equation}

 where $y_{1}$ and $y_{2}$ are the two species of yeast with the parameters $b_{1}, \; b_{2}, \; K_{1}, \; K_{2}, \; \alpha, \; \beta$ describing the characteristics of the yeast species.

a. Determine the equilibrium solutions for this differential equation. Express your answer in terms of the parameters $b_{1}, \; b_{2}, \; K_{1}, \; K_{2}, \; \alpha, \; \beta$.
b. @gause_experimental_1932 computed the following values of the parameters: $b_{1}=0.21827, \; b_{2}=0.06069, \; K_{1}=13.0, \; K_{2}=5.8, \; \alpha=3.15, \; \beta=0.439$. Using these values and your results from part a, what would be the predicted values for the equilibrium solutions? Is there anything odd about the values for these equilibrium solutions?
c. Use the function `rk4` to solve this system of differential equations numerically and plot your solutions. Use initial conditions of $y_{1}(0)=.375$ and $y_{2}(0)=.291$, with $\Delta t = 1$ and $N=600$.


```

<!--chapter:end:06-coupledSystems.Rmd-->

# Exact Solutions to Differential Equations {#exact-solns-07}

Chapters \@ref(euler-04), \@ref(phase-05), and \@ref(coupled-06) studied numerical and qualitative tools to analyze differential equations. Phase planes and slope fields helped to determine the long-term stability of an equilibrium solution. Beyond these approaches, it is also helpful to know the *exact* solution to a differential equation. In this chapter we will study three techniques to determine exact solutions to differential equations, making connections to some tools that you know from calculus. We briefly illustrate the methods but also have lots of exercises for you to practice these techniques, with problems in and out of a given context. Let's get started!

## Verify a solution
The first approach is direct verification also known as the guess and check method.\index{differential equation!direct verification} Consider the differential equation shown in Equation \@ref(eq:exp-growth-07):

\begin{equation}
\frac{dS}{dt} = 0.7 S (\#eq:exp-growth-07)
\end{equation}

Direct verification starts with a candidate solution and then checks to see if the candidate solution is consistent with the differential equation. If we have an initial value problem (e.g. $S(0)=4$), then the candidate solution is also consistent with the initial condition.

For example, let's verify if the function $\tilde{S}(t) = 5 e^{0.7t}$ is a solution to Equation \@ref(eq:exp-growth-07). We do this by differentiating $\tilde{S}(t)$, which, using our knowledge of calculus, is $0.7 \cdot 5 e^{0.7t}$. Finally, by rearrangement, $\displaystyle \frac{d\tilde{S}}{dt} = 0.7 \cdot 5 e^{0.7t} = 0.7 e^{0.7t}$. Therefore the function $\tilde{S}$ *is* a solution to the differential equation. Let's build off this to try other candidate functions:

```{example verify-07}
Verify if the following functions are solutions to Equation \@ref(eq:exp-growth-07):
        
- $\tilde{R}(t) = 10e^{0.7t}$
- $\tilde{P}(t) = e^{0.7t}$
- $\tilde{Q}(t) = 5e^{0.7t}$
- $\tilde{F}(t)=3$
- $\tilde{G}(t)=0$


```

```{solution}
First apply direct differentiation to each of these functions (this represents the left hand side of each differential equation):

- $\tilde{R}(t) = 10e^{0.7t} \rightarrow \tilde{R}'(t) = 7e^{0.7t}$
- $\tilde{P}(t) = e^{0.7t} \rightarrow \tilde{P}'(t) = 0.7e^{0.7t}$
- $\tilde{Q}(t) = 5e^{0.7t} \rightarrow \tilde{Q}'(t) = 3.5e^{0.7t}$
- $\tilde{F}(t)=3 \rightarrow \tilde{F}'(t) = 0$
- $\tilde{G}(t)=0 \rightarrow \tilde{G}'(t) = 0$

Next compare each of these solutions to the right hand side of Equation \@ref(eq:exp-growth-07):
        
- $0.7\tilde{R}(t) = 0.7 \cdot 10e^{0.7t} \rightarrow  7e^{0.7t}$
- $0.7\tilde{P}(t) = 0.7 e^{0.7t}$
- $0.7\tilde{Q}(t) = 0.7 \cdot 5e^{0.7t} \rightarrow = 3.5e^{0.7t}$
- $0.7 \tilde{F}(t)=0.7 \cdot 3 \rightarrow 2.1$
- $0.7 \tilde{G}(t)=0.7 \cdot 0 \rightarrow  0$        

Notice how in the candidate functions (with the exception of $\tilde{F}(t)$) the right hand side of each equation equals the left hand side. When that is the case, our candidate functions are indeed solutions to Equation \@ref(eq:exp-growth-07)!
```


Verifying a solution to a differential equation relies on your knowledge of differentiation versus other more involved methods, which may be an under-appreciated approach. In some instances you may not be given a candidate function as in Example \@ref(exm:verify-07). Deciding "what function to try" is the hardest step, but a safe bet would be an exponential equation (especially if the right hand side involves the dependent variable). As we will see in Chapter \@ref(eigenvalues-18) the guess and check approach will help us to determine general solutions to systems of linear differential equations.

### Superposition of solutions
Related to the verification method is a concept called superposition of solutions. Here is how this works: if you have two known solutions to a differential equation, then the sum (or difference) is a solution as well. Let's look at an example:

```{example super-07}
Show that $\tilde{R}(t) + \tilde{Q}(t) = 5e^{0.7t} + e^{0.7t}$ from Example \@ref(exm:verify-07) is a solution to Equation \@ref(eq:exp-growth-07).
```

```{solution}
By direct differentiation, $\tilde{R}'(t) + \tilde{Q}'(t) = 3.5e^{0.7t} + 0.7e^{0.7t}$. Furthermore, $0.7 \cdot (\tilde{R}(t) + \tilde{Q}(t)) = 0.7 \cdot (5e^{0.7t} + e^{0.7t}) = 3.5 e^{0.7t} + 0.7 e^{0.7t}$, which equals $\tilde{R}'(t) + \tilde{Q}'(t)$.
```

Example \@ref(exm:super-07) illustrates the principle that different solutions to a differential equation can be added together and produce a new solution. More generally, adding two solutions together is an example of a *linear combination* of solutions, and we can state this more formally:

> If $x(t)$ and $y(t)$ are solutions to the differential equation $z' = f(t,z)$, then $c(t) = a \cdot x(t) + b \cdot y(t)$ are also solutions, where $a$ and $b$ are constants.

  

## Separable differential equations
The next techninque is called *separation of variables*.\index{differential equation!solution technique!separation of variables} This method has a defined workflow (Separate $\rightarrow$ Integrate $\rightarrow$ Solve), which we illustrate by considering the following differential equation:

\begin{equation}
\frac{dy}{dt} = yt^{2} (\#eq:sep-de-07)
\end{equation}

**Separate:** This step uses algebra to collect variables involving $x$ on one side of the equation, and the variables involving $y$ on the other (Equation \@ref(eq:sep1-07)):

\begin{equation}
\frac{1}{y} dy = t^{2} \; dt (\#eq:sep1-07)
\end{equation}

**Integrate:** This step computes the antiderivative of both sides of Equation \@ref(eq:sep1-07):

\begin{equation}
\begin{split}
\int  \frac{1}{y} dy = \ln(y) + C_{1}. \\
\int t^{2} \; dt  = \frac{1}{3} t^{3} + C_{2}
\end{split} (\#eq:int1-07)
\end{equation}

You may remember from calculus that whenever you compute antiderivatives to always include a $+C$ (hence the $C_{1}$ and $C_{2}$ in Equation \@ref(eq:int1-07)). For separable differential equations it is okay just to keep only one of the $+C$ terms in Equation \@ref(eq:int1-07), which usually is best on the side of the independent variable (in this case $t$). Since both sides of the separated equation are equal, we can rewrite Equation \@ref(eq:int1-07) on a single line (Equation \@ref(eq:int3-07)):

\begin{equation}
\ln(y) = \frac{1}{3} t^{3} + C (\#eq:int3-07)
\end{equation}

**Solve:** This last step solves Equation \@ref(eq:int3-07) for the dependent variable $y$:

\begin{equation}
\ln(y) =\frac{1}{3} t^{3} + C \rightarrow e^{\ln(y)} = e^{\frac{1}{3} t^{3} + C} = e^{C} \cdot e^{\frac{1}{3} t^{3}} \rightarrow y = Ce^{\frac{1}{3} t^{3}}  (\#eq:sol1-07)
\end{equation}

We are in business! Notice how in Equation \@ref(eq:sol1-07) at each step we just kept the constant to be $C$, since exponentiating a constant will still be constant.

To summarize, the workflow for the separating of variables technique is the following:

1. **Separate** the variables on one side of the equation.
2. **Integrate** both sides individually.
3. **Solve** for the dependent variable.


## Integrating factors
Chapters \@ref(intro-01) and \@ref(euler-04) examined a model for the spread of Ebola where that was proportional to the number infected:\index{differential equation!solution technique!integrating factor}

\begin{equation}
\frac{dI}{dt} = .023(13600-I) = 312.8  - .023I (\#eq:infected-07)
\end{equation}


While Equation \@ref(eq:infected-07) can be solved via separation of variables, let's try a different approach to illustrate another useful technique. First let's write the terms in Equation \@ref(eq:infected-07) that involve $I$ on one side of the equation:

\begin{equation}
\frac{dI}{dt} + .03I = 30. (\#eq:infected-2-07)
\end{equation}

What we are going to do is multiply both sides of this Equation \@ref(eq:infected-2-07) by $e^{.023t}$ (I'll explain more about that later):

\begin{equation}
\frac{dI}{dt} \cdot e^{.023t} + .023I \cdot e^{.023t} = 312.8  \cdot e^{.023t} (\#eq:infected-3-07)
\end{equation}

Hmmm - this seems like we are making the differential equation harder to solve, doesn't it?  However the left hand side of Equation \@ref(eq:infected-3-07) is actually the derivative of the expression $I \cdot e^{.023t}$ (courtesy of the product rule from calculus). Let's take a look:


\begin{equation}
\frac{d}{dt} \left( I \cdot e^{.023t} \right) = \frac{dI}{dt} \cdot e^{.023t} + I \cdot .023 e^{.023t} (\#eq:infected-4-07)
\end{equation}

Equation \@ref(eq:infected-4-07) allows us to express the left hand side of Equation \@ref(eq:infected-3-07) as a derivative and then integrate both sides:

\begin{equation}
\begin{split}
\frac{d}{dt} \left( I \cdot e^{.023t} \right) &= 312.8 \cdot e^{.023t} \rightarrow \\
\int \frac{d}{dt} \left( I \cdot e^{.023t} \right) \; dt &= \int 312.8 \cdot e^{.023t} \; dt \rightarrow \\
I \cdot e^{.023t} &= 13600 \cdot e^{.023t} + C
\end{split} (\#eq:infected-5-07)
\end{equation}

All that is left to do is to solve Equation \@ref(eq:infected-5-07) in terms of $I(t)$ by dividing by $e^{.023t}$, labeled as $I_{1}(t)$ (Equation \@ref(eq:i1)):

\begin{equation}
I_{1}(t) = 13600 + Ce^{-.023t}  (\#eq:i1)
\end{equation}

Cool!  The function $f(t)=e^{.023t}$ is called an _integrating factor_.\index{differential equation!solution technique!integrating factor} Let's explore this technique with a second example:

```{example}
Apply the integrating factor technique to determine a general solution to the differential equation:

\begin{equation}
\frac{dI}{dt} = .023t (13600-I) = 312.8 t  - .023 t \cdot I (\#eq:infected-v2-07)
\end{equation}

(Equation \@ref(eq:infected-v2-07) is a modification of Equation \@ref(eq:infected-07), where the rate of infection is time dependent.)
```

```{solution}
Re-writing Equation \@ref(eq:infected-v2-07) we have:

\begin{equation}
\frac{dI}{dt} + .023 t \cdot I = 312.8 t (\#eq:infected-v2-2-07)
\end{equation}

To write the left hand side of Equation \@ref(eq:infected-v2-2-07) as the derivative of a product of functions, multiply the _entire_ differential equation by $\displaystyle e^{\int .023t \; dt} = e^{.0115 t^{2}}$. This term is called the _integrating factor_ (Equation \@ref(eq:infected-v2-3-07)):

\begin{equation}
\frac{dI}{dt} \cdot e^{ .0115 t^{2}} + .023 t \cdot I \cdot e^{0.0115 t^{2}} = 312.8 t \cdot e^{0.0115 t^{2}} (\#eq:infected-v2-3-07)
\end{equation}

Rewrite the left hand side of Equation \@ref(eq:infected-v2-3-07) with the product rule:
  
\begin{equation}
\frac{dI}{dt} \cdot e^{ 0.0115 t^{2}} + .023 t \cdot I \cdot e^{0.0115 t^{2}} = \frac{d}{dt} \left( I \cdot e^{0.0115  t^{2}} \right) (\#eq:infected-v2-4-07)
\end{equation}

Next integrate Equation \@ref(eq:infected-v2-4-07):

\begin{equation}
\begin{split}
\frac{d}{dt} \left( I \cdot e^{.0115 t^{2}} \right) &= 312.8 t \cdot e^{.0115 t^{2}}  \rightarrow \\
\int \frac{d}{dt} \left( I \cdot e^{.0115  t^{2}} \right) \; dt &= \int 312.8t \cdot e^{.0115 t^{2}}  \; dt \rightarrow \\
I \cdot e^{0.0115 t^{2}} &= 27200 \cdot e^{.0115 t^{2}} + C
\end{split} (\#eq:infected-v2-5-07)
\end{equation}

The final step is to write the Equation \@ref(eq:infected-v2-5-07) in terms of $I(t)$; we will label this solution as $I_{2}(t)$:

\begin{equation}
I_{2}(t) =  27200 + C e^{-.0115 t^{2}}  (\#eq:i2)
\end{equation}

Figure \@ref(fig:if-compare) compares the solutions $I_{1}(t)$ and $I_{2}(t)$ when the initial condition (in both cases) is 10 (so $I_{1}(0)=I_{2}(0)=10$). Notice how the extra time-dependent factor in Equation \@ref(eq:infected-v2-07) makes the cases grow quickly before leveling off.
```


```{r if-compare,warning=FALSE,message=FALSE,echo=FALSE,fig.cap='Comparison of two integrating factor solutions, Equation \\@ref(eq:i1) in red and Equation \\@ref(eq:i2) in blue.'}
t <- seq(0,100,by=0.05)

# I_{1}(t) = 13600 + Ce^{-.023t}  (\#eq:i1)
# I_{2}(t) =  27200 + C e^{-.0115 t^{2}}  (\#eq:i2)

solutions <- tibble(t,
                    soln1 = 13600-13590*exp(-.023*t),
                    soln2 = 27200-27190*exp(-.0115*t^2))


solutions %>%
  pivot_longer(cols=c("soln1","soln2"),names_to="type",values_to="value") %>%
  ggplot(aes(x=t,y=value,color=type)) + geom_line(size=1) +
  labs(x="Time",y="Number Infected") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name = "Infection rate:", labels = c("Constant", "Time-dependent")) 
```



The integrating factor approach can be applied for differential equations that can be written in the form $\displaystyle \frac{dy}{dt} + f(t) \cdot y = g(t)$, using the following workflow:

1. **Determine** the _integrating factor_ $\displaystyle e^{\int f(t) \; dt}$. Hopefully the integral $\displaystyle \int f(t) \; dt$ is easy to compute!
2. **Multiply** the integrating factor across your equation to rewrite the differential equation as $\displaystyle \frac{d}{dt} \left( y \cdot e^{\int f(t) \; dt} \right) = g(t) \cdot e^{\int f(t) \; dt}$.
3. **Evaluate** the integral $\displaystyle H(t) = \int g(t) \cdot e^{\int f(t) \; dt} \; dt$. This looks intimidating - but hopefully is manageable to compute!  Don't forget the $+C$!
4. **Solve** for $y(t)$:  $\displaystyle y(t) = H(t) \cdot e^{-\int f(t) \; dt} + C e^{-\int f(t) \; dt}$.



## Applying the verification method to coupled equations {#verify-broad-07}
Finally, the method of verifying a solution helps to introduce a useful solution technique for systems of differential equations. We are going to study a simplified version of the lynx-hare model from Chapter \@ref(coupled-06). Equation \@ref(eq:lh-07) assumes both lynx and hares decline at a rate proportional to their respective population sizes, with the decline in the lynx population representing predation:

\begin{equation}
\begin{split}
\frac{dH}{dt} &= -b H  \\
\frac{dL}{dt} & = b H  - d L
\end{split} (\#eq:lh-07)
\end{equation}


Based on these simplified assumptions a good approach is to assume a solution that is exponential for both $H$ and $L$ (Equation \@ref(eq:lh-sol-07)):

\begin{equation}
\begin{split}
\tilde{H}(t) &= C_{1} e^{\lambda t} \\
\tilde{L}(t) &= C_{2} e^{\lambda t}
\end{split} (\#eq:lh-sol-07)
\end{equation}

Notice that Equation \@ref(eq:lh-sol-07) has $C_{1}$, $C_{2}$, and $\lambda$ as parameters.^[If you have taken a course in linear algebra, you may recognize that we are assuming the solution is a vector of the form $\vec{v} = \vec{C} e^{\lambda t}$.] Let's apply the verification method to determine expressions for $C_{1}$, $C_{2}$, and $\lambda$ that are consistent with Equation \@ref(eq:lh-07). By differentiation of Equation \@ref(eq:lh-sol-07), we have the following (Equation \@ref(eq:lh-sol-diff-07)):

\begin{equation}
\begin{split}
\frac{d\tilde{H}}{dt} &= \lambda C_{1} e^{\lambda t} \\
\frac{d\tilde{L}}{dt}  &= \lambda C_{2} e^{\lambda t}
\end{split} (\#eq:lh-sol-diff-07)
\end{equation}


Comparing Equation \@ref(eq:lh-sol-diff-07) to Equation \@ref(eq:lh-07) we can show that

\begin{equation}
\begin{split}
\lambda C_{1} e^{\lambda t} &= - b C_{1} e^{\lambda t} \\
 \lambda C_{2} e^{\lambda t} &= b  C_{1} e^{\lambda t} - d C_{2} e^{\lambda t}
\end{split} (\#eq:lh-sol-diff2-07)
\end{equation}

Let's rearrange Equation \@ref(eq:lh-sol-diff-07) some more:

\begin{equation}
\begin{split}
 (\lambda + b) C_{1} e^{\lambda t} &= 0\\
 (\lambda + d) C_{2} e^{\lambda t} &=b  C_{1} e^{\lambda t}
\end{split} (\#eq:lh-sol-diff2a-07)
\end{equation}

Notice that for the second expression in Equation \@ref(eq:lh-sol-diff2a-07) we can express $\displaystyle C_{1} e^{\lambda t}$ as $\displaystyle\frac{(\lambda + d)}{b} C_{2} e^{\lambda t}$. Next, we can substitute this expression for $C_{1} e^{\lambda t}$ into the first expression of Equation \@ref(eq:lh-sol-diff2a-07):

\begin{equation}
 (\lambda + b) \frac{(\lambda + d)}{b} C_{2} e^{\lambda t} = 0 
\end{equation}

If we assume that $b \neq 0$, then we have the following simplified expression (Equation \@ref(eq:lh-sol-diff3-07)):

\begin{equation}
(\lambda +b) (\lambda +d) C_{2} e^{\lambda t}  = 0 (\#eq:lh-sol-diff3-07)
\end{equation}


Because the exponential function in Equation \@ref(eq:lh-sol-diff3-07) never equals zero, the only possibility is that $(\lambda + b)(\lambda + d)=0$, or that $\lambda = -b$ or $\lambda = -d$.^[Remember: if expressions multiply to zero, then the only possibility is that at least one of them is zero. The process outlined here finds the *eigenvalues* and *eigenvectors* of a system of equations. We will study these concepts in Chapter \@ref(eigenvalues-18).]

Next we need to determine values of $C_{1}$ and $C_{2}$. We can do this by going back to the the second expression in Equation \@ref(eq:lh-sol-diff2-07), which we rearrange to Equation \@ref(eq:lh-sol-diff4-07):

\begin{equation}
(\lambda + d) C_{2} e^{\lambda t} -b  C_{1} e^{\lambda t}=0 (\#eq:lh-sol-diff4-07)
\end{equation}

Let's analyze Equation \@ref(eq:lh-sol-diff4-07) for each of the values of $\lambda$:

- Case $\lambda = -d$  
For this case we have Equation \@ref(eq:lh-sol-case1-07):

\begin{equation}
(-d +d) C_{2} e^{-d t} - b  C_{1} e^{-d t} =0 \rightarrow   -b  C_{1} e^{-d t} =0 (\#eq:lh-sol-case1-07)
\end{equation}

The only way for Equation \@ref(eq:lh-sol-case1-07) to be consistent and remain zero is if $C_{1}=0$. We don't have any restrictions on $C_{2}$, so the general solution is given with Equation \@ref(eq:lh-sol-f1-07): 

\begin{equation}
\begin{split}
\tilde{H}(t) &=0 \\
\tilde{L}(t) &= C_{2} e^{-d t}
\end{split} (\#eq:lh-sol-f1-07)
\end{equation}

- Case $\lambda = -d$
For this situation, Equation \@ref(eq:lh-sol-diff4-07) becomes Equation \@ref(eq:lh-sol-case2-07): 

\begin{equation}
\left( (-d +b) C_{2} - d  C_{1} \right) e^{-d t} =0 (\#eq:lh-sol-case2-07)
\end{equation}


The only way for Equation \@ref(eq:lh-sol-case2-07) to be consistent is if $\left( (-d +b) C_{2} - d  C_{1} \right)=0$, or if $\displaystyle C_{2} = \left( \frac{d}{-d + b} \right) C_{1}$. In this case, Equation \@ref(eq:lh-sol-f2-07) then represents the general solution: 

\begin{equation}
\begin{split}
\tilde{H}(t) &=  C_{1} e^{-d t} \\
\tilde{L}(t) &= \left( \frac{d}{-d + b} \right) C_{1} e^{-d t} (\#eq:lh-sol-f2-07)
\end{split}
\end{equation}

The parameter $C_{2}$ in Equation \@ref(eq:lh-sol-f2-07) can be determined by the initial condition. Notice that we need to have $d \neq b$ or our solution will be undefined.

Finally we can write down a general solution to Equation \@ref(eq:lh-07) by combining our Equations \@ref(eq:lh-sol-f1-07) and \@ref(eq:lh-sol-f2-07) by superposition (Equation \@ref(eq:lh-sol-f3-07)):

\begin{equation}
\begin{split}
H(t) &=  C_{1} e^{-d t} \\
L(t) &= \left( \frac{d}{-d + b} \right) C_{1} e^{-d t} + C_{2} e^{-b t}
\end{split} (\#eq:lh-sol-f3-07)
\end{equation}


The method outlined here only works on *linear* differential equations (i.e. it wouldn't work if there was a term such as $kHL$ in Equation \@ref(eq:lh-07)). In Chapter \@ref(eigenvalues-18) explores this method more systematically to determine general solutions to linear systems of equations.


As you can see, there are a variety of techniques that can be applied in the solution of differential equations. Many more solution techniques exist - but by and large the techniques presented here probably will be your "go-tos" when working to find an exact solution to a differential equation.

## Exercises

```{exercise}
Determine the value of $C$ when $I(0)=10$ for the two equations:
        
\begin{equation}
\begin{split}
I_{1}(t) = 1000 + Ce^{-.03t} \\
I_{2}(t) =  1000 + C e^{-0.015 t^{2}}
\end{split}
\end{equation}

```

```{exercise}
Solve Equation \@ref(eq:infected-07) using the separation of variables technique.
```

```{exercise}
Verify that $I_{2}(t) =  N + C e^{-0.5 k t^{2}}$ is the solution to the differential equation $\displaystyle \frac{dI}{dt} = kt (N-I)$.  Set $N=3$ and $C=1$. Plot $I_{2}(t)$ with various values of $k$ ranging from .001 to .1. What effect does $k$ have on the solution?
```

```{exercise}
Consider the following model of an infection:
  
  \begin{equation}
\begin{split}
\frac{dS}{dt} &= -k SI \\
\frac{dI}{dt} &= k SI - \beta I
\end{split}
\end{equation}

Use this equation to solve the following questions:
  
  a. Show that $\displaystyle \frac{I'}{S'} = -1 + \frac{\beta}{k} \frac{1}{S}$, where $\displaystyle S'=\frac{dS}{dt}$ and $\displaystyle I'=\frac{dI}{dt}$. We will call $\displaystyle \frac{I'}{S'} = \frac{dI}{dS}$.
b. Using separation of variables, show that the general solution to  $\displaystyle \frac{I'}{S'} = -1 + \frac{\beta}{k} \frac{1}{S}$ is $\displaystyle I(S) = -S + \frac{\beta}{k} \ln (S) + C$.
c. At the beginning of the epidemic, $S_{0}+I_{0} = N$, where $N$ is the total population size. Use this fact to determine $C$ in the equation $\displaystyle I_{0} = - S_{0} +  \frac{\beta}{k} \ln (S_{0}) + C$.
d. Using your previous answer, show that $\displaystyle I(S) = N- S + \frac{\beta}{k} \ln \left(\frac{S}{S_{0}} \right)$.
e. Plot a solution curve for $I(S)$ with $\beta = 1$, $k=0.1$, $N=100$, and $S_{0}=5$.
  
```

```{exercise}
(Inspired by @scholz_first-order_2014) A chemical reaction $2A \rightarrow C + D$ can be modeled with the following differential equation:
        
\begin{equation}
\frac{dA}{dt} = -2 k A^{2}
\end{equation}

Apply the method of separation of variables to determine a general solution for this differential equation.
```

```{exercise}
(Inspired by @berg_mathematical_2011) Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of $S$ in the blood is the following:

\begin{equation}
\frac{dS}{dt} = I + p \cdot (W - S)
\end{equation}

where the parameter $I$ represents the active uptake of salt, $p$ is the permeability of the skin, and $W$ is the salinity in the water. For this problem, set $I = 0.1$ (10% / hour), $p = 0.05$ hr$^{-1}$, $W = 0.4$ (40% salt concentration), and $S(0)=0.6$ (60% salt concentration).

a. Generate a slope field of the differential equation $\displaystyle \frac{dS}{dt} = 0.1 + 0.05 \cdot (.6 - S)$.
b. Apply integrating factors to solve the differential equation $\displaystyle \frac{dS}{dt} = 0.1 + 0.05 \cdot (.6 - S)$.
c. Does your solution conform to the slope field diagram?

```


```{exercise}
Which of the following differential equations can be solved via separation of variables?

a. $\displaystyle \frac{dy}{dx} = x^{2} + xy$
b. $\displaystyle \frac{dy}{dx} = e^{x+y}$
c. $\displaystyle \frac{dy}{dx} = y \cdot \cos(2+x)$
d. $\displaystyle \frac{dy}{dx} = \ln x + \ln y$
e. $\displaystyle \frac{dy}{dx} = x \cdot (y^{2}+2)$


Once you have identified which ones can be solved via separation of variables, apply that technique to solve each differential equation.
```


```{exercise}
Consider the following differential equation $\displaystyle \frac{dP}{dt} = - \delta P$, $P(0)=P_{0}$, where $\delta$ is a constant parameter.


a. Solve this equation using the method of separation of variables.
b. Solve this equation using an integrating factor.
c. Your two solutions from the two methods should be the same - are they?


```


```{exercise}
A differential equation that relates a consumer's nutrient content (denoted as $y$) to the nutrient content of food (denoted as $x$) is given by: 

\begin{equation}
\frac{dy}{dx} = \frac{1}{\theta} \frac{y}{x},
\end{equation}

where $\theta \geq 1$ is a constant. Apply separation of variables to determine the general solution to this differential equation.
```


```{exercise}
Apply separation of variables to determine general solutions to the following systems of differential equations:
  
\begin{equation}
\begin{split} 
\frac{dx}{dt} &= x \\ 
\frac{dy}{dt} &= y
\end{split} (\#eq:uncoupled-07)
\end{equation}

(Equation \@ref(eq:uncoupled-07) is an example of an *uncoupled* system of equations.)
```



<!-- %Thornley pg 80 -->
```{exercise}
(Inspired by @thornley_plant_1990) A plant grows proportional to its current length $L$. Assume this proportionality constant is $\mu$, whose rate also decreases proportional to its current value. The system of equations that models this plant growth is the following:
  
\begin{equation}
\begin{split}
\frac{dL}{dt}  = \mu L \\ 
\frac{d\mu}{dt}  = -k \mu \\
 \mbox{($k$ is a constant parameter)}
\end{split}
\end{equation}

Apply separation of variables to determine the general solutions to this system of equations.

```



```{exercise}
Apply the verification method developed in Chapter \@ref(verify-broad-07) to determine the general solution to the following system of differential equations:
  
\begin{equation}
\begin{split}
\frac{dx}{dt} &= x-y   \\
\frac{dy}{dt} & = 2y
\end{split}
\end{equation}

```


```{exercise}
Apply the integrating factors technique to determine the solution to the differential equation $\displaystyle \frac{dI}{dt} = (N-I) = kN  - kI$, where $k$ and $N$ are parameters.
```




```{exercise}
For each of the following differential equations:

- Determine equilibrium solutions for the differential equation.
- Apply separation of variables to determine general solutions to the following differential equations.
- Choose reasonable values of any parameters and plot the solution curve for an initial condition that you select.


a. $\displaystyle \frac{dy}{dx} = -\frac{x}{y}$
  
b. $\displaystyle \frac{dy}{dx} = 8-y$
  
c. $\displaystyle \frac{dW}{dt} = k (N-W)$  ($k$ and $N$ are constant parameters)

d. $\displaystyle \frac{dR}{dt} =-aR \ln \frac{R}{K}$  ($a$ and $K$ are constant parameters)

```


```{exercise}
Consider the following differential equation, where $M$ represents a population of mayflies and $t$ is time (given in months), and $\delta$ is a mortality rate (units % mayflies / month):
  
\begin{equation}
\frac{dM}{dt} = - \delta \cdot M
\end{equation}

Determine the general solution to this differential equation and plot a few different solution curves with different values of $\delta>0$. Assume that $M(0) = 10,000$. Describe the effect of changing $\delta$ on your solution.
```


```{exercise}
An alternative model of mayfly mortality is the following:
  
\begin{equation}
\displaystyle \frac{dM}{dt} = - \delta(t) \cdot M,
\end{equation}
where $\delta(t)$ is a time dependent mortality function. Determine a solution and plot a solution curve (assuming $M(0)=10,000$ and over the interval from $0 \leq t \leq 5$, assuming time is scaled appropriately) for this differential equation when $\delta(t)$ has the following forms:

  
a. $\delta(t) = 1$
b. $\delta(t) = 2t$
c. $\delta(t) = 1-e^{-t}$
d. $\delta(t) = 1+e^{-t}$

Provide a reasonable biological explanation justifying the use of these alternative mayfly models. 
```


<!--chapter:end:07-exactSolutions.Rmd-->

# (PART) Parameterizing Models with Data {.unnumbered}

# Linear Regression and Curve Fitting {#linear-regression-08}


## What is parameter estimation?
Chapters \@ref(intro-01) - \@ref(exact-solns-07) introduced the idea of modeling with rates of change. There is much more to be stated regarding qualitative analyses of a differential equation (Chapters \@ref(phase-05) and \@ref(coupled-06)), which we will return to starting in Chapter \@ref(linearsystems-15). But for the moment, let's pause and recognize that a key motivation for modeling with rates of change is to quantify observed phenomena. 

Oftentimes, we wish to compare model outputs to measured data. While that may seem straightforward, sometimes models have parameters (such as $k$ and $\beta$ for Equation \@ref(eq:flu-quarantine-06) in Chapter \@ref(coupled-06)). Parameter estimation is the process of determining model parameters from data.\index{parameter!estimation} Stated differently:

> **Parameter estimation** is the process that determines the set of parameters $\vec{\alpha}$ that minimize the difference between data $\vec{y}$ and the output of the function $f(\vec{x}, \vec{\alpha})$ and measured error $\vec{\sigma}$.

Over the next several chapters we will examine aspects of _parameter estimation_. Sometimes parameter estimation is synonymous with "fitting a model to data" and can also be called _data assimilation_ or _model-data fusion_.\index{data assimilation}\index{model!data fusion} We can address the parameter estimation problem from several different mathematical areas: _calculus_  (optimization), _statistics_ (likelihood functions), and _linear algebra_ (systems of linear equations). We will explore how we define "best" over several chapters, but let's first explore techniques of how this is done in `R` using simple linear regression.^[We will use `R` a lot in this chapter to make plots - so please visit Chapter \@ref(r-intro-02) if you need some reminders on plotting in R.] Let's get started!


## Parameter estimation for global temperature data
Let's take a look at a specific example. Table \@ref(tab:temp-table) shows anomalies in average global temperature since 1880, relative to 1951-1980 global temperatures.^[Data provided by NOAA: [https://climate.nasa.gov/vital-signs/global-temperature/](https://climate.nasa.gov/vital-signs/global-temperature/).] This dataset can be found in the `demodelr` package with the name `global_temperature`. To name our variables let $Y=\mbox{ Year since 1880 }$ and $T= \mbox{ Temperature anomaly}$.

```{r temp-table, echo=FALSE, results= 'asis'}
library(tidyverse)
library(demodelr)

kable(t(global_temperature[1:7, ]), caption = "First few years of average global temperature anomalies. The anomaly represents the global surface temperature relative to 1951-1980 average temperatures.")
```

We will be working with these data to fit a function $f(Y,\vec{\alpha})=T$. In order to fit a function in `R` we need three essential elements, distilled into a workflow of: Identify $\rightarrow$ Construct $\rightarrow$ Compute

- **Identify** data for the formula to estimate parameters. For this example we will use the tibble (or data frame) `global_temperature`.
- **Construct** the regression formula we will use for the fit. We want to do a linear regression so that $T = a+bY$. How we represent the regression formula in `R` is with `temperature_anomaly ~ 1 + year_since_1880`. Notice that this regression formula must include _named columns from your data_. Said differently, this regression formula "defines a linear regression where the factors are a constant term and one is proportional to the predictor variable."  It is helpful to assign this regression formula as a variable: `regression_formula <- temperature_anomaly ~ 1 + year_since_1880`. In Chapter \@ref(beyond-linear-08) we will discuss other types of regression formulas. 
- **Compute** the regression with the **command** `lm` (which stands for *l*inear *m*odel).


That's it!  So if we need to do a linear regression of global temperature against year since 1880, it can be done with the following code:

```{r}
regression_formula <- temperature_anomaly ~ 1 + year_since_1880

linear_fit <- lm(regression_formula, data = global_temperature)

summary(linear_fit)
```


What is printed on the console (and shown above) is the summary of the fit results. This summary contains several interesting things that you would study in advanced courses in statistics, but here is what we will focus on:

- The estimated **coefficients** (starting with `Coefficients:` above) of the linear regression. The column `Estimate` lists the constants in front of our regression formula $y=a+bx$. What follows is the statistical error for that estimate. The other additional columns concern statistical tests that show significance of the estimated parameters.
- One helpful thing to look at is the **Residual standard error** (starting with `Residual standard error` above), which represents the overall, total effect of the differences between the model predicted values of $\vec{y}$ and the measured values of $\vec{y}$. The goal of linear regression is to minimize this model-data difference.\index{residual!standard error}

The summary of the statistical fit is a verbose readout, which may prohibit quickly identifying the regression coefficients or plotting the fitted results. Thankfully the `R` package called `broom` can help us! The `broom` package produces model output in what is called "tidy" data format. You can read more about `broom` from its [documentation.](https://broom.tidymodels.org/index.html)

Since we are only going to use one or two functions from this package, I am going to refer to the functions I need with the syntax `PACKAGE_NAME::FUNCTION`. 

First we will make a data frame with the predicted coefficients from our linear model, as shown with the following code that you can run on your own:


```{r, eval=FALSE}
global_temperature_model <- 
  broom::augment(linear_fit, data = global_temperature)

glimpse(global_temperature_model)
```

```{r,echo=FALSE}
global_temperature_model <- 
  broom::augment(linear_fit, data = global_temperature)
```

Notice how the `augment` command takes the results from `linear_fit` with the data `global_temperature` to produce model estimated results (under the variable named `.fitted`.^[I like appending `_model` to the original name of the data frame to signify we are working with modeled components.]  There is a lot to unpack with this new data frame, but the important ones are the columns `year_since_1880` (the independent variable) and `.fitted`, which represents the fitted coefficients.

Finally, Figure \@ref(fig:fitted-global-temp-08) compares the data to the fitted regression line (also known as the "best fit line"). 

```{r, eval = FALSE}
ggplot(data = global_temperature) +
  geom_point(aes(x = year_since_1880, y = temperature_anomaly),
    color = "red",
    size = 2
  ) +
  geom_line(
    data = global_temperature_model,
    aes(x = year_since_1880, y = .fitted)
  ) +
  labs(
    x = "Year Since 1880",
    y =  "Temperature anomaly"
  )
```

```{r fitted-global-temp-08,fig.cap="Global temperature anomaly data along with the fitted linear regression line.",echo=FALSE}
ggplot(data = global_temperature) +
  geom_point(aes(x = year_since_1880, y = temperature_anomaly),
    color = "red",
    size = 1
  ) +
  geom_line(
    data = global_temperature_model,
    aes(x = year_since_1880, y = .fitted)
  ) +
  labs(
    x = "Year Since 1880",
    y = expression("Temperature anomaly "~`(`^o~C~`)`)
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 12),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 12)
  ) 
```


A word of caution: this example is one of an exploratory data analysis illustrating parameter estimation with `R`. Global temperature is a measurement of many different types of complex phenomena integrated from the local, regional, continental, and global levels (and interacting in both directions). Global temperature anomalies cannot be distilled down to a simple linear relationship between time and temperature. What Figure \@ref(fig:fitted-global-temp-08) *does* suggest though is that over time the average global temperature has increased. I encourage you to study the pages at [NOAA](https://climate.nasa.gov/) to learn more about the scientific consensus in modeling climate (and the associated complexities - it is a fascinating scientific problem that will need YOUR help to solve it!)

## Moving beyond linear models for parameter estimation {#beyond-linear-08}
We can also estimate parameters or fit additional polynomial models such as the equation
$y = a + bx + cx^{2} + dx^{3} ...$  (here the estimated parameters $a$, $b$, $c$, $d$, ...). There is a key distinction here: the regression formula is *nonlinear* in the predictor variable $x$, but *linear* with respect to the parameters. Incorporating these regression formulas in `R` modifies the structure of the regression formula. A few templates are show in Table \@ref(tab:reg-ex-08):

Table: (\#tab:reg-ex-08) Comparison of model equations to regression formulas used for `R`. The variable $y$ is the response variable and $x$ the predictor variable. Notice the structure `I(..)` is needed for `R` to signify a factor of the form $x^{n}$.

**Equation** | **Regression Formula**
-------------| -------------
    $y=a+bx$ | `y ~ 1 + x`
    $y=a$ |  `y ~ 1` 
    $y=bx$ | ` y ~ -1+x ` 
    $y=a+bx+cx^{2}$ |  ` y ~ 1 + x + I(x^2) `
    $y=a+bx+cx^{2}+dx^{3}$ |  `y~ 1 + x + I(x^2) + I(x^3) `




### Can you linearize your model?
We can estimate parameters for nonlinear models in cases where the function can be transformed mathematically to a linear equation. Here is one example: while the equation $y=ae^{bx}$ is nonlinear with respect to the parameters, it can be made linear by a *logarithmic transformation*  of the data:\index{logarithmic transformation}

\begin{equation}
\ln(y) = \ln(ae^{bx}) = \ln(a) + \ln (e^{bx}) = \ln(a) + bx
\end{equation}

The advantage to this approach is that the growth rate parameter $b$ is easily identifiable from the data, and the value of $a$ is found by exponentiation of the fitted intercept value. The disadvantage is that you need to do a log transform of the $y$ variable first before doing any fits.



```{example enzyme-08}
A common nonlinear equation in enzyme kinetics is the *Michaelis-Menten* law, which states that the rate of the uptake of a substrate $V$ is given by the equation:
  
\begin{equation}
V = \frac{V_{max} s}{s+K_{m}},
\end{equation}

where $s$ is the amount of substrate, $K_{m}$ is half-saturation constant, and $V_{max}$ the maximum reaction rate. (Typically $V$ is used to signify the "velocity" of the reaction.)

Consider you have the following data (from @keener_mathematical_2009):
  
*s* (mM) | 0.1 | 0.2 | 0.5 | 1.0 | 2.0 | 3.5 | 5.0
-------------| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | -------------
 *V* (mM / s) | 0.04 | 0.08 | 0.17 | 0.24 | 0.32 | 0.39 | 0.42

    
Apply parameter estimation techniques to estimate $K_{m}$ and $V_{max}$ and plot the resulting fitting curve with the data.
```

```{solution}
The first thing that we will need to do is to define a data frame (`tibble`) for these data:
```


```{r}
enzyme_data <- tibble(
  s = c(0.1, 0.2, 0.5, 1.0, 2.0, 3.5, 5.0),
  V = c(0.04, 0.08, 0.17, 0.24, 0.32, 0.39, 0.42)
)
```

Figure \@ref(fig:enzyme-data-08) shows a plot of $s$ and $V$:

```{r enzyme-data-08,fig.cap="Scatterplot of enzyme substrate data from Example \\@ref(exm:enzyme-08).",echo=FALSE}
p1<- ggplot(data = enzyme_data) +
  geom_point(aes(x = s, y = V),
             size = 2,
             color='red'
  ) +
  labs(
    x = "s (mM)",
    y = "V (mM / s)"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )

p1
```

Figure \@ref(fig:enzyme-data-08) definitely suggests a nonlinear relationship between $s$ and $V$. To dig a little deeper, try running the following code that plots the reciprocal of $s$ and the reciprocal of $V$ (do this on your own):

```{r,eval=FALSE}
ggplot(data = enzyme_data) +
  geom_point(aes(x = 1 / s, y = 1 / V),
    color = "red",
    size = 2
  ) +
  labs(
    x = "1/s (1/mM)",
    y = "1/V (s / mM)"
  )
```

Notice how easy it was to plot the reciprocals of $s$ and $V$ inside the `ggplot` command. Here's how to see this with the provided equation (and a little bit of algebra):

\begin{equation}
V = \frac{V_{max} s}{s+K_{m}} \rightarrow \frac{1}{V} = \frac{s+K_{m}}{V_{max} s} = \frac{1}{V_{max}} + \frac{1}{s} \frac{K_{m}}{V_{max}}
\end{equation}


In order to do a linear fit to the transformed data we will use the regression formulas defined above and the handy structure `I(VARIABLE)` and plot the transformed data with the fitted equation (do this on your own as well):^[The process outlined here forms a *Lineweaver-Burk* plot.]


```{r,eval=FALSE}

# Define the regression formula
enzyme_formula <- I(1 / V) ~ 1 + I(1 / s)

# Apply the linear fit
enzyme_fit <- lm(enzyme_formula,data = enzyme_data)

# Show best fit parameters
summary(enzyme_fit)

# Added fitted data to the model
enzyme_data_model <- broom::augment(enzyme_fit, data = enzyme_data)

# Compare fitted model to the data
ggplot(data = enzyme_data) +
  geom_point(aes(x = 1 / s, y = 1 / V),
    color = "red",
    size = 2
  ) +
  geom_line(
    data = enzyme_data_model,
    aes(x = 1 / s, y = .fitted)
  ) +
  labs(
    x = "1/s (1/mM)",
    y = "1/V (s / mM)"
  )
```

In Exercise \@ref(exr:enzyme-fit-08) you will use the coefficients from your linear fit to determine $V_{max}$ and $K_{m}$. When plotting the fitted model values with the original data (Figure \@ref(fig:enzyme-fitted-normal-08)), we need to take the reciprocal of the column `.fitted` when we apply `augment` because the response variable in the linear model is $1 / V$ (confusing, I know!). For convenience, the code that does the all the fitting is shown below:^[You may notice that in Figure \@ref(fig:enzyme-fitted-normal-08) the fitted curve seems to look like a piecewise linear function. This is mainly due to the distribution of data - if you have several gaps between measurements, the fitted curve looks smoother.]

```{r,eval=FALSE}
# Define the regression formula
enzyme_formula <- I(1 / V) ~ 1 + I(1 / s)

# Apply the linear fit
enzyme_fit <- lm(enzyme_formula,data = enzyme_data)

# Added fitted data to the model
enzyme_data_model <- broom::augment(enzyme_fit, data = enzyme_data)


ggplot(data = enzyme_data) +
  geom_point(aes(x = s, y = V),
    color = "red",
    size = 2
  ) +
  geom_line(
    data = enzyme_data_model,
    aes(x = s, y = 1 / .fitted)
  ) +
  labs(
    x = "s (mM)",
    y = "V (mM / s)"
  )
```


```{r enzyme-fitted-normal-08,fig.cap="Scatterplot of enzyme substrate data from Example \\@ref(exm:enzyme-08) along with the fitted curve.",echo=FALSE}
# Define the regression formula
enzyme_formula <- I(1 / V) ~ 1 + I(1 / s)

# Apply the linear fit
enzyme_fit <- lm(enzyme_formula,data = enzyme_data)

# Added fitted data to the model
enzyme_data_model <- broom::augment(enzyme_fit, data = enzyme_data)


p1 +
  geom_line(
    data = enzyme_data_model,
    aes(x = s, y = 1 / .fitted)
  ) +
  labs(
    x = "s (mM)",
    y = "V (mM / s)"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) 
```


## Parameter estimation with nonlinear models
In many cases you will not be able to write your model in a linear form by applying functional transformations. Here's the good news: you can still do a non-linear curve fit using the function `nls`, which is similar to the command `lm` with some additional information. Let's return to an example from Chapter \@ref(r-intro-02) where we examined the weight of a dog named Wilson (Figure \@ref(fig:wilson-weight-02)). One model for the dog's weight $W$ from the days since birth $D$ is a saturating exponential equation (Equation \@ref(eq:wilson-08)):


\begin{equation}
W =f(D,a,b,c)= a - be^{-ct}, (\#eq:wilson-08)
\end{equation}

where we have the parameters $a$, $b$, and $c$. We can apply the command `nls` (nonlinear least squares) to estimate these parameters. Because `nls` is an iterative numerical method it needs a starting value for these parameters (here we set $a = 75$, $b=30$, and $c=0.01$). Determining a starting point can be tricky - it does take some trial and error.


```{r}
# Define the regression formula
wilson_formula <- weight ~ a - b * exp(-c * days)

# Apply the nonlinear fit
nonlinear_fit <- nls(formula = wilson_formula,
  data = wilson,
  start = list(a = 75, b = 30, c = 0.01)
)

# Summarize the fit parameters
summary(nonlinear_fit)
```

Similar as before, we can `augment` the data to display the fitted curve. 

However once you have your fitted model, you can still plot the fitted values with the coefficients (try this out on your own).

```{r,eval=FALSE}

# Augment the model
wilson_model <- broom::augment(nonlinear_fit, data = wilson)

# Plot the data with the model
ggplot(data = wilson) +
  geom_point(aes(x = days, y = weight),
    color = "red",
    size = 2
  ) +
  geom_line(
    data = wilson_model,
    aes(x = days, y = .fitted)
  ) +
  labs(
    x = "Days since birth",
    y = "Weight (pounds)"
  )
```

## Towards model-data fusion
The `R` language (and associated packages) has many excellent tools for parameter estimation and comparing fitted models to data. These tools are handy for first steps in parameter estimation.

More broadly the technique of estimating models from data can also be called *data assimilation* or *model-data fusion*. Whatever terminology you happen to use, you are combining the best of both worlds: combining observed measurements with what you expect *should* happen, given the understanding of the system at hand.

We are going to dig into data assimilation even more - and one key tool is understanding likelihood functions, which we will study in the next chapter.

 
## Exercises

```{exercise}
Determine if the following equations are linear with respect to the parameters. Assume that $y$ is the response variable and $x$ the predictor variable.


a. $y=a + bx+cx^{2}+dx^{3}$
b. $y=a \sin (x) + b \cos (x)$
c. $y = a \sin(bx) + c \cos(dx)$
d. $y = a + bx + a\cdot b x^{2}$
e. $y = a e^{-x} + b e^{x}$
f. $y = a e^{-bx} + c e^{-dx}$


```


<!-- Idea taken from https://stattrek.com/regression/linear-transformation.aspx -->
```{exercise}
Each of the following equations can be written as linear with respect to the parameters, through applying some elementary transformations to the data. Write each equation as a linear function with respect to the parameters. Assume that $y$ is the response variable and $x$ the predictor variable.


a. $y=ae^{-bx}$
b. $y=(a+bx)^{2}$
c. $\displaystyle y =  \frac{1}{a+bx}$
d. $y = c x^{n}$


```



```{exercise}
Use the dataset `global_temperature` and the function `lm` to answer the following questions:


a. Complete the following table, which represents various regression fits to global temperature anomaly $T$ (in degrees Celsius) and years since 1880 (denoted by $Y$). In the table **Coefficients** represent the values of the parameters $a$, $b$, $c$, etc. from your fitted equation; **P** =  number of parameters; **RSE** = Residual standard error.


**Equation** | **Coefficients** | **P** | **RSE**
------------- | ------------- | ------------- | -------------
     $T=a+bY$ | | | 
    $T=a+bY+cY^{2}$ | | | 
     $T=a+bY+cY^{2}+dY^{3}$ | | | 
     $T=a+bY+cY^{2}+dY^{3}+eY^{4}$ | | | 
   $T=a+bY+cY^{2}+dY^{3}+eY^{4}+fY^{5}$ | | | 
      $T=a+bY+cY^{2}+dY^{3}+eY^{4}+fY^{5}+gY^{6}$ | | | 

b. After making this table, choose the polynomial of the function that you believe fits the data best. Provide reasoning and explanation why you chose the polynomial that you did.
c. Finally show the plot of your selected polynomial with the data.


```


```{exercise log-linear-08}
An equation that relates a consumer's nutrient content (denoted as $y$) to the nutrient content of food (denoted as $x$) is given by: $\displaystyle y = c x^{1/\theta},$ where $\theta \geq 1$ and $c>0$ are both constants.


a. Use the dataset `phosphorous` to make a scatterplot with `algae` as the predictor (independent) variable and `daphnia` the response (dependent) variable.
b. Show that you can linearize the equation $\displaystyle y = c x^{1/\theta}$ with logarithms.
c. Determine a linear regression fit for your new linear equation.
d. Determine the value of $c$ and $\theta$ in the original equation with the parameters from the linear fit.



```



```{exercise}
Similar to Exercise \@ref(exr:log-linear-08), do a non-linear least squares fit for the dataset `phosphorous` to the equation $\displaystyle y = c x^{1/\theta}$. For a starting point, you may use the values of $c$ and $\theta$ from Exercise \@ref(exr:log-linear-08). Then make a plot of the original `phosphorous` data with the fitted model results.
```




 <!-- Keener vol1 pg 44 and pg 11 -->
```{exercise enzyme-fit-08}
Example \@ref(exm:enzyme-08) guided you through the process to linearize the following equation: 
  
  \begin{equation}
V = \frac{V_{max} s}{s+K_{m}},
\end{equation}

where $s$ is the amount of substrate, $K_{m}$ is half-saturation constant, and $V_{max}$ the maximum reaction rate. (Typically $V$ is used to signify the "velocity" of the reaction.) When doing a fit of the reciprocal of $s$ with the reciprocal of $V$, what are the resulting values of $V_{max}$ and $K_{m}$?


```



```{exercise}
Following from Example \@ref(exm:enzyme-08) and Exercise \@ref(exr:enzyme-fit-08), apply the command `nls` to conduct a nonlinear least squares fit of the enzyme data to the equation:
  
  \begin{equation}
V = \frac{V_{max} s}{s+K_{m}},
\end{equation}

where $s$ is the amount of substrate, $K_{m}$ is the half-saturation constant, and $V_{max}$ the maximum reaction rate. As starting points for the nonlinear least squares fit, you may use the values of $K_{m}$ and $V_{max}$ that were determined from Example \@ref(exm:enzyme-08). Then make a plot of the actual data with the fitted model curve.


```

 
  <!-- See rScripts file for code -->
```{exercise temperature-08}

Consider the following data which represent the temperature over the course of a day:

 **Hour** | **Temperature** 
|:------:|:-----:|
 0 | 54 |
 1 | 53 |
 2 | 55 |
 3 | 54 |
 4 | 58 |
 5 | 58 |
 6 | 61 |
 7 | 63 | 
 8 | 67 | 
 9 | 66 |
 10 | 67 |
 11 | 69 |
 12 | 68 | 
 13 | 68 | 
 14 | 66 |
 15 | 67 |
 16 | 63 |
 17 | 60 |
 18 | 59 |
 19 | 57 |
 20 | 56 |
 21 | 53 |
 22 | 52 |
 23 | 54 |
 24 | 53 |



a. Make a scatterplot of these data, with the variable \textbf{Hour} on the horizontal axis.
b. A function that describes these data is $\displaystyle T = A + B \sin \left( \frac{\pi}{12} \cdot H \right) + C \cos \left( \frac{\pi}{12} \cdot H \right)$, where $H$ is the hour and $T$ is the temperature. Explain why this equation is linear for the parameters $A$, $B$, and $C$.
c. Define a `tibble` that includes the variables $T$, $\displaystyle \sin \left( \frac{\pi}{12} \cdot H \right)$, $\displaystyle \cos \left( \frac{\pi}{12} \cdot H \right)$.
d. Do a linear fit on your new data frame to report the values of $A$, $B$, and $C$.
e. Define a new `tibble` that has a sequence in $H$ starting at 0 from 24 with at least 100 data points, and a value of $T$ (`T_fitted`) using your coefficients of $A$, $B$, and $C$.
f. Add your fitted curve to the scatterplot. How do your fitted values compare to the data?

```

```{exercise}
Use the data from Exercise \@ref(exr:temperature-08) to conduct a nonlinear fit (use the function `nls`) to the equation $\displaystyle T = A + B \sin \left( \frac{\pi}{12} \cdot H \right) + C \cos \left( \frac{\pi}{12} \cdot H \right)$. Good starting points are  $A=50$, $B=1$, and $C=-10$.
```

<!--chapter:end:08-linearRegression.Rmd-->

# Probability and Likelihood Functions {#likelihood-09}

In Chapter \@ref(linear-regression-08) we began to the process of parameter estimation. We revisit parameter estimation here by applying _likelihood functions_, which is a topic from probability and statistics.\index{likelihood!function} Probability is the association of a set of observable events to a quantitative scale between 0 and 1. Informally, a value of zero means that event is not possible; 1 means that it definitely can happen.^[See @devore_modern_2021 for a more refined definition of probability.] We will only consider continuous events with the range of parameter estimation problems examined here. 

This chapter will introduce likelihood functions but also discuss some interesting visualization techniques of multivariable functions and contour plots. As with Chapter \@ref(linear-regression-08) we are starting to build out some `R` skills and techniques that you can apply in other contexts. Let's get started!


## Linear regression on a small dataset
Table \@ref(tab:limited-data-09) displays a dataset with a limited number of points where we wish to fit the function $y=bx$:

Table: (\#tab:limited-data-09) A small, limited dataset.

| *x* | 1 | 2 | 4 | 4 | 
|:------:|:-----:|:-----:|:-----:|:-----:|
| *y* | 3 | 5 | 4 | 10 |



For this example we are forcing the intercept term to equal zero - for most cases you will just fit the linear equation (see Exercise \@ref(exr:full-linear) where you will consider the intercept $a$). Figure \@ref(fig:quick-scatter-09) displays a quick scatterplot of these data:

```{r quick-scatter-09,warning=FALSE,message=FALSE,echo=FALSE,fig.cap="A scatterplot of a small, limited dataset (Table \\@ref(tab:limited-data-09))."}
data.frame(x = c(1, 2, 4, 6), y = c(3, 5, 4, 10)) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2, color = "red") +
  xlim(c(0, 10)) +
  ylim(c(0, 15)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) 
```

The goal here is to work to determine the value of $b$ that is most *likely* - or consistent - with the data. However, before we tackle this further we need to understand how to quantify most _likely_ in a mathematical sense. In order to do this, we need to take a quick excursion into continuous probability distributions.

## Continuous probability density functions
Consider Figure \@ref(fig:normal-shaded-09), which may be familiar to you as the normal distribution or the bell curve:

```{r normal-shaded-09,fig.cap='The standard normal distribution, with a shaded area between $x=\\pm 1$',echo=FALSE}
ggplot() +
  geom_area(
    data = data.frame(x_val = seq(-5, 5, length = 200),
                      y_val = dnorm(seq(-5, 5, length = 200))),
    aes(x = x_val, y = y_val), alpha = .6
  ) +
  geom_area(
    data = data.frame(x_val = seq(-1, 1, length = 20),
                      y_val = dnorm(seq(-1, 1, length = 20))),
    aes(x = x_val, y = y_val), alpha = .8
  ) +
  xlab("x") +
  ylab("f(x)") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_fill_colorblind()
```


We tend to think of the plot and the associated function $f(x)$ as something with input and output (such as $f(0)=$ `r round(dnorm(0),digits=4)`). However because it is a probability density function, the *area* between two points yields the probability of an event to fall within two values as shown in Figure \@ref(fig:normal-shaded-09).


In this case, the numerical value of the shaded area would represent the probability that our measurement is in the interval $-1 \leq x \leq 1$. The value of the area, or the probability, is `r round(pnorm(1)-pnorm(-1),digits=5)`. Perhaps when you studied calculus the area was expressed as a definite integral:  $\displaystyle \int_{-1}^{1} f(x) \; dx=$ `r round(pnorm(1)-pnorm(-1),digits=5)`, where $f(x)$ is the formula for the probability density function for the normal distribution. Here, the formula for the normal distribution $f(x)$ is given by Equation \@ref(eq:normal-09), where $\mu$ is the mean and $\sigma$ is the standard deviation.^[Were you ever asked to find the antiderivative of $\displaystyle \int e^{-x^{2}} \; dx$? You may recall that there is easy antiderivative - and to find values of definite integrals you need to approximate numerically. Thankfully `R` does that work using sophisticated numerical integration techniques!]

\begin{equation}
f(x)=\frac{1}{\sqrt{2 \pi} \sigma } e^{-(x-\mu)^{2}/(2 \sigma^{2})} (\#eq:normal-09)
\end{equation}



With this intuition we can summarize key facts about probability density functions:

- $f(x) \geq 0$ (this means that probability density functions are positive values)
- Area integrates to one (in probability, this means we have accounted for all of our outcomes)


## Connecting probabilities to linear regression
Now that we have made that small excursion into probability, let's return to the parameter estimation problem. Another way to phrase this problem is to examine the probability distribution of the model-data residual\index{residual!model-data} for each measurement $\epsilon_{i}$ (Equation \@ref(eq:model-data-resid-09)):

\begin{equation}
\epsilon_{i} = y_{i} - f(x_{i},\vec{\alpha} ). (\#eq:model-data-resid-09)
\end{equation}

The approach with likelihood functions assumes a particular probability distribution on each residual. One common assumption is that the model-data residual is normally distributed. In most applications the mean of this distribution is zero ($\mu=0$) and the standard deviation $\sigma$ (which could be specified as measurement error, etc.). We formalize this assumption with a likelihood function $L$ in Equation \@ref(eq:likelihood-resid-09).

\begin{equation}
L(\epsilon_{i}) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\epsilon_{i}^{2} / 2 \sigma^{2} } (\#eq:likelihood-resid-09)
\end{equation}

To extend this further across all measurements, we use the idea of *independent, identically distributed* measurements so the joint likelihood of **all** the residuals (each $\epsilon_{i}$) is the product of the individual likelihoods (Equation \@ref(eq:likelihood-prod-09). The assumption of independent, identically distributed is a common one. As a note of caution you should always evaluate if this is a valid assumption for more advanced applications.

\begin{equation}
L(\vec{\epsilon}) = \prod_{i=1}^{N} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\epsilon_{i}^{2} / 2 \sigma^{2} } (\#eq:likelihood-prod-09)
\end{equation}



We are making progress here; however to fully characterize the solution we need to specify the parameters $\vec{\alpha}$. A simple redefining of the likelihood function where we specify the measurements ($x$ and $y$) and parameters ($\vec{\alpha}$) is all we need (Equation \@ref(eq:likelihood-prod-param-09)).

\begin{equation}
L(\vec{\alpha} | \vec{x},\vec{y} )= \prod_{i=1}^{N}  \frac{1}{\sqrt{2 \pi} \sigma} \exp(-(y_{i} - f(x_{i},\vec{\alpha} ))^{2} / 2 \sigma^{2} )  (\#eq:likelihood-prod-param-09)
\end{equation}

Now with Equation \@ref(eq:likelihood-prod-param-09) we have a function where the best parameter estimate is the one that optimizes the likelihood.

Returning to our original linear regression problem (Table \@ref(tab:limited-data-09) and Figure \@ref(fig:quick-scatter-09)), we want to determine the $b$ for the function $y=bx$. Equation \@ref(eq:small-data-likely) then characterizes the likelihood of $b$, given the data $\vec{x}$ and $\vec{y}$:

\begin{equation}
L(b | \vec{x},\vec{y} ) = \left( \frac{1}{\sqrt{2 \pi} \sigma}\right)^{4} e^{-\frac{(3-b)^{2}}{2\sigma^{2}}} \cdot e^{-\frac{(5-2b)^{2}}{2\sigma^{2}}}  \cdot e^{-\frac{(4-4b)^{2}}{2\sigma^{2}}}  \cdot e^{-\frac{(10-4b)^{2}}{2\sigma^{2}}} (\#eq:small-data-likely)
\end{equation}

For the purposes of our argument here, we will assume $\sigma=1$. Figure \@ref(fig:small-likelihood-plot) shows a plot of the likelihood function $L(b | \vec{x},\vec{y} )$.

```{r small-likelihood-plot,warning=FALSE,message=FALSE,echo=FALSE,fig.cap='The likelihood function (Equation \\@ref(eq:small-data-likely)) for the small dataset (Table \\@ref(tab:limited-data-09)), with the value of the maximum likelihood at $b=1.865$ marked with a vertical line.'}
b <- seq(0, 5, 0.01)
new_data <- data.frame(x = c(1, 2, 4, 4), y = c(3, 5, 4, 10))
lb <- map(.x = b, .f = ~ (1 / (2 * pi)^length(new_data$y)) * exp(-sum((new_data$y - .x * new_data$x)^2) / 4)) %>% as.numeric()

data.frame(b, lb) %>%
  ggplot(aes(x = b, y = lb)) +
  geom_line() +
  geom_vline(xintercept = b[which.max(lb)], color = "red") +
  ylab("L(b|x,y)") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) 
```


Note that in Figure \@ref(fig:small-likelihood-plot) the values of $L(b | \vec{x},\vec{y} )$ on the vertical axis are really small. (This typically may be the case; see Exercise \@ref(exr:show-small-likely).) An alternative to the small numbers in $L(b)$ is to use the log-likelihood (Equation \@ref(eq:loglikely)):

\begin{equation}
\begin{split}
\ln(L(\vec{\alpha} | \vec{x},\vec{y} )) &=  N \ln \left( \frac{1}{\sqrt{2 \pi} \sigma} \right) - \sum_{i=1}^{N} \frac{ (y_{i} - f(x_{i},\vec{\alpha} )^{2}}{ 2 \sigma^{2}} \\
 & = - \frac{N}{2} \ln (2) - \frac{N}{2} \ln(\pi) - N \ln( \sigma) - \sum_{i=1}^{N} \frac{ (y_{i} - f(x_{i},\vec{\alpha} )^{2}}{ 2 \sigma^{2}}
\end{split} (\#eq:loglikely)
\end{equation}

In Exercise \@ref(exr:small-data-optimize) you will be working on how to transform the likelihood function $L(b)$ to the log-likelihood $\ln(L(b))$ and showing that Equation \@ref(eq:small-data-likely) is maximized at $b=1.865$. The data with the fitted line is shown in Figure \@ref(fig:quick-scatter-fit-09).

```{r quick-scatter-fit-09,warning=FALSE,message=FALSE,echo=FALSE,fig.cap="A scatterplot of a small dataset (Table \\@ref(tab:limited-data-09)) with fitted line $y=1.865x$ from optimizing Equation \\@ref(eq:small-data-likely)."}
data.frame(x = c(1, 2, 4, 6), y = c(3, 5, 4, 10)) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2, color = "red") +
  xlim(c(0, 10)) +
  ylim(c(0, 15)) +
  geom_abline(slope=1.865) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) 
```



## Visualizing likelihood surfaces
Next we are going to examine a second example from @gause_experimental_1932 which modeled the growing of yeast in solution. This classic paper examines the biological principal of *competitive exclusion*, how one species can out-compete another one for resources. Some of the data from @gause_experimental_1932 is encoded in the data frame `yeast` in the `demodelr` package. For this example we are going to examine a model for one species growing without competition. Figure \@ref(fig:yeast-quick-09) shows a scatterplot of the `yeast` data.

```{r eval = FALSE}
### Make a quick ggplot of the data

ggplot() +
  geom_point(
    data = yeast,
    aes(x = time, y = volume),
    color = "red",
    size = 2
  ) +
  labs(x = "Time", y = "Volume")
```

```{r yeast-quick-09, fig.cap="Scatterplot of *Sacchromyces* volume growing by itself in a container.",echo=FALSE}
### Make a quick ggplot of the data

p_gause <- ggplot() +
  geom_point(
    data = yeast,
    aes(x = time, y = volume),
    color = "red",
    size = 2
  ) +
  labs(x = "Time", y = "Volume") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) 

p_gause
```


We are going to assume the population of yeast (represented with the measurement of volume) over time changes according to the differential equation $\displaystyle \frac{dy}{dt} = -by \frac{(K-y)}{K}$, where $y$ is the population of the yeast, and $b$ represents the growth rate, and $K$ is the carrying capacity of the population. 

Equation \@ref(eq:gause-09) shows the solution to this differential equation, where the additional parameter $a$ can be found through application of the initial condition $y_{0}$. 

\begin{equation}
y = \frac{K}{1+e^{a-bt}} (\#eq:gause-09)
\end{equation}

In @gause_experimental_1932 the value of $a$ was determined by solving the initial value problem $y(0)=0.45$. In Exercise \@ref(exr:solve-gause) you will show that $\displaystyle a = \ln \left( \frac{K}{0.45} - 1 \right)$. Equation \@ref(eq:gause-09) then has two parameters: $K$ and $b$. Here we are going to explore the likelihood function to try to determine the best set of values for the two parameters $K$ and $b$ using a function in the `demodelr` package called `compute_likelihood`. Inputs to the `compute_likelihood` function are the following:

- A function $y=f(x,\vec{\alpha})$
- A dataset $(\vec{x},\vec{y})$
- Ranges of your parameters $\vec{\alpha}$. 

The `compute_likelihood` function also has an optional input `logLikely` that allows you to specify if you want to compute the likelihood or the log-likelihood. The default is that `logLikely` is `FALSE`, meaning that the normal likelihoods are plotted.\index{likelihood!surface}


First we will define the equation used to compute our model in the likelihood. As with the functions `euler` or `systems` in Chapter \@ref(euler-04) we need to define this function:
```{r}
library(demodelr)

# Define the solution to the differential equation with
# parameters K and bGause model equation
gause_model <- volume ~ K / (1 + exp(log(K / 0.45 - 1) - b * time))


# Identify the ranges of the parameters that we wish to investigate
kParam <- seq(5, 20, length.out = 100)
bParam <- seq(0, 1, length.out = 100)


# Allow for all the possible combinations of parameters
gause_parameters <- expand.grid(K = kParam, b = bParam)

# Now compute the likelihood
gause_likelihood <- compute_likelihood(
  model = gause_model,
  data = yeast,
  parameters = gause_parameters,
  logLikely = FALSE
  )
```

Ok, let's break this code down step by step:

- The line `gause_model <- volume ~ K/(1+exp(log(k/0.45-1)-b*time))` identifies the formula that relates the variables `time` to `volume` in the dataset `yeast`.
- We define the ranges (minimum and maximum values) for our parameters by defining a sequence. Because we want to look at *all possible combinations* of these parameters we use the command `expand.grid`.
- The input `logLikely = FALSE` to `compute_likelihood` reports back likelihood values.

Some care is needed in defining the number of points (`length.out = 100`) in the sequence that we want to evaluate - we will have $100^{2}$ different combinations of $K$ and $b$ on our grid, which does take time to evaluate.


The output to `compute_likelihood` is a list, which is a flexible data structure in `R`. You can think of this as a collection of items - which could be data frames of different sizes. In this case, what gets returned are two data frames: `likelihood`, which is a data frame of likelihood values for each of the parameters and `opt_value`, which reports back the values of the parameters that optimize the likelihood function. Note that the optimum value is *an approximation*, as it is just the optimum from the input values of $K$ and $b$ provided on our grid. Let's take a look at the reported optimum values, which we can do with the syntax `LIST_NAME$VARIABLE_NAME`, where the dollar sign ($) helps identify which variable from the list you are investigating.

```{r, fig.show='hold'}
gause_likelihood$opt_value
```

It is also important to visualize this likelihood function. For this dataset we have the two parameters $K$ and $b$, so the likelihood function will be a *likelihood surface*, rather than a two-dimensional plot. To visualize this in `R` we can use a contour diagram. Figure \@ref(fig:gause-likely-plot) displays this countour plot.

```{r gause-likely-plot,fig.cap="Likelihood surface and contour lines for the `yeast` dataset."}

# Define the likelihood values
my_likelihood <- gause_likelihood$likelihood

# Make a contour plot
ggplot(data = my_likelihood) +
  geom_tile(aes(x = K, y = b, fill = l_hood)) +
  stat_contour(aes(x = K, y = b, z = l_hood))
```

Similar to before, let's take this step by step:

- The command `my_likelihood` just puts the likelihood values in a data frame.
- The `ggplot` command is similar to that used before.
- We use `geom_tile` to visualize the likelihood surface. There are three required inputs from the `my_likelihood` data frame: the `x` and `y` axis data values and the `fill` value, which represents the height of the likelihood function.
- The command  `stat_contour` draws the contour lines, or places where the likelihood function has the same value. Notice how we used `z = l_hood` rather than `fill` here. This function helps "smooth" out any jaggedness in the contours.

In Figure \@ref(fig:gause-likely-plot) there appears to be a large region where the likelihood has the same value. (Admittedly I chose some broad parameter ranges for $K$ and $b$). We can refine that by producing a second contour plot that focuses in on parameters closer to the calculated optimum value at $K=13$ and $b=0.07$ (Figure \@ref(fig:revised-gause-likelihood)):

```{r revised-gause-likelihood,fig.cap="Zoomed in likelihood surface. for the `yeast` dataset. The computed location of the optimum value is shown as a red point.",echo=FALSE}

# Gause model equation
gause_model <- volume ~ K / (1 + exp(log(K / 0.45 - 1) - b * time))


# Identify the (new) ranges of the parameters that we wish to investigate
kParam <- seq(11, 14, length.out = 100)
bParam <- seq(0.1, 0.3, length.out = 100)


# Allow for all the possible combinations of parameters
gause_parameters_rev <- expand.grid(K = kParam, b = bParam)


gause_likelihood_rev <- compute_likelihood(
  model = gause_model,
  data = yeast,
  parameters = gause_parameters_rev,
  logLikely = FALSE
  )

# Report out the optimum values
opt_value_rev <- gause_likelihood_rev$opt_value

opt_value_rev


# Define the likelihood values
my_likelihood_rev <- gause_likelihood_rev$likelihood

# Make a contour plot
ggplot(data = my_likelihood_rev) +
  geom_tile(aes(x = K, y = b, fill = l_hood)) +
  stat_contour(aes(x = K, y = b, z = l_hood)) +
  geom_point(data = opt_value_rev, aes(x = K, y = b), color = "red")
```

The reported values for $K$ (12.8) and $b$ (0.241) may be close to what was reported from Figure \@ref(fig:gause-likely-plot). Notice that in Figure \@ref(fig:revised-gause-likelihood) I also added in the location of the optimum point with `geom_point()`.


As a final step, once you have settled on the value that optimizes the likelihood function, is to compare the optimized parameters against the data (Figure \@ref(fig:gause-model-data-09)):


```{r eval = FALSE}

# Define the parameters and the times to evaluate:
my_params <- gause_likelihood_rev$opt_value
time <- seq(0, 60, length.out = 100)

# Get the right hand side of your equations
new_eq <- gause_model %>%
  formula.tools::rhs()

# This collects the parameters and data into a list
in_list <- c(my_params, time) %>% as.list()

# The eval command evaluates your model
out_model <- eval(new_eq, envir = in_list)


# Now collect everything into a data frame:
my_prediction <- tibble(time = time, volume = out_model)


ggplot() +
  geom_point(
    data = yeast,
    aes(x = time, y = volume),
    color = "red",
    size = 2
  ) +
  geom_line(
    data = my_prediction,
    aes(x = time, y = volume)
  ) +
  labs(x = "Time", y = "Volume")
```

```{r gause-model-data-09,fig.cap="Model and data comparison of the `yeast` dataset from maximum likelihood estimation.",echo = FALSE}

# Define the parameters and the times to evaluate:
my_params <- gause_likelihood_rev$opt_value
time <- seq(0, 60, length.out = 100)

# Get the right hand side of your equations
new_eq <- gause_model %>%
  formula.tools::rhs()

# This collects the parameters and data into a list
in_list <- c(my_params, time) %>% as.list()

# The eval command evaluates your model
out_model <- eval(new_eq, envir = in_list)


# Now collect everything into a data frame:
my_prediction <- tibble(time = time, volume = out_model)

p_gause +
  geom_line(
    data = my_prediction,
    aes(x = time, y = volume)
  ) +
  labs(x = "Time", y = "Volume")
```


All right, this code block has some new commands and techniques that need explaining. Once we have the parameter estimates we need to compute the modeled values.

- First we define the `params` and the `time` we wish to evaluate with our model.
- We need to evaluate the right hand side of $\displaystyle y = \frac{K}{1+e^{a+bt}}$, so the definition of `new_eq` helps to do that, using the package `formula.tools`.
- The `%>%` is the `tidyverse` [pipe](https://r4ds.had.co.nz/pipes.html#pipes). This is a very useful command to help make code more readable!
- `in_list <- c(params,my_time) %>% as.list()` collects the parameters and input times in one list to evaluate the model with `out_model <- eval(new_eq,envir=in_list)`
- We make a data frame called `my_prediction` so we can then plot.

And the rest of the plotting commands you should be used to. This activity focused on the likelihood function - Exercise \@ref(exr:log-likelihood-yeast) has you repeat this analysis with the log-likelihood function.

## Looking back and forward
This chapter covered a lot of ground - from probability and likelihood functions to computing and visualizing these. A good strategy for a likelihood function is to visualize the function and then explore to find values that optimize the likelihood or log-likelihood function. This approach is one example of *successive approximations* or using an iterative method to determine a solution.\index{successive approximations} While this chapter focused on optimizing a likelihood function with one or two parameters, the successive approximation method does generalize to more parameters. However searching for parameters becomes tricky (read: tedious and slow) in high-dimensional spaces. In later chapters we will explore numerical methods to accelerate convergence to an optimum value. 

## Exercises

```{exercise solve-gause}
Algebraically solve the equation $\displaystyle 0.45 = \frac{K}{1+e^{a}}$ for $a$.
```



```{exercise show-small-likely}
Compute the values of $L(b|\vec{x},\vec{y})$ and $\ln(L(b|\vec{x},\vec{y}))$ for each of the data points in Equation \@ref(eq:small-data-likely) when $b=1.865$ and $\sigma=1$. (This means that these 4 values would be multiplied or added when you compute the full likelihood function.) Explain if the likelihood or log-likelihood would be easier to calculate in instances when the number of observations is large. 
```

```{exercise log-likelihood-yeast}
Visualize the likelihood function for the `yeast` dataset, but in this case report out and visualize the log-likelihood. (This means that you are setting the option `logLikely = TRUE` in the `compute_likelihood` function.)  Compare the log-likelihood surface to Figure \@ref(fig:revised-gause-likelihood).
```


```{exercise small-data-invest}
When we generated our plot of the likelihood function in Figure \@ref(fig:small-likelihood-plot) we assumed that $\sigma=1$ in Equation (9.6). For this exercise you will explore what happens in Equation (9.6) as $\sigma$ increases or decreases.


a. Use desmos (\url{www.desmos.com/calculator}) or `R` to generate a plot of Equation \@ref(eq:small-data-likely). What happens to the shape of the likelihood function as $\sigma$ increases?
b. How does the estimate of $b$ change as $\sigma$ changes?
c. The spread of the distribution (in terms of it being more peaked or less peaked) is a measure of uncertainty of a parameter estimate. How does the resulting parameter uncertainty change as $\sigma$ changes?

```

```{exercise small-data-optimize}
Using Equation \@ref(eq:small-data-likely) with $\sigma = 1$:


a. Apply the natural logarithm to both sides of this expression. Using properties of logarithms, show that the log-likelihood function is $\displaystyle \ln(L(b)) =-2 \ln(2) - 2 \ln (\pi) -\frac{(3-b)^{2}}{2}-\frac{(5-2b)^{2}}{2}-\frac{(4-4b)^{2}}{2}-\frac{(10-4b)^{2}}{2}$.

b. Make a plot of the log-likelihood function (in desmos or `R`).

c. At what values of $b$ Where is this function optimized? Does your graph indicate that it is a maximum or a minimum value?
  
d. Compare this likelihood estimate for $b$ to what was found in Figure \@ref(fig:small-likelihood-plot). 

```


```{exercise full-linear}
Consider the linear model $y=a+bx$ for the following dataset:


| *x* | *y*  | 
|:------:|:-----:|
| 1 | 3 |
| 2 | 5 |
| 4 | 4 |
| 4 | 10 |


a. With the function `compute_likelihood`, generate a contour plot of both the likelihood and log-likelihood functions. You may assume $0 \leq a \leq 5$ and $0 \leq b \leq 5$.
b. Make a scatterplot of these data with the equation $y=a+bx$ with your maximum likelihood parameter estimates.
c. Earlier when we fit $y=bx$ we found $b=1.865$. How does adding $a$ as a model parameter affect your estimate of $b$?

```


```{exercise}
For the function $\displaystyle P(t)=\frac{K}{1+e^{a+bt}}$, with $P(0)=P_{0}$, determine an expression for the parameter $a$ in terms of $K$, $b$, and $P_{0}$.
```

```{exercise}
The values returned by the maximum likelihood estimate for Equation \@ref(eq:gause-09) were a little different from those reported in @gause_experimental_1932:


**Parameter** | **Maximum Likelihood Estimate**  | **@gause_experimental_1932** 
|:------:|:-----:|:-----:|
$K$ | 12.7 |  13.0 |
$b$ | 0.24242 |  0.21827 |
  
Using the `yeast` dataset, plot the function $\displaystyle y = \frac{K}{1+e^{a-bt}}$ (setting $\displaystyle a = \ln \left( \frac{K}{0.45} - 1 \right)$) using both sets of parameters. Which approach (the Maximum Likelihood estimate or @gause_experimental_1932) does a better job representing the data?
```

```{exercise}
An equation that relates a consumer's nutrient content (denoted as $y$) to the nutrient content of food (denoted as $x$) is given by: $\displaystyle y = c x^{1/\theta}$, where $\theta \geq 1$ and $c>0$ are both constants.


a. Use the dataset `phosphorous` to make a scatterplot with the variable `algae` on the horizontal axis, `daphnia` on the vertical axis.
b. Generate a contour plot for the likelihood function for these data. You may assume $0 \leq c \leq 5$ and $1 \leq \theta \leq 20$. What are the values of $c$ and $\theta$ that optimize the likelihood? *Hint:* for the dataset `phosphorous` be sure to use the variables $x=$`algae` and $y=$`daphnia`.
c. Add the fitted curve to your scatterplot and evaluate your fitted results.

```


```{exercise}
A dog's weight $W$ (pounds) changes over $D$ days according to the following function:

\begin{equation}
W =f(D,p_{1},p_{2})= \frac{p_{1}}{1+e^{2.462-p_{2}D}},
\end{equation}

where $p_{1}$ and $p_{2}$ are parameters.

a. This function can be used to describe the data `wilson`. Make a scatterplot with the `wilson` data. What is the long term weight of the dog? 
b. Generate a contour plot for the likelihood function for these data. What are the values of $p_{1}$ and $p_{2}$ that optimize the likelihood?  *You may assume that $p_{1}$ and $p_{2}$ are both positive.*
c. With your values of $p_{1}$ and $p_{2}$ add the function $W$ to your scatterplot and compare the fitted curve to the data.

```


<!--chapter:end:09-likelihood.Rmd-->

# Cost Functions qnd Bayes' Rule {#cost-fns-10}
Chapter \@ref(likelihood-09) introduced likelihood functions as an approach to tackle parameter estimation. However this is not the only approach to understand model-data fusion. This chapter introduces *cost functions*, which estimates parameters from data using a least squares approach.\index{cost function} Want to know a secret? Cost functions are very closely related to log-likelihood functions. This chapter will explore this idea some more, first by exploring model-data residuals, defining a cost function, and then connecting them back to likelihood functions. To complete the circle, this chapter ends by discussing Bayes' Rule, which will further strengthen the connection between cost and likelihood functions. Let's get started!

## Cost functions and model-data residuals
Let's revisit the linear regression problem from Chapter \@ref(likelihood-09). Recall Table \@ref(tab:limited-data-09) from Chapter \@ref(likelihood-09). With these data we wanted to fit a function of the form $y=bx$ (forcing the intercept term to be zero). We will extend Table \@ref(tab:limited-data-09) to include the model-data residual computed as $y-bx$ in Table \@ref(tab:md-resid-10):\index{residual!model-data}


Table: (\#tab:md-resid-10) A small, limited dataset (Table \@ref(tab:limited-data-09)) with the computed model-data residual with parameter $b$, along with model-data residuals for different values of $b$. 

| $x$ | $y$  | $bx$ | $y-bx$ | $b=1$ | $b=3$ | $b=-1$ |
|:------:|:-----:|:-----|:-----:|:-----:|:-----|:-----|
| 1 | 3 | $b$ | $3-b$ | 2 | 0 | 4 |
| 2 | 5 | $2b$ | $5-2b$ | 3 | -1 | 7 | 
| 4 | 4 | $4b$ | $4-4b$ | 0 | -8 | 8  |
| 4 | 10 | $4b$ | $10-4b$ | 6 | -2 | 14 |

Also included in Table \@ref(tab:md-resid-10) are the model-data residual values for different values of $b$. Notably values of the residuals can be negative and some can be positive - which makes it tricky to assess the "best" value of $b$ from the residuals alone. (If we found a value of $b$ where the residuals were all zero, then we would have the "best" value of $b$!^[To be fair, that means the data would be perfectly on a line; not too interesting of a problem, right?]).

To assess the overall residuals as a function of the value of $b$, we need to take into consideration not just the value of the residual (positive or negative), but rather some way to measure the overall distance of *all* the residuals from a given value of $b$. One way to define that is with a function that squares each residual (so that negative and positive values don't cancel each other) and adds each of those results together. We call this the *sum squared residuals*.\index{residual!sum square} So for example, the sum squared residual when $b=1$ is shown in Equation \@ref(eq:b-1-10):

\begin{equation}
\mbox{ Sum square residual: } 2^{2}+3^{2}+0^{2}+6^{2} = 49 (\#eq:b-1-10)
\end{equation}

The other square residuals are $68$ when $b=3$ and $325$ when $b=-1$. So of these choices for $b$, the one that minimizes the square residual is $b=1$.

Let's generalize this to determine a function to compute the sum square residual for any value of $b$. This function, denoted as $S(b)$, is called the cost function (Equation \@ref(eq:sb-10)):\index{cost function}

\begin{equation}
S(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 (\#eq:sb-10)
\end{equation}


Equation \@ref(eq:sb-10) is a function of one variable ($b$). Figure \@ref(fig:quadsb) shows a graph of $S(b)$. Notice how the plot of $S(b)$ is a nice quadratic function, with a minimum at $b=1.865$. Did you notice that this value for $b$ is the same value for the minimum that we found from  Equation \@ref(eq:small-data-likely) in Chapter \@ref(likelihood-09)?  In Exercise \@ref(exr:cost-min-10) you will use calculus to determine the optimum value of $S(b)$.

```{r quadsb,echo=FALSE,fig.cap='Plot of Equation \\@ref(eq:sb-10). The vertical line denotes the minimum value at $b=1.865$.'}
new_data <- data.frame(x = c(1, 2, 4, 4), y = c(3, 5, 4, 10))
b <- seq(0, 5, 0.01)
lb <- map(.x = b, .f = ~ (sum((new_data$y - .x * new_data$x)^2))) %>% as.numeric()

data.frame(b, lb) %>%
  ggplot(aes(x = b, y = lb)) +
  geom_line() +
  geom_vline(xintercept = b[which.min(lb)], color = "red") +
  labs(y = "S(b)") +
  annotate(
    geom = "curve", x = 2.5, y = 260, xend = 1.9, yend = 200,
    curvature = .3, arrow = arrow(length = unit(2, "mm"))
  ) +
  annotate(geom = "text", x = 2.6, y = 260, label = "Minimum of S(b) \n at b = 1.865", hjust = "left") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()

```


### Accounting for uncertainty
The cost function can also incorporate uncertainty in the value of the response variable $y$. We will define this uncertainty as $\sigma$ and have it be the same for each value $y_{i}$. In some cases the uncertainty may vary from measurement to measurement - but the concepts presented here can generalize. To account for this uncertainty we divide each of the square residuals in Equation \@ref(eq:sb-10) by $\sigma^{2}$, as shown in Equation \@ref(eq:sb-general-10) using $\sum$ notation.

\begin{equation}
S(\vec{\alpha}) = \sum_{i=1}^{N} \frac{(y_{i}-f(x,\vec{\alpha}))^{2}}{\sigma^{2}} (\#eq:sb-general-10)
\end{equation}

As an example, comparing Equation \@ref(eq:sb-10) to Equation \@ref(eq:sb-general-10) we have $N=4$, $f(x_{i},\vec{\alpha} ) =bx$, and $\sigma = 1$.

### Comparing cost and log-likelihood functions
Chapter \@ref(likelihood-09) defined the log-likelihood function (Equation \@ref(eq:loglikely)), which for the small dataset we are studying is represented with Equation \@ref(eq:loglikely-10) where $\sigma = 1$ and $N=4$:

\begin{equation}
\begin{split}
\ln(L(b | \vec{x},\vec{y} )) &= -2 \ln(2) - 2 \ln (\pi) - 2 \ln(1) -\frac{(3-b)^{2}}{2}-\frac{(5-2b)^{2}}{2} \\
&-\frac{(4-4b)^{2}}{2}-\frac{(10-4b)^{2}}{2} \\
&= -2 \ln(2) - 2 \ln (\pi) -\frac{(3-b)^{2}}{2}-\frac{(5-2b)^{2}}{2} \\
& -\frac{(4-4b)^{2}}{2}-\frac{(10-4b)^{2}}{2}
\end{split} (\#eq:loglikely-10)
\end{equation}

(Note that in Equation \@ref(eq:loglikely-10) the expression $-2 \ln(1)$ is 0.) If we compare Equation \@ref(eq:loglikely-10) with Equation \@ref(eq:sb-10), then we have $\ln(L(b | \vec{x},\vec{y} )) = -2 \ln(2) - 2 \ln (\pi) - \frac{1}{2} \cdot S(b)$. **This is no coincidence**: log-likelihood functions are similar to cost functions!  While Equation \@ref(eq:loglikely-10) contains some extra factors, they only shift vertically or expand the graph of $\ln(L(b | \vec{x},\vec{y} ))$ compared to $S(b)$. This "coincidence" is only true when $\sigma$ is the same for all $y_{i}$. Can you explain why?

For both log-likelihood or cost functions the goal is optimization. Vertically shifting or expanding a function does not change the *location* of an optimum value (Why?  Think back to derivatives from Calculus I). In fact, a quadratic cost function yields the same results as the log-likelihood function assuming the residuals are normally distributed.


## Further extensions to the cost function
The cost function $S(b)$ can be extended additionally to incorporate other types of data. For example, if we knew there was a given range of values that would make sense (say $b$ is near 1.3 with a standard deviation of 0.1), we should be able to incorporate this information into the cost function. We do this by adding an additional term to Equation \@ref(eq:sb-10) ($\tilde{S}(b)$, Equation \@ref(eq:priorcost-10), also is graphed in Figure \@ref(fig:priorcost)).

\begin{equation}
\tilde{S}(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 + \frac{(b-1.3)^2}{0.1^2} (\#eq:priorcost-10)
\end{equation}

```{r priorcost,echo=FALSE, warning=FALSE,fig.cap='Comparing two cost functions $S(b)$ (black) and $\\tilde{S}(b)$ (black dashed line)'}
new_data <- data.frame(x = c(1, 2, 4, 4), y = c(3, 5, 4, 10))
b <- seq(0, 5, 0.01)
lb <- map(.x = b, .f = ~ (sum((new_data$y - .x * new_data$x)^2) + (.x - 1.3)^2 / 0.1^2)) %>% as.numeric()
lb_orig <- map(.x = b, .f = ~ (sum((new_data$y - .x * new_data$x)^2))) %>% as.numeric()

data.frame(b, lb, lb_orig) %>%
  ggplot(aes(x = b)) +
  geom_line(aes(y = lb_orig), color = "black") +
  geom_line(aes(y = lb), color = "black", linetype = "dashed") +
  geom_vline(xintercept = b[which.min(lb_orig)], color = "red") +
  geom_vline(xintercept = b[which.min(lb)], color = "blue",linetype="dashed") +
  labs(x = "b", y = "Cost Function") +
  annotate(
    geom = "curve", x = 2.5, y = 260, xend = 1.9, yend = 200,
    curvature = .3, arrow = arrow(length = unit(2, "mm"))
  ) +
  annotate(geom = "text", x = 2.6, y = 260, label = "Original optimum \n at b = 1.865", hjust = "left") +
  annotate(
    geom = "curve", x = 0.9, y = 260, xend = 1.4, yend = 200,
    curvature = .3, arrow = arrow(length = unit(2, "mm"))
  ) +
  annotate(geom = "text", x = 0.1, y = 300, label = "New optimum \n at b = 1.45", hjust = "left") +



  # geom_text(aes(x=0.1,y=300,label="The original \n minimum value"),hjust="left") +
  # geom_text(aes(x=3,y=200,label="The new minimum \n with prior information"),hjust="left") +
  # geom_curve(aes(x = 0.5, y = 300, xend = 1.3, yend = 750),
  #            curvature = 0.05, angle = 15,
  #            arrow = arrow(length = unit(0.25,"cm"))) +
  # geom_curve(aes(x = 3.5, y = 400, xend = 1.9, yend = 550),
  #            curvature = 0.05, angle = 15,
  #            arrow = arrow(length = unit(0.25,"cm"))) +
  ylim(c(0, 400)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()

```

Aha!  Figure \@ref(fig:priorcost) shows how the revised cost function $\tilde{S}(b)$ changes the optimum value. Numerically this works out to be $\tilde{b}=$ `r b[which.min(lb)]`. In Exercise \@ref(exr:cost-min-10) you will verify this new minimum value and compare the results to the fitted value of $b=1.86$.

Adding this prior information seems like an effective approach. Many times a study wants to build upon the existing body of literature and to take that into account. This approach of including prior information into the cost function can also be considered a *Bayesian* approach.\index{Bayes!approach}

## Conditional probabilities and Bayes' rule
<!-- Adapted from Silver, *The Signal and the Noise*, 2012. -->
We first need to understand Bayes' rule and conditional probability, with an example.

```{example president-pop}
The following table shows results from a survey of people's views on the economy (optimistic or pessimistic) and whether or not they voted for the incumbent President in the last election. Percentages are reported as decimals. Probability tables are a clever way to organize this information. 

| Probability | Optimistic | Pessimistic | Total |
|:------:|:-----:|:-----|:-----|
| Voted for the incumbent President | 0.20 | 0.20 | 0.40 |
| Did not vote for incumbent President | 0.15 | 0.45 | 0.60 | 
| Total | 0.35 | 0.65 | 1.00  |

Compute the probability of having an optimistic view on the economy.
```

```{solution}
Based on the probability table, we define the following probabilities:

- The probability you voted for the incumbent President *and* have an **optimistic** view on the economy is 0.20
- The probability you **did not** vote for the incumbent President *and* have an **optimistic** view on the economy is 0.15
- The probability you voted for the incumbent President *and* have an **pessimistic** view on the economy is 0.20
- The probability you **did not** vote for the incumbent President *and* have an **pessimistic** view on the economy is 0.45

We calculate the probability of having an **optimistic view on the economy**  by adding the probabilities with an optimistic view, whether or not they voted for the incumbent President. For this example, this probability sums to 0.20 + 0.15 = 0.35, or 35%.

On the other hand, the probability you have a pessimistic view on the economy is 0.20 + 0.45 = 0.65, or 65%. Notice how the two of these together (probability of optimistic and pessimistic views of the economy is 1, or 100% of the outcomes.)

```

### Conditional probabilities
Next, let's discuss conditional probabilities. A conditional probability is the probability of an outcome given some previous outcome, or $\mbox{Pr} (A | B)$, where Pr means "probability of an outcome" and $A$ and $B$ are two different outcomes or events. In probability theory you might study the following law of conditional probability:
 
\begin{equation}
\begin{split}
\mbox{Pr}(A \mbox { and } B) &= \mbox{Pr} (A \mbox{ given } B) \cdot  \mbox{Pr}(B) \\
 &= \mbox{Pr} (A | B) \cdot  \mbox{Pr}(B) \\
  &= \mbox{Pr} (B | A) \cdot  \mbox{Pr}(A)
\end{split} (\#eq:law-cond-prop)
\end{equation}

Typically when expressing conditional probabilities we remove "and" and write $P(A \mbox{ and } B)$ as $P(AB)$ and "given" as $P(A \mbox{ given } B)$ as $P(A|B)$.

```{example}
Continuing with Example \@ref(exm:president-pop), sometimes people believe that your views of the economy [influence whether you are going to vote for the incumbent President in an election.](https://www.cbsnews.com/news/how-much-impact-can-a-president-have-on-the-economy/) Use the information from the table in Example \@ref(exm:president-pop) to compute the probability you voted for the incumbent President *given* you have an optimistic view of the economy.
```

```{solution pres-pop-con}
To compute the  probability you voted for the incumbent President *given* you have an optimistic view of the economy is a rearrangement of Equation \@ref(eq:law-cond-prop):

\begin{equation}
\begin{split}
\mbox{Pr(Voted for incumbent President | Optimistic View on Economy)} = \\
\frac{\mbox{Pr(Voted for incumbent President and Optimistic View on Economy)}}{\mbox{Pr(Optimistic View on Economy)}} = \\
\frac{0.20}{0.35} = 0.57
\end{split} (\#eq:econ-cond-prop)
\end{equation}

So if you have an optimistic view on the economy, there is a 57% chance you will vote for the incumbent President. Contrast this result to the probability that you voted for the incumbent President (Example \@ref(exm:president-pop)), which is only 40%. Perhaps your view of the economy does indeed influence whether or not you would vote to re-elect the incumbent President.
```


### Bayes' rule
Using the incumbent President and economy example as a framework, we will introduce [*Bayes' Rule*](https://en.wikipedia.org/wiki/Bayes%27_theorem), which is a re-arrangment of the rule for conditional probability:\index{Bayes!rule}

\begin{equation}
\mbox{Pr} (A | B) = \frac{ \mbox{Pr} (B | A) \cdot  \mbox{Pr}(A)}{\mbox{Pr}(B) } (\#eq:bayes-rule-10)
\end{equation}

It turns out Bayes' Rule is a really helpful way to understand how we can systematically incorporate this prior information into the likelihood function (and by association the cost function). For parameter estimation our goal is to estimate parameters, given the data. Another way to state Bayes' Rule in Equation \@ref(eq:bayes-rule-10) is using terms of parameters and data:


\begin{equation}
\mbox{Pr}( \mbox{ parameters } | \mbox{ data }) = \frac{\mbox{Pr}( \mbox{ data } | \mbox{ parameters }) \cdot \mbox{ Pr}( \mbox{ parameters }) }{\mbox{Pr}(\mbox{ data }) } (\#eq:bayes-rule-data-10)
\end{equation}

While Equation \@ref(eq:bayes-rule-data-10) seems pretty conceptual, here are some key highlights:

- In practice, the term $\mbox{Pr}( \mbox{ data } | \mbox{ parameters })$ in Equation \@ref(eq:bayes-rule-data-10) is the likelihood function (Equation \@ref(eq:likelihood-prod-param-09)).
- The term $\mbox{Pr}( \mbox{ parameters })$ is the probability distribution of the *prior information* on the parameters, specifying the probability distribution functions for the given context. When this distribution is the same as $\mbox{Pr}( \mbox{ data } | \mbox{ parameters })$ (typically normally distributed), prior information has a multiplicative effect on the likelihood function ($\mbox{Pr}( \mbox{ parameters } | \mbox{ data })$). (Or an additive effect on the log-likelihood function.) *This is good news!* When we added that additional term for prior information into $\tilde{S}(b)$ in Equation \@ref(eq:priorcost-10), we accounted for the prior information correctly. In Exercise \@ref(exr:log-cost-verify) you will explore how the log-likelihood is related to the cost function. 
-  The expression $\mbox{Pr}( \mbox{ parameters } | \mbox{ data })$ is the start of a framework for a probability density function, which should integrate to unity. (You will explore this more if you study probability theory.) This denominator term is called a [normalizing constant](https://stats.stackexchange.com/questions/12112/normalizing-constant-in-bayes-theorem). Since our overall goal is to select parameters that optimize $\mbox{Pr}( \mbox{ parameters } | \mbox{ data })$, the expression in the denominator ($\mbox{Pr}(\mbox{ data })$ ) does not change the *location* of the optimum values. 


## Bayes' rule in action
Wow - we made some significant progress in our conceptual understanding of how to incorporate models and data! Let's see how this applies  to our linear regression problem ($y=bx$). We have the following assumptions:

- **Assumption 1:** The data are independent, identically distributed. We can then write the likelihood function as the following:

\begin{equation}
\mbox{Pr}(\vec{y} | b) = \left( \frac{1}{\sqrt{2 \pi} \sigma}\right)^{4} e^{-\frac{(3-b)^{2}}{2\sigma^{2}}} \cdot e^{-\frac{(5-2b)^{2}}{2\sigma^{2}}}  \cdot e^{-\frac{(4-4b)^{2}}{2\sigma^{2}}}  \cdot e^{-\frac{(10-4b)^{2}}{2\sigma^{2}}}
\end{equation}

- **Assumption 2:** Prior knowledge expects us to say that $b$ is normally distributed with mean 1.3 and standard deviation 0.1. Incorporating this information allows us to write the following:

\begin{equation}
\mbox{Pr}(b) =\frac{1}{\sqrt{2 \pi} \cdot 0.1} e^{-\frac{(b-1.3)^{2}}{2 \cdot 0.1^{2}}}
\end{equation}

When we combine the two pieces of information, the probability of $b$, given the data $\vec{y}$, is the following:

\begin{equation}
\mbox{Pr}(b | \vec{y}) \approx e^{-\frac{(3-b)^{2}}{2\sigma^{2}}} \cdot e^{-\frac{(5-2b)^{2}}{2\sigma^{2}}}  \cdot e^{-\frac{(4-4b)^{2}}{2\sigma^{2}}}  \cdot e^{-\frac{(10-4b)^{2}}{2\sigma^{2}}} \cdot e^{-\frac{(b-1.3)^{2}}{2 \cdot 0.1^{2}}} (\#eq:likelihood-bayes-10)
\end{equation}


Notice we are ignoring the terms $\displaystyle \left( \frac{1}{\sqrt{2 \pi} \cdot \sigma }\right)^{4}$ and $\displaystyle \frac{1}{\sqrt{2 \pi} \cdot 0.1}$, because per our discussion above not including them does not change the *location* of the optimum value, only the value of the likelihood function. The plot of $\mbox{Pr}(b | \vec{y})$, assuming $\sigma = 1$ is shown in Figure \@ref(fig:likelihoodbayes):


```{r likelihoodbayes,warning=FALSE,message=FALSE,echo=FALSE,fig.cap='Equation \\@ref(eq:likelihood-bayes-10) with optimum value at $b=1.45$ denoted in a blue dashed line. '}
b <- seq(0, 3, 0.01)
new_data <- data.frame(x = c(1, 2, 4, 4), y = c(3, 5, 4, 10))
lb <- map(.x = b, .f = ~ exp(-sum((new_data$y - .x * new_data$x)^2) / 2)) %>% as.numeric()
lb_rev <- map(.x = b, .f = ~ exp(-sum((new_data$y - .x * new_data$x)^2) / 2) * exp(-(.x - 1.3)^2 / (2 * 0.1^2))) %>% as.numeric()
data.frame(b, lb, lb_rev) %>%
  ggplot(aes(x = b)) +
  #geom_line(aes(y = lb)) +
  geom_line(aes(y = lb_rev)) +
  geom_vline(xintercept = b[which.max(lb_rev)], color = "blue",linetype="dashed") +
  #geom_vline(xintercept = b[which.max(lb)], color = "red") +
  ylab("Posterior Probability") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()

```

It looks like the value that optimizes our posterior probability is $b=$ `r b[which.max(lb_rev)]`. This is similar the value of $\tilde{b}$ from Equation \@ref(eq:priorcost-10). Again, *this is no coincidence*. Adding in prior information to the cost function or using Bayes' Rule are equivalent approaches.

## Next steps
Now that we have seen the usefulness of cost functions and Bayes' Rule we can begin to apply this to larger problems involving more equations and data. In order to do that we need to explore some computational methods to scale this problem up - which we will do in subsequent chapters.


## Exercises

<!-- Other potential problems: -->
<!-- - Do some likelihood functions with uniform prior (the simple ones) -->
<!-- -  -->
<!-- HW: they adjust the distribution to a uniform on the simple linear regression -->
<!-- Write out the likelihood function (try programming for something simple) -->
<!-- Verify that dividing by the data doesn't affect the location of the optimum -->
<!-- Prove a uniform distribution doesn't affect the function -->

```{exercise cost-min-10}
The following problem works with Table \@ref(tab:limited-data-09) to determine the value of $b$ with the function $y=bx$ as in this chapter.


a. Using calculus, show that the cost function $S(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2$ has a minimum value at $b=1.86$.
b. What is the value of $S(1.865)$?
c. Use a similar approach to determine the minimum of the revised cost function $\tilde{S}(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 + (b-1.3)^2$. Call this value $\tilde{b}$.
d. How do the values of $S(1.865)$ and $\tilde{S}(\tilde{b})$ compare?
e. Make a plot of the cost functions $S(b)$ and $\tilde{S}(b)$  to verify the optimum values.
f. Make a scatter plot with the data and the function $y=bx$ and $y=\tilde{b}x$. How do the two estimates compare with the data?

```


```{exercise}
Use calculus to determine the optimum value of $b$ for Equation \@ref(eq:loglikely-10). Do you obtain the same value of $b$ for Equation \@ref(eq:sb-10)?
```

 <!-- From van den Berg, pg 59, exercise 3.13 -->
```{exercise phos-09}
(Inspired from @berg_mathematical_2011) Consider the nutrient equation $\displaystyle y = c x^{1/\theta}$ using the dataset `phosphorous`.


a. Write down a formula for the objective function $S(c,\theta)$ that characterizes this equation (that includes the dataset `phosphorous`).
b. Fix $c=1.737$. Make a `ggplot` of $S(1.737,\theta)$ for $1 \leq \theta \leq 10$.
c. How many critical points does this function have over this interval?  Which value of $\theta$ is the global minimum?

```



```{exercise}
Use the cost function $S(1.737,\theta)$ from Exercise \@ref(exr:phos-09) to answer the following questions:

a. Researchers believe that $\theta \approx 7$. Re-write $S(1.737,\theta)$ to account for this additional (prior) information.
b. How does the inclusion of this additional information change the shape of the cost function and the location of the global minimum?
c. Finally, reconsider the fact that $\theta \approx 7 \pm .5$ (as prior information). How does that modify $S(1.737,\theta)$ further and the location of the global minimum?

```




```{exercise}
One way to generalize the notion of prior information using cost functions is to include a term that represents the degree of uncertainty in the prior information, such as $\sigma$. For the problem $y=bx$ this leads to the following cost function: $\displaystyle \tilde{S}_{revised}(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 + \frac{(b-1.3)^2}{\sigma^{2}}$.


Use calculus to determine the optimum value for $\tilde{S}_{revised}(b)$, expressed in terms of $\tilde{b}_{revised} = f(\sigma)$ (your optimum value will be a function of $\sigma$). What happens to $\tilde{b}_{revised}$ as $\sigma \rightarrow \infty$?
```





```{exercise log-cost-verify}
For this problem you will minimize some generic functions.


a. Using calculus, verify that the optimum value of $y=ax^{2}+bx+c$ occurs at $\displaystyle x=-\frac{b}{2a}$. (You can assume $a>0$.)
b. Using calculus, verify that a critical point of $z=e^{-(ax^{2}+bx+c)^{2}}$ also occurs at $\displaystyle x=-\frac{b}{2a}$. *Note: this is a good exercise to practice your differentiation skills!*
c. Algebraically show that $\ln(z) = -y$.
d. Explain why $y$ is similar to a cost function $S(b)$ and $z$ is similar to a likelihood function.


```


```{exercise}
This problem continues the re-election of the incumbent President and viewpoint on the economy in Example \@ref(exm:president-pop). Determine the following conditional probabilities:
  
  a. Determine the probability that you **voted for the incumbent President** given that you have a **pessimistic view on the economy**.
  b. Determine the probability that you **did not vote for the incumbent President** given that you have an **pessimistic view on the economy**. 
  c. Determine the probability that you **did not vote for the incumbent President** given that you have an **optimistic view on the economy**. 
  d. Determine the probability that you have an **pessimistic view on the economy** given that you **voted for the incumbent President**.
  e. Determine the probability that you have an **optimistic view on the economy** given that you **did not vote for the incumbent President**.
  
```





```{exercise}
Incumbents have an advantage in re-election due to wider name recognition, which may boost their re-election chances, as shown in the following table:


| Probability | Being elected | Not being elected | Total |
|:------:|:-----:|:-----|:-----|
| Having name recognition | 0.55 | 0.25 | 0.80 |
| Not having name recognition | 0.05 | 0.15 | 0.20 | 
| Total | 0.60 | 0.40 | 1.00  |


Use Bayes' Rule to determine the probability of being elected, given that you have name recognition. 


```




```{exercise}
Demonstrate how Bayes' Rule differs from the law of conditional probability.
```





<!--chapter:end:10-costFunctions.Rmd-->

# Sampling Distributions and the Bootstrap Method {#bootstrap-11}

In Chapters and \@ref(likelihood-09) and \@ref(cost-fns-10) we saw how the parameter estimation problem is related to optimizing the likelihood or cost function. In most cases - and especially when the model is linear - optimization is straightforward. However for *nonlinear* models the cost function is trickier. Consider the cost function shown in Figure \@ref(fig:nonlin-cost), which tries to optimize $\theta$ for the nutrient equation $\displaystyle y = 1.737 x^{1/\theta}$ using the dataset `phosphorous`:

```{r nonlin-cost,fig.cap="Nonlinear cost function plot for `phosphorous` data set with the model $\\displaystyle y = 1.737 x^{1/\\theta}$.",echo=FALSE}
# Can we do this with compute likelihood?
my_model <- daphnia ~ 1.737 * algae^(1 / theta)

# This allows for all the possible combinations of parameters
parameters <- tibble(theta = seq(1, 25, length.out = 200))


out_values <- compute_likelihood(my_model, phosphorous, parameters, logLikely = TRUE)

out_values$likelihood %>%
  ggplot(aes(theta, y = l_hood)) +
  geom_line() +
  labs(x = expression(theta), y = expression(S(theta))) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```

While Figure \@ref(fig:nonlin-cost) shows a clearly defined minimum around $\theta \approx 6$, the shape of the cost function is not quadratic like Figure \@ref(fig:priorcost) in Chapter \@ref(cost-fns-10). For nonlinear models direct optimization of the cost function using techniques learned in calculus may not be computationally feasible.

An alternative approach to optimization relies on the idea of *sampling*, which randomly selects parameter values and then computes the cost function. After a certain amount of time or iterations, all the values of the cost function are compared to each other to determine the optimum value. We will refine the concept of sampling to determine the optimum value in subsequent chapters (Chapters \@ref(metropolis-12) and \@ref(mcmc-13)), but this chapter develops some foundations in sampling - we will study how to plot histograms in `R` and then apply these to the *bootstrap* method, which relies on random sampling.\index{bootstrap} Let's get started!

## Histograms and their visualization

To introduce the idea of a histogram, consider Table \@ref(tab:snow-table), which is a sample of the dataset `snowfall` in the `demodelr` package. The `snowfall` dataset is measurements of precipitations collected at weather stations in the Twin Cities (Minneapolis, Saint Paul, and surrounding suburbs) from a spring snowstorm. These data come from a cooperative [network of local observers](https://www.cocorahs.org/ViewData/ListDailyPrecipReports.aspx). Yes, it can snow in Minnesota in April. Sigh.

```{r snow-table,echo=FALSE}
knitr::kable(snowfall[1:5, ], caption = "Weather station data from a Minnesota snowstorm.")
```

The header row in Table \@ref(tab:snow-table) also lists the names of the associated variables in this data frame. We are going to focus on the variable `snowfall`. A histogram is an easy way to view the distribution of measurements, using `geom_histogram` (Figure \@ref(fig:initial-histogram-snow-11)). You may recall that a histogram represents the frequency count of a random variable, typically binned over certain intervals.

```{r eval = FALSE}
ggplot(data = snowfall) +
  geom_histogram(aes(x = snowfall)) +
  labs(
    x = "Snowfall amount",
    y = "Number of observations"
  )
```


```{r initial-histogram-snow-11,echo=FALSE,fig.cap="Initial histogram of snowfall data in Table \\@ref(tab:snow-table).",warning=FALSE,message=FALSE}
ggplot(data = snowfall) +
  geom_histogram(aes(x = snowfall)) +
  labs(
    x = "Snowfall amount",
    y = "Number of observations"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```

This code introduces `geom_histogram`. To create the historgram we use the aesthetic (`aes(x = snowfall)`) to display the histogram for the `snowfall` column in the dataset `snowfall`. 

At the `R` console you may have received a warning about such as `` stat_bin()` using `bins = 30`. Pick better value with `binwidth` ``. The function `geom_histogram` doesn't guess a bin width, but one rule of thumb is to select the number of bins to be equal to the square root of the number of observations (16 in this instance). So let's adjust the number of bins to `r round(sqrt(length(demodelr::snowfall$snowfall)),digits=0)` in Figure \@ref(fig:adjusted-histogram-snow-11).

```{r eval = FALSE}

ggplot() +
  geom_histogram(data = snowfall, aes(x = snowfall), bins = 4) +
  labs(
    x = "Snowfall amount",
    y = "Number of observations"
  )
```



```{r adjusted-histogram-snow-11,fig.cap="Adjusted histogram of snowfall data in Table \\@ref(tab:snow-table) with the number of bins set to 4.",echo=FALSE}

ggplot() +
  geom_histogram(data = snowfall, aes(x = snowfall), bins = 4) +
  labs(
    x = "Snowfall amount",
    y = "Number of observations"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```



## Statistical theory: sampling distributions


### Sampling the empirical distribution

The histogram of snowfall measurements (Figure \@ref(fig:adjusted-histogram-snow-11)) from the snowfall data (Table \@ref(tab:snow-table)) represents an *empirical probability distribution* of measurements. \index{probability distribution!empirical} While it is great to have all these observations, a key question is: *What is an estimate for the representative amount of snowfall for this storm?* The snowfall measurements illustrate the difference between what statisticians call a *population* (the true distribution of measurements) and a *sample* (what people observe).\index{probability distribution!population}\index{probability distribution!sample distribution} To go a little deeper, let's define a random variable $S$ for this population, which has an (unknown) probability density function associated with it. The measurements shown Table \@ref(tab:snow-table) are *samples* of this distribution. While the average of the precipitation data (`r round(mean(snowfall$snowfall,na.rm=TRUE),digits=2)` inches) might be a defensible value for the average amount of snow that fell, what we don't know is *how well* this empirical mean approximates the expected value of the distribution for $S$.

Each of the entries in Table \@ref(tab:snow-table) represents a measurement made by a particular observer. To get the true distribution we would need to add more observers, but that isn't realistic for this case as the snowfall event has already passed - we can't go "back in time".

One way is to generate a *bootstrap sample*, which is a sample of the original dataset with replacement. The workflow that we will apply is the following:

> Do once $\rightarrow$ Do several times $\rightarrow$ Visualize $\rightarrow$ Summarize

**"Do once"** is the first step, using sampling with replacement. This process is easily done with the `R` command `slice_sample` (you should try this out yourself), which in the following code assigns a sample to the variable `p_new`:

```{r,eval=FALSE}
p_new <- slice_sample(snowfall, prop = 1, replace = TRUE)
```

Let's break this code down:

-   We are sampling the `snowfall` data frame with replacement (`replace = TRUE`). If you are uncertain about how sampling with replacement works, here is one visual: say you have each of the snowfall measurements written on a piece of paper in a hat. You draw one slip of paper, record the measurement, and then place that slip of paper back in the hat to draw again, until you have as many measurements as the original data frame.

- The command `prop=1` means that we are sampling 100% of the `snowfall` data frame.

Once we have taken a sample, we can then compute the mean (average) and the standard deviation of the sample:

```{r}
slice_sample(snowfall, prop = 1, replace = TRUE) %>%
  summarize(
    mean = mean(snowfall, na.rm = TRUE)
  )
```

How the above code works is to first do the sampling, and then the summary. The command `summarize` collapses the `snowfall` data frame and computes the mean and the standard deviation `sd` of the column `snowfall`. We have the command `na.rm=TRUE` to remove any `NA` values that may affect the computation. 

**"Do several times"** is the second step. Here we are going to rely on some powerful functionality from the [`purrr` package](https://purrr.tidyverse.org/). This package has the wonderful command `map_df`, which allows you to efficiently repeat a process several times and return a data frame as output. Evaluate the following code on your own:


```{r,eval=FALSE}
purrr::map_df(
  1:10,
  ~ (
    slice_sample(snowfall, prop = 1, replace = TRUE) %>%
      summarize(
        mean = mean(snowfall, na.rm = TRUE)
      )
  ) # Close off the tilde ~ ()
) # Close off the map_df
```

Let's review this code step by step: 

-   The basic structure is `map_df(1:N,~(COMMANDS))`, where `N` is the number of times you want to run your code (in this case `N=10`).
-   The second part `~(COMMANDS)` lists the different commands we want to re-run (here the resampling of our dataset and then subsequent summarizing).


What should be returned is a data frame that lists the `mean` of each bootstrap sample. The process of randomly sampling a dataset is called *bootstrapping*.

I can appreciate that this programming might be a little tricky to understand and follow - don't worry - the goal is to give you a tool that you can easily adapt to a situation. 

**"Visualize"** is the step where we will use a histogram to examine the distribution of the bootstrap samples. The following code does this all, changing the number of bootstrap samples to 1000:

```{r eval = FALSE}

bootstrap_samples <- purrr::map_df(
  1:1000,
  ~ (
    slice_sample(snowfall, prop = 1, replace = TRUE) %>%
      summarize(
        mean_snow = mean(snowfall, na.rm = TRUE)
      )
  ) # Close off the tilde ~ ()
) # Close off the map_df

# Now make the histogram plots for the mean and standard deviation:
ggplot(bootstrap_samples) +
  geom_histogram(aes(x = mean_snow)) +
  ggtitle("Distribution of sample mean")
```


```{r bootsample,fig.cap="Histogram of 1000 bootstrap samples for the mean of the snowfall dataset.",warning=FALSE,message=FALSE,echo=FALSE}

bootstrap_samples <- purrr::map_df(
  1:1000,
  ~ (
    slice_sample(snowfall, prop = 1, replace = TRUE) %>%
      summarize(
        mean_snow = mean(snowfall, na.rm = TRUE)
      )
  ) # Close off the tilde ~ ()
) # Close off the map_df

# Now make the histogram plots for the mean:
ggplot(bootstrap_samples) +
  geom_histogram(aes(x = mean_snow)) +
  ggtitle("Distribution of sample mean") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind() +
  labs(x = "Snowfall amount bootstrap means", y = "Number of observations")
```

Excellent! This is shaping up nicely. Once we have sampled as much we want, *then* investigate the distribution of the computed sample statistics (we call this the *sampling distribution*). It turns out that the statistics of the sampling distribution (such as the mean or the standard deviation) will approximate the population distribution statistics when the number of bootstrap samples is large (and 1000 is sufficiently large in this instance).

**"Summarize"** is the final step, where we compute summary statistics of the distribution of bootstrap means. We will do this by computing a confidence interval, which comes from the percentiles of the distribution. 

Here is how we would compute these different statistics using the `quantile` command shown below:

```{r}
# Compute the average of the distribution:
mean(bootstrap_samples$mean_snow)

# Compute the 2.5% and 97.5% of the distribution:
quantile(bootstrap_samples$mean_snow, probs = c(0.025, .975))
```

Notice how we used the `probs=c(0.025,.975)` command to compute the different 2.5% and 97.5% quantiles of the distribution of the sample means.
Let's discuss the distribution of bootstrap means. The 2.5 percentile is approximately 14.3 inches. This means 2.5% of the distribution is at 14.3 inches or less. The 97.5 percentile is approximately 17.8 inches, so 97.5% of the distribution is 17.8 inches or less. If we take the difference between 2.5% and 97.5% that is 95%, so 95% of the distribution is contained between 14.3 and 17.8 inches. If we are using the bootstrap mean, we would report that the average precipitation is 16.1 inches with a 95% confidence interval of 14.3 to 17.8 inches. The confidence interval is to give some indication of the uncertainty in the measurements.



## Summary and next steps
The idea of sampling with replacement, generating a parameter estimate, and then repeating over several iterations is at the heart of many computational parameter estimation methods, such as Markov Chain Monte Carlo methods that we will explore in Chapter \@ref(mcmc-13). Bootstrapping (and other sampling with replacement techniques) makes nonlinear problems more tractable.

This chapter extended your abilities in `R` by showing you how to generate histograms, sample a dataset, and compute statistics. The goal here is to give you examples that you can re-use in this chapter's exercises. Enjoy!


## Exercises

```{exercise}
Histograms are an important visualization tool in descriptive statistics. Read the following essays on histograms, and then summarize 2-3 important points of what you learned reading these articles.


- [Visualizing histograms](http://tinlizzie.org/histograms/)
- [How histograms work](https://flowingdata.com/2017/06/07/how-histograms-work/)
- [How to read histograms and use them in R](https://flowingdata.com/2014/02/27/how-to-read-histograms-and-use-them-in-r/)

```

```{exercise}
Display the bootstrap histogram of 1000 bootstrap samples for the standard deviation of the `snowfall` dataset. From this bootstrap distribution (for the standard deviation) what is the mean and 95% confidence interval?
```

<!-- Taken from 373 textbook Fall 2020, pg 31 -->

```{exercise snow-eurasia}
(Inspired by @devore_modern_2021) Average snow cover from 1970 - 1979 in October over Eurasia (in million km$^{2}$) was reported as the following:

\begin{equation*}
\{6.5, 12.0, 14.9, 10.0, 10.7, 7.9, 21.9, 12.5, 14.5, 9.2\}
\end{equation*}


a. Create a histogram for these data.
b. Compute the sample mean and median of this dataset.
c. What would you report as a representative or typical value of snow cover for October?  Why?
d. The 21.9 measurement looks like an outlier. What is the sample mean excluding that measurement?

```



```{exercise}
Consider the equation $\displaystyle S(\theta)=(3-1.5^{1/\theta})^{2}$ for $\theta>0$. This function is an idealized example for the cost function in Figure \@ref(fig:nonlin-cost).


a. What is $S'(\theta)$?
b. Make a plot of $S'(\theta)$. What are the locations of the critical points?
c. Algebraically solve $S'(\theta)=0$. Does your computed critical point match up with the graph?

```



```{exercise}
Repeat the bootstrap sample for the mean of the snowfall dataset (`snowfall`) where the number of bootstrap samples is 10,000. Report the median and confidence intervals for the bootstrap distribution. What do you notice as the number of bootstrap samples increases?
```



```{exercise}
Using the data in Exercise \@ref(exr:snow-eurasia), do a bootstrap sample with $N=1000$ to compute the a bootstrap estimate for the mean October snowfall cover in Eurasia. Compute the mean and 95% confidence interval for the bootstrap distribution.

```

```{r summary-11, echo=FALSE, fig.cap = "Example computing a confidence interval with the `summary` command.",out.width="25%"}

knitr::include_graphics("figures/11-bootstrap/summary-output-11.png")
```

```{exercise}
We computed the 95% confidence interval using the `quantile` command. An alternative approach to summarize a distribution is with the `summary` command, as shown in Figure \@ref(fig:summary-11). We call this command using `summary(data_frame)`, where `data_frame` is the particular data frame you want to summarize. The output reports the minimum and maximum values of a dataset. The output `1st Qu.` and `3rd Qu.` are the 25th and 75th percentiles.

Do 1000 bootstrap samples using the data in Exercise \@ref(exr:snow-eurasia) and report the output from the `summary` command.
```


```{exercise}
The dataset `precipitation` lists rainfall data from a fall storm that came through the Twin Cities.


a. Make an appropriately sized histogram for the precipitation observations.
b. What is the mean precipitation across these observations?
c. Do a bootstrap estimate with $N=100$ and $N=1000$ and plot their respective histograms.
d. For each of your bootstrap samples ($N=100$ and $N=1000$) compute the mean and 95% confidence interval for the bootstrap distribution.
e. What would you report for the mean and its 95% confidence interval for this storm?

```

<!--chapter:end:11-bootstrap.Rmd-->

# The Metropolis-Hastings Algorithm {#metropolis-12}

Cost or likelihood functions (Chapters \@ref(likelihood-09) and \@ref(cost-fns-10)) are a powerful approach to estimate model parameters for a dataset. Bootstrap sampling (Chapter \@ref(bootstrap-11)) is an efficient computational method to extend the reach of a dataset to estimate population level parameters. With all these elements in place we will discuss a powerful algorithm that will efficiently sample a likelihood function to estimate parameters for a model. Let's get started!


## Estimating the growth of a dog

In Chapter \@ref(r-intro-02) we introduced the dataset `wilson`, which reported data on the [weight of the puppy Wilson](http://bscheng.com/2014/05/07/modeling-logistic-growth-data-in-r/) as he grew, shown again in Figure \@ref(fig:wilson-data-12). Because we will re-use the scatter plot in Figure \@ref(fig:wilson-data-12) several times, we define the variable `wilson_data_plot` so we don't have to re-copy the code over and over.^[Making a base plot and repeatedly adding to it is a really good coding practice for `R`.]

```{r eval = FALSE}
wilson_data_plot <- ggplot(data = wilson) +
  geom_point(aes(x = days, y = weight), size = 1, color = "red") +
  labs(
    x = "D (Days since birth)",
    y = "W (Weight in pounds)"
  )

wilson_data_plot
```

```{r wilson-data-12,fig.cap="Weight of the dog Wilson over time.",echo=FALSE}
wilson_data_plot <- ggplot(data = wilson) +
  geom_point(aes(x = days, y = weight), size = 1, color = "red") +
  labs(
    x = "D (Days since birth)",
    y = "W (Weight in pounds)"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind() + ylim(c(0,80))

wilson_data_plot
```

Let $D$ be the days since birth and $W$ Wilson's weight in pounds, Equation \@ref(eq:wilson-sat-12) is a model that describes Wilson's weight over time:

\begin{equation}
W =f(D,p_{1})= \frac{p_{1}}{1+e^{(p_{2}-p_{3}D)}} (\#eq:wilson-sat-12)
\end{equation}

Equation \@ref(eq:wilson-sat-12) has three different parameters $p_{1}$, $p_{2}$, and $p_{3}$ to be estimated. For convenience we will set $p_{2}= 2.461935$ and  $p_{3} = 0.017032$ and estimate $p_{1}$, which for this model represents the long-term weight for Wilson, or a horizontal asymptote for Equation \@ref(eq:wilson-sat-12) (Exercise \@ref(exr:limit-12)).

Let's take an initial guess for the parameter $p_{1}$. From Figure \@ref(fig:wilson-data-12) a reasonable guess for $p_{1}$ would be 78. As we did with Figure \@ref(fig:wilson-data-12), we are going to store the updated plot as a variable. Try this code out on your own. 

```{r wilson-with-guess-12,fig.cap="Weight of the dog Wilson with initial guess $p_{1}=78$.",eval = FALSE}

# Define a data frame for W = f(D,78)
days <- seq(0, 1500, by = 1)

p1 <- 78
p2 <- 2.461935
p3 <- 0.017032

weight <- p1 / (1 + exp(p2 - p3 * days))
my_guess_78 <- tibble(days, weight)

### Now add our guess of p1 = 78 to the plot.
my_guess_plot <- wilson_data_plot +
  geom_line(
    data = my_guess_78, color = "red",
    aes(x = days, y = weight)
  )

my_guess_plot
```



Perhaps let's try another guess for $p_{1}$ a little lower than $p_{1}=78$. Re-run the prior code, but this time set $p_{1}=65$. Does that model better represent the `wilson` data?

## Likelihood ratios for parameter estimation {#lr-12}
We have two potential values for $p_{1}$ (78 or 65). While you may be able to decide which parameter is better "by eye", let's discuss a way to quantify this some more. How we do that is with the likelihood function for these data. We will apply the standard assumptions that the nineteen measurements of Wilson's weight over time are all independent and identically distributed, creating the following likelihood function (Equation \@ref(eq:wilson-like-12)): 

\begin{equation}
L(p_{1}) = \prod_{i=1}^{19} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(W_{i}-f(D_{i},p_{1}))^{2}}{2 \sigma^{2}}} (\#eq:wilson-like-12)
\end{equation}

We can easily compute the associated likelihood values for Equation \@ref(eq:wilson-like-12) with the `R` function `compute_likelihood` from Chapter \@ref(likelihood-09):


```{r}
# Define the model we are using
my_model <- weight ~ p1 / (1 + exp(p2 - p3 * days))


# Define a tibble for the two different estimates of p1
parameters <- tibble(
  p1 = c(78, 65),
  p2 = 2.461935,
  p3 = 0.017032
)

# Compute the likelihood and return the likelihood from the list
out_values <-
  compute_likelihood(my_model, wilson, parameters)$likelihood

# Return the likelihood from the list:
out_values
```

Hopefully this code seems familiar to you from Chapter \@ref(likelihood-09), but of note are the following:

- We want to compare two values of `p1`, so when we defined `parameters` we included the two values of $p_{1}$ when defining `parameters`. The same values of `p2` and `p3` will apply to both. Don't believe me? Type `parameters` at the console line to see!
- Recall that when we apply `compute_likelihood` a list is returned (`likelihood` and `opt_value`). For this case we just want the `likelihood` data frame, hence the code `$likelihood` at the end of `compute_likelihood`.

So we computed $L(78)$=`r out_values$l_hood[[1]]` and $L(65)$=`r out_values$l_hood[[2]]`. Since $L(65)>L(78)$ we would say $p_{65}$ is the more likely parameter value.

Notice how we computed $L(65)$ and $L(78)$ separately and then compared the two values. Another approach is to examine the *ratio* of the likelihoods (Equation \@ref(eq:like-ratio-12)):

\begin{equation}
\mbox{ Likelihood ratio: } \frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) } (\#eq:like-ratio-12)
\end{equation}

The utility of the likelihood ratio is that we can say that if the likelihood ratio is *greater* than 1, $p_{1}^{proposed}$ is preferred.\index{likelihood!ratio} If this ratio is *less* than 1, $p_{1}^{current}$ is preferred.

Applying Equation \@ref(eq:like-ratio-12) with $p_{1}^{proposed}=65$ and $p_{1}^{current}=78$, we have $\displaystyle \frac{ L(65) }{ L(78) }=$ `r round(out_values$l_hood[[2]]/out_values$l_hood[[1]])`, further confirming $p_{1}=65$ is more likely compared to the value of $p_{1}=78$.

### Iterative improvement with likelihood ratios
We can improve on estimating $p_{1}$ for Equation \@ref(eq:wilson-sat-12) by continuing to compute likelihood ratios. However, since $p_{1}=65$ is the more likely value (currently), then we will set $p_{1}^{current}=65$ for Equation \@ref(eq:like-ratio-12).

To simplify things, let's define a function that will quickly compute Equation \@ref(eq:wilson-sat-12) for this dataset:

```{r}
# A function that computes the likelihood ratio for Wilson's weight
likelihood_ratio_wilson <- function(proposed, current) {

  # Define the model we are using
  my_model <- weight ~ p1 / (1 + exp(p2 - p3 * days))


  # This allows for all the possible combinations of parameters
  parameters <- tibble(
    p1 = c(current, proposed),
    p2 = 2.461935,
    p3 = 0.017032
  )

  # Compute the likelihood and return the likelihood from the list
  out_values <-
    compute_likelihood(my_model, wilson, parameters)$likelihood

  # Return the likelihood from the list:
  ll_ratio <- out_values$l_hood[[2]] / out_values$l_hood[[1]]

  return(ll_ratio)
}

# Test the function out:
likelihood_ratio_wilson(65, 78)
```

You should notice that the reported likelihood ratio matches up with our earlier computations!  Perhaps a better guess for $p_{1}$ would be somewhere between 65 and 78. Let's try to compute the likelihood ratio for $p_{1}=70$ compared to $p_{1}=65$. Try computing `likelihood_ratio_wilson(70,65)` - you should see that it is about 7.5 million times more likely!


I think we are onto something - Figure \@ref(fig:all-three-wilson) compares the modeled values of Wilson's weight for the different parameters:

```{r all-three-wilson,fig.cap="Comparison of our three estimates for Wilson's weight over time.",echo=FALSE,warning=FALSE,message = FALSE}
# Define a data frame for W = f(D,65)
days <- seq(0, 1500, by = 1)

p2 <- 2.461935
p3 <- 0.017032

mass_1 <- 78 / (1 + exp(p2 - p3 * days))
mass_2 <- 65 / (1 + exp(p2 - p3 * days))
mass_3 <- 70 / (1 + exp(p2 - p3 * days))

my_guess_all <- tibble(days, `78` = mass_1, `65` = mass_2, `70` = mass_3) %>%
  pivot_longer(cols = c(-"days"))

# Now add our guess of p1 = 70 to the plot.
wilson_data_plot +
  geom_line(data = my_guess_all, aes(x = days, y = value, color = name)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name = expression(Value ~ of ~ p[1]))
```


So now, let's try $p_{1}=74$ and compare the likelihoods: $\displaystyle \frac{ L(74) }{ L(70) }$=`r likelihood_ratio_wilson(64,70)`. This seems to be *less* likely because the ratio was significantly less than one. If we are doing a hunt for the *best* optimum value, then perhaps we would reject $p_{1}=74$ and keep moving on, perhaps selecting another value closer to 70.

While rejecting $p_{1}=74$ as less likely, a word of caution is warranted. For non-linear problems we want to be extra careful that we do not accept a parameter value that leads us to a *local* (not global) optimum. A way to avoid this is to compare the calculated likelihood ratio to a uniform random number $r$ between 0 and 1.

At the `R` console type `runif(1)` - this creates one random number from the uniform distribution (remember the default range of the uniform distribution is $0 \leq p_{1} \leq 1$). The `r` in `runif(1)` stands for *random*. When I tried `runif(1)` I received a value of 0.126. Since the likelihood ratio is smaller than the random number I generated, we will *reject* the value of $p_{1}=74$ and try again, keeping 70 as our value.

The process to keep the proposed value based on some decision metric is called a *decision step*.

## The Metropolis-Hastings algorithm for parameter estimation
Section \@ref(lr-12) outlined an iterative approach of applying likelihood ratios to estimate $p_{1}$. Let's organize all the work in a table (Table \@ref(tab:mh-table-12)).

Table: (\#tab:mh-table-12) Organizational table of Metropolis-Hastings algorithm to estimate $p_{1}$ from the `wilson` dataset.

| Iteration | Current value of $p_{1}$  | Proposed value of $p_{1}$ | $\displaystyle\frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) }$ | Value of `runif(1)` | Accept proposed $p_{1}$?
|:------:|:-----:|:-----|:-----:|:-----:|:-----:|
| 0 | 78 | NA | NA | NA | NA
| 1 | 78 | 65 | 17.55936 | NA | yes
| 2 | 65 | 70 | 7465075 | NA | yes
| 3 | 70 | 74 | 0.09985308 | 0.126 | no
| 4 | 70 | ... | ... | ... | ...

Table \@ref(tab:mh-table-12) is the essence of what is called the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm).\index{Metropolis-Hastings algorithm} The goal of this algorithm method is to determine the parameter set that optimizes the likelihood function, or makes the likelihood ratio greater than unity. Here are the key components for this algorithm:


1. A defined likelihood function.
2. A starting value for your parameter.
3. A proposed value for your parameter.
4. Comparison of the likelihood ratios for the proposed to the current value ($\displaystyle \mbox{ Likelihood ratio: } \frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) }$). Parameter values that increase the likelihood will be preferred. 
5. A decision to accept the proposed parameter value. If the likelihood ratio is greater than 1, then we accept this value. However if the likelihood ratio is less than 1, we generate a random number $r$ (using `runif(1)`) and use this following process:
  - If $r$ is less than the likelihood ratio we **accept** (keep) the proposed parameter value. 
  - If $r$ is greater than the likelihood ratio we **reject** the proposed parameter value.


### Improving the Metropolis-Hastings algorithm
We have done the Metropolis-Hastings algorithm "by hand", which may seem tedious to do, but it helps build your own iterative understanding of the underlying process. Here's the good news: we can easily automate the Metropolis-Hastings algorithm, which we will explore in Chapter \@ref(mcmc-13). To note, there are several modifications we can do to make the Metropolis-Hastings algorithm a more efficient and robust method:


- While we have focused on implementation of the Metropolis-Hastings algorithm with one parameter, this is easily extended to sets of parameter values (e.g. estimating $p_{1}$, $p_{2}$, and $p_{3}$ in Equation \@ref(eq:wilson-sat-12)). However it may take longer to determine the global optimum because we change one parameter value at a time.
- As you would expect, the more times we iterate through this process, the better. Your initial guesses probably weren't that great (or close to the global optimum), so a common procedure is to throw out the first percentage of iterations and call that the "burn-in" period.
- Different (usually in parallel) "chains" of parameter estimates are used. Each chain starts from a different starting point. Once the number of iterations is reached, the final parameter set is chosen from the chain with the optimized likelihood. The practice of running multiple chains is a safeguard to prevent the algorithm converging on a local optimum.
- Since the likelihood ratio may generate large or small numbers, the ratio of log-likelihoods is usually implemented. A log-likelihood ratio then computes the *difference* between the proposed and current parameter values. Computing the difference between two numbers is computationally easier than dividing two numbers. Here is some sample code that implements the log-likelihood ratio:

```{r eval=FALSE}

# A function to computes the LOG likelihood ratio for Wilson's weight
log_likelihood_ratio_wilson <- function(proposed, current) {

  # Define the model we are using
  my_model <- weight ~ p1 / (1 + exp(p2 - p3 * days))


  # This allows for all the possible combinations of parameters
  parameters <- tibble(
    p1 = c(current, proposed),
    p2 = 2.461935,
    p3 = 0.017032
  )

  # Compute the likelihood and return the likelihood from the list
  # Notice we've set logLikely = TRUE to compute the log likelihood
  out_values <-
    compute_likelihood(my_model,
      wilson,
      parameters,
      logLikely = TRUE
    )$likelihood

  # Return the likelihood from the list
  # here we compute the DIFFERENCE of likelihoods:
  ll_ratio <- out_values$l_hood[[2]] - out_values$l_hood[[1]]

  return(ll_ratio)
}
```

- We can systematically explore the parameter space, where the jump distance changes depending on if we are always accepting new parameters or not. This process has several different implementations, but one is called *simulated annealing*.\index{simulated annealing}


## Exercises
```{exercise}
Using `likelihood_ratio_wilson`, explain why `likelihood_ratio_wilson(65,78)`$\neq$`likelihood_ratio_wilson(78,65)`.
```


```{exercise limit-12}
Show that $\displaystyle \lim_{D \rightarrow \infty} \frac{p_{1}}{1+e^{(p_{2}-p_{3}D)}} = p_{1}$. *Hint: use the fact that* $e^{A-B}= e^{A}e^{-B}$. Note that $p_{1}$, $p_{2}$, and $p_{3}$ are all positive parameters.
```

```{exercise}
Simplify the expression $\displaystyle \ln \left( \frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) } \right)$.
```

```{exercise wilson-more-12}
Using the dataset `wilson` from this chapter, complete 10 iterations of the Metropolis-Hastings algorithm by continuing Table \@ref(tab:mh-table-12). See if you can get the value of $p_{1}$ to 2 decimal places of accuracy. Be sure to include a plot of the data and the model with the final estimated value of $p_{1}$.
```


```{exercise}
Apply 10 iterations of the Metropolis-Hastings Algorithm to estimate $\theta$ for the nutrient equation $\displaystyle y=1.737 x^{1/\theta}$  using the dataset `phosphorous`, where $y=$`daphnia` and $x=$`algae`. You will first need to construct a likelihood ratio similar to the function `likelihood_ratio_wilson` or `log_likelihood_ratio_wilson` in this chapter. Compare your final estimated value of $\theta$ with the data in one plot.
```


```{exercise}
An alternative model for the dog's mass is the following differential equation:

\begin{equation}
\frac{dW}{dt} = -k (W-p_{1})
\end{equation}


a. Apply separation of variables and $W(0)=5$ and the value of $p_{1}$ from Exercise \@ref(exr:wilson-more-12) to determine the solution for this differential equation.
b. Apply 10 iterations of the Metropolis-Hastings algorithm to estimate the value of $k$ to three decimal places accuracy. The true value of $k$ is between 0 and 1.
c. Compare your final estimated value of $k$ with the data in one plot.

```


```{exercise full-linear-12}
Consider the linear model $y=6.94+bx$ for the following dataset:


| *x* | -0.365  | -0.14 | -0.53  | -0.035 | 0.272  |
|:------:|:-----:|:------:|:-----:|:------:|:-----:|
| *y* | 6.57 | 6.78  | 6.39  | 6.96  | 7.20 |

  
Apply 10 iterations of the Metropolis-Hastings algorithm to determine $b$.
```



```{exercise}
For the `wilson` dataset, repeat three steps of the parameter estimation to determine $p_{1}$ as in this chapter, but this time use `log_likelihood_ratio_wilson` to estimate $p_{1}$. Which function (`likelihood_ratio_wilson` or `log_likelihood_ratio_wilson`) do you think is easier in practice?
```

<!--chapter:end:12-metropolis.Rmd-->

# Markov Chain Monte Carlo Parameter Estimation {#mcmc-13}

We have explored likelihood functions, iterative methods, and the Metropolis-Hastings algorithm. In this chapter all these together introduce a sophisticated parameter estimation algorithm called Markov Chain Monte Carlo (MCMC) parameter estimation, which has a rich history [@richey_evolution_2010].\index{Markov Chain Monte Carlo}  MCMC methods can be highly computational; more importantly you already have the skills in place to understand *how* the MCMC method works. To do the heavy lifting we will rely on functions from the `demodelr` package. Let's get started!

## The recipe for MCMC
The MCMC approach is a systematic exploration to determine the set of parameters that optimizes the value of the log-likehood function, given the data. It may be helpful to think of the MCMC method as a recipe, and in order to "run" the MCMC method, you will need four key ingredients:

- _Model_: a function that we have for our dynamics (this is $\displaystyle \frac{d\vec{y}}{dt} = f(\vec{y},\vec{\alpha},t)$), or an empirical equation $\vec{y}=f(\vec{x},\vec{\alpha})$.
- _Data_: a data frame (`tibble`) or a spreadsheet file (to read into `R`) of the data you wish to use for parameter estimation.
- _Parameter bounds_: upper and lower bounds on your parameter values. We typically assume an initial uniform distribution on the parameters.
- _Initial conditions_: (optional) needed if your model is a differential equation.
- _Run diagnostics_: specifications for the MCMC code, which may include how long you will run the code and other aspects of the MCMC algorithm.

We will work step by step through two examples of an application of the MCMC algorithm using both a differential equation and an empirical model. Example code is provided so you can also run your own estimates. The workflow that we will use is:

\newpage

> Define the model, parameters, and data $\rightarrow$ Determine MCMC settings $\rightarrow$ Compute MCMC estimate $\rightarrow$ Analyze results.

Having an established workflow helps to breakdown the process step by step, making it easier to check for any coding errors.

## MCMC parameter estimation with an empirical model
The first step of our workflow is to "Define the model and parameters". Here we return to the problem exploring of the phosphorous content in algae (denoted by $x$) compared to the phosphorous content in daphnia (denoted by $y$), and estimating $c$ and $\theta$ from Equation \@ref(eq:phos-13).

\begin{equation}
y = c \cdot x^{1/\theta} (\#eq:phos-13)
\end{equation}

The parameters $c$ and $\theta$ from Equation \@ref(eq:phos-13) range from $0 \leq c \leq 2$ and $1 \leq \theta \leq 20$. To define the model we use similar code to how we defined models in Chapter \@ref(linear-regression-08). Then to define the parameters we will use a `tibble`, specifying the upper and lower bounds:

```{r}
## Step 1: Define the model and parameters
phos_model <- daphnia ~ c * algae^(1 / theta)

# Define the parameters that you will use with their bounds
phos_param <- tibble(
  name = c("c", "theta"),
  lower_bound = c(0, 1),
  upper_bound = c(2, 20)
)
```

Notice how we defined that `tibble` called `phos_param`, which has three columns: `name` which contains the name of the variables in our model (`c` and `theta`), the lower (`lower_bound`) and upper (`upper_bound`) for the parameters (listed in the same order as the parameters listed in `name`).

The data that we use is the dataset `phosphorous`, which is already located in the `demodelr` package).


The next two steps in our workflow (Determine MCMC settings $\rightarrow$ Compute MCMC estimate) are combined together below:

```{r, eval = FALSE}
## Step 2: Determine MCMC settings
# Define the number of iterations
phos_iter <- 1000

## Step 3: Compute MCMC estimate
phos_mcmc <- mcmc_estimate(
  model = phos_model,
  data = phosphorous,
  parameters = phos_param,
  iterations = phos_iter
)
```

The variable `phos_iter` specifies how many iterations we will run of the MCMC method. Notice that `mcmc_estimate` has several inputs, which for convenience we write on separate lines. There are four required inputs to the function `mcmc_estimate` and several predefined inputs; you will explore these further in Exercise \@ref(exr:mcmc-input-13).

The function `mcmc_estimate` may take some time (which is OK). But once it finishes a `tibble` is produced, which we call `phosphorous_mcmc` (run this code on your own):

```{r eval=FALSE}
glimpse(phos_mcmc)
```

Notice `phos_mcmc` contains four columns:

 - `accept_flag` tells you if at that particular iteration the MCMC estimate was accepted or not. This is a categorical variable of `TRUE` or `FALSE`
 - `l_hood` is the value of the likelihood for that given iteration.
 - The values of the parameters follow on the next few lines. Notice that $\theta$ is written as `theta` in the resulting data frame.

The final step of our workflow is to "Analyze results". Fortunately the `demodelr` package has a function called `mcmc_analyze` to help you:

```{r, eval=FALSE}
## Step 4: Analyze results:
mcmc_analyze(
  model = phos_model,
  data = phosphorous,
  mcmc_out = phos_mcmc
)
```

The function `mcmc_analyze` filters `phos_mcmc` whenever the variable `accept_flag` is `TRUE`. This function will compute parameter statistics (e.g. median and 95% confidence intervals) to be displayed at the console. In addition this function will generate two different types of graphs.^[To see each graph, you can click the left arrow button on the Plots tab in the lower right hand pane of the RStudio window.] Let's examine each one individually. The first plot (Figure \@ref(fig:phos-pp-13) is called a pairwise parameter plot, which is a collection of different plots together in a square matrix pattern, sized to the number of parameters that were estimated.


```{r phos-pp-13, echo=FALSE,out.width = "80%",fig.align='center',fig.cap="Pairwise parameter histogram from the MCMC parameter estimation with Equation \\ref{eq:phos-13}."}
knitr::include_graphics("figures/13-mcmc/histogram.png")
```

Along the diagonal of Figure \@ref(fig:phos-pp-13) is a histogram of the accepted parameter values from the Metropolis algorithm. Depending on the results that you obtain, you may have some interesting shaped histograms. Generally they are grouped in the following ways:

- *well-constrained:* the parameter takes on a definite, well-defined value. The parameter $c$ seems to behave like this.
- *edge-hitting:* the parameter seems to cluster near the edges of its value.
- *non-informative:* the histogram looks like a uniform distribution. The parameter $\theta$ has some indications of being edge hitting, but we would need more iterations in order to confirm.

The off-diagonal plots in Figure \@ref(fig:phos-pp-13) are interesting as well. These plots are a scatter plot of the accepted values for the two parameters in the particular row and column, and the upper off-diagonal reports the correlation coefficient $r$ from simple linear regression of the variables in that particular row and column. The asterisks (`*`) denote the degree of significance of the linear correlation.

Examining the off-diagonal terms in Figure \@ref(fig:phos-pp-13) helps ascertain the degree of *equifinality* in a particular set of variables [@beven_equifinality_2001].\index{equifinality}  In Figure \@ref(fig:phos-pp-13) it looks like as *c* increases, $\theta$ decreases. This degree of linear coupling means that we may not be able to independently resolve each parameter separately. Each model and parameter estimation is different, so be prepared to be surprised!

The presence of equifinality in a model does not mean the parameter estimation is a failure - just that we need to be aware of these relationships. Perhaps we may be able to go out in the field and measure a parameter (for example $c$) more carefully, narrowing the range of accepted values.

The second figure that is produced from `mcmc_analyze` displays an *ensemble* estimate of the model results with the data (Figure \@ref(fig:phos-out-13)). The ensemble average plot provides a high-level model-data overview. Figure \@ref(fig:phos-out-13) is generated when the model (`phos_mcmc`) is run for each accepted parameter estimate in `phos_mcmc`. At each of the data points in Figure \@ref(fig:phos-out-13) a [boxplot](https://ggplot2.tidyverse.org/reference/geom_boxplot.html) is produced from the model runs. The median, and the 25th and 75th percentile make up the box. The whiskers extend no further than 1.5 of the difference between the 25th and 75th percentile. By eye, it seems that the model and estimated parameters fit the data, but there is still wide variation in the model predictions. Perhaps this variation is caused by relative wide confidence intervals on our parameters (Figure \@ref(fig:phos-pp-13)).


```{r phos-out-13, echo=FALSE,out.width="80%",fig.align='center',fig.cap="Ensemble output results from the MCMC parameter estimation with Equation \\ref{eq:phos-13}."}

knitr::include_graphics("figures/13-mcmc/output-plot.png")
```

## MCMC parameter estimation with a differential equation model
Next let's try parameter estimation with a differential equation model. Here the measured data are solutions to a differential equation, which contains unknown parameters. Once the MCMC method proposes a parameter, then the differential equation needs to be solved numerically with Euler's or a Runge-Kutta method before evaluating the likelihood function.^[If we knew the function that solves the differential equation, then we would have an empirical model.]

The example that we are going to use relates to land use management, in particular a coupled system between a resource (such as a national park) and the number of visitors it receives [@sinay_simple_2006]. The tourism model relies on two nondimensional scaled variables, $R$ which is the amount of the resource (as a percentage) and $V$ the percentage of visitors that could visit (also as a percentage):

\begin{equation}
\begin{split}
\frac{dR}{dt}&=R\cdot (1-R)-aV \\ 
\frac{dV}{dt}&=b\cdot V \cdot (R-V)
\end{split} (\#eq:tourism-13)
\end{equation}

Equation \@ref(eq:tourism-13) has two parameters $a$ and $b$, which relate to how the resource is used up as visitors come ($a$) and how as the visitors increase, word of mouth leads to a negative effect of it being too crowded ($b$).

For this case we are going to use a pre-defined dataset of the number of resources and visitors to a national park as reported in @sinay_simple_2006 (this is the `parks` dataset in the `demodelr` package) which is plotted in Figure \@ref(fig:park-data-13).


```{r park-data-13,fig.cap="Scaled data on resources and visitors to a national park over time.",echo=FALSE}
parks %>%
  pivot_longer(cols = c("visitors", "resources")) %>%
  ggplot(aes(x = time, y = value)) +
  # geom_line(size = 0.75) +
  facet_grid(name ~ ., scales = "free_y") +
  geom_point(size = 2) +
  labs(x = "Time", y = "Proportion") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```

As the visitors $V$ increase in Figure \@ref(fig:park-data-13), the percentage of the resources $R$ decreases. Notably though, the data on both variables may be limited. In Figure \@ref(fig:park-data-13) the proportion of visitors ranges from 0.9 to 1, and the resources ranges up to 0.03. Perhaps from this limited dataset given we can estimate the parameters $a$ and $b$ and then forecast out as time increases. We will estimate the parameters $a$ and $b$ in Equation \@ref(eq:tourism-13) with the data shown in Figure \@ref(fig:park-data-13). We are going to assume that $0 \leq a \leq 30$ and $0 \leq b \leq 5$. We will still use the same workflow (Define the model, parameters, and data $\rightarrow$ Determine MCMC settings $\rightarrow$ Compute MCMC estimate $\rightarrow$ Analyze results) as we did in estimating parameters for an empirical model. Since this workflow was presented earlier we will combine the first three steps below: 


```{r, eval = FALSE}
## Step 1: Define the model, parameters, and data
# Define the tourism model
tourism_model <- c(
  dRdt ~ resources * (1 - resources) - a * visitors,
  dVdt ~ b * visitors * (resources - visitors)
)

# Define the parameters that you will use with their bounds
tourism_param <- tibble(
  name = c("a", "b"),
  lower_bound = c(10, 0),
  upper_bound = c(30, 5)
)

## Step 2: Determine MCMC settings
# Define the initial conditions
tourism_init <- c(resources = 0.995, visitors = 0.00167)

deltaT <- .1 # timestep length
n_steps <- 15 # must be a number greater than 1

# Define the number of iterations
tourism_iter <- 1000

## Step 3: Compute MCMC estimate
tourism_out <- mcmc_estimate(
  model = tourism_model,
  data = parks,
  parameters = tourism_param,
  mode = "de",
  initial_condition = tourism_init,
  deltaT = deltaT,
  n_steps = n_steps,
  iterations = tourism_iter
)
```

Notice how `mcmc_estimate` has some additional arguments. Most important is the option `mode "de"`, where `de` stands for *differential equation*. (The default mode is `emp`, or *empirical* model - like the `phosphorous` data set.)  If the `de` mode is specified, then you also need to define the initial conditions (`tourism_init`), $\Delta t$ (`deltaT`), and timesteps (`n_steps`) in order to generate the numerical solution. 

Visualizing the data also is done with `mcmc_analyze`:
```{r,eval=FALSE}
## Step 4: Analyze results
mcmc_analyze(
  model = tourism_model,
  data = parks,
  mcmc_out = tourism_out,
  mode = "de",
  initial_condition = tourism_init,
  deltaT = deltaT,
  n_steps = n_steps
)
```

Examining the parameter histograms (Figure \@ref(fig:tourism-pp-13)) shows $b$ to be well-constrained. The histogram for $a$ seems like it could be well-constrained - but we may need to run more iterations to confirm this.


```{r tourism-pp-13, echo=FALSE,out.width = "70%",fig.align='center',fig.cap="Pairwise parameter histogram of MCMC parameter estimation results with Equation \\ref{eq:tourism-13}."}
knitr::include_graphics("figures/13-mcmc/histogram-tourism.png")
```


The model results and confidence intervals show good agreement to the data (Figure \@ref(fig:tourism-out-13)). Additionally the model forecasts out in time confirming that as visitors increase, the resources in the national park will decrease due to overuse. In contrast to Figure \@ref(fig:phos-out-13), the black line in Figure \@ref(fig:tourism-out-13) represents the median and the grey shading is the 95% confidence interval for all timesteps defined in solving the model.

```{r tourism-out-13, echo=FALSE,out.width = "80%",fig.align='center',fig.cap="Ensemble output results from the MCMC parameter estimation for Equation \\ref{eq:tourism-13}."}
knitr::include_graphics("figures/13-mcmc/output-tourism.png")
```


## Timing your code
As you can imagine the more iterations we have the better our parameter estimates will be. However, this means the full estimate with that number of iterations will take some more time. Before doing that, you first should get an estimate for the length of time it takes to run this code. Fortunately `R` has a stopwatch function. Let's check this out with one iteration of the phosphorous dataset:

```{r}
# This "starts" the stopwatch
start_time <- Sys.time()

# Compute a single mcmc estimate
phosphorous1_mcmc <- mcmc_estimate(
  model = phos_model,
  data = phosphorous,
  parameters = phos_param,
  iterations = 1
)

# End the stopwatch
end_time <- Sys.time()

# Determine the difference between the start and end times
end_time - start_time
```

Timing the code for one iteration gives you a ballpark estimate for a full MCMC parameter estimate. If we were to run *N* MCMC iterations, a good benchmark would be to multiply the time difference (`end_time - start_time`) by *N*. Performance time varies by computer and the other programs / apps that are running at the same time. However, this gives you an estimate of what to expect.^[I am a big fan of "set it and forget it" - meaning I set up the code before I go to sleep and it is ready in the morning!]


## Further extensions to MCMC
For the examples in this chapter we limited the number of iterations to a smaller number to make the results computationally feasible. However we can extend the MCMC approach in two notable ways:

- One approach is to separate the data into two different sets - one for optimization and one for validation. In this approach the "optimization data" consists of a certain percentage of the original dataset, leaving the remaining to validate the forward forecasts. This is a type of cross-validation approach, and is generally preferred because you are demonstrating the strength of your model ability against non-optimized data.

- We also run multiple "chains" of optimization, starting from a different value in parameter space. What we do then after running each of these chains is to select the one with the best log-likelihood value, and run *another* MCMC iteration starting at that value.  The idea is with a different chain we have sampled the parameter space and are hopefully starting near an optimum value.

As you can see, the MCMC algorithm is an extremely powerful technique for parameter estimation. While MCMC may take additional time and programming skill to analyze - it is definitely worth it!



## Exercises
```{exercise}
Re-run both of the MCMC examples in this chapter, but increase the number of iterations to $10,000$. Analyze your results from both cases. How does increasing the number of iterations affect the posterior parameter estimates and their confidence intervals?  Does the log-likelihood value change?
```


```{exercise}
Time the MCMC parameter estimate for the `phosphorous` dataset for $1$ iteration. Then time the MCMC parameter estimate for $10$, $100$, and $1000$ iterations, recording the times for each one. Make a scatterplot with the number of iterations on the horizontal axis and time on the vertical axis. How would you characterize the relationship between the number of iterations and the time it takes to run the code? How long would it take to compute an MCMC estimate with $10,000$ iterations?
```

```{exercise mcmc-input-13}
The function `mcmc_estimate` has several other input variables that are set to default values. What are they and how would you explain their use? (*Hint:* to see the documentation associated with this function type `?mcmc_estimate` at the `R` console.)
```

```{exercise}
For the `parks` data (Equation (13.2)) studied in this chapter, compare the 1:1 and the posterior parameter plots (Figure \@ref(fig:tourism-pp-13)). Write a summary of each panel of the plot. Apply your understanding of equifinality and other observations to determine by how much you have estimated the parameters $a$ and $b$ from the data.


```


```{exercise yeast-v1-13}
Run an MCMC parameter estimation on the dataset `yeast` from @gause_experimental_1932, where the equation for the volume of yeast $V$ over time is given by the following equation for a yeast growing in isolation:

\begin{equation}
V = \frac{K}{1+e^{a-bt}},
\end{equation}

where $K$ is the carrying capacity, $a$ and $b$ respective rate constants.

a. Show that when $V(0)=0.45$, $a = \ln(K/0.45-1)$.
b. Rewrite the $V(t)$ equation without $a$.
c. With the `yeast` data, perform an MCMC estimate for this equation. (*Reminder:* $\ln(5)$) is implemented as `log(5)` in `R`.)

Use the following settings for your MCMC parameter estimation:
  
- $K:$ 1 to 20
- $b$: 0 to 1
- 1000 iterations


When setting up the MCMC method, be sure to name the variables in your model to match the `yeast` data frame.

d. Report all outputs from the MCMC estimation (this includes parameter estimates, confidence intervals, log-likelihood values, and any graphs). Compare your results to the results from Exercise \@ref(exr:yeast-v2-13).

```


```{exercise yeast-v2-13}
Another model for this growth of yeast is the function $\displaystyle V= K + Ae^{-bt}$.

a. Show that when $V(0)=0.45$, $A = K - 0.45$.
b. Rewrite the initial equation without $A$.
c. With the `yeast` data, apply an MCMC estimate for this equation.

Use the following settings for your MCMC parameter estimation:

- $K:$ 1 to 20
- $b$: 0 to 1
- 1000 iterations

When setting up the MCMC method, be sure to name the variables in your model to match the `yeast` data frame.

d. Report all outputs from the MCMC estimation (this includes parameter estimates, confidence intervals, log-likelihood values, and any graphs). Compare your results to the results from Exercise \@ref(exr:yeast-v1-13).

```



```{exercise}
Run an MCMC parameter estimation on the dataset `wilson` according to the following differential equation:

\begin{equation}
\frac{dP}{dt} = b(N-P),
\end{equation}

where $P$ represents the mass of the dog. Use the following settings for your MCMC parameter estimation:
  
- $N$: 60 to 90
- $b$: 0 to .01
- $P(0)=5$
- $\Delta t = 1$ day
- Number of timesteps: 1500
- Number of iterations: 1000

When setting up the MCMC method, be sure to name the variables in your model to match the `wilson` data frame. Be sure to report all outputs from the MCMC estimation (this includes parameter estimates, confidence intervals, log-likelihood values, and any graphs).

```

<!--chapter:end:13-mcmc.Rmd-->

# Information Criteria {#information-criteria-14}

```{r echo=FALSE}
# Load this up here because we need the rmse function
library(modelr)
```

In Exercises \@ref(exr:yeast-v1-13) and \@ref(exr:yeast-v2-13) of Chapter \@ref(mcmc-13) we introduced two different empirical models for fitting the growth of `yeast` $V$ over time $t$. One model is a logistic model ($\displaystyle V = \frac{K}{1+e^{a-bt}}$), whereas the second model is a saturating function ($\displaystyle V= K + Ae^{-bt}$). A plot comparing MCMC parameter estimates for the two models is shown in Figure \@ref(fig:yeast-compare-14).

```{r yeast-compare-14,fig.cap="Comparison of models for the growth of yeast in culture. Dots represent measured values from @gause_experimental_1932.",echo=FALSE}
yeast_out <- tibble(
  time = seq(0, 60, length.out = 200),
  model1 = 12.8 / (1 + exp(log(12.8 / 0.45 - 1) - 0.242 * time)),
  model2 = 14.7 + (0.45 - 14.7) * exp(-0.0493 * time)
) %>%
  pivot_longer(cols = c(-"time"))

ggplot() +
  geom_point(data = yeast, aes(x = time, y = volume), size = 2, color = "red") +
  geom_line(data = yeast_out, aes(x = time, y = value, color = name, linetype = name), size = 1) +
  labs(x = "Time (days)", y = bquote("Volume " (cm^3)), color = "Model", linetype = "Model") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(
    breaks = c("model1", "model2"),
    labels = c("Logistic model", "Saturating model"),
    name = NULL
  ) +
  scale_linetype_discrete(
    breaks = c("model1", "model2"),
    labels = c("Logistic model", "Saturating model"),
    name = NULL
  )
```


Figure \@ref(fig:yeast-compare-14) raises an interesting question. Sometimes we have multiple, convergent models to describe a context or situation. While having these different options is good, we also like to know which is the *best* model. How would you decide that?

This chapter focuses on objective criteria to assess what is called the *best approximating model* [@burnham_model_2002]. We will explore what are called *information criteria*, which is developed from statistical theory.\index{information criteria} Let's get started!

## Model assessment guidelines

The first step is to develop some guidelines and metrics for model evaluation. Here would be the start of a list of things to consider, represented as questions: 

- The model complexity - how many equations do we have?
- The number of parameters - a few or many?
- Do the model outputs match the data?
- How will model prediction compare to any newly collected measurements?
- Are the trends accurately represented (especially for timeseries data)?
- Is the selected model easy to use, simulate, and forecast?


I may have hinted at some of these guidelines in earlier chapters. These questions are related to one another - and answering these questions (or ranking criteria for them) is at the heart of the topic of _model selection_.

Perhaps you may be asking, why bother? Aren't more models better? Let's talk about a specific example, for which we return to the dataset `global_temperature` in the `demodelr` library. Recall this dataset represents the average global temperature anomaly relative to 1951-1980. When we did linear regression with this dataset in Chapter \@ref(linear-regression-08) the quadratic and cubic models were approximately the same (Figure \@ref(fig:global-temp-14)):

```{r global-temp-14,echo=FALSE,fig.cap="Comparison of global temperature anomaly dataset with various polynomial fitted models."}
regression_formula0 <- temperature_anomaly ~ 1 + year_since_1880
regression_formula1 <- temperature_anomaly ~ 1 + year_since_1880 + I(year_since_1880^2)
regression_formula2 <- temperature_anomaly ~ 1 + year_since_1880 + I(year_since_1880^2) + I(year_since_1880^3)
regression_formula3 <- temperature_anomaly ~ 1 + year_since_1880 + I(year_since_1880^2) + I(year_since_1880^3) + I(year_since_1880^4)
fit0 <- lm(regression_formula0, data = global_temperature)
fit1 <- lm(regression_formula1, data = global_temperature)
fit2 <- lm(regression_formula2, data = global_temperature)
fit3 <- lm(regression_formula3, data = global_temperature)

smooth_data0 <- data.frame(x = global_temperature[[1]], y = predict(fit0))
smooth_data1 <- data.frame(x = global_temperature[[1]], y = predict(fit1))
smooth_data2 <- data.frame(x = global_temperature[[1]], y = predict(fit2))
smooth_data3 <- data.frame(x = global_temperature[[1]], y = predict(fit3))

ggplot(data = global_temperature, aes(x = year_since_1880, y = temperature_anomaly)) +
  geom_point(color = "red", size = 1) +
  geom_line(data = smooth_data0, aes(x = x, y = y, color = "0"), size = 1.0, linetype = 1) +
  geom_line(data = smooth_data1, aes(x = x, y = y, color = "1"), size = 1.0, linetype = 2) +
  geom_line(data = smooth_data2, aes(x = x, y = y, color = "2"), size = 1.0, linetype = 3) +
  geom_line(data = smooth_data2, aes(x = x, y = y, color = "3"), size = 1.0, linetype = 4) +
  labs(
    x = "Year Since 1880",
    y = expression("Temperature anomaly "~`(`^o~C~`)`)
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 10),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 10)
  ) +
  scale_color_colorblind(labels = c("Linear", "Quadratic", "Cubic", "Quartic"), name = NULL)
```

The variation in the different model fits for Figure \@ref(fig:global-temp-14) shows how different, but similar, the model results can be depending on the choice of regression function. Table \@ref(tab:gt-ll-14) displays summary results for the log-likelihood and the root mean square error (RMSE).^[The root mean square error is the computed as $\displaystyle \sqrt{\frac{\sum (y_{i}-f(x_{i}))^{2}}{N}}$.] In some cases, the log-likelihood decreases (indicating a more likely model), supported by the decrease in the RMSE indicating the fitted model more closely matches the observations. However the decrease in the log-likelihood and the RMSE is changing less as the complexity of the model (i.e. a higher degree polynomial) increases.

Table: (\#tab:gt-ll-14) Comparison of model fits for global temperature anomaly dataset shown in Figure \@ref(fig:global-temp-14).

| Model | Log-likelihood^[Remember, log-likelihoods can be positive or negative; see Chapter \@ref(likelihood-09).] | RMSE |
| :-----------: | :-----------: | :-----------: |
Linear | `r round(logLik(fit0),digits=3)` | `r round(rmse(fit0,data=global_temperature),digits=3)` |
Quadratic | `r round(logLik(fit1),digits=3)` | `r round(rmse(fit1,data=global_temperature),digits=3)` |
Cubic | `r round(logLik(fit2),digits=3)` |`r round(rmse(fit2,data=global_temperature),digits=3)` |
Quartic | `r round(logLik(fit3),digits=3)` | `r round(rmse(fit3,data=global_temperature),digits=3)` |

Further model evaluation can be examined by the following:

- Compare the measured values of $\vec{y}$ to the modeled values of $\vec{y}$ in a 1:1 plot. Does $g$ do a better job predicting $\vec{y}$ than $f$?
- Related to that, compare the likelihood function values of $f$ and $g$. We would favor the model that has the lower log-likelihood.
- Compare the number of parameters in each model $f$ and $g$. We would favor the model that has the fewest number of parameters.

Given the above question, we can state the model selection problem as the following:

> When we have two $f(\vec{x}, \vec{\alpha})$ and $g(\vec{x}, \vec{\beta})$ for the data $\vec{y}$, how would we determine which one ($f$ or $g$ or perhaps another alternative model) is the best approximating model?


## Information criteria for assessing competing models
_Information criteria_ evaluate the tradeoff between model complexity (i.e. the number of parameters used) and with the log-likelihood (a measure of how well the model fits the data). There are several types of information criteria, but we are going to focus on two:

- The __Akaike Information Criterion__ ($AIC$, @akaike_new_1974) is the most commonly used information criteria: 

\begin{equation} 
AIC = -2 LL_{max} + 2 P
(\#eq:aic)
\end{equation}

- An alternative to the $AIC$ is the __Bayesian Information Criterion__ ($BIC$, @schwartz_estimating_1978)

\begin{equation}
BIC = -2 LL_{max} + P \ln (N)
(\#eq:bic)
\end{equation}

In Equations \@ref(eq:aic) and \@ref(eq:bic), $N$ is the number of data points, $P$ is the number of estimated parameters, and $LL_{max}$ is the log-likelihood for the parameter set that maximized the likelihood function. Equations \@ref(eq:aic) and \@ref(eq:bic) show the dependence on the log-likelihood function and the number of parameters. For both the $AIC$ and $BIC$  a lower value of the information criteria indicates greater support for the model from the data.

Notice how easy the $AIC$ and $BIC$ are to compute in Equations \@ref(eq:aic) and \@ref(eq:bic) (assuming you have the information at hand). When an empirical model fit is computed (i.e. using the command `lm`), `R` computes these easily with the functions `AIC` or `BIC`. To apply them you need to first do the model fit (with the function `lm`. Try this out by running the following code on your own:^[You can compute the log-likelihood with the function `logLik(fit)`, where `fit` is the result of your linear model fits.]:

```{r,eval=FALSE}
regression_formula <- temperature_anomaly ~ 1 + year_since_1880
fit <- lm(regression_formula, data = global_temperature)
AIC(fit)
BIC(fit)
```


Table \@ref(tab:gt-ic-14) compares $AIC$ and $BIC$ for the models fitted using the global temperature anomaly dataset:

<!-- Table: (\#tab:gt-ic-14) Comparison of the $AIC$ and $BIC$ for global temperature anomaly data shown in Figure \@ref(fig:global-temp-14). -->

Table:  (\#tab:gt-ic-14) Comparison of the $AIC$ and $BIC$ for global temperature anomaly dataset shown in Figure \@ref(fig:global-temp-14).

| Model      | $AIC$                          | $BIC$                         |
| :--------: | :----------------------------: | :----------------------------:| 
| Linear     | `r round(AIC(fit0),digits=3)`  | `r round(BIC(fit0),digits=3)` | 
| Quadratic  | `r round(AIC(fit1),digits=3)`  | `r round(BIC(fit1),digits=3)` | 
| Cubic      | `r round(AIC(fit2),digits=3)`  | `r round(BIC(fit2),digits=3)` | 
| Quartic    | `r round(AIC(fit3),digits=3)`  | `r round(BIC(fit3),digits=3)` | 

Table \@ref(tab:gt-ic-14) shows that the cubic model is the better approximating model for both the $AIC$ and the $BIC$.


## A few cautionary notes

- Information criteria are relative measures. In a study it may be more helpful to report the change in the information criteria, or even a ratio (see @burnham_model_2002 for a detailed analysis).
- Information criteria are not cross-comparable across studies. If you are pulling in a model from another study, it is helpful to re-calculate the information criteria.
- An advantage to the $BIC$ is that it measures tradeoffs between favoring a model that has the fewer number of data needed to estimate parameters. Other information criteria examine the distribution of the likelihood function and parameters.


__The upshot:__ Information criteria are _one_ piece of evidence to help you to evaluate the best approximating model. You should do additional investigation (parameter evaluation, model-data fits, forecast values) in order to help determine the best model.

## Exercises
```{exercise}
You are investigating different models for the growth of a yeast species in a population where $V$ is the rate of reaction and $s$ is the added substrate:

\begin{equation*}
\begin{split}
\mbox{Model 1: } & V =  \frac{V_{max} s}{s+K_{m}} \\
\mbox{Model 2: } & V = \frac{K}{1+e^{-a-bs}} \\
\mbox{Model 3: } & V= K + Ae^{-bs}
\end{split}
\end{equation*}

With a dataset of 7 observations you found that the log-likelihood for Model 1 is `26.426`, for Model 2 the log-likelihood is is `15.587`, and for Model 3 the the log-likelihood is `21.537`. Apply the $AIC$ and the $BIC$ to evaluate which model is the best approximating model. Be sure to identify the number of estimated parameters for each model.
```


```{exercise}
An equation that relates a consumer's nutrient content (denoted as $y$) to the nutrient content of food (denoted as $x$) is given by: $\displaystyle y = c x^{1/\theta}$, where $\theta \geq 1$ and $c$ are parameters. We can apply linear regression to the dataset $(x, \; \ln(y) )$, so the intercept of the linear regression equals $\ln(c)$ and the slope equals $1 / \theta$.


a. Show that you can write this equation as a linear equation by applying a logarithm to both sides and simplifying.
b. With the dataset `phosphorous`, take the logarithm of the `daphnia` variable and then determine a linear regression fit for your new linear equation. What are the reported values of the slope and intercept from the linear regression, and by association, $c$ and $\theta$?
c. Apply the function `logLik` to report the log-likelihood of the fit.
d. What are the reported values of the $AIC$ and the $BIC$?
e. An alternative linear model is the equation $y = a + b \sqrt{x}$. Use the R command `sqrt_fit <- lm(daphnia~I(sqrt(algae)),data = phosphorous)` to first obtain a fit for this model. Then compute the log-likelihood and the $AIC$ and the $BIC$. Of the two models (the log-transformed model and the square root model), which one is the better approximating model?

```


<!-- From Burnham and Anderson pg 135 of pdf -->
```{exercise}
(Inspired by @burnham_model_2002) You are tasked with the job of investigating the effect of a pesticide on water quality, in terms of its effects on the health of the plants and fish in the ecosystem. Different models can be created that investigate the effect of the pesticide. Different types of reaction schemes for this system are shown in Figure \@ref(fig:pesticide-ch3), where $F$ represents the amount of pesticide in the fish, $W$ the amount of pesticide in the water, and $S$ the amount of pesticide in the soil. The prime (e.g. $F'$, $W'$, and $S'$) represent other bound forms of the respective state. In all seven different models can be derived.

These models were applied to a dataset with 36 measurements of the water, fish, and plants. The table for the log-likelihood for each model is shown below:

| Model  | 1a  | 2a | 2b | 3a | 3b |4a | 4b |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Log-likelihood    | -90.105  | -71.986 | -56.869 |  -31.598 | -31.563 | -8.770 | -14.238 |





a. Use Figure \@ref(fig:pesticide-ch3) to identify the number of parameters for each model.
b. Apply the $AIC$ and the $BIC$ to the data in the above table to determine which is the best approximating model.


```

```{exercise}
Use the information shown in Table \@ref(tab:gt-ll-14) to compute (by hand) the $AIC$ and the $BIC$ for each of the models for the `global_temperature` dataset (there are 142 observations). Do your results conform to what is presented in Table \@ref(tab:gt-ic-14)? How far off are your results? What would be a plausible explanation for the difference?
```

<!--chapter:end:14-informationCriteria.Rmd-->

# (PART) Stability Analysis for Differential Equations {.unnumbered}  


# Systems of Linear Differential Equations {#linearsystems-15}

Here we delve into a deeper understanding of differential equations by examining long term stability of equilibrium solutions. As a first step, Chapter \@ref(linearsystems-15) focuses on *linear* systems of differential equations, such as Equation \@ref(eq:example-ch15):\index{differential equation!linear system}

\begin{equation}
\begin{split} 
\frac{dx}{dt} &= 2x \\ 
\frac{dy}{dt} &= x+y
\end{split} (\#eq:example-ch15)
\end{equation}

Equation \@ref(eq:example-ch15) is a linear system of differential equations \index{differential equation!linear system} because it does contain terms such as $y^{2}$ or $\sin(x)$ on the right hand side of the equation. This chapter focuses on visualizing the phase plane for linear systems and determining the equilibrium solutions. Let's get started!


## Linear systems of differential equations and matrix notation
Another way to express Equation \@ref(eq:example-ch15) is with matrix notation:\index{matrix notation}

\begin{equation}
\begin{split}
\begin{pmatrix} \frac{dx}{dt} \\ \frac{dy}{dt} \end{pmatrix} &=  \begin{pmatrix} 2x \\ x+y \end{pmatrix} \\
&=\begin{pmatrix} 2 & 0 \\ 1 &  1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}
\end{split} (\#eq:example-ch15a)
\end{equation}

(Note: we can also use the prime notation ($x'$ or $y'$) to signify  $\displaystyle \frac{dx}{dt}$ or $\displaystyle \frac{dy}{dt}$.) We can also represent Equation \@ref(eq:example-ch15a) in a compact vector notation: $\displaystyle \frac{ d \vec{x} }{dt} = A \vec{x}$, where for this example $\displaystyle \vec{A}=\begin{pmatrix} 2 & 0 \\ 1 &  1 \end{pmatrix}$.

Now let's generalize. A system of linear equations:

\begin{equation}
\begin{split}
\frac{dx}{dt} &= ax+by \\ 
\frac{dy}{dt} &= cx+dy
\end{split} 
\end{equation}

can be expressed in the following way:


\begin{equation}
\begin{pmatrix} \frac{dx}{dt} \\ \frac{dy}{dt} \end{pmatrix} =  \begin{pmatrix} ax+by \\ cx+dy \end{pmatrix} =  \begin{pmatrix} a & b \\ c &  d \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} (\#eq:example-ch15b)
\end{equation}

Equation \@ref(eq:example-ch15) is an example of a *coupled* system of equations, mainly due to the expression $\displaystyle \frac{dy}{dt}=x+y$. An example of an *uncoupled* system of equations would be $\displaystyle \frac{dx}{dt}=3x$ and $\displaystyle \frac{dy}{dt}=-2y$, which could be solved with separation of variables.\index{differential equation!coupled system}\index{differential equation!uncoupled system}


## Equilibrium solutions
Now let's discuss equilibrium solutions for Equation \@ref(eq:example-ch15). Recall equilibrium solutions are places where both $\displaystyle \frac{dx}{dt}=0 \mbox{ and } \frac{dy}{dt}=0$. Since $\displaystyle \frac{dx}{dt}=2x$, an equilibrium solution would be $x=0$.\index{equilibrium solution}  Substituting $x=0$ into $\displaystyle \frac{dy}{dt}=x+y$ also shows $y=0$ is the corresponding equilibrium solution for $y$. 

For a general linear system of differential equations, it might be helpful to imagine what we should _expect_ for an equilibrium solution. Think back to calculus - what types of functions have a derivative that equals zero?  (Hopefully constant functions comes to mind!) The equilibrium solution is then $x=0$ and $y=0$.

Here is an amazing fact: it turns out **any linear system of differential equations has the origin as its only equilibrium solution.**  One way to verify this fact is to examine the theory behind solutions for linear systems of equations in linear algebra. You might be wondering why there is all the fuss with equilibrium solutions - especially the origin ($x=0$ and $y=0$)^[Another name for the origin equilibrium solution is the _trivial equilibrium_. Can you see why it is trivial?]. So while equilibrium solutions are not a terribly interesting question at the moment, the _stability_ of solutions is. In order to understand what I mean by stability, let's re-examine how to generate phase planes from Chapter \@ref(coupled-06).

## The phase plane
The phase plane is helpful here to understand the stability of an equilibrium solution. Remember that the phase plane shows the motion of solutions, visualized as a vector. For the system we examined earlier let’s take a look at the phase plane. Here is some `R` code from the `demodelr` package to help you visualize the phase plane for Equation \@ref(eq:example-ch15), shown in Figure \@ref(fig:ex-ch15fig).^[It is okay to refer back to Chapter \@ref(coupled-06) for a refresher on how the `phaseplane` command works.]

```{r eval = FALSE}
# For a two variable system of differential equations
# we need to define dx/dt and dy/dt separately:


linear_eq <- c(
  dxdt ~ 2 * x,
  dydt ~ x + y
)

# Now we plot the solution.
phaseplane(
  system_eq = linear_eq,
  x_var = "x",
  y_var = "y"
)
```

```{r ex-ch15fig,fig.cap="phase plane for Equation \\@ref(eq:example-ch15).",echo=FALSE}
# For a two variable system of differential equations
# we need to define dx/dt and dy/dt separately:


linear_eq <- c(
  dxdt ~ 2 * x,
  dydt ~ x + y
)

# Now we plot the solution.
phaseplane(
  system_eq = linear_eq,
  x_var = "x",
  y_var = "y"
) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```

Notice how in the Figure \@ref(fig:ex-ch15fig) phaseline the arrows seem to spin out from the origin? We are going to discuss that below - but based on what we see we might expect the stability of the origin to be _unstable_. 

## Non-equilibrium solutions and their stability 
Even though we have already identified the equilibrium solution for Equation \@ref(eq:example-ch15), there are other non-equilibrium solutions. Here are two non-equilibrium solutions that you can verify:

- **Solution 1:** $\displaystyle s_{1}(t) = \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ e^{t} \end{pmatrix}$
- **Solution 2:** $\displaystyle s_{2}(t) = \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} e^{2t} \\ e^{2t} \end{pmatrix}$ 

These two non-equilibrium solutions can aid us in understanding the stability of the equilibrium solutions. All of the solutions contain terms that are exponential growth, indicating movement away from the equilibrium solution at $x=0$, $y=0$. This is even more evident when we plot the solutions with the phase plane by defining a new data frame and plotting $s_{1}(t)$ and $s_{2}(t)$ with `geom_path` (Figure \@ref(fig:ex-ch15fig-sol)):


```{r ex-ch15fig-sol,fig.cap="phase plane for Equation \\@ref(eq:example-ch15), with straight line solutions $s_{1}(t)$ and $s_{2}(t)$.",echo=FALSE,warning = FALSE, message = FALSE}


# Define the straight line solutions:
solution1 <- tibble(
  t = seq(0, 1, length.out = 20),
  x = 0,
  y = exp(t),
  soln = 1
)

solution2 <- tibble(
  t = seq(0, 2, length.out = 20),
  x = exp(2 * t),
  y = exp(2 * t),
  soln = 2
)

solution <- rbind(solution1, solution2)

# Our differential equation from before:
systems_eq <- c(
  dxdt ~ 2 * x,
  dydt ~ x + y
)

# Plot the solution, first saving to a variable:
phaseplane(systems_eq, "x", "y") +
  geom_path(data = solution, aes(x = x, y = y, color = as.factor(soln)), size = 2) +
  xlim(c(-4, 4)) + ylim(c(-4, 4)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(labels = c(expression("s"[1] * "(t)"), expression("s"[2] * "(t)")), name = "Solutions:")
```

The arrows in the phase plane of Figure \@ref(fig:ex-ch15fig-sol) show how $s_{1}(t)$ and $s_{2}(t)$ move away from the solution. The phase plane suggests for Equation \@ref(eq:example-ch15) that the equilibrium solution at the origin is unstable because _both_ the arrows in both directions seem to be pointing away from the origin. 

Also notice that in Figure \@ref(fig:ex-ch15fig-sol) the solutions $s_{1}(t)$ and $s_{2}(t)$ in the $xy$ plane are straight lines!  It turns out that these straight line solutions are quite useful - we will study them in a later chapter.

We can also investigate stability _algebraically_ for each solution ($s_{1}(t)$ and $s_{2}(t)$). We will organize our solutions in vector format, factoring out the exponential functions in each of the expressions:

- Solution 1: $\displaystyle  \vec{s}_{1}(t) = \begin{pmatrix} 0 \\ e^{t} \end{pmatrix}= \begin{pmatrix} 0 \\ e^{t} \end{pmatrix} =e^{t}  \begin{pmatrix} 0 \\ 1  \end{pmatrix}$
- Solution 2:  $\displaystyle \vec{s}_{2}(t) = \begin{pmatrix} e^{2t} \\ e^{2t} \end{pmatrix}= \begin{pmatrix} e^{2t} \\ e^{2t} \end{pmatrix} = e^{2t}  \begin{pmatrix} 1 \\ 1\end{pmatrix}$ 

The vectors $\displaystyle  \begin{pmatrix} 0 \\ 1\end{pmatrix}$ and $\displaystyle  \begin{pmatrix} 1 \\ 1\end{pmatrix}$ are the lines $x=0$ and $y=x$, as shown in Figure \@ref(fig:ex-ch15fig-sol). By factoring out the exponential functions we can see the straight line solutions!  

To further investigate stability we investigate the long term behavior of the exponential functions that are multiplying the straight line solutions. Notice that $\displaystyle \lim_{t \rightarrow \infty}   e^{t}$ and $\displaystyle \lim_{t \rightarrow \infty}   e^{2t}$ *both* do not have a finite value^[In other words, $\lim_{t \rightarrow \infty}  e^{t}=\infty$ and $\lim_{t \rightarrow \infty}  e^{2t}=\infty$.], so we conclusively classify the equilibrium solution as “unstable".\index{equilibrium solution!unstable} Conversely, if *both* of the exponential functions had exponential decrease we would classify the equilibrium solution as "stable". \index{equilibrium solution!stable}

Finally, with these straight line solutions we can create other solutions as a linear combination of $s_{1}(t)$ and $s_{2}(t)$. For example, we can define another solution which we will call $s_{3}(t)$, where $\vec{s}_{3}(t)=\vec{s}_{1}(t) -0.1 \vec{s}_{2}(t)$:


```{r ex-ch15fig-sol-2,fig.cap="Phase plane for Equation \\@ref(eq:example-ch15), with straight line solutions $s_{1}(t)$, $s_{2}(t)$, and $s_{3}(t)$.",,warning = FALSE, message = FALSE,echo=FALSE}

# Define the straight line solutions:
solution1 <- tibble(
  t = seq(0, 2, length.out = 20),
  x = 0,
  y = exp(t),
  soln = 1
)

solution2 <- tibble(
  t = seq(0, 2, length.out = 20),
  x = exp(2 * t),
  y = exp(2 * t),
  soln = 2
)

solution3 <- solution2 %>%
  mutate(
    x = solution1$x - 0.1 * x,
    y = solution1$y - 0.1 * y,
    soln = 3
  )



solution <- rbind(solution1, solution2, solution3)

# Our differential equation from before:
systems_eq <- c(
  dxdt ~ 2 * x,
  dydt ~ x + y
)

# Plot the solution, first saving to a variable:
phaseplane(systems_eq, "x", "y") +
  geom_path(data = solution, aes(x = x, y = y, color = as.factor(soln)), size = 2) +
  xlim(c(-4, 4)) + ylim(c(-4, 4)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(
    labels = c(
      expression("s"[1] * "(t)"),
      expression("s"[2] * "(t)"),
      expression("s"[3] * (t))
    ),
    name = "Solutions:"
  )
```


So to recap the following about straight line solutions to two-dimensional linear systems:

- Straight line solutions have the form $\displaystyle \vec{s}(t) = e^{\lambda t} \cdot \vec{v}$. Methods to determine $\lambda$ and $\vec{v}$ will be studied in later chapters.

- For a two-dimensional linear system, you generally will have two straight line solutions $\vec{s}_{1}$ and $\vec{s}_{2}$. This means you will have two different values of $\lambda$ ($\lambda_{1}$ and $\lambda_{2}$). The most general solution to the system of differential equations is the linear sum of the $s_{1}(t)$ and $s_{2}(t)$:
$\vec{x}(t) = c_{1} \cdot \vec{s}_{1}(t) + c_{2} \cdot \vec{s}_{2}(t)$.
- If both values of $\lambda$ are *greater* than 0, the equilibrium solution is *unstable*.
- If both values of $\lambda$ are *less* than 0, the equilibrium solution is *stable*.
- Geometrically these straight line solutions are lines that pass through the origin in the $xy$ plane.

In the exercises you will look at additional examples to understand the behavior of linear systems.


## Exercises

<!-- Used code coeff_cal in rScripts/15-linear-systems-code -->
```{exercise matrix}
Write the following systems of equations in matrix notation ($\displaystyle \frac{ d \vec{x} }{dt} = A \vec{x}$):

a. $\displaystyle \frac{dx}{dt} = 2x-6y, \;  \frac{dy}{dt} = x-2y$
b. $\displaystyle \frac{dx}{dt} = 9x-22y, \;  \frac{dy}{dt} = 3x-7y$
c. $\displaystyle \frac{dx}{dt} = 4x - 2y, \;  \frac{dy}{dt} = 2x - 2y$
d. $\displaystyle \frac{dx}{dt}= 4x-15y, \; \frac{dy}{dt}=2x-7y$
e. $\displaystyle \frac{dx}{dt} = 3x-18y, \;  \frac{dy}{dt} = x-5y$
f. $\displaystyle \frac{dx}{dt} = 5x-12y, \;  \frac{dy}{dt} = x-2y$

```


```{exercise}
Verify that $\displaystyle s_{1}(t) =  \begin{pmatrix} 0 \\ e^{t} \end{pmatrix}$ and  $\displaystyle s_{2}(t) = \begin{pmatrix} e^{2t} \\ e^{2t} \end{pmatrix}$ are both solutions for Equation \@ref(eq:example-ch15).

```


```{exercise}
Verify that $\displaystyle s_{3}(t) = \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} -0.1e^{2t} \\ e^{t} - 0.1e^{2t} \end{pmatrix}$ is a solution for Equation \@ref(eq:example-ch15).

```


```{exercise}
Generate a new solution for Equation \@ref(eq:example-ch15) that is a linear combination of $s_{1}(t)$ and $s_{2}(t)$ and plot your solution with the phase plane for Equation \@ref(eq:example-ch15).
```


```{exercise}
Verify that $x=0$, $y=0$, and $z=0$ are solutions to the differential equation


\begin{equation}
\begin{split}
\frac{dx}{dt} &= 5x-4y + z \\ 
\frac{dy}{dt} &= y - 9z \\
\frac{dz}{dt} &= 7x-z \\
\end{split}
\end{equation}

```



```{exercise}
Explain why we call $x=0$ and $y=0$ _equilibrium_ solutions to the general linear differential equation $\displaystyle \frac{ d \vec{x} }{dt} = A \vec{x}$. In other words, why is the word _equilibrium_ important?  (Hint: Think about what the solution curves for $x$ and $y$ would be in this case.)
```




```{exercise}
Generate a phase plane for each of the systems in Exercise \@ref(exr:matrix) and classify the stability of the equilibrium solution at $x=0$ and $y=0$ as stable, unstable, or uncertain, providing reasons for your conclusion.

```


```{exercise}
This problem considers the differential equation 

\begin{equation}
\begin{split}
\frac{dx}{dt} &= x+y \\ 
\frac{dy}{dt} &= y-x
\end{split}
\end{equation}


a. Use the command `phaseplane` to create a phase plane of this differential equation.
b. Using the option `plot_points`, change the number of arrows shown to 5 and 20 (2 different plots). What do you notice about the updated phase plane?
c. Change the viewing window (`x_window` and `y_window`) from the default to minus 10 to 10 in both axes. Now change the number of arrows shown to 5 and 20 (2 different plots). What do you notice about the updated phase plane?

```


<!-- Adapted from LW pg 164 -->
<!-- tr = -a det = 1 -->
```{exercise}
Consider the following differential equation:
  
\begin{equation}
\begin{split}
\frac{dx}{dt} &= -ax-y \\ 
\frac{dy}{dt} &= x
\end{split}
\end{equation}


a. Write this system in the form $\displaystyle \frac{d\vec{x}}{dt}=A \vec{x}$.
b. Let $a= -3, \; -1, \; 0, \; 1, \; 3$. Generate a phase plane for each of these values of $a$.
c. With each of your phase plane plots, characterize the behavior of the equilibrium solution as $a$ changes.

```








<!-- %\marginnote{Exercises a-c,e Based on Example 4.9 page 166 in \lw. d on 1c, pg 163 in lw}  -->



```{exercise}
Consider the following differential equation:

\begin{equation}
\begin{split}
  \frac{dx}{dt} &= -y \\ 
  \frac{dy}{dt} &= x
\end{split}
\end{equation}



a. Generate a phase plane diagram of this system. What do you notice?
b. Verify that $x(t)=A \cos(t)$ and $y(t)=A \sin(t)$ is a solution to this differential equation.
c. An equation of a circle of radius $R$ is $x^{2}+y^{2}=R^{2}$. Use implicit differentiation to differentiate this equation to get an expression for $\displaystyle \frac{dy}{dx}$. How does your solution compare to the ratio of $\displaystyle \frac{y'}{x'}$ from your differential equation? (Note: $\displaystyle y' = \frac{dy}{dt}$ and $\displaystyle x' = \frac{dx}{dt}$)
d. Verify that $x_{2}(t)=A \cos(t) + B \sin(t)$ and $y_{2}(t)=A \sin(t)-B \cos(t)$ also is a solution to the differential equation. Choose $A=1$ and $B=1$ to make a parametric plot of the solution. What do you notice in your parametric plot?

```

<!--chapter:end:15-linearSystems.Rmd-->

# Systems of Nonlinear Differential Equations {#nonlinear-16}


## Introducing nonlinear systems of differential equations
In Chapter \@ref(linearsystems-15) we discussed systems of linear equations. For this chapter we focus on _non_-linear systems of equations. We previously discussed coupled (nonlinear) systems of equations in Chapter \@ref(coupled-06), but we will dig in a little deeper here.

Consider the following nonlinear system of equations with the associated phase plane in Figure \@ref(fig:phase-ex1-16):

\begin{equation}
\begin{split} 
\frac{dx}{dt} &= y-1 \\  
\frac{dy}{dt} &= x^{2}-1 
\end{split} (\#eq:ex1-ch16)
\end{equation}

```{r phase-ex1-16,echo=FALSE,fig.cap="Phase plane for Equation \\@ref(eq:ex1-ch16)."  }
# Define the range we wish to use to evaluate this vector field

system_eq <- c(
  dx ~ y - 1,
  dy ~ x^2 - 1
)

phaseplane(system_eq, "x", "y", 
           x_window = c(-2, 2), 
           y_window = c(-2, 2)
           ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()

```

Wow!  The phase plane in Figure \@ref(fig:phase-ex1-16) looks really interesting. Let's dig into this deeper to understand the phase plane better.

## Zooming in on the phase plane
One way to investigate the phase plane is to zoom in on interesting chapters for Figure \@ref(fig:phase-ex1-16). In the upper left corner there is some swirling action, so let's zoom in somewhat (remember you can adjust the window size in `phaseplane` with the option `x_window` and `y_window`):

```{r zoom-ch16,fig.cap="Zoomed in phase plane for Equation \\@ref(eq:ex1-ch16)." ,echo=FALSE }
# Define the range we wish to use to evaluate this vector field

system_eq <- c(
  dx ~ y - 1,
  dy ~ x^2 - 1
)

phaseplane(system_eq, "x", "y", 
           x_window = c(-1.5, -0.5), 
           y_window = c(0.5, 1.5)
           ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
  
 
```

Something interesting seems to be happening at the point $(x,y)=(-1,1)$ in Figure \@ref(fig:zoom-ch16). Let's take a look at what happens if we evaluate our differential equation at $(x,y)=(-1,1)$:

\begin{equation}
\begin{split} 
\frac{dx}{dt} &= 1-1 = 0 \\ 
\frac{dy}{dt} &= (-1)^{2}-1  = 0
\end{split}
\end{equation}

Aha!  So the point $(-1,1)$ is an equilibrium solution. In later chapters we will discuss _why_ we are observing the behavior with the swirling arrows. For now, the key point from Figure \@ref(fig:zoom-ch16) is to recognize that _nonlinear systems_ can have nonzero equilibrium solutions.

Next, there seems to be a second interesting point in the upper right corner of Figure \@ref(fig:phase-ex1-16). Let's zoom in near the point $(x,y)=(1,1)$:

```{r zoom-ch16a,fig.cap="Another zoomed in phase plane for Equation \\@ref(eq:ex1-ch16).",echo=FALSE  }
# Define the range we wish to use to evaluate this vector field
system_eq <- c(
  dx ~ y - 1,
  dy ~ x^2 - 1
)

phaseplane(system_eq, "x", "y", 
           x_window = c(0.5, 1.5), 
           y_window = c(0.5, 1.5)
           ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```

It seems like there is a _second_ equilibrium solution at the point $(1,1)$!  Let's confirm this:

\begin{equation}
\begin{split} 
\frac{dx}{dt} &= 1-1 = 0 \\ 
\frac{dy}{dt} &= (1)^{2}-1  = 0
\end{split}
\end{equation}

By zooming in on the phase plane we learned something important about nonlinear systems and how they might differ compared to linear systems. In Chapter \@ref(linearsystems-15) we learned that the origin is the only equilibrium solution for a linear system of differential equations. On the other hand, nonlinear systems of equations may have _multiple_ equilibrium solutions.

## Determining equilibrium solutions with nullclines
To determine an equilibrium solution for a system of differential equations we first need to find the intersection of different nullclines. We do this by setting each of the rate equations ($\displaystyle \frac{dx}{dt}$ or $\displaystyle \frac{dy}{dt}$) equal to zero. Equation \@ref(eq:ex1-ch16) has two nullclines:

\begin{equation}
\begin{split} 
\frac{dx}{dt} = 0 &\rightarrow y-1 = 0\\  
\frac{dy}{dt} = 0 & \rightarrow x^{2}-1 = 0
\end{split} (\#eq:ex1-ch16-null)
\end{equation}

So, solving for both nullclines in Equation \@ref(eq:ex1-ch16-null) we have that $y=1$ or $x = \pm 1$. You can visually see the phase plane with the nullclines in Figure \@ref(fig:ex16-phaseplane), where we will add the nullclines and equilibrium solutions into the plot.

```{r ex16-phaseplane,fig.cap="Phase plane for Equation \\@ref(eq:ex1-ch16), with nullclines and equilibrium solutions shown.",echo=FALSE }
# Define the range we wish to use to evaluate this vector field

system_eq <- c(
  dx ~ y - 1,
  dy ~ x^2 - 1
)

phaseplane(system_eq, "x", "y", x_window = c(-2, 2), y_window = c(-2, 2)) + # The values in quotes are the labels for the axes
  stat_function(fun = function(x) 1, geom = "line", aes(colour = "x' = 0"), size = 1,linetype="dashed") +
  geom_vline(data = tibble(xint = c(-1, 1)), aes(xintercept = xint, colour = "y' = 0"), size = 1) + geom_point(data = tibble(xint = c(-1, 1), yint = c(1, 1)), aes(x = xint, y = yint), color = "red", size = 4) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_colour_colorblind(name = "Nullclines:",labels=c("x'=0","y'=0"))
```

In Figure \@ref(fig:ex16-phaseplane) we can see equilibrium solutions occur where a nullcline for $x'=0$ intersects with a nullcline where $y'=0$.

## Stability of an equilibrium solution
The idea of stability of an equilibrium solution for a nonlinear system is intuitively similar to that of a linear system: the equilibrium is stable when all the phase plane arrows point towards the equilibrium solution. For Equation \@ref(eq:ex2-ch16), the equilibrium solution at $(x,y)=(1,1)$ is _unstable_ because in Figure \@ref(fig:zoom-ch16a) some of the arrows point towards the equilibrium solution, whereas others point away from it. For Figure \@ref(fig:zoom-ch16) it is a little harder to tell stability of the equilibrium solution at $(x,y)=(-1,1)$. At this point we won't discuss more specifics of determining stable versus unstable equilibrium solutions. If the phase plane suggests that the equilibrium solution is stable or unstable, then you have established some good intuition that can be confirmed with additional analyses.

## Graphing nullclines in a phase plane


Let's look at another example, but this time we will focus on generating graphs for the nullclines.

\begin{equation}
\begin{split} 
\frac{dx}{dt} &= x-0.5yx \\  
\frac{dy}{dt} &= yx -y^{2} 
\end{split} (\#eq:ex2-ch16)
\end{equation}

Figure \@ref(fig:ex16-2-phaseplane) shows the phase plane for this example. Can you guess where an equilibrium solution would be?

```{r eval = FALSE }
# Define the range we wish to use to evaluate this vector field
system_eq <- c(
  dx ~ x - 0.5 * y * x,
  dy ~ y * x - y^2
)

p1 <- phaseplane(system_eq, "x", "y", 
                 x_window = c(0, 4),
                 y_window = c(0, 4)
                 )

p1
```

```{r ex16-2-phaseplane,fig.cap="Phase plane for Equation \\@ref(eq:ex2-ch16).",echo=FALSE  }
# Define the range we wish to use to evaluate this vector field
system_eq <- c(
  dx ~ x - 0.5 * y * x,
  dy ~ y * x - y^2
)

p1 <- phaseplane(system_eq, "x", "y", 
                 x_window = c(0, 4),
                 y_window = c(0, 4)
                 ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )


p1
```


Notice how the code used to generate Figure  \@ref(fig:ex16-2-phaseplane) stores the phase plane in the variable `p1` and the displays it. This will make things easier when we plot the nullclines.
Speaking of nullclines, let's find them:

\begin{equation}
\begin{split} 
\frac{dx}{dt} = 0 & \rightarrow x-0.5yx = 0 \\  
\frac{dy}{dt} = 0 & \rightarrow yx -y^{2} = 0
\end{split}
\end{equation}

The algebra is becoming a little more involved. Factoring $x-0.5yx = 0$ we have $x \cdot (1 - 0.5 y) = 0$, so either $x=0$ or $y=2$. Factoring the second equation we have $y \cdot (x - y) = 0$, so either $y=0$ or $x=y$. Notice how this second nullcline is a function of $x$ and $y$. The following code plots the phase plane (`p1`) along with the nullclines (try this code out on your own):

```{r eval = FALSE}

# Define the nullclines for dx/dt = 0 (red):

# x = 0
nullcline_x1 <- tibble(x = 0,
                       y=seq(0,4,length.out=100)
)

# y = 0.5
nullcline_x2 <- tibble(x = seq(0,4,length.out=100),
                       y=2
)

# Define the nullclines for dy/dt = 0 (blue):
# y = 0
nullcline_y1 <- tibble(x = seq(0,4,length.out=100),
                       y=0 
)

# y = x
nullcline_y2 <- tibble(x = seq(0,4,length.out=100),
                       y=x
)

# Add the nullclines onto the phase plane
p1 +
  geom_line(data = nullcline_x1,aes(x=x,y=y),color='red') +
  geom_line(data = nullcline_x2,aes(x=x,y=y),color='red') +
  geom_line(data = nullcline_y1,aes(x=x,y=y),color='blue') +
  geom_line(data = nullcline_y2,aes(x=x,y=y),color='blue')

```

For each nullcline we define a data frame (`tibble`) that encodes the relevant information so we can plot it. In order to accomplish this we defined a sequence of values ranging from the plot window of 0 to 4 for the *other* variable. For nullclines where $y$ was a function of $x$ we defined a sequence of values for $x$ and defined $y$ accordingly.

For Equation \@ref(eq:ex2-ch16) the equilibrium solutions are $(x,y)=(0,0)$, $(x,y)=(2,2)$. You may be tempted to think that $(0,2)$ is also an equilibrium solution - however - $x=0$ and $y=2$ are equations for the $x$ nullcline. It is easy to forget, but equilibrium solutions are determined from the intersection of _distinct_ nullclines.

Now that we have seen how nonlinear systems are different from linear systems, Chapter \@ref(jacobian-17) will introduce tools for analysis for the stability of equilibrium solutions.



## Exercises
```{exercise}
Equation \@ref(eq:ex2-ch16) equilibrium solutions are $(x,y)=(0,0)$, $(x,y)=(2,2)$. Zoom in on the phase plane at each of those points to determine the stability of the equilibrium solutions. (Set the window between $-0.5 \leq x \leq 0.5$ and $-0.5 \leq x \leq 0.5$ for the $(x,y)=(0,0)$ equilibrium solution.)
  
```

```{exercise}
Consider the following nonlinear system of equations, which is a modification of Equation \@ref(eq:ex1-ch16):
  
\begin{equation}
\begin{split}
\frac{dx}{dt} &= y-x  \\
\frac{dy}{dt} &= x^{2}-1
\end{split}
\end{equation}


a. What are the equations for the nullclines for this differential equation?
b. What are the equilibrium solutions for this differential equation?
c. Generate a phase plane that includes all equilibrium solutions (use the window  $-2 \leq x \leq 2$ and $-2 \leq y \leq 2$)
d. Based on the phase plane, evaluate the stability of the equilibrium solution.

```



```{exercise}
Consider the following nonlinear system of equations:
  
\begin{equation}
\begin{split}
\frac{dx}{dt} &= x - .5xy  \\
\frac{dy}{dt} &= .5yx-y
\end{split}
\end{equation}


a. What are the equations for the nullclines for this differential equation?
b. What are the equilibrium solutions for this differential equation?
c. Generate a phase plane that includes all equilibrium solutions.
d. Based on the phase plane, evaluate the stability of the equilibrium solution.


```


<!-- Modified from LW pg 157 158 #3 -->
```{exercise}
(Inspired by @logan_mathematical_2009) A population of fish $F$  has natural predators $P$. A model that describes this interaction is the following:

\begin{equation}
\begin{split}
\frac{dF}{dt} &= F - .3FP  \\
\frac{dP}{dt} &= .5FP - P
\end{split}
\end{equation}



a. What are the equations for the nullclines for this differential equation?
b. What are the equilibrium solutions for this differential equation?
c. Generate a phase plane that includes all the equilibrium solutions.
d. Based on the phase plane, evaluate the stability of the equilibrium solution.

```



```{exercise}
Consider the following system:

\begin{equation}
\begin{split}
\frac{dx}{dt} &= y^{2} \\
\frac{dy}{dt} &= - x
\end{split}
\end{equation}


a. What are the nullclines for this system of equations?
b. What is the equilibrium solution for this system of equations?
c. Generate a phase plane that includes the equilibrium solution. Set the viewing window to be $-0.5 \leq x \leq 0.5$ and $-0.5 \leq y \leq 0.5$.
d. Based on the phase plane, evaluate the stability of the equilibrium solution.

```



```{exercise}
The *Van der Pol Equation* is a second-order differential equation used to study radio circuits: $x'' + \mu \cdot (x^{2}-1) x' + x = 0$, where $\mu$ is a parameter.


a. Let $x'=y$ (note: $\displaystyle x' = \frac{dx}{dt}$). Show that with this change of variables the Van der Pol equation can be written as a system: 
  
\begin{equation}
\begin{split} 
\frac{dx}{dt} &= y \\ 
\frac{dy}{dt} &= -x-\mu \cdot  (x^{2}-1)y
\end{split}
\end{equation}

b. By determining the nullclines, verify that the only equilibrium solution is $(x,y)=(0,0)$.
c. Make a phase plane for different values of $\mu$ ranging from $-3$, $-1$, $0$, $1$, $3$. Set your $x$ and $y$ windows to range between $-1$ to $1$.
d. Based on the phase planes that you generate, evaluate the stability of the equilibrium solution as $\mu$ changes.


```


<!-- Strogatz pg 246, 259-260. LW pg 183 #11 - cell differentiation -->
```{exercise}
(Inspired by @strogatz_nonlinear_2015) Consider the following nonlinear system:

\begin{equation}
\begin{split}
\frac{dx}{dt} &= y-x \\
\frac{dy}{dt} &=-y + \frac{5x^2}{4+x^{2}}
\end{split}
\end{equation}


a. What are the equations for the nullclines?
b. Using desmos (or some other graphing utility), graph the two nullclines simultaneously. What are the intersection points?
c. Generate a phase plane for this system that contains all the equilibrium solutions.
d. Let's say instead that $\displaystyle \frac{dx}{dt} = bx-y$, where $b$ is a parameter such that $0 \leq b \leq 2$. Using desmos (or some other graphing utility), how many equilibrium solutions do you have as $b$ changes?


```


<!-- LW pg 182 -->
```{exercise}
(Inspired by @logan_mathematical_2009) Let $C$ be the amount of carbon in a forest ecosystem, with $P$ as the rate of increase in carbon due to photosynthesis. Herbivores $H$ consume carbon on the following predator-prey model:

\begin{equation}
\begin{split}
\frac{dC}{dt}&=P- bHC \\
\frac{dH}{dt} &= e\cdot bHC-dC
\end{split}
\end{equation}

In the above equation, $b$, $e$, and $d$ are all parameters greater than zero.

a. What are the equations for the nullclines?
b. Set $e=b=d=1$. Plot the equations of the nullclines. How many equilibrium solutions does this system have?
c. Determine the equilibrium solutions for this system of equations, expressed in terms of the parameters $b$, $e$, and $d$.

```





<!--chapter:end:16-nonlinearSystems.Rmd-->

# Local Linearization and the Jacobian {#jacobian-17}

Chapters \@ref(linearsystems-15) and \@ref(nonlinear-16) focused on systems of differential equations and using phase planes to determine a preliminary classification of any equilibrium solution. In this chapter we study local linearization and the associated Jacobian matrix.\index{local linearization}\index{Jacobian}  These tools are used to analyze stability of equilibrium solutions for a nonlinear system, thereby building a bridge between nonlinear and linear systems of equations. Let's get started!

## Competing populations
Let's take a look at a familiar example from Chapter \@ref(likelihood-09), specifically the experiments of growing different species of yeast together [@gause_experimental_1932]. Equation \@ref(eq:yeast-comp-17) (adapted from the one presented in @gause_experimental_1932) represents the volume of two species of yeast (which we will call $Y$ and $N$) growing in the same solution:

\begin{equation}
\begin{split}
\frac{dY}{dt} &= .2 Y \left( \frac{13-Y-2N}{13} \right)  \\  
\frac{dN}{dt} &= .06 N \left( \frac{6-N-0.4Y}{6} \right)  \\ 
\end{split} (\#eq:yeast-comp-17)
\end{equation}

While Equation \@ref(eq:yeast-comp-17) is a tricky model to consider, the terms $2N$ and $0.4Y$ represent the effect that $Y$ and $N$ have on each other since they are competing for the same resource. If these terms weren't present, both $Y$ and $N$ would follow a logistic growth (verify this on your own).

Equation \@ref(eq:yeast-comp-17) has 4 equilibrium solutions: $(Y,N)=(0,0)$, $(Y,N)=(13,0)$, $(Y,N)=(0,6)$, and $(Y,N)=(5,4)$ (Exercise \@ref(exr:yeast-ss-17)). Figure \@ref(fig:yeast-orig-17) shows the phase plane for this system along with the equilibrium solutions.^[I encourage you to use the `phaseplane` command to re-create Figure \@ref(fig:yeast-orig-17). If you do, be sure to use the options `x_window` and `y_window` to set things correctly.]

```{r yeast-orig-17,fig.cap="Phase plane for Equation \\@ref(eq:yeast-comp-17), with equilibrium solutions shown as red points.",echo=FALSE}
# Define the range we wish to evaluate this vector field
Y_window <- c(0, 20)
N_window <- c(0, 10)

yeast_eq <- c(
  dY ~ .2 * Y * (13 - Y - 2 * N) / 13,
  dN ~ .06 * N * (6 - N - 0.4 * Y) / 6
)

# Define the points where the
# equilibrium solutions are located
eq_solns <- tibble(
  Y = c(0, 13, 0, 5),
  N = c(0, 0, 6, 4)
)

# Reminder: The values in quotes are the labels for the axes
# We plot the phase plane and add on the points.
phaseplane(yeast_eq, "Y", "N",
  x_window = Y_window,
  y_window = N_window
) +
  geom_point(
    data = eq_solns,
    aes(x = Y, y = N),
    color = "red",
    size = 2
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```

Let's take a closer look at the phase plane near the equilibrium solution $(Y,N)=(5,4)$ in Figure \@ref(fig:yeast-closer):

```{r,label="yeast-closer",fig.cap="A zoomed in view of Equation \\@ref(eq:yeast-comp-17) near the $(Y,N)=(5,4)$ equilibrium solution.",echo=FALSE,warning=FALSE}
# Define the range we wish to evaluate this vector field
Y_window <- c(4, 6)
N_window <- c(3, 5)

yeast_eq <- c(
  dY ~ .2 * Y * (13 - Y - 2 * N) / 13,
  dN ~ .06 * N * (6 - N - 0.4 * Y) / 6
)

# Define the points where the
# equilibrium solutions are located
eq_solns <- tibble(
  Y = c(0, 13, 0, 5),
  N = c(0, 0, 6, 4)
)

# Reminder: The values in quotes are the labels for the axes
# We plot the phase plane and add on the points.
phaseplane(yeast_eq, "Y", "N",
  x_window = Y_window,
  y_window = N_window
) +
  geom_point(
    data = eq_solns,
    aes(x = Y, y = N),
    color = "red",
    size = 2
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind() +
  xlim(Y_window) +
  ylim(N_window)
```

The phase plane in Figure \@ref(fig:yeast-closer) _looks_ like this equilibrium solution is stable (the arrows seem to suggest a "swirling" into this equilibrium solution. However, another way to verify this is by applying a **locally linear approximation**. Better stated, because Equation \@ref(eq:yeast-comp-17) is a system of differential equations, we will construct a **tangent plane approximation** around $Y=5$, $N=4$. Let's review how to do that next.

## Tangent plane approximations
For a multivariable function $f(x,y)$, the tangent plane approximation \index{tangent!plane}  at the point $x=a$, $y=b$ is given by Equation \@ref(eq:tangent-plane-17):

\begin{equation}
L(x,y) = f(a,b) + f_{x}(a,b) \cdot (x-a) + f_{y}(a,b) \cdot (y-b), (\#eq:tangent-plane-17)
\end{equation}

where $f_{x}$ is the partial derivative of $f(x,y)$ with respect to $x$ and $f_{y}$ is the partial derivative of $f(x,y)$ with respect to $y$.

We will apply Equation \@ref(eq:tangent-plane-17) to Equation \@ref(eq:yeast-comp-17) at the equilibrium solution at $(Y,N)=(5,4)$. Since we have *two* equations, we need to compute two tangent plane approximations (one for each equation). The right hand sides for Equation \@ref(eq:yeast-comp-17) look complicated, but we can expand them to identify $f(Y,N)=.2Y - .03 Y N - .015 Y^{2}N$ and $g(Y,N)= .06N-.01 N^{2} - .004YN$.

First consider $f(Y,N)$. Let's compute the partial derivatives for $f(Y,N)$ at the equilibrium solution:


\begin{equation}
\begin{split}
f_{Y} = .2 - .03N - .03YN & \rightarrow f_{Y}(5,4)=-.52 \\
f_{N} = .03Y - .015Y^{2} & \rightarrow f_{N}(5,4)=-.225
\end{split}
\end{equation}

We also know that $f(5,4)=0$. As a result, the tangent plane approximation for $f(Y,N)$ is given by Equation \@ref(eq:tangent-plane-f-17):


\begin{equation}
f(Y,N) \approx  -.52 \cdot (Y-5) - .225 \cdot (N-4)  (\#eq:tangent-plane-f-17)
\end{equation}


Likewise if we consider $g(Y,N)= .06N-.01 N^{2} - .004YN$, we have:

\begin{equation}
\begin{split}
g_{Y} &= -.004N  \rightarrow g_{Y}(5,4)=-.016 \\
g_{N} &= .06 -.02N-.004Y  \rightarrow g_{N}(5,4)=-.04
\end{split}
\end{equation}

We also know that $g(5,4)=0$. As a result, the tangent plane approximation for $g(Y,N)$ is given by Equation \@ref(eq:tangent-plane-g-17):

\begin{equation}
g(H,L) \approx  -.016  \cdot (Y-5) -.04 \cdot (N-4) (\#eq:tangent-plane-g-17)
\end{equation}

So, at the equilibrium solution $(Y,N)=(5,4)$, Equation \@ref(eq:yeast-comp-17)  behaves like the following system of equations (Equation \@ref(eq:yeast-17-54)):

\begin{equation}
\begin{split}
\frac{dY}{dt} &= -.52 \cdot (Y-5) - .225 \cdot (N-4)\\
\frac{dN}{dt} &= -.016  \cdot (Y-5) -.04 \cdot (N-4)
\end{split} (\#eq:yeast-17-54)
\end{equation}



## The Jacobian matrix
Equation \@ref(eq:yeast-17-54) looks like a linear system of equations; however, we first need to re-define our system with the change of variables $y = Y-5$, $n = N-4$, which essentially is a shift of the variables so the equilibrium solution is at the origin (Equation \@ref(eq:yeast-17-54-shift)): 

\begin{equation}
\begin{split}
\frac{dy}{dt} &= -.52 y - .225 n\\
\frac{dn}{dt} &= -.016  y -.04 n
\end{split} (\#eq:yeast-17-54-shift)
\end{equation}

Then we can write Equation \@ref(eq:yeast-17-54-shift) in matrix form using the tools from Chapter \@ref(linearsystems-15):

\begin{equation}
\begin{pmatrix} y' \\ n' \end{pmatrix} =\begin{pmatrix} -.52 & -.225 \\ -.016 &  -.04 \end{pmatrix} \begin{pmatrix} y \\ n \end{pmatrix}
\end{equation}

We define the matrix $\displaystyle J= \begin{pmatrix} -.52 & -.225 \\ -.016 &  -.04 \end{pmatrix}$ as the Jacobian matrix.\index{Jacobian}

More generally, let's say we have the following system of differential equations, with an equilibrium solution at $(x,y)=(a,b)$:

\begin{equation}
\begin{split}
\frac{dx}{dt} &= f(x,y) \\
\frac{dy}{dt} &= g(x,y)
\end{split}
\end{equation}

The _Jacobian matrix_ at that equilibrium solution is:

\begin{equation}
J_{(a,b)} =\begin{pmatrix} f_{x}(a,b) & f_{y}(a,b) \\ g_{x}(a,b) &  g_{y}(a,b) \end{pmatrix} (\#eq:jacobian-17)
\end{equation}

The notation $J_{(a,b)}$ signifies the Jacobian matrix evaluated at the equilibrium solution $(x,y)=(a,b)$. The Jacobian matrix (Equation \@ref(eq:jacobian-17)) follows naturally from tangent plane approximations (Equation \@ref(eq:tangent-plane-17)). In later chapters we will use the Jacobian matrix to investigate stability of nonlinear systems.

The Jacobian matrix is part of the following system of linear equations (Equation \@ref(eq:linear-jacobian-17)):

\begin{equation}
\begin{split}
\frac{dX}{dt} &= f_{x}(a,b) \cdot X + f_{y}(a,b) \cdot Y \\
\frac{dY}{dt} &= g_{x}(a,b) \cdot X + g_{y}(a,b) \cdot Y 
\end{split} (\#eq:linear-jacobian-17)
\end{equation}

You may recognize that Equation \@ref(eq:yeast-17-54) was a specific example of Equation \@ref(eq:linear-jacobian-17). The variables $X=x-a$ and $Y=y-b$ help translate the tangent plane equation into a linear system. Also notice how Equation \@ref(eq:jacobian-17) does not include the terms $f(a,b)$ or $g(a,b)$ from Equation \@ref(eq:tangent-plane-17). This is because of the fact that we are building our tangent plane approximation at an equilibrium solution, so $f(a,b)=g(a,b)=0$! 

One more additional note: when we visualize the phase plane for a Jacobian matrix, we center the window at the origin $(x,y)=(0,0)$ (rather than at the equilibrium solution $(a,b)$) because Equation \@ref(eq:linear-jacobian-17) is a linear system, with an equilibrium solution at the origin. While we don't discuss it here, the Jacobian matrix also extends to higher order systems as well.


### An unstable equilibrium solution
Let's return to Equation \@ref(eq:yeast-comp-17) and investigate the Jacobian at the origin $(Y,N)=(0,0)$. The Jacobian matrix at that solution is the following:

\begin{equation}
J_{(0,0)} =\begin{pmatrix} .6 & 0 \\0 &  .2 \end{pmatrix} (\#eq:yeast-00-17)
\end{equation}

(You should verify this on your own). This Jacobian matrix leads to an uncoupled system of linear equations where $Y' = 0.2Y$ and $N'=0.6N$. In this case, both $Y$ and $N$ are growing exponentially. This type of behavior supports the idea that both $Y$ and $N$ are growing away from the origin, as indicated in Figure \@ref(fig:yeast-orig-17).

The Jacobian matrix helps confirm some of our intuition from examining the phase plane of a system of differential equations. At the heart of the Jacobian matrix is that idea that we can understand the dynamics of a nonlinear system through examining a closely related linear system. Chapter \@ref(eigenvalues-18) will continue to build on this idea, giving you a tool to quantitatively analyze the stability of an equilibrium solution.



## Exercises


```{exercise yeast-ss-17}
Using algebra, show that the 4 equilibrium solutions to Equation \@ref(eq:yeast-comp-17) are $(Y,N)=(0,0)$, $(Y,N)=(13,0)$, $(Y,N)=(0,6)$, and $(Y,N)=(5,4)$ *Hint:* Perhaps first determine the nullclines for each solution.
```

```{exercise}
Construct the Jacobian matrices for the equilibrium solutions $(Y,N)=(13,0)$ and $(Y,N)=(0,6)$ to Equation \@ref(eq:yeast-comp-17).
```

 
```{exercise}
By solving directly, show that $(H,L)=(0,0)$ and $(4,3)$ are equilibrium solutions to the following system of equations:

\begin{equation}
\begin{split}
\frac{dH}{dt} &= .3 H - .1 HL \\
\frac{dL}{dt} &=.05HL -.2L
\end{split}
\end{equation}
```


```{exercise}
A system of two differential equations has a Jacobian matrix at the equilibrium solution $(0,0)$ as the following:
  
  \begin{equation}
J_{(0,0)}=\begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}
\end{equation}

What would be a system of differential equations that would produce that Jacobian matrix?

```


```{exercise}
Consider the following nonlinear system:

\begin{equation}
\begin{split} 
\frac{dx}{dt} &= y-1 \\ 
\frac{dy}{dt} &= x^{2}-1
\end{split}
\end{equation}


a. Verify that this system has equilibrium solutions at $(-1,1)$ and $(1,1)$.
b. Determine the linear system associated with the tangent plane approximation at the equilibrium solution $(x,y)=(-1,1)$ and $(1,1)$ (two separate linear systems).
c. Construct the Jacobian matrix at the equilibrium solutions at $(-1,1)$ and $(1,1)$.
d. With the Jacobian matrix, visualize a phase plane at these equilbrium solutions to estimate stability of the equilibrium solution.


```



<!-- Strogatz pg 259-260. LW pg 183 #11 - cell differentiation -->
```{exercise}
(Inspired by @strogatz_nonlinear_2015) Consider the following nonlinear system:

\begin{equation}
\begin{split}
\frac{dx}{dt} &= y-x \\
\frac{dy}{dt} &=-y + \frac{5x^2}{4+x^{2}}
\end{split}
\end{equation}


a. Verify that the point $(x,y)=(1,1)$ is an equilibrium solution.
b. Determine the linear system associated with the tangent plane approximation at the equilibrium solution $(x,y)=(1,1)$.
c. Construct the Jacobian matrix at this equilibrium solution.
d. With the Jacobian matrix, visualize a phase plane at that equilbrium solution to estimate stability of the equilibrium solution.


```



```{exercise}
Consider the following system:

\begin{equation}
\begin{split}
\frac{dx}{dt} &= y^{2} \\
\frac{dy}{dt} &= - x
\end{split}
\end{equation}


a. Determine the equilibrium solution.
b. Visualize a phase plane of this system of differential equations.
c. Construct the Jacobian at the equilibrium solution.
d. Use the fact that $\displaystyle \frac{dy}{dt} / \frac{dx}{dt} = \frac{dy}{dx}$, which should yield a separable differential equation that will allow you to solve for a function $y(x)$. Plot several solutions of $y(x)$. How does that solution compare to the phase plane from the Jacobian matrix?

```



```{exercise}
The *Van der Pol Equation* is a second-order differential equation used to study radio circuits. In Chapter \@ref(nonlinear-16) you showed that the differential equations $x'' + \mu \cdot (x^{2}-1) x' + x = 0$, where $\mu$ is a parameter can be written as a system of equations:

\begin{equation}
\begin{split} 
\frac{dx}{dt} &= y \\ 
\frac{dy}{dt} &= -x-\mu (x^{2}-1)y
\end{split}
\end{equation}

a. Determine the general Jacobian matrix $J_{(x,y)}$ for this system of equations.
b. The point $(0,0)$ is an equilibrium solution. Evaluate the Jacobian matrix at the point $(0,0)$. Your Jacobian matrix will depend on $\mu$.
c. Evaluate your Jacobian matrix at the $(0,0)$ equilibrium solution for different values of $\mu$ ranging from $-3$, $-1$, 0, 1, 3.
d. Make a phase plane for the Jacobian matrices at each of the values of $\mu$.
e. Based on the phase planes that you generate, evaluate the stability of the equilibrium solution as $\mu$ changes.


```




<!-- Modified from LW pg 157 158 #3 -->
```{exercise}
(Inspired by @logan_mathematical_2009) A population of fish $F$  has natural predators $P$. A model that describes this interaction is the following:

\begin{equation}
\begin{split}
\frac{dF}{dt} &= F - .3FP  \\
\frac{dP}{dt} &= .5FP - P
\end{split}
\end{equation}


a. What are the equilibrium solutions for this differential equation?
b. Construct a Jacobian matrix for each of the equilibrium solutions.
c. Based on the phase plane from the Jacobian matrices, evaluate the stability of the equilibrium solutions.

```


```{exercise}
(Inspired by @pastor_mathematical_2008) The amount of nutrients (such as carbon) in soil organic matter is represented by $N$, whereas the amount of inorganic nutrients in soil is represented by $I$. A system of differential equations that describes the turnover of inorganic and organic nutrients is the following:

\begin{equation}
\begin{split}
\frac{dN}{dt} &= L + kdI - \mu N I - \delta N  \\
\frac{dI}{dt} &= \mu N I - k d I - \delta I
\end{split}
\end{equation}


a. Verify that $\displaystyle N = \frac{L}{\delta}, \; I = 0$ and $\displaystyle N = \frac{kd+\delta}{\mu}, \; I = \frac{L \mu - \delta k d - \delta^{2}}{\mu \delta}$ are equilibrium solutions for this system.
b. Construct a Jacobian matrix for each of the equilibrium solutions.


```



<!-- Adapted from LW pg 121 -->
```{exercise}
(Inspired by @logan_mathematical_2009 \& @kermack_contribution_1927) A model for the spread of a disease where people recover is given by the following differential equation:

\begin{equation}
\begin{split}
\frac{dS}{dt} &= -\alpha SI \\
\frac{dI}{dt} &= \alpha SI - \gamma I \\
\frac{dR}{dt} &= \gamma I
\end{split}
\end{equation}

Assume this population has $N=1000$ people.

a. Determine the equilibrium solutions for this system of equations.
b. Construct the Jacobian for each of the equilibrium solutions.
c. Let $\alpha=0.001$ and $\gamma = 0.2$. With the Jacobian matrix, generate the phase plane (using the equations for $\displaystyle \frac{dS}{dt}$ and $\displaystyle \frac{dI}{dt}$ only) for all of the equilibrium solutions and classify their stability.


```




```{tikz,glucose,warning=FALSE,message=FALSE,echo=FALSE,fig.cap="Glucose transporter reaction schemes.",fig.width=4,fig.height=3}


\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black,node distance = 3 cm]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=1.5cm]

%% keener pg 65 - need to adapt somewhat with se
\node [vspecies] (A) {$c_{e}$} ;
\node [vspecies, right of = A] (B) {$p_{e}$} ;
\node [vspecies, below of = A] (C) {$c_{i}$} ;
\node [vspecies, below of = B] (D) {$p_{i}$} ;
\node [right of = D,node distance = 1.5cm] (model) {inside};
\node [right of = B,node distance = 1.5cm] (model2) {outside};

\draw [->] ([yshift=3pt]A.east) --  node[above] {\small{$k_+ S_{e}$}} ([yshift=3pt]B.west) ;
\draw [->] ([yshift=-3pt]B.west) --  node[below] {\small{$k_-$}} ([yshift=-3pt]A.east) ;

\draw [->] ([yshift=3pt]C.east) --  node[above] {\small{$k_+ S_{i}$}} ([yshift=3pt]D.west) ;
\draw [->] ([yshift=-3pt]D.west) --  node[below] {\small{$k_-$}} ([yshift=-3pt]C.east) ;

\draw [->] ([xshift=-3pt]A.south) --  node[left] {\small{$k$}} ([xshift=-3pt]C.north) ;
\draw [->] ([xshift=3pt]C.north) --  node[right] {\small{$k$}} ([xshift=3pt]A.south) ;

\draw [->] ([xshift=-3pt]B.south) --  node[left] {\small{$k$}} ([xshift=-3pt]D.north) ;
\draw [->] ([xshift=3pt]D.north) --  node[right] {\small{$k$}} ([xshift=3pt]B.south) ;



\end{tikzpicture}

```



<!-- Problem adapted from Keener pg 64 and 65 2.4.1 glucose transport -->
```{exercise}
(Inspired by @keener_mathematical_2009) The chemical glucose is transported across the cell membrane using carrier proteins. These proteins can have different states (open or closed) that can be bound to a glucose substrate. The schematic for this reaction is shown in Figure \@ref(fig:glucose). The system of differential equations describing this reaction is:
  
\begin{equation}
\begin{split}
\frac{dp_{i}}{dt} &= k p_{e} - k p_{i} + k_{+} s_{i}c_{i}-k_{i}p_{i} \\
\frac{dp_{e}}{dt} &= k p_{i} - k p_{e} + k_{+} s_{e}c_{e}-k_{-}p_{e} \\
\frac{dc_{i}}{dt} &= k c_{e} - k c_{i} + k_{-} p_{i}-k_{+}s_{i}c_{i} \\
\frac{dc_{e}}{dt} &= k c_{i} - k c_{e} + k_{-} p_{e}-k_{+}s_{e}c_{e}
\end{split}
\end{equation}


a. We can reduce this to a system of three equations. First show that $\displaystyle \frac{dp_{i}}{dt} + \frac{dp_{e}}{dt}+ \frac{dc_{i}}{dt} + \frac{dc_{e}}{dt} = 0$. Given that $p_{i}+p_{e}+c_{i}+c_{e}=C_{0}$, where $C_{0}$ is constant, use this equation to eliminate $p_{i}$ and write down a system of three equations.
b. Determine the equilibrium solutions for this new system of three equations.
c. Construct the Jacobian matrix for each of these equilibrium solutions.

```

<!--chapter:end:17-jacobian.Rmd-->

# What are Eigenvalues? {#eigenvalues-18}

## Introduction

This chapter focuses on developing a tool to understand the stability of an equilibrium solution. This tool is determining eigenvalues and eigenvectors.\index{eigenvalue}\index{eigenvector} We connect eigenvectors and eigenvalues back to straight-line solutions introduced in Chapter \@ref(linearsystems-15). You will see how eigenvalues are determined is through solving a polynomial equation. Finally we investigate how the values of the eigenvalues are reflected in the directions of the arrows in the phase plane. There is a lot here to unpack, so let's get started! 


## Straight line solutions
Consider this following linear system of equations:

\begin{equation} (\#eq:system-ch18)
\begin{split} 
\frac{dx}{dt} &= 2x-y \\
\frac{dy}{dt} &= 2x+5 
\end{split}
\end{equation}

In Chapter \@ref(linearsystems-15) we identified two straight line solutions:

- Solution 1: $\displaystyle  \vec{s}_{1}(t) =e^{4t}  \begin{pmatrix} 1 \\ -2  \end{pmatrix}$
- Solution 2:  $\displaystyle \vec{s}_{2}(t) = e^{3t}  \begin{pmatrix} -1 \\ 1\end{pmatrix}$

Let's verify that Solution 2 is indeed a solution to this linear system. First we will take the derivative of Solution 2:

\begin{equation}
\frac{d}{dt} \left( \vec{s}_{2}(t)  \right) = 3 e^{3t}  \begin{pmatrix} -1 \\ 1\end{pmatrix} = \begin{pmatrix} -3e^{3t} \\ 3e^{3t} \end{pmatrix}
\end{equation}


Let's compare this solution to the right hand side of the differential equation, recognizing that the $x$ component of $\vec{s}_{2}(t)$ is $-e^{3t}$ and the $y$ component of $\vec{s}_{2}(t)$ is $e^{3t}$:
\begin{equation}
\begin{split}
2x-y &= -2e^{3t}-e^{3t} = -3e^{3t} \\
2x+5y &= -2e^{3t}+5e^{3t} = 3e^{3t}
\end{split}
\end{equation}

So, indeed $\vec{s}_{2}(t)$ _is_ a solution to the differential equation. However something interesting is occurring. Notice how $\displaystyle \frac{d}{dt} \left( \vec{s}_{2}(t)  \right)$ equals $3 \vec{s}_{2}(t)$, which was the same as the right hand side of the differential equation.

While we wrote the right hand side of Equation \@ref(eq:system-ch18) component by component, we could also write it as $A \vec{x}$, where $\displaystyle A = \begin{pmatrix} 2 & -1 \\ 2 &  5 \end{pmatrix}$. Because we verified $\vec{s}_{2}(t)$ was a solution to the differential equation, we could also have said that $A \vec{s}_{2}(t) = 3 \vec{s}_{2}(t)$.

So we have two interesting facts here:

- A straight line solution to a system of linear differential equations $(\displaystyle \frac{d \vec{x}}{dt} = A \vec{x}$) has the form $\vec{s}(t) = c_{1} e^{\lambda t} \; \vec{v}$, where $c_{1}$ is a constant and $\vec{v}$ a constant vector.
- Differentiating $\vec{s}(t)$ yields $\displaystyle \frac{d}{dt} \left(  \vec{s}(t) \right) = \lambda \vec{s}(t)$.
- Consequently $\lambda \vec{s}(t)=A \vec{s}(t)$ in order for $\vec{s}(t)$ to be a solution.

All of these facts (in particular $\lambda \vec{s}(t)=A \vec{s}(t)$) set up an interesting equation: $\displaystyle \lambda c_{1} e^{\lambda t}\vec{v} = c_{1} e^{\lambda t} A \vec{v}$. Applying concepts from linear algebra, in order for the solution $\vec{s}(t)$ to be consistent, $A \vec{v} - \lambda I \vec{v} = \vec{0}$, where $\vec{0}$ is a vector of all zeros and $I$ is called the _identity matrix_, or a square matrix with ones along the diagonal and zero everywhere else. The goal is to find a $\lambda$ and $\vec{v}$ consistent with this equation. \index{identity matrix}


Let's apply some terminology here. For these special straight line solutions, we give a particular name to $\vec{v}$ - we call it the *eigenvector*. The name we give to $\lambda$ is the *eigenvalue*. (Eigen means _own_ in German - get it?) \index{eigenvalue}\index{eigenvector}

So how do we _determine_ an eigenvalue or eigenvector? We do this by first determining the eigenvalues $\lambda$. This is done by solving the equation $\det (A - \lambda I ) =0$ for $\lambda$, where $\det(M)$ is the determinant. Once the eigenvalues are found, we then compute the eigenvectors by solving the equation $A \vec{v} - \lambda \vec{v} = \vec{0}$.

Let's take a time out. I recognize that we are starting to get deeper into linear algebra which may be some unfamiliar concepts. However we will just highlight key results that we will need - so hopefully that will give you a leg up when you study linear algebra - it is a great topic! Let's get to work.


## Computing eigenvalues and eigenvectors
Let's dig into understanding the equation $\det (A - \lambda I ) =0$ for a two-linear system of differential equations. In this case, $A$ is the matrix $\displaystyle  \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, for which then $A - \lambda I$ is the following  matrix:

\begin{equation} (\#eq:2by2-ch18)
A - \lambda I=\begin{pmatrix} a - \lambda & b \\ c & d-\lambda \end{pmatrix}
\end{equation}

The determinant of a 2 $\times$ 2 matrix is formed by the product of the diagonal entries less the product of the off-diagonal entries. For Equation \@ref(eq:2by2-ch18), $\det(A-\lambda I)=0$ is the equation $(a-\lambda)(d-\lambda)-bc=0$. Our goal is to solve this equation for $\lambda$, which are the eigenvalues for this system. 


```{example evec-18}
Compute the eigenvalues for the matrix $\displaystyle A =  \begin{pmatrix} -1 & 1 \\ 0 &  3 \end{pmatrix}$.
```

```{solution}
The matrix $A-\lambda I$ is
$\displaystyle A-\lambda I =  \begin{pmatrix} -1-\lambda & 1 \\ 0 &  3-\lambda \end{pmatrix}$. So we have:

\begin{equation}
\det(A-\lambda I) =  (-1-\lambda)(3-\lambda) - 0 = 0
\end{equation}

Solving the equation $(-1-\lambda)(3-\lambda)=0$ yields two eigenvalues: $\lambda = -1$ or $\lambda = 3$.
```



More generally the equation $\det(A - \lambda I)$ yields a polynomial equation in $\lambda$. We call this equation the _characteristic polynomial_ and denote it by $f(\lambda)$.\index{characteristic polynomial}  In the case of a two-dimensional system of equations, $f(\lambda)$ will be a quadratic equation (see Exercise \@ref(exr:ch-eq-exr-18)). 


Once we have determined the eigenvalues, we next compute the eigenvectors associated with each eigenvalue. Remember that an eigenvector is a vector $\vec{v}$ consistent with $A \vec{v} = \lambda \vec{v}$ or $A \vec{v} - \lambda \vec{v} =\vec{0}$. How we do this is through algebra, as is done in the following example:

```{example evec-18-2}
Compute the eigenvectors for the matrix $\displaystyle A =  \begin{pmatrix} -1 & 1 \\ 0 &  3 \end{pmatrix}$ from Example \@ref(exm:evec-18).
```

```{solution}
First for general $\lambda$, consider the expression $A \vec{v} - \lambda \vec{v} =\vec{0}$, where $\displaystyle \vec{v} = \begin{pmatrix} v_{1} \\ v_{2} \end{pmatrix}$:

\begin{equation}
 A \vec{v} - \lambda \vec{v}  =  \vec{0} \rightarrow \begin{pmatrix} -v_{1} +v_{2} - \lambda v_{1} = 0  \\ 3v_{2} - \lambda v_{2} =0 \end{pmatrix} (\#eq:eval-ex-ch18)
\end{equation}

We use the two expressions ($-v_{1} +v_{2} - \lambda v_{1} = 0$ and $3v_{2} - \lambda v_{2} =0$) in Equation \@ref(eq:eval-ex-ch18) to determine the eigenvector $\vec{v}$. We need to consider both eigenvalues ($\lambda = -1$ and $\lambda =3$) separately to yield two different eigenvectors: 

| - **Case 1 $\lambda = -1$**: The first expression in  Equation \@ref(eq:eval-ex-ch18) yields $-v_{1} +v_{2} + v_{1} = 0$, or $v_{2}=0$ after simplifying. For the second expression we have $3v_{2} + v_{2} =0 \rightarrow 4v_{2} = 0$, so that tells us again that $v_{2}=0$. Notice we've determined a value for the second component $v_{2}$, but not $v_{1}$. In this case, we say that the first component to the vector $\vec{v}$ is a *free parameter*.\index{parameter!free}, so the eigenvector can be expressed as $\displaystyle \begin{pmatrix} c_{1} \\ 0 \end{pmatrix}$, where $c_{1}$ is the free parameter. Another way to express this eigenvector is $\displaystyle c_{1} u_{1}$, with $u_{1}=\begin{pmatrix} 1 \\ 0 \end{pmatrix}$. The eigenvector in this case is $\displaystyle \begin{pmatrix} 1 \\ 0 \end{pmatrix}$. (We generally write eigenvectors without the arbitrary constants.) This particular straight line solution is $\displaystyle s_{1}(t)=c_{1}e^{-t}u_{1}$, where $c_{1}$ is a free variable.


&nbsp;

| - **Case 2 $\lambda = 3$**: For the second equation we have $3v_{2} - 3v_{2}=0$, which is always true. However in the first equation we have $- v_{1} + v_{2} - 3v_{1} = 0$, or $v_{2} = 4v_{1}$. In this case, $v_{1}$ can be a free parameter; however, $v_{2}$ will have to be four times the value of $v_{1}$. Hence, this particular straight line solution is $\displaystyle s_{2}(t)=e^{3t} \begin{pmatrix} c_{2} \\ 4 c_{2} \end{pmatrix}$, or also as $\displaystyle s_{2}(t)=c_{2} e^{3t} \vec{u}_{2}$, with $\displaystyle \vec{u}_{2}= \begin{pmatrix} 1 \\ 4 \end{pmatrix}$. The eigenvector in this case is $\displaystyle \begin{pmatrix} 1 \\ 4 \end{pmatrix}$.

| Notice that in both of our cases we had a free variable ($c_{1}$ or $c_{2}$), which are also constants in our final solution for the differential equation.
```

Once we have computed the eigenvalues and eigenvectors, we are now ready to express the most general solution for a system of differential equations. For a two-dimensional system of linear differential equations ($\displaystyle \frac{d}{dt} \vec{x} = A \vec{x}$), the most general solution is given by Equation \@ref(eq:gen-soln-18):

\begin{equation}
\vec{x}(t) = c_{1} e^{\lambda_{1}t} \vec{v}_{1} + c_{2} e^{\lambda_{2}t} \vec{v}_{2} (\#eq:gen-soln-18)
\end{equation}

```{example}
What is the solution to the differential equation $\displaystyle \frac{d}{dt} \vec{x} =  \begin{pmatrix} -1 & 1 \\ 0 &  3 \end{pmatrix} \vec{x}$?
```

```{solution}
Since we have already computed the eigenvalues and eigenvectors, our most general solution for this differential equation is:

\begin{equation*}
\vec{x} = c_{1} e^{- t} \begin{pmatrix} 1 \\ 0 \end{pmatrix}  + c_{2} e^{3t} \begin{pmatrix} 1 \\ 4 \end{pmatrix},
\end{equation*}

with $c_{1}$ and $c_{2}$ defined as constants.
```

### Computing eigenvalues with demodelr
While computing eigenvalues and eigenvectors is a good algebraic exercise, we can also program this in `R` using the function `eigenvalues` from the `demodelr` package. The syntax works where $\displaystyle A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ is entered in as `eigenvalues(a,b,c,d,matrix_rows)` where `matrix_rows` is the number of rows.^[If you have a 2 by 2 matrix, you can leave out `matrix_rows` (so just `eigenvalues(a,b,c,d)`) as the default is a 2 by 2 matrix.] What gets returned from the function will be the eigenvalues and eigenvectors for any square matrix.

Let's compute the eigenvalues for the matrix $\displaystyle \begin{pmatrix} -1 & 1 \\ 0 & 3 \end{pmatrix}$:

```{r}
# For a two-dimensional equation the code assumes 
# the default is a 2 by 2 matrix.

demodelr::eigenvalues(matrix_entries = c(-1, 1, 0, 3),
            matrix_rows = 2)

```


Notice that the eigenvalues and the eigenvectors get returned with the `eigenvalues` function. How you read the output for the eigenvector is that `X1` is the eigenvector associated with the first eigenvalue ($\lambda = 3$) and `X2` is the eigenvector associated with the second eigenvalue ($\lambda = -1$). The eigenvector associated with $\lambda=3$ is a little different from what we computed - `R` will _normalize_ the vector, which means that its total length will be one.^[The length of a vector $\vec{v}$ is denoted as $||\vec{v}||$ and is computed the following way: $||\vec{v}||=\sqrt{v_{1}^{2}+v_{2}^{2}+...+v_{n}^{2}}$. We normalize a vector to a length of 1 by dividing each component by its length.] For example $\displaystyle \vec{v}_{2} = \begin{pmatrix} 1 \\ 4 \end{pmatrix}$, so $||\vec{v}|| = \sqrt{5}$, and the normalized vector is  $\displaystyle \begin{pmatrix} 1/\sqrt{5} \\ 4/\sqrt{5} \end{pmatrix}$, which you can verify is the same as the reported eigenvector from the `eigenvalues` function.



## What do eigenvalues tell us?
Here the focus of the chapter changes a little bit. Now we focus on understanding how the phase plane for a differential equation gives clues about the stability for an equilibrium solution. This is intentional: once we have found the eigenvalues, determining eigenvectors can seem rather mundane at times (and perhaps heavy on the algebra). Studying the eigenvalues helps us understand the qualitative nature of the solution to a differential equation. Let's think about the characteristic equation $f(\lambda)$ for a two-dimensional system of differential equations:

\begin{equation}
\begin{split}
f(\lambda) &= \det(A - \lambda I) \\
&= (a - \lambda)(d-\lambda)-bc \\
&= \lambda^{2} - (a+d) \lambda + ad-bc
\end{split} (\#eq:cheq-2-ch12)
\end{equation}

Notice how $f(\lambda)$ is a quadratic equation. You may recall that quadratic equations have zero, one, or two distinct solutions. If there are no solutions, we say the solutions are imaginary (more on that later). Also the signs of solutions may be positive or negative. There are so many different combinations!  What types of phase planes do all those different types of eigenvalues produce? The following examine representative examples of all the possible eigenvalues you may obtain for a two-dimensional linear system of differential equations (which can be generalized to higher-dimensional systems).


### Sources: all eigenvalues positive
Consider the differential equation in Equation \@ref(eq:source-ex-18):

\begin{equation} 
\begin{pmatrix} x' \\ y' \end{pmatrix} =\begin{pmatrix} 2 & 0 \\ 1 &  1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}  (\#eq:source-ex-18)
\end{equation}

Computing the eigenvalues for Equation \@ref(eq:source-ex-18) shows that they are both positive:
```{r}
eigenvalues(c(2, 0, 1, 1))
```

The phase plane for this matrix $A$ is shown in Figure \@ref(fig:source-18):

```{r source-18,echo=FALSE, fig.cap="Phase plane for Equation \\@ref(eq:source-ex-18), which shows the equilibrium solution is a source (also known as an unstable node)."}
# For a two-variable system of differential equations we need to define dx/dt and dy/dt separately:
phaseplane(c(dx ~ 2 * x, dy ~ x + y), "x", "y", c(-4, 4), c(-4, 4)) +
  geom_point(aes(x = 0, y = 0), size = 3, color = "red") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```

Notice how the phase plane in Figure \@ref(fig:source-18) has all the arrows pointing from the origin. In this case we call the origin equilibrium solution a _source_, or also an _unstable node_.\index{equilibrium solution!source}\index{equilibrium solution!unstable node} Plotting the components of $\vec{x}(t)$ as functions of $t$ would show the dependent values increase exponentially as time increases.



### Sinks: all eigenvalues negative
Consider the differential equation in Equation \@ref(eq:sink-ex-18), which is a _slight_ modification from Equation \@ref(eq:source-ex-18):

\begin{equation}
\begin{pmatrix} x' \\ y' \end{pmatrix} =\begin{pmatrix} -2 & 0 \\ 1 &  -1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}  (\#eq:sink-ex-18)
\end{equation}

The eigenvalues for Equation \@ref(eq:sink-ex-18) are both negative (verify this on your own). The resulting phase plane for Equation \@ref(eq:sink-ex-18) then has all the arrows pointing towards the origin, shown in Figure \@ref(fig:sink-18).

```{r sink-18,echo=FALSE, fig.cap="Phase plane for Equation \\@ref(eq:sink-ex-18), which shows the equilibrium solution is a sink (also known as a stable node)."}
# For a two-variable system of differential equations we need to define dx/dt and dy/dt separately:

phaseplane(c(dx ~ -2 * x, dy ~ x - y), "x", "y", c(-4, 4), c(-4, 4)) +
  geom_point(aes(x = 0, y = 0), size = 3, color = "red") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```

Based on the phase plane in Figure \@ref(fig:sink-18), solutions to Equation \@ref(eq:sink-ex-18) would asymptotically approach the origin. We say the equilibrium solution is a  _sink_, also known as a _stable node_.\index{equilibrium solution!sink}\index{equilibrium solution!stable node} 


### Saddle nodes: one positive and one negative eigenvalue
Consider the differential equation in Equation \@ref(eq:saddle-ex-18):

\begin{equation} 
\begin{pmatrix} x' \\ y' \end{pmatrix} =\begin{pmatrix} 3 & -2 \\ 1 &  -1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} (\#eq:saddle-ex-18)
\end{equation}

Equation \@ref(eq:saddle-ex-18) has $\lambda_{1}=2.414$ and $\lambda_{2}=-0.414$ (verify this on your own). Because the differential equation has one positive and one negative eigenvalue the equilibrium solution the phase plane for this differential equation looks a little different, as is shown in Figure \@ref(fig:saddle-18):

```{r saddle-18,echo=FALSE, fig.cap="Phase plane for Equation \\@ref(eq:saddle-ex-18), which shows the equilibrium solution is a saddle node."}
# For a two-variable system of differential equations we need to define dx/dt and dy/dt separately:

phaseplane(c(dx ~ 3 * x - 2 * y, dy ~ x - y), "x", "y", c(-4, 4), c(-4, 4)) +
  geom_point(aes(x = 0, y = 0), size = 3, color = "red") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```

This equilibrium solution is called a _saddle node_.\index{equilibrium solution!saddle}  From the horizontal direction, the arrows point *away* from the origin, but in the vertical direction the arrows point *towards* the origin. This behavior is caused by the opposing signs of the eigenvalues - one part of the solution in Equation \@ref(eq:gen-soln-18) (the one associated with the negative eigenvalue) decays asymptotically to zero. The other positive eigenvalue is associated with the asymptotically unstable, giving the solution trajectory the shape of a saddle.


### Spirals: imaginary eigenvalues
Consider the differential equation in Equation \@ref(eq:spiral-sink-ex-18):

\begin{equation}
\begin{pmatrix} x' \\ y' \end{pmatrix} =\begin{pmatrix} -3 & -8 \\ 4 &  -6 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} (\#eq:spiral-sink-ex-18)
\end{equation}

There are two eigenvalues to this system: $\lambda = -4.5+5.45i$ and $\lambda = -4.5 - 5.45i$ (you can confirm this on your own). In this case the $i$ means the eigenvalues are imaginary. Notice how the eigenvalues are similar, but the signs on the second term differ. We say the eigenvalues are _complex conjugates_ of each other, and write them in the form $\lambda = \alpha \pm \beta i$.\index{complex conjugate} In this example $\alpha = -4.5$ and $\beta = 5.45$. Figure \@ref(fig:spiral-sink-18) shows the phase plane for this system.

```{r spiral-sink-18,echo=FALSE, fig.cap="Phase plane for Equation \\@ref(eq:spiral-sink-ex-18), which shows the equilibrium solution is a spiral sink."}
# For a two-variable system of differential equations we need to define dx/dt and dy/dt separately:

phaseplane(c(dx ~ -3 * x - 8 * y, dy ~ 4 * x - 6 * y), "x", "y", c(-4, 4), c(-4, 4), ) +
  geom_point(aes(x = 0, y = 0), size = 3, color = "red") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```

The phase plane in Figure \@ref(fig:spiral-sink-18) has some spiraling motion to it. Why does that occur? Imaginary eigenvalues can occur when the characteristic equation $\det(A-\lambda I)=0$ has imaginary solutions.\index{eigenvalue!imaginary} More generally, we say $\lambda = \alpha \pm \beta i$. Because the eigenvalues are complex, we would also expect the eigenvectors to be complex as well (i.e. $\vec{v} \pm i \vec{w}$). Don't let the term _imaginary_ fool you: by using properties from complex analysis it can be shown that when eigenvalues are imaginary, the template for the solution is given in Equation \@ref(eq:imag-eval-18):

\begin{equation}
\vec{x}(t) = c_{1} e^{\alpha t} ( \vec{w} \cos (\beta t) - \vec{v} \sin (\beta t)) + c_{2} e^{\alpha t} ( \vec{w} \cos (\beta t) + \vec{v} \sin (\beta t)) (\#eq:imag-eval-18)
\end{equation}

The trigonometric terms in Equation \@ref(eq:imag-eval-18) suggest that the solution has some periodic behavior if we plot the components of $\vec{x}(t)$ as functions of $t$. But when we plot the solution in the $xy$ plane that periodic behavior gets translated to spiraling motion in Figure \@ref(fig:spiral-sink-18). When $\alpha < 0$ we say the equilibrium solution is a _spiral sink_ because the exponential terms in Equation \@ref(eq:imag-eval-18) decay asymptotically to zero.\index{equilibrium solution!spiral sink}


As you would expect when $\alpha > 0$ we classify a phase plane as a _spiral source_ (shown in Equation \@ref(eq:spiral-source-ex-18) and Figure \@ref(fig:spiral-source-18)).\index{equilibrium solution!spiral source} 

\begin{equation}
\begin{pmatrix} x' \\ y' \end{pmatrix} =\begin{pmatrix} 4 & -5 \\ 3 &  2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}  (\#eq:spiral-source-ex-18)
\end{equation} ().


```{r spiral-source-18,echo=FALSE, fig.cap="Phase plane for Equation \\@ref(eq:spiral-source-ex-18), which shows the equilibrium solution is a spiral source."}
# For a two-variable system of differential equations we need to define dx/dt and dy/dt separately:

phaseplane(c(dx ~ 4 * x - 5 * y, dy ~ 3 * x + 2 * y), "x", "y", c(-4, 4), c(-4, 4), ) +
  geom_point(aes(x = 0, y = 0), size = 3, color = "red") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```

The final case for imaginary eigenvalues is when $\alpha = 0$, which is termed a _center_.\index{equilibrium solution!center}  As an example, let's examine the phase plane for the system in Equation \@ref(eq:center-ex-18):

\begin{equation} 
\begin{pmatrix} x' \\ y' \end{pmatrix} =\begin{pmatrix} 0 & -1 \\ 1 &  0 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} (\#eq:center-ex-18)
\end{equation}

```{r center-18,echo=FALSE, fig.cap="Phase plane for Equation \\@ref(eq:center-ex-18), which shows the equilibrium solution is a center."}
# For a two-variable system of differential equations we need to define dx/dt and dy/dt separately:

phaseplane(c(dx ~ -y, dy ~ x), "x", "y", c(-4, 4), c(-4, 4)) +
  geom_point(aes(x = 0, y = 0), size = 3, color = "red") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```

Notice how the phase plane arrows in Figure \@ref(fig:center-18) neither spin in nor out, but seem to point in a circle. In fact, the formula of the solution to this equation can be represented as a circle. The solution to this differential equation is a circle, which is found with $\alpha = 0$ in Equation \@ref(eq:imag-eval-18).


### Repeated eigenvalues
For repeated eigenvalues (where $\lambda_{1}=\lambda_{2}$) the stability of the solution still depends on the sign of the sole eigenvalue $\lambda$.\index{eigenvalue!repeated} However in this case the form of the solution changes as shown in Equation \@ref(eq:repeat-eval-18):

\begin{equation}
\vec{x}(t) = \left( c_{1} \vec{v}_{1} + c_{2} \vec{v}_{2} \right) e^{\lambda t} + c_{2} \vec{v}_{1} t e^{\lambda t} (\#eq:repeat-eval-18)
\end{equation}



## Concluding thoughts

As you can see there is a lot of interesting behavior with eigenvalues and eigenvectors!  But in all cases, stability of the equilibrium solution really focuses on the eigenvalues and their relative (positive or negative) sign. How the straight line solutions _approach_ the equilibrium solution is a function of the eigenvectors.





## Exercises
```{exercise}
What are the characteristic equations for the following systems of differential equations?

a. $\displaystyle \frac{dx}{dt} = 4x, \;  \frac{dy}{dt} = -y$
b. $\displaystyle \frac{dx}{dt} = x+y, \;  \frac{dy}{dt} = x-y$
c. $\displaystyle \frac{dx}{dt} = 9x +15y, \;  \frac{dy}{dt} = 7x + 2y$

```



```{exercise}
Verify that $\displaystyle s_{1}(t) =  \begin{pmatrix} e^{3t} \\ -e^{3t} \end{pmatrix}$ is a solution to the system of differential equations $\displaystyle \frac{dx}{dt} = 2x-y$ and $\displaystyle \frac{dy}{dt} = 2x+5y$.

```



```{exercise}
The matrix $\displaystyle \begin{pmatrix} 2 & -1 \\ 2 & 5 \end{pmatrix}$ has $\lambda = 4$ as an eigenvalue. Use this information to calculate (by hand) the eigenvector $\vec{v}$ associated with this eigenvalue.
```



```{exercise}
Compute the eigenvalues and eigenvectors for the following linear systems. Based on the eigenvalues, classify if the equilibrium solution is stable or unstable. Finally write down the most general solution for the system of equations.


a. $\displaystyle \frac{dx}{dt} = 2x-6y, \;  \frac{dy}{dt} = x-2y$
b. $\displaystyle \frac{dx}{dt} = 9x-22y, \;  \frac{dy}{dt} = 3x-7y$
c. $\displaystyle \frac{dx}{dt} = 4x - 2y, \;  \frac{dy}{dt} = 2x - 2y$
d. $\displaystyle \frac{dx}{dt}= 4x-15y, \; \frac{dy}{dt}=2x-7y$
e. $\displaystyle \frac{dx}{dt} = 3x-18y, \;  \frac{dy}{dt} = x-5y$
f. $\displaystyle \frac{dx}{dt} = 5x-12y, \;  \frac{dy}{dt} = x-2y$



```

```{exercise}
Consider the following nonlinear system:


\begin{equation}
\begin{split}
\frac{dx}{dt} &= y-x \\
\frac{dy}{dt} &=-y + \frac{5x^2}{4+x^{2}}
\end{split}
\end{equation}  


a. Previously you verified that $(x,y)=(1,1)$ is an equilibrium solution for this system. What is the Jacobian matrix at that equilibrium solution?
b. Generate a phase plane for the Jacobian matrix.
c. What are the eigenvalues for the Jacobian matrix at the equilbrium solution?
d. Based on the eigenvalues, how would you classify the stability of the equilibrium solution?


```

```{exercise}
Consider the following nonlinear system:

\begin{equation}
\begin{split}
\frac{dx}{dt} &= x-2y-xy \\
\frac{dy}{dt} &= -y+xy- 2xy^{3}
\end{split}
\end{equation}  


a. Verify that $(x,y)=(0,0)$ is an equilibrium solution for this system. What is the Jacobian matrix at that equilibrium solution?
b. Generate a phase plane for the Jacobian matrix.
c. What are the eigenvalues for the Jacobian matrix at the equilbrium solution?
d. Based on the eigenvalues, how would you classify the stability of the equilibrium solution?


```



```{exercise}
Consider the following system:

\begin{equation}
\begin{split}
\frac{dx}{dt} &= y^{2} \\
\frac{dy}{dt} &= - x
\end{split}
\end{equation} 


a. There is one equilibrium solution to this system of equations. What is it?
b. What is the Jacobian matrix for this equilibrium solution?
c. Generate a phase plane for the Jacobian matrix.
d. What are the eigenvalues for the Jacobian matrix at the equilbrium solution?
e. Based on the eigenvalues, how would you classify the stability of the equilibrium solution?


```



```{exercise}
Consider the general system of differential equations $\displaystyle \frac{d}{dt} \vec{x} = A \vec{x}$.


a. Given the function $\vec{s}(t)=e^{\lambda t} \vec{v}$, where $\vec{v}$ is a constant vector, what is an expression for $\displaystyle \frac{d}{dt} \vec{s}(t)$?
b. Given the function $\vec{s}(t)=e^{\lambda t} \vec{v}$, where $\vec{v}$ is a constant vector, what is an expression for $A \; \vec{s}(t)$? 
c. Now use the previous results in the expression $\displaystyle \frac{d}{dt} \vec{s}(t) = A \vec{s}(t)$. Explain why it must be the case that $\lambda \vec{v} = A \vec{v}$ (assuming $\vec{v} \neq 0$).


```


```{exercise ch-eq-exr-18}
In this chapter we learned that for a two-dimensional matrix $\displaystyle A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, eigenvalues can be found by solving the characteristic equation $\det(A-\lambda I)=0$, or $\lambda^{2} - (a+d) \lambda + (ad-bc) = 0$. Use the quadratic formula to get an expression for the eigenvalues $\lambda$ in terms of $a$,$b$, $c$, and $d$.
```





<!--chapter:end:18-eigenvalues.Rmd-->

# Qualitative Stability Analysis {#stability-19}

Chapter \@ref(eigenvalues-18) introduced eigenvalues in order to classify equilibrium solutions for a linear system of differential equations. However let's face an ugly truth: determining eigenvalues via the characteristic polynomial isn't easy. Even with a two-dimensional system of equations you may resort to using the quadratic formula. Here's the good news: this chapter will develop other tools that can circumvent finding roots of a polynomial. In order to do that, we will need to understand some general relationships about the characteristic polynomial for a two-dimensional system of linear differential equations. Let's get started!



## The characteristic polynomial (again)
Consider the following two-dimensional linear system, where $a$, $b$, $c$, and $d$ can be any number:

\begin{equation}
\begin{pmatrix} x' \\ y' \end{pmatrix} =  \begin{pmatrix} ax+by \\ cx+dy \end{pmatrix} =  \begin{pmatrix} a & b \\ c &  d \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}
\end{equation}

Recall that eigenvalues are found by solving $\displaystyle \det (A - \lambda I ) =0$; $\displaystyle \det (A - \lambda I )$ is computed in Equation \@ref(eq:det-eq-19).

\begin{equation}
\det \begin{pmatrix} a - \lambda & b \\ c & d-\lambda \end{pmatrix} = (a-\lambda)(d-\lambda) - bc (\#eq:det-eq-19)
\end{equation}

If we multiply out Equation \@ref(eq:det-eq-19) we obtain the characteristic polynomial:

\begin{equation}
f(\lambda)=\lambda^{2} - (a+d) \lambda + ad - bc (\#eq:ch-eq-19)
\end{equation}

Notice how the terms of Equation \@ref(eq:ch-eq-19) can be expressed as functions of the entries of the matrix $A$ which are ($a$, $b$, $c$, $d$). In fact, in linear algebra the term $a+d$ is the sum of the diagonal entries, also known as the **trace** of a matrix, denoted as $\mbox{tr}(A)$. \index{trace}  And you may recognize that $ad-bc$ is the same as $\det(A)$.\index{determinant} So our characteristic polynomial can be rewritten as solving Equation \@ref(eq:tr-det-19):\index{characteristic polynomial}

\begin{equation}
f(\lambda)=\lambda^{2} - \mbox{tr}(A)\lambda + \det(A) (\#eq:tr-det-pre-19)
\end{equation}
```{example ch-eq-18}
Determine the characteristic polynomial $f(\lambda)$ for the system $\vec{x}'=Ax$ where $\displaystyle A=  \begin{pmatrix} -1 & 1 \\ 0 &  3 \end{pmatrix}$. Solve for the eigenvalues to classify the stability of the equilibrium solution.
```


```{solution}
We can see that $\det(A)= -1(3) - 0(1) = -3$ and tr$(A)=2$, so our characteristic equation is $\lambda^{2}-2\lambda -3$. If we solve $\lambda^{2}-2\lambda -3=0$ we have $(\lambda-3)(\lambda+1)=0$, so our eigenvalues are $\lambda=3$ and $\lambda=-1$. Since one eigenvalue is positive and the other one is negative, the equilibrium solution is a saddle node.
```


As shown in Example \@ref(exm:ch-eq-18), Equation \@ref(eq:tr-det-pre-19) may be a computationally easier way to determine eigenvalues. Here is *another* way to think about eigenvalues. Let's say we have two eigenvalues $\lambda_{1}$ and $\lambda_{2}$, so $f(\lambda_{1})=0$ and $f(\lambda_{2})=0$. However we can also examine Equation \@ref(eq:tr-det-19) in terms of the roots of $f(\lambda)$, denoted as $\lambda_{1}$ and $\lambda_{2}$. We make no assumptions about whether $\lambda_{1}$ or $\lambda_{2}$ are real, imaginary, or equal in value. However, since they solve the equation $f(\lambda)=0$, this also means that $(\lambda-\lambda_{1})(\lambda-\lambda_{2})=0$. If we multiply out this equation we have $\lambda^{2}-(\lambda_{1}+\lambda_{2}) \lambda + \lambda_{1} \lambda_{2}=0$. If we compare this equation with Equation \@ref(eq:tr-det-pre-19) we have:

\begin{equation}
\begin{split}
f(\lambda) &=\lambda^{2}-(\lambda_{1}+\lambda_{2}) \lambda + \lambda_{1} \lambda_{2} \\
&= \lambda^{2} - \mbox{tr}(A)\lambda + \det(A)
\end{split} (\#eq:tr-det-19)
\end{equation}

Equation \@ref(eq:tr-det-19) allows us to identify that $\mbox{tr}(A) = \lambda_{1}+\lambda_{2}$ and $\det(A) = \lambda_{1} \lambda_{2}$, or the trace of $A$ is the *sum* of the two eigenvaules and the determinant of $A$ is the *product* of the eigenvalues. Let's explore this a little more.

## Stability with the trace and determinant
The equality in Equation \@ref(eq:tr-det-19) uncovers some neat relationships - in particular tr$(A)=(\lambda_{1}+\lambda_{2})$ and $\det(A)=\lambda_{1}+\lambda_{2}$. Table \@ref(tab:eval-compare-19) synthesizes all these relationships to provide an alternative pathway to understand stability of an equilibrium solution with the trace and determinant:


Table: (\#tab:eval-compare-19) Comparison of the stability of an equilibrium solution in relation to the signs of an eigenvalue, the trace of the matrix $A$, or the determinant of the matrix $A$.

Sign of $\lambda_{1}$ | Sign of $\lambda_{2}$  | Tendency of equilibrium solution | Sign of tr$(A)=\lambda_{1}+\lambda_{2}$ | Sign of $\det(A)=\lambda_{1} \cdot \lambda_{2}$
-------------| ------------- | ------------- | -------------| ------------- |
    Positive | Positive | Source | Positive | Positive
    Negative |  Negative | Sink | Negative | Positive
   Positive  | Negative | Saddle | ? | Negative
    Negative |  Positive | Saddle | ? | Negative
   
For the moment we will only consider real non-zero values of the eigenvalues - more specialized cases will occur later. But examining the above table carefully:

- If $\det(A)$ is *negative*, then the equilibrium solution is a *saddle*.
- If $\det(A)$ is *positive* and tr$(A)$ is *negative*, then the equilibrium solution is a *sink*.
- If $\det(A)$ and tr$(A)$ are both *positive*, then the equilibrium solution is a *source*.

```{example}
Use the trace and determinant relationships to classify the stability of the equilibrium solution for the linear system $\vec{x}'=A\vec{x}$ where $\displaystyle A=  \begin{pmatrix} -1 & 1 \\ 0 &  3 \end{pmatrix}$.
```


```{solution}
We can see that $\det(A)= -1(3) - 0(1) = -3$ and tr$(A)=2$. Since the determinant is negative, the equilibrium solution must be a saddle node.
```


Knowing the relationships between the trace and determinant for a two-dimensional system of equations is a pretty quick and easy way to investigate stability of equilibrium solutions!

Another way to graphically represent the stability of solutions is with the _trace-determinant plane_ (shown in Figure \@ref(fig:trace-det-19)), with tr$(A)$ on the horizontal axis and det$(A)$ on the vertical axis:\index{trace-determinant plane}

```{tikz, trace-det-19,warning=FALSE,message=FALSE,echo=FALSE,fig.cap="The trace-determinant plane, illustrating stability of an equilibrium solution based the values of the trace and determinant. See also Table \\@ref(tab:eval-compare-19)."}
\begin{tikzpicture}
%\draw[help lines, color=gray!30, dashed] (-4.9,-4.9) grid (4.9,4.9);
\draw[<->,ultra thick] (-5,0)--(5,0) node[right]{tr$(A)$};
\draw[<->,ultra thick] (0,-1)--(0,3) node[above]{det$(A)$};
%\draw[thick,scale=0.5,domain=-5:5,smooth,variable=\x,blue] plot ({\x},{0.3*\x*\x});
\node at (0,-1.5) {Saddle node};
\node at (-3,1.5) {Sink};
\node at (3,1.5) {Source};
\end{tikzpicture}
```


While Figure \@ref(fig:trace-det-19) only determines if an equilibrium solution is a sink, source, or saddle node, it can be extended further to include spiral sinks and spiral nodes. Here's how: first we will apply the quadratic formula to Equation \@ref(eq:tr-det-19) to solve directly for the eigenvalues as a function of the trace and determinant:

\begin{equation}
\lambda_{1,2}= \frac{\mbox{tr}(A)}{2} \pm \frac{\sqrt{ (\mbox{tr}(A))^2-4 \det(A)}}{2} (\#eq:tr-det-soln-19)
\end{equation}

While Equation \@ref(eq:tr-det-soln-19) seems like a more complicated expression, it can be shown to be consistent with our above work. Imaginary eigenvalues can be a spiral source or sink depending on their location in the trace-determinant plane \index{trace-determinant plane} (Figure \@ref(fig:trace-det-ref)). The dividing curve is setting the discriminant of Equation \@ref(eq:tr-det-soln-19) to 0, which yields the quadratic equation $\displaystyle \det(A) = \frac{\mbox{tr}(A))^2}{4}$ (blue dashed curve in Figure \@ref(fig:trace-det-ref)). When $\displaystyle 0 < \det(A)<\frac{\mbox{tr}(A))^2}{4}$, then the solution is a sink or a source depending on the sign of tr$(A)$. Likewise, when $\displaystyle 0 < \frac{\mbox{tr}(A))^2}{4} <\det(A)$, then the solution is a spiral sink or a spiral source depending on the sign of tr$(A)$.

```{tikz, trace-det-ref,warning=FALSE,message=FALSE,echo=FALSE,fig.cap="Revised trace-determinant plane, illustrating all possible cases for classification of the stability of an equilibrium solution based on the values of the trace and determinant. See also Table \\@ref(tab:eval-compare-19)."}

\begin{tikzpicture}
%\draw[help lines, color=gray!30, dashed] (-4.9,-4.9) grid (4.9,4.9);
\draw[<->,ultra thick] (-5,0)--(5,0) node[right]{tr$(A)$};
\draw[<->,ultra thick] (0,-1)--(0,3) node[above]{det$(A)$};
\draw[thick,scale=0.5,domain=-5:5,smooth,variable=\x,blue,dashed] plot ({\x},{0.3*\x*\x});
\node at (0,-1.5) {Saddle node};
\node at (-3,1.5) {Sink};
\node at (3,1.5) {Source};
\node[fill=white] at (0,1.5) {Center};
\node[align=center] at (1,2.3) {Spiral \\ source};
\node[align=center] at (-1,2.3) {Spiral \\ sink};
\end{tikzpicture}

```

The one case that we haven't considered in our stability table is a *center* equilibrium. For this equilibrium solution, the eigenvalues ($\lambda_{1,2}$) equal $\pm \beta i$. Additionally, a center equilibrium occurs when the value of tr$(A)$ is exactly zero and $\det(A)$ is positive (Exercise \@ref(exr:center-eq-19)). 


```{example sink-spiral-19}
Use the trace and determinant relationships to classify the stability of the equilibrium solution for the linear system of differential equations:
  
  \begin{equation}
\begin{pmatrix} x' \\ y' \end{pmatrix} =  \begin{pmatrix} -x+y \\ x-4y  \end{pmatrix}
\end{equation}
  
```

```{solution}
The matrix for this system of differential equations is $\displaystyle A= \begin{pmatrix} -1& 1 \\ 1 & -4 \end{pmatrix}$. This means that tr$(A)=-5$ and $\det(A)=3$. Since (tr$(A)$)$^{2}/4=25/4$, which is greater than $\det(A)$, this equilibrium solution is a sink. You can verify on your own that the equilibrium solution is a sink by generating this system's phase plane.
```

As you can see, the trace-determinant plane (Figure \@ref(fig:trace-det-ref)) is a quick way to analyze stability of an equilibrium solution that does not require heavy algebraic analysis.

## A workflow for stability analysis
Chapters \@ref(linearsystems-15) to this one have covered a lot of ground, so perhaps it is prudent to summarize a workflow for stability analysis for a system of differential equations $\displaystyle \frac{d}{dt} \vec{x} = f(\vec{x},\vec{\alpha})$, where $\vec{\alpha}$ is a vector of parameters:

1. Determine nullclines by solving $f(\vec{x},\vec{\alpha})=0$. Equilibrium solutions occur where all the distinct nullclines intersect.
2. Construct the Jacobian matrix for each equilibrium solution.
3. Analyze stability of the equilbrium solution by computing eigenvalues of the Jacobian matrix (or use the methods in this chapter).

You can bypass the first few steps if the system is already linear ($\displaystyle \frac{d}{dt} \vec{x} = A \vec{x}$). You can summarize this workflow with Nullclines $\rightarrow$ Jacobian $\rightarrow$ Eigenvalues. There are a lot of separate pieces to analyze stability of a differential equation - but being systematic and careful with your approach helps.

## Stability for higher-order systems of differential equations.
The trace-determinant plane is a really useful approach to analyze stability of an equilibrium solution. However, one huge caveat is that the methods outlined in this chapter only apply with a two-dimensional system of differential equations.

The stability of an equilibrium solution depends on the signs of the eigenvalues, which are also a function of roots of the characteristic polynomial $f(\lambda)$. For higher-order systems the [Routh-Hurwitz stability criterion](https://en.wikipedia.org/wiki/Routh%E2%80%93Hurwitz_stability_criterion) can help determine stability, but computational complexity increases with higher-order systems. Another apporach with several differential equations may be to apply equilibrium analyses to reduce the system to two differential equations. (See @keener_mathematical_2009 for many examples related to human physiology.)

In the final analysis, there are deep connections between eigenvalues and the structure of the matrix $A$ (whether or not it arises from a linear system of differential equations or a Jacobian matrix). Examining stability of an equilibrium solution as it depends on an unspecified parameter is called a bifuraction analysis, which we will study in Chapter \@ref(bifurcation-20)).\index{bifurcation} There is more to this story, so let's forge ahead!


## Exercises


```{exercise}
Compute the trace and determinant for each of these systems of differential equations. Use the trace-determinant condition to classify the stability of the equilibrium solutions. Verify your stability results are consistent when analyzing stability by calculating the eigenvalues.

a. $\displaystyle x' = 2x-6y, \;  y' = x-2y$
b. $\displaystyle x' = 9x-22y, \;  y' = 3x-7y$
c. $\displaystyle x' = 4x - 2y, \;  y' = 2x - 2y$
d. $\displaystyle x'= 4x-15y, \; y'=2x-7y$
e. $\displaystyle x' = 3x-18y, \;  y' = x-5y$
f. $\displaystyle x' = 5x-12y, \;  y' = x-2y$

```
&nbsp;

<!-- modified from LW pg 164, changed things around a little bit -->
```{exercise}
Consider the linear system of differential equations:
  
\begin{equation}
\begin{split}
x'&=ax-y \\
y' &= -x+ ay
\end{split}
\end{equation}

Apply the relationships between the trace and determinant to classify the stability of the equilibrium solution for different values of $a$. Be sure to include cases where the system will be a spiral source or sink.

```





<!-- Strogatz pg 259-260. LW pg 183 #11 - cell differentiation -->
```{exercise}
Consider the following nonlinear system:

\begin{equation}
\begin{split}
x'&=y-x \\
y' &= -y + \frac{5x^{2}}{4+x^{2}}
\end{split}
\end{equation}

a. In Chapter \@ref(nonlinear-16) you verified that $(x,y)=(1,1)$ is an equilibrium solution for this system. What is the Jacobian matrix for this equilibrium solution?
b. What is tr$(J)$ and det$(J)$ for this equilibrium solution?
c. Evaluate the stability of the equilibrium solution using relationships between the trace and determinant. You may use a graph to plot $\det(J)$.


```



<!-- Keener, pg 411 with I = 0 -->
```{exercise}
(Inspired by @keener_mathematical_2009) Consider the following model of a neuron, with the two variables $v$ and $w$:
  
  \begin{equation}
\begin{split}
x' &= y-x^{3} +3x^{2} \\
y' &= 1 - 5x^{2} - y \\
\end{split} (\#eq:fh-nagumo-keener-19)
\end{equation}

a. Solve each of the nullclines as a function of $y$.
b. Using desmos or some other graphing utility, determine the equilibrium solutions.
c. Determine the Jacobian matrix for each of the equilibrium solutions.
d. Apply the trace-determinant conditions to determine stability of the equilibrium solutions

```


<!-- %LW #12 pg 183 Modified so algebraically easier (0,0) and (2,2) and (4,0) -->
```{exercise}
(Inspired by @logan_mathematical_2009) Consider the following predator-prey model, where the carrying capacity of the predator ($y$) depends on the prey population ($x$):

  \begin{equation}
\begin{split}
x' &= \frac{2}{3} x \cdot \left(1- \frac{x}{4} \right) - \frac{1}{6} xy \\
y' &= 0.5y \cdot \left(1 - \frac{y}{x} \right)
\end{split}
\end{equation}


a. There are three equilibrium solutions for this differential equation. What are they? *Hint:* first determine where $y'=0$ and then substitute your solutions into $x'=0$.
b. Visualize the phase plane for this system of differential equations.
c. Compute the Jacobian matrix for all of the equilibrium solutions.
d. Use the trace-determinant relationships to evaluate the stability of the equilibrium solutions. Is the trace-determinant analysis consistent with your phase plane?

```



<!-- adapted LW pg 183 #9 -->
```{exercise}
(Inspired by @logan_mathematical_2009) Let $C$ be the amount of carbon in a forest ecosystem, with $P$ as the rate of increase due to photosynthesis. Herbivores $H$ consume carbon on the following predator-prey model:

\begin{equation}
\begin{split}
\frac{dC}{dt}&=P - aC - bHC \\
\frac{dH}{dt} &= ebHC-dH
\end{split}  (\#eq:carbon-19)
\end{equation}

The parameters $a$ and $d$ represent the removal of carbon and herbivores from this system, and $b$ the consumption of carbon by the herbivores at some efficiency $e$. All parameters are greater than zero. Use this information to answer the following questions:
  
  
a. Construct the general Jacobian matrix for this system of differential equations.
b. What are the $H$ nullclines for this system of differential equations? Your nullclines will be a function of the parameters.
c. Use the $H$ nullclines to determine the two equilibrium solutions for Equation \@ref(eq:carbon-19). Under what conditions will the equilibrium solutions be positive?
d. Evaluate tr($J$) and det($J$) at each of your equilibrium solutions. What do you think the stability of the equilibrium solutions would be?


```


```{exercise}
(Inspired by @pastor_mathematical_2008) The amount of nutrients (such as carbon) in soil organic matter is represented by $N$, whereas the amount of inorganic nutrients in soil is represented by $I$. A system of differential equations that describes the turnover of inorganic and organic nutrients is the following:

\begin{equation}
\begin{split}
\frac{dN}{dt} &= L + kdI - \mu N I - \delta N  \\
\frac{dI}{dt} &= \mu N I - k d I - \delta I ,
\end{split}
\end{equation}


a. Construct the general Jacobian matrix for this system of differential equations.
b. An equilibrium solution to this system of differential equations is $\displaystyle N = \frac{L}{\delta}, \; I = 0$. Determine tr$(J)$ and det($J$) for this equilibrium solution.
c. Express conditions on the parameter $\mu$ (as a function of the other parameters) that determine when this equilibrium solution a saddle node (you may assume that $\delta>0$ and $\mu > 0$)? If $\mu$ represents the rate conversion of nutrients to inorganic matter, and $\delta$ is the removal of nutrients from the system, what does this condition mean in a biological sense?

```


```{exercise}
Apply the quadratic formula to $\lambda^{2} - \mbox{tr}(A)\lambda + \det(A)=0$ to obtain Equation \@ref(eq:tr-det-soln-19).
```



<!-- (x-(a+bi))(x-(a-bi)) = x^2 +(a+bi)(a-bi) -x(a+bi)-x(a-bi) = x^2 -2a x + (a^2+b^2)  -->
```{exercise}
Assume that you have two complex conjugate eigenvalues: $\lambda_{1} = a + bi$ and $\lambda_{2} = a - bi$.


a. What is an expression for $\lambda_{1} + \lambda_{2}$?
b. What is an expression for $\lambda_{1} \cdot \lambda_{2}$?
c. Explain why your answers from the previous two questions mean that tr$(A)=2a$ and $\det(A)=a^{2}+b^{2}$.
d. Create a linear two-dimensional system of differential equations where the equilibrium solution at the origin is a spiral sink. Show your system and the corresponding phase plane. 


```



```{exercise center-eq-19}
Consider a two-dimensional system where tr$(A)=0$ and det$(A)>0$.


a. Given those conditions, explain why $\lambda_{1} + \lambda_{2}=0$ and $\lambda_{1} \cdot \lambda_{2}>0$.
b. What does $\lambda_{1} + \lambda_{2}=0$ tell you about the relationship between $\lambda_{1}$ and $\lambda_{2}$?
c. What does $\lambda_{1} \cdot \lambda_{2}>0$ tell you about the relationship between $\lambda_{1}$ and $\lambda_{2}$?
d. Look back to your previous two responses. First explain why $\lambda_{1}$ and $\lambda_{2}$ must be imaginary eigenvalues (in other words, not real values). Then explain why $\lambda_{1,2}= \pm bi$.
e. Given these constraints, what would the phase plane for this system be?
f. Create a linear two-dimensional system where tr$(A)=0$ and det$(A)>0$. Show your system and the phase plane. 


```







<!--chapter:end:19-qualitativeStability.Rmd-->

# Bifurcation {#bifurcation-20}

In this chapter we will use *bifurcation* to examine how the stability of an equilibrium solution changes as the value of a parameter changes. This is a great topic of study that (by necessity) requires you to think of stability of an equilibrium solution on multiple levels. You are up for the challenge; let's get started!


## A series of equations
Consider the differential equation $\displaystyle x' = 1-x^{2}$. This differential equation has an equilibrium solution at $x=\pm 1$. To classify the stability of the equilibrium solutions we apply the following test for stability of an equilibrium solution that we developed in Chapter \@ref(phase-05):


- If $f'(y^{*})>0$ at an equilibrium solution, the equilibrium solution $y=y^{*}$ will be unstable.
- If $f'(y^{*}) <0$ at an equilibrium solution, the equilibrium solution $y=y^{*}$ will be stable.
- If $f'(y^{*}) = 0$, we cannot conclude anything about the stability of $y=y^{*}$.

Applying this test, we know $f(x)=1-x^2$ and $f'(x)=-2x$. Since $f'(1)=-2$ and $f'(-1)=2$, then the respective equilibrium solution $x=1$ is stable and the equilibrium solution at $x=-1$ is unstable.

Let's modify and extend this example further. Consider two more differential equations:

 - $\displaystyle x' = -1-x^{2}$: This differential equation does not have any equilibrium solutions, so we do not need to apply the stability test.
 - $\displaystyle x' = -x^{2}$:  This differential equation has an equilibrium solution at $x=0$; the stability test cannot apply because $f'=-2x$ and $f'(0)=0$. The general solution to this differential equation is $\displaystyle x(t)=\frac{1}{t+C}$ (Exercise \@ref(exr:dxdt-x2-20)), which apart from the vertical asymptote at $t=-C$ is always decreasing for $t>0$. So the equilibrium solution at $x=0$ is not stable.


```{r eq1-ex2-3, echo=FALSE,results='hide',warning=FALSE,fig.cap="Phase plane with associated solutions for $\\displaystyle x'=c-x^{2}$ for different values of $c$. The dashed grey lines are equilibrium solutions.",fig.width=4,fig.height=12,fig.pos="!p"}
system_eq <- c(dxdt ~ 1,
               dydt ~ 1-y^2)


system_eq_rev <- c(dxdt ~ 1-x^2)

initialCondition <- tibble(value = c(2,1.5,1,0.5,0,-0.5,-1,-1.5,-2),sim=1:9,vector="x") %>% relocate(vector) %>%
  group_by(sim) %>%
  nest()


out_values <- initialCondition %>%
  mutate(result = map(.x=data,.f=~rk4(system_eq_rev,
                                      initial_condition=deframe(.x),
                                      deltaT=.1,
                                      n_steps = 20))) %>%
  select(-data) %>%
  unnest(cols=c(result))


p1 <- phaseplane(system_eq,'x','y',x_window = c(-0.1,2),y_window = c(-2,2)) +
  geom_path(data=out_values,aes(x=t,y=x,color=as.factor(sim),group=sim),inherit.aes = TRUE,size=1) + 
  scale_x_continuous(limits=c(-0.1,2)) +
    scale_y_continuous(limits=c(-2,2)) +
 guides(color="none")+ ggtitle(expression("A: x' = 1"~-x^2)) + xlab("t") + ylab("x") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  geom_hline(yintercept = c(-1,1),linetype='dashed',color='grey')



system_eq <- c(dxdt ~ 1,
               dydt ~ -1-y^2)


system_eq_rev <- c(dxdt ~ -1-x^2)

initialCondition <- tibble(value = c(2,1.5,1,0.5,0,-0.5,-1,-1.5,-2),sim=1:9,vector="x") %>% relocate(vector) %>%
  group_by(sim) %>%
  nest()


out_values <- initialCondition %>%
  mutate(result = map(.x=data,.f=~rk4(system_eq_rev,
                                      initial_condition=deframe(.x),
                                      deltaT=.1,
                                      n_steps = 20))) %>%
  select(-data) %>%
  unnest(cols=c(result))


p2 <- phaseplane(system_eq,'x','y',x_window=c(-0.1,2),y_window=c(-2,2)) +
  geom_path(data=out_values,aes(x=t,y=x,color=as.factor(sim),group=sim),inherit.aes = TRUE,size=1) + 
  scale_x_continuous(limits=c(-0.1,2)) +
    scale_y_continuous(limits=c(-2,2)) +
  guides(color="none")+ ggtitle(expression("B: x' = -1"~-x^2)) + xlab("t") + ylab("x") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
###


system_eq <- c(dxdt ~ 1,
               dydt ~ -y^2)


system_eq_rev <- c(dxdt ~ -x^2)

initialCondition <- tibble(value = c(2,1.5,1,0.5,0,-0.5,-1,-1.5),sim=1:8,vector="x") %>% relocate(vector) %>%
  group_by(sim) %>%
  nest()


out_values <- initialCondition %>%
  mutate(result = map(.x=data,.f=~rk4(system_eq_rev,
                                      initial_condition=deframe(.x),
                                      deltaT=.1,
                                      n_steps = 20))) %>%
  select(-data) %>%
  unnest(cols=c(result))


p3 <- phaseplane(system_eq,'x','y',x_window=c(-0.1,2),y_window=c(-2,2)) +
  geom_path(data=out_values,aes(x=t,y=x,color=as.factor(sim),group=sim),inherit.aes = TRUE,size=1) + 
  scale_x_continuous(limits=c(-0.1,2)) +
    scale_y_continuous(limits=c(-2,2)) +
  guides(color="none")+ ggtitle(expression("C: x' ="~-x^2)) + xlab("t") + ylab("x") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  geom_hline(yintercept = 0,linetype='dashed',color='grey')


gridExtra::grid.arrange(p1,p2,p3, 
             ncol = 1, nrow = 3)
```


Figure \@ref(fig:eq1-ex2-3) builds on an interesting pattern in our series of differential equations. Let's build on these three examples in a more general context. Consider the differential equation $\displaystyle x' = c-x^{2}$, which is a generalization of our examples. (For our examples $c=1$ $-1$, and $0$ respectively.)

The value of $c$ influences the phase line and the resulting solution. Steady states to $\displaystyle x' = c-x^{2}$ are at $x^{*}=\pm \sqrt{c}$. We can also test out the stability of our steady states using the stability test, with $f(x)=c-x^{2}$ and $f'(x)=-2x$. 

If $c>0$ we have two steady states, summarized in the following table:

Equilibrium solution | $f'(x^{*})$  | Tendency of solution | 
-------------| ------------- | ------------- | 
    $x^{*}=\sqrt{c}$ | $-2 \sqrt{c}$ | Stable 
    $x^{*}=-\sqrt{c}$ |  $2 \sqrt{c}$ | Unstable 

(You should verify that the stability result we initially found when $c=1$ matches the table.)

If $c=0$ there is only one steady state, summarized, in the following table:

Equilibrium solution | $f'(x^{*})$  | Tendency of solution | 
-------------| ------------- | ------------- | 
   $x^{*}=0$  | 0 | Inconclusive 

Even though in this case the stability test is inconclusive, based on the phase plane for $x'=-x^{2}$ (Figure \@ref(fig:eq1-ex2-3)), the equilibrium solution $x^{*}=0$ is unstable. If the initial condition $x(0)$ is greater than 0, the solution flows towards $x=0$, but if the initial condition is less than zero, the solution flows away from $x=0$. This type of behavior is similar to a one-dimensional analogue to a saddle node from Chapter \@ref(eigenvalues-18).

Finally, when $c<0$ then there are no steady states because the $\sqrt{c}$ will be an imaginary number.^[While eigenvalues can be imaginary, equilibrium solutions for the contexts studied here can only be real.] 


Notice how different values of $c$ influence both the *value* $(x^{*}=\pm \sqrt{c})$ and the *stability* of the equilibrium solution (stable / unstable). Rather than making a series of tables, we can represent the dependence of the equilibrium solution and its stability in what is called a *bifurcation diagram* (Figure \@ref(fig:saddle-bifur-20)).

```{r saddle-bifur-20,engine='tikz',warning=FALSE,message=FALSE,echo=FALSE,fig.cap="A saddle node bifurcation for the differential equation $\\displaystyle x'=c-x^{2}$.",fig.align='center'}


\begin{tikzpicture}
%\draw[help lines, color=gray!30, dashed] (-4.9,-4.9) grid (4.9,4.9);
\draw[<->,ultra thick] (-3,0)--(3,0) node[right]{$c$};
\draw[<->,ultra thick] (0,-1)--(0,3) node[above]{$x^{*}$};
\draw[->,thick,scale=0.5,domain=0:5,smooth,variable=\x,blue] plot (\x,{sqrt(\x)});
\draw[->,thick,scale=0.5,domain=0:5,dashed,variable=\x,blue] plot (\x,-{sqrt(\x)});
\node at (3,-1.5) {Unstable};
\node at (3,1.5) {Stable};

\end{tikzpicture}


```

Let's talk about Figure \@ref(fig:saddle-bifur-20). The graph represents the value of the equilibrium solution ($x^{*}$, vertical axis) as a function of the parameter $c$ (horizontal axis). Since equilibrium solutions are characterized by $x^{*}=\pm \sqrt{c}$ we have the "sideways parabola", traced in blue in Figure \@ref(fig:saddle-bifur-20). When $c<0$, there is no equilibrium solution, (so nothing is plotted in the second and third quadrants of Figure \@ref(fig:saddle-bifur-20)). The difference between the solid and dashed lines in Figure \@ref(fig:saddle-bifur-20) is used to distinguish between a stable equilibrium solution ($x^{*}=\sqrt{c}$ when $c>0$) and an unstable equilibrium solution ($x^{*}=-\sqrt{c}$ when $c>0$). It is so cool that *all* the information about the equilibrium solution and its stability is contained in Figure \@ref(fig:saddle-bifur-20)!

The bifurcation structure of $\displaystyle x'=c-x^{2}$ is called a *saddle-node* bifurcation. To give another context, it might be helpful to think of this $c$ like a tuning knob. As $c>0$ we will always have two different equilibrium solutions that are symmetrical based on the value of $c$. The positive equilibrium solution will be stable, the other unstable. As $c$ approaches zero these equilibrium solutions will collapse into each other. If $c$ is negative, the equilibrium solution disappears.

\newpage

```{r bifuc-b-20, echo=FALSE,results='hide',warning=FALSE,fig.cap="Phase plane for Equation \\@ref(eq:b-bifurc-20) when $b=-2$"}
eq_1 <- c( dx ~ 3*x -2*y, dy~x+y)

theta <- seq(0,2*pi,length.out=20)
initialCondition <- tibble(x = sin(theta),
                           y = cos(theta),
                           sim = 1:20) %>%
  pivot_longer(cols=c("x","y")) %>%
  group_by(sim) %>%
  nest()

out_values <- initialCondition %>%
  mutate(result = map(.x=data,.f=~rk4(eq_1,
                                      initial_condition=deframe(.x),
                                      deltaT=.1,
                                      n_steps = 200))) %>%
  select(-data) %>%
  unnest(cols=c(result)) %>%
  arrange(sim,t)
  

pb1 <- phaseplane(eq_1,'x','y',c(-5,5),c(-5,5)) +
  geom_path(data=out_values,aes(x=x,y=y,color=as.factor(sim),group=sim),inherit.aes = TRUE,size=1) +   scale_x_continuous(limits=c(-5,5)) +
    scale_y_continuous(limits=c(-5,5)) + guides(color="none")+ ggtitle(expression("A: b = -2")) + geom_point(data=tibble(x=0,y=0),aes(x=x,y=y),color='red',size=2) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 18),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 18)
  )


###

```

```{r bifuc-b-20-2, echo=FALSE,results='hide',warning=FALSE,fig.cap="Phase plane for Equation \\@ref(eq:b-bifurc-20) when $b=0$"}
eq_2 <- c( dx ~ 3*x +0*y, dy~x+y)

theta <- seq(0,2*pi,length.out=20)
initialCondition <- tibble(x = sin(theta),
                           y = cos(theta),
                           sim = 1:20) %>%
  pivot_longer(cols=c("x","y")) %>%
  group_by(sim) %>%
  nest()

out_values <- initialCondition %>%
  mutate(result = map(.x=data,.f=~rk4(eq_2,
                                      initial_condition=deframe(.x),
                                      deltaT=.1,
                                      n_steps = 200))) %>%
  select(-data) %>%
  unnest(cols=c(result))

pb2<- phaseplane(eq_2,'x','y',c(-5,5),c(-5,5)) +
  geom_path(data=out_values,aes(x=x,y=y,color=as.factor(sim),group=sim),inherit.aes = TRUE,size=1) +
  scale_x_continuous(limits=c(-5,5)) +
    scale_y_continuous(limits=c(-5,5)) +
  guides(color="none")+ ggtitle(expression("B: b = 0")) + geom_point(data=tibble(x=0,y=0),aes(x=x,y=y),color='red',size=2) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 18),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 18)
  )

```



```{r bifuc-b-20-3, echo=FALSE,results='hide',warning=FALSE,fig.cap="Phase planes for Equation \\@ref(eq:b-bifurc-20) for different values of $b$.",fig.width=4,fig.height=12}
eq_2 <- c( dx ~ 3*x +4*y, dy~x+y)

theta <- seq(0,2*pi,length.out=20)
initialCondition <- tibble(x = sin(theta)*runif(20,min=1,max=2),
                           y = cos(theta)*runif(20,min=1,max=2),
                           sim = 1:20) %>%
  pivot_longer(cols=c("x","y")) %>%
  group_by(sim) %>%
  nest()

out_values <- initialCondition %>%
  mutate(result = map(.x=data,.f=~rk4(eq_2,
                                      initial_condition=deframe(.x),
                                      deltaT=.1,
                                      n_steps = 200))) %>%
  select(-data) %>%
  unnest(cols=c(result))

pb3 <- phaseplane(eq_2,'x','y',x_window = c(-5,5),y_window = c(-5,5)) +
  geom_path(data=out_values,aes(x=x,y=y,color=as.factor(sim),group=sim),inherit.aes = TRUE,size=1) + 
  scale_x_continuous(limits=c(-5,5)) +
    scale_y_continuous(limits=c(-5,5)) +
  guides(color="none")+ 
  ggtitle(expression("C: b = 4")) + geom_point(data=tibble(x=0,y=0),aes(x=x,y=y),color='red',size=2) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 18),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 18)
  )

grid.arrange(pb1,pb2,pb3,nrow=3,ncol=1)
```



## Bifurcations with systems of equations 
We can also examine how bifurcation plays a role with systems of differential equations. 

As another example, let's determine the behavior of solutions near the origin for the system of equations:
 \begin{equation}
 \frac{\vec{dx}}{dt} = \begin{pmatrix} 3 & b \\ 1 & 1 \end{pmatrix} \vec{x}. (\#eq:b-bifurc-20)
 \end{equation}

This equation has one free parameter $b$ that we will analyze using the trace determinant conditions developed in Chapter \@ref(stability-19). Let's call the matrix $A$, so the tr$(A)=4$ and $\det(A)=3-b$. Since the trace is always positive either the equilibrium solution will be a saddle if $\det(A)<0$, or when $3<b$. We have a spiral source when $\det(A)>0$ (this means $3 > b$) and $\det(A) > (\mbox{tr}(A))^{2}/4$, or when $3-b > 4$, which leads to $b<-1$. Figure \@ref(fig:bifuc-b-20-3)A-C displays the phase planes for different values of $b$ along with sample solution curves.

To summarize, Equation \@ref(eq:b-bifurc-20) has the following dynamics depending on the value of $b$:

- When $b < -1$, the equilibrium solution will be a spiral source.
- When $-1 < b < 3$, the equilibrium solution will be a source.
- When $3<b$, the equilibrium solution will a saddle.


Another approach to analyzing Equation \@ref(eq:b-bifurc-20) is to compute the eigenvalues directly, which in this case are $\displaystyle \lambda_{1,2}(b)=2 \pm \sqrt{b+1}$. Creating a plot of the eigenvalues (Figure \@ref(fig:bifurc-b-lambda)) can also help explain the bifurcation structure. When $b<-1$, the eigenvalues are imaginary, with Re$(\lambda_{1,2}(b)=2)=2$, so the equilibrium solution is a spiral source. When $-1 < b < 3$, both eigenvalues are positive, so the equilibrium solution is a source. Finally, when $3 < b$, one eigenvalue is positive and the other is negative, confirming our analyses with the trace-determinant plane.

```{r bifurc-b-lambda,echo=FALSE,results='hide',warning=FALSE,fig.cap="Bifurcation diagram for Equation \\@ref(eq:b-bifurc-20). The vertical axis shows the value of the eigenvalues $\\lambda$ (red and blue curves) as a function of the parameter $b$. The annotations represent the stability of the original equilibrium solution." }

in_data <- tibble(b=seq(-1,5,length.out=100),y1=2+sqrt(1+b),y2=2-sqrt(1+b))

text_vals <- tibble(x=c(-2.25,1,4),y=3,val=c("Spiral\nsource","Source","Saddle"))

p1 <- in_data %>%
  ggplot() + 
  geom_line(aes(x=b,y=y1),color='red',size=1) +
  geom_line(aes(x=b,y=y2),color='blue',size=1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_vline(xintercept = c(-1,3),linetype='dashed') +
  labs(y=expression(lambda)) + xlim(c(-2.4,5)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 18),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 18)
  )
p1 +
  geom_text(data = text_vals,aes(x=x,y=y,label=val),hjust='left',vjust='bottom')
```

The benefit of a bifurcation diagram is to provide a complete understanding of the dynamics of the system *as a function of the parameters*. In this chapter we examined *one-parameter* bifurcations (for example we looked the stability of the equilibrium solution as it depends on *c* or *b*), but bifurcations can also be extended further to two parameter bifurcation families, applying similar methods. In general the methods are similar to what we have done.

## Functions as equilibrium solutions: limit cycles 
In the previous examples the stability of an equilibrium solution changed depending on the value of a parameter. Typically equilibrium solutions are a single point in the phase plane. Another way we can represent an equilibrium solution is with a *function*. As an example, consider the following highly nonlinear system in Equation \@ref(eq:limit-cycle-20):
 
\begin{equation}
\begin{split}
\frac{dx}{dt} =-y-x(x^2+y^2-1) \\ 
\frac{dy}{dt}=x-y(x^2+y^2-1)
\end{split} (\#eq:limit-cycle-20)
\end{equation}

The phase plane for Equation \@ref(eq:limit-cycle-20) is shown in Figure \@ref(fig:limit-cycle-20-f). You can verify that Equation \@ref(eq:limit-cycle-20) has an equilibrium solution at the point  $x=0$, $y=0$. However Figure \@ref(fig:limit-cycle-20-f) suggests there might be other equilibrium solutions when various solution curves are plotted with the phase plane.

```{r limit-cycle-20-f, warning=FALSE,echo=FALSE,fig.width=3,fig.height=3,fig.cap="Phase plane for Equation \\@ref(eq:limit-cycle-20) with different solution curves. Notice the equilibrium solution described by the equation $x^{2}+y^{2}=1$."}
mu <- 1

mu_eq <- c( dx ~ -y-x*(x^2+y^2-1), dy~x-y*(x^2+y^2-1))


initialCondition <- tibble(x = c(1,0.3,0.7,-0.3,0.7,0.7,0.9,1.1),
                           y = c(0.5,.5,.5,.5,-0.5,-0.5,0,0.5),
                           sim = 1:8) %>%
  pivot_longer(cols=c("x","y")) %>%
  group_by(sim) %>%
  nest()


out_values <- initialCondition %>%
  mutate(result = map(.x=data,.f=~rk4(mu_eq,
                                      initial_condition=deframe(.x),
                                      deltaT=.1,
                                      n_steps = 200))) %>%
  select(-data) %>%
  unnest(cols=c(result))

phaseplane(mu_eq,'x','y',x_window=c(-1.5,1.5),y_window=c(-1.5,1.5),plot_points = 10) +
  geom_path(data=out_values,aes(x=x,y=y,color=as.factor(sim),group=sim),inherit.aes = TRUE,size=1) + xlim(c(-1.5,1.5)) + ylim(c(-1.5,1.5)) + guides(color="none") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
  


```

What is interesting in Figure \@ref(fig:limit-cycle-20-f) is that the solution tends towards a circle of radius 1 (or the equation $x^{2}+y^{2}=1$). This is an example of an equilibrium solution that is a *curve* rather than a specific point. We can describe the phase plane with a new variable $X$ that represents the distance from the origin (a radius $r$) by transforming this system from $x$ and $y$ to a single new variable $X$ (see Exercise \@ref(exr:limit-transform-20)).

\begin{equation}
\frac{dX}{dt} = -X(X-1) \mbox{ where } X=r^{2} (\#eq:limit-X-20)
\end{equation}

How Equation \@ref(eq:limit-cycle-20) transforms to Equation \@ref(eq:limit-X-20) is by applying a polar coordinate transformation to this system. With stability analysis for Equation \@ref(eq:limit-X-20) we can show that the equilibrium solution $X=0$ is unstable (meaning the origin $x=0$ and $y=0$ is an unstable equilibrium solution) and the circle of radius 1 is a stable equilibrium solution (which is the equation $x^{2}+y^{2}=1$). In this case we would say $r=1$ is a *stable limit cycle*.\index{limit cycle}\index{limit cycle!stable}  You will study a similar system in Exercises \@ref(exr:limit-transform-20) and \@ref(exr:hopf-20).

Equation \@ref(eq:limit-X-20) is an example of next steps with studying the qualitative analysis of systems. We can extend out Equation \@ref(eq:limit-cycle-20) further to introduce a parameter $\mu$ that, as $\mu$ changes, undergoes a bifurcation as $\mu$ increases. This is an example of a *Hopf bifurcation*.\index{bifurcation!Hopf} 

## Bifurcations as analysis tools
The most important part in studying bifurcations is analyzing examples. This chapter has several exercises where you will construct bifurcation diagrams for one- and two-dimensional systems of differential equations. As a reminder, constructing sample phase lines / phase planes before analyzing stability and the bifurcation structure is always helpful to build understanding.

Bifurcation analysis is a fascinating field of study that combines knowledge of differential equations, geometry, and other types of advanced mathematics. For further information, please see the texts by @strogatz_nonlinear_2015, @perko_differential_2001, and @kuznetsov_elements_2004.



## Exercises
```{exercise}
Explain why $\displaystyle x'= 1+x^{2}$ does not have any equilibrium solutions.
```

```{exercise dxdt-x2-20}
Use separation of variables to verify that the general solution to $\displaystyle x' = -x^{2}$ is $\displaystyle x(t)=\frac{1}{t+C}$.
```

```{exercise}
Apply local linearization to classify stability of the following differential equations:

a. $\displaystyle \frac{dx}{dt} = x-x^{2}$
b. $\displaystyle \frac{dx}{dt} = -x^{2}$
c. $\displaystyle \frac{dx}{dt} = -x-x^{2}$

```

```{exercise}
Consider the differential equation $\displaystyle x' = cx-x^{2}$. What are equations that describe the dependence of the equilibrium solution on the value of $c$?  Once you have that figured out, plot the bifurcation diagram, with the parameter $c$ along the horizontal axis. This bifurcation is called a *transcritical* bifurcation.\index{bifurcation!transcritical}
```

```{exercise}
Consider the differential equation $\displaystyle x' = cx-x^{3}$. What are equations that describe the dependence of the equilibrium solution on the value of $c$?  Once you have that figured out plot the bifurcation diagram, with the parameter $c$ along the horizontal axis. This bifurcation is called a *pitchfork* bifurcation.\index{bifurcation!pitchfork}
```


```{exercise}
Construct a bifurcation diagram for the differential equation $\displaystyle x'=c+x^{2}$
```



```{exercise}
Consider the differential equation $\displaystyle x' = x(x-1)(b-x)$. The differential equation has equilibrium solutions at $x^{*}=0$, $x^{*}=1$, and $x^{*}=b$, where $b > 0$.

a. Use desmos or some other plotting software to investigate the effect of the number of roots as $b$ increases from a value of 0.
b. Analyze the stability of each of these equilibrium solutions. (You may want to multiply out the right hand side of the differential equation.) Whether a given equilibrium solution is stable may depend on the value of $b$.
c. Construct a bifurcation diagram for all three solutions together, with $b$ on the horizontal axis and the value of $x^{*}$ on the vertical axis.


```

```{exercise}
Consider the system of differential equations:

\begin{equation}
\begin{pmatrix} x' \\ y' \end{pmatrix} =  \begin{pmatrix} -x  \\ cy - y^{2}  \end{pmatrix}
\end{equation}

a. What are the equilibrium solutions for this (uncoupled) system of equations?
b. Evaluate stability of the equilibrium solutions as a function of the parameter $c$.
c. Construct a few representative phase planes to verify your analysis.

```


```{exercise}
Consider the following linear system of differential equations:
  
 \begin{equation}
 \frac{d}{dt}\vec{x} = \begin{pmatrix} 3 & b \\ b & 1 \end{pmatrix} \vec{x}.
 \end{equation}

a. Verify that the characteristic polynomial is $f(\lambda,b)=\lambda^{2}-4\lambda+(3-b^{2})$.
b. Solve $f(\lambda,b)=0$ with the quadratic formula to obtain an expression for the eigenvalues as a function of $b$, that is $\lambda_{1,2}(b)$.
c. Using the eigenvalues, classify the stability of the equilibrium solution as $b$ changes.
d. Generate a few representative phase planes to verify your analysis.
e. Create a plot similar to Figure \@ref(fig:bifurc-b-lambda) showing the bifurcation structure.

```

<!-- LW pg 164 -->
```{exercise}
Consider the linear system of differential equations:

\begin{equation}
\begin{split}
\frac{dx}{dt}&=cx-y \\
\frac{dy}{dt} &= -x+cy
\end{split}
\end{equation}

a. Determine the characteristic polynomial ($f(\lambda,c)$) for this system of equations.
b. Solve $f(\lambda,c)=0$ with the quadratic formula to obtain an expression for the eigenvalues as a function of $c$, that is $\lambda_{1,2}(c)$.
c. Using the eigenvalues, classify the stability of the equilibrium solution as $c$ changes.
d. Generate a few representative phase planes to verify your analysis.
e. Create a plot similar to Figure \@ref(fig:bifurc-b-lambda) showing the bifurcation structure.

```


```{exercise limit-transform-20}
 Consider the following highly nonlinear system:

\begin{equation}
\begin{split}
\frac{dx}{dt} =-y-x(x^2+y^2-1) \\
\frac{dy}{dt}=x-y(x^2+y^2-1)
\end{split}
\end{equation}

We are going to transform the system by defining new variables $x=r \cos \theta$ and $y=r \sin \theta$. Observe that $r^2=x^2+y^2$.


a. Consider the equation $r^2=x^2+y^2$, where $r$, $x$, and $y$ are all functions of time. Apply implicit differentiation to determine a differential equation for $\displaystyle \frac{d(r^{2})}{dt}$, expressed in terms of $x$, $y$, $\displaystyle \frac{dx}{dt}$ and $\displaystyle \frac{dy}{dt}$.

b. Multiply the above equations $\displaystyle \frac{dx}{dt}$ by $2x$ and $\displaystyle \frac{dy}{dt}$ by $2y$ on both sides of the equation. Then add the two equations together. You should get an expression for $\displaystyle \frac{d(r^{2})}{dt}$ in terms of $x$ and $y$.

c. Rewrite the equation for the right hand side of $\displaystyle \frac{d(r^{2})}{dt}$ in terms of $r^{2}$.

d. Use your equation that you found to verify that 

\begin{equation}
\frac{dX}{dt} = -2X(X-1), \mbox{ where } X=r^{2}
\end{equation}

e. Verify that $X=1$ is a stable node and $X=0$ is unstable.

f. As discussed in this chapter, this system has a stable limit cycle. What quick and easy modification to our system could you do to the system to ensure that this is an unstable limit cycle?  Justify your work.

```

```{exercise hopf-20}
Construct a bifurcation diagram for $\displaystyle \frac{dX}{dt} = - 2X(X-\mu)$,; $\mu$ is a parameter. Explain how you can apply that result to understanding the bifurcation diagram of the system:

\begin{equation}
\begin{split}
\frac{dx}{dt} =-y- x(x^2+y^2-\mu) \\
\frac{dy}{dt}=x- y(x^2+y^2-\mu)
\end{split}
\end{equation}
  
This system is an example of a \emph{Hopf bifurcation}.
```



<!-- adapted LW pg 185, get some more citations in here -->
```{exercise}
(Inspired by @logan_mathematical_2009) The immune response to HIV can be described with differential equations. In the early stages (before the body is swamped by the HIV virions) the dynamics of the virus can be described by the following system of equations, where $v$ is the virus load and $x$ the immune response:
  
\begin{equation}
\begin{split}
\frac{dv}{dt}&=rv - pxv \\
\frac{dx}{dt} &= cv-bx
\end{split}
\end{equation}

You may assume that all parameters are positive.

a. Explain the various terms in this model and their biological meaning.
b. Determine the equilibrium solutions.
c. Evaluate the Jacobian for each of the equilibrium solutions.
d. Construct a bifurcation diagram for each of the equilibrium solutions.

```



<!--chapter:end:20-bifurcation.Rmd-->

# (PART) Stochastic Differential Equations {.unnumbered} 


# Stochastic Biological Systems {#stoch-sys-21}


## Introducing stochastic effects
Up to this point we have studied *deterministic* differential equations.\index{differential equation!deterministic} We use the word deterministic because given an initial condition and parameters, the solution trajectory is known. In this part we are going to study *stochastic* differential equations or SDEs for short.\index{differential equation!stochastic} A stochastic differential equation means that the differential equation is subject to random effects - either in the parameters (which may cause a change in the stability for a time) or in the variables themselves.

Stochastic differential equations can be studied using computational approaches. This part will give you an introduction to SDEs with some focus on solution techniques, which I hope you will be able to apply in other contexts relevant to you. Understanding how to model SDEs requires learning some new mathematics and approaches to numerical simulation. Let's get started!

<!-- ### Author: JMZ, modified from Logan and Wolesesnky "Mathematical Methods in Biology" -->
<!-- ### Purpose: Create a sandhill crane model of discrete dynamics x(t+1)=r*x(t), as detailed on page 311 -->
<!-- ### r = 1+b-d, where b is the birth rate, d is the death rate. -->
<!-- ### b and d are drawn from normally distributed random variables. -->

## A discrete dynamical system
Let's focus on an example that involves discrete dynamical systems. Moose are large animals (part of the deer family), weighing 1000 pounds, that can be found in [Northern Minnesota](https://www.dnr.state.mn.us/mammals/moose.html).\index{discrete dynamical system!moose model}\index{model!moose population}  The moose population was 8000 in the early 2000s, but recent [surveys](https://files.dnr.state.mn.us/wildlife/moose/moosesurvey.pdf) show the population is maybe stabilized at 3000.


A starting model that describes their population dynamics is the discrete dynamical system in Equation \@ref(eq:moose):

\begin{equation}
M_{t+1} = M_{t} + b \cdot M_{t} - d \cdot M_{t}, (\#eq:moose)
\end{equation}

where $M_{t}$ is the population of the moose in year $t$, and $b$ the birth rate and $d$ the death rate. Equation \@ref(eq:moose) can be reduced down to $M_{t+1}=r M_{t}$ where $r=1+b-d$ is the net birth/death rate. This model states that the population of moose in the next year is proportional to the current population.

Equation \@ref(eq:moose) is a little bit different from a continuous dynamical system, but can be simulated pretty easily by defining a function. 

```{r}

M0 <- 3000 # Initial population of moose
N <- 5 # Number of years we simulate

moose <- function(r) {
  out_moose <- array(M0, dim = N+1)
  for (i in 1:N) {
    out_moose[i + 1] <- r * out_moose[i]
  }
  return(out_moose)
}
```

Notice how the function `moose` returns the current population of moose after $N$ years with the net birth rate $r$. Let's take a look at the results for different values of $r$ (Figure \@ref(fig:moose-det-21)).

```{r moose-det-21,echo=FALSE,fig.cap="Simulation of the moose population with different birth rates."}

moose_rates <- tibble(
  years = 0:N,
  r0.4 = moose(0.4),
  r0.8 = moose(0.8),
  r1.1 = moose(1.1)
) %>%
  pivot_longer(cols = c(-"years"))


ggplot(data = moose_rates) +
  geom_line(aes(x = years, y = value, color = name)) +
  labs(
    x = "Years",
    y = "Moose"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name = "Net birth rate:", labels = c("r = 0.4", "r = 0.8", "r = 1.1"))
```

Notice how for some values of $r$ the population starts to decline, stay the same, or increase. To analyze Equation \@ref(eq:moose), just like with a continuous differential equation we want to look for solutions that are in steady state, or ones where the population is staying the same. In other words this means that $M_{t+1}=M_{t}$, or $M_{t}=rM_{t}$. If we simplify this expression this means that $M_{t}-r M_{t}=0$, or $(1-r)M_{t}=0$. Assuming that $M_{t}$ is not equal to zero, then this equation is consistent only when $r=1$. This makes sense: we know $r=1-b-d$, so the only way this can be one is if $b=d$, or the births balance out the deaths.

Okay, so we found our equilibrium solution. The next goal is to determine the general solution to Equation \@ref(eq:moose). In Chapter \@ref(exact-solns-07) for continuous differential equations, a starting point for a general solution was an exponential function. For discrete dynamical systems we will also assume a general solution is exponential, but this time we represent the solution as $M_{t}=M_{0} \cdot v^{t}$, which is an exponential equation. The parameter $M_{0}$ is the initial population of moose (here it equals 3000). Now let's determine $v$ in Equation \@ref(eq:moose):

\begin{equation}
M_{t+1} = r M_{t} \rightarrow 3000 \cdot v^{t+1} = r \cdot 3000 \cdot v^{t}
\end{equation}

Our goal is to figure out a value for $v$ that is consistent with this expression. Just like we did with continuous differential equations we can arrange the following equation, using the fact that $v^{t+1}=v^{t}\cdot v$:

\begin{equation}
3000 v^{t} (v-r) = 0
\end{equation}

Since we assume $v\neq 0$, the only possibility is if $v=r$. Equation \@ref(eq:moose-soln) represents the general solution for Equation \@ref(eq:moose):

\begin{equation}
M_{t}=3000 r^{t} (\#eq:moose-soln)
\end{equation}

We know that if $r>1$ we have exponential growth exponential decay when $r<1$ exponential decay, consistent with our results above.

There is some comfort here: just like in continuous systems we find eigenvalues that determine the stability of the equilibrium solution. For discrete dynamical systems the stability is based on the value of an eigenvalue relative to 1 (not 0). Note: this is a good reminder to be aware if the model is based in continuous or discrete time!

## Environmental stochasticity
It may be the case that environmental effects drastically change the net birth rate from one year to the next. For example during snowy winters the net birth rate changes because it is difficult to find food [@carroll_modeling_2013]. For our purposes, let's say that in snowy winters $r$ changes from $1.1$ to $0.7$. This would be a pretty drastic effect on the system - when $r=1.1$ the moose population grows exponentially and when $r=0.7$ the moose population decays exponentially.

A snowy winter occurs randomly. One way to model this randomness is to create a conditional statement based on the probability of it being snowy, defined on a scale from 0 to 1. How we implement this is by writing a function that draws a uniform random number each year and adjust the net birth rate:


```{r}
# We use the snowfall_rate  as an input variable

moose_snow <- function(snowfall_prob) {
  out_moose <- array(M0, dim = N+1)
  for (i in 1:N) {
    r <- 1.1 # Normal net birth rate
    if (runif(1) < snowfall_prob) { # We are in a snowy winter
      r <- 0.7 # Decreased birth rate
    }
    out_moose[i + 1] <- r * out_moose[i]
  }
  return(out_moose)
}
```



Figure \@ref(fig:moose-out-21) displays different solution trajectories of the moose population over time for different probabilities of a deep snowpack.

```{r moose-out-21,echo=FALSE,fig.cap= "Moose populations with different probability of adjusting to deep snowpacks."}

M0 <- 100 # Initial population of moose
N <- 10 # Number of years we simulate

# We use the probability of adjusting to snow as a variable
moose_snow_pop <- tibble(
  years = 0:N,
  p0.25 = moose_snow(0.25),
  p0.5 = moose_snow(0.5),
  p0.75 = moose_snow(0.75)
) %>%
  pivot_longer(cols = c(-"years"))


ggplot(data = moose_snow_pop) +
  geom_line(aes(x = years, y = value, color = name)) +
  labs(
    x = "Years",
    y = "Moose"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size=10),
    legend.text = element_text(size = 10),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name = "Probability of\nsnowy winter:", labels = c("p = 0.25", "p = 0.5", "p = 0.75"))

```

If you tried generating Figure \@ref(fig:moose-out-21) on your own you would not obtain the same figure. We are drawing random numbers for each year, so you should have different trajectories. While this may seem like a problem, one key thing that we will learn later in Chapter \@ref(stoch-sim-22) is there is a stronger underlying signal when we compute *multiple* simulations and then compute an ensemble average.

As you can see when the probability of a snowy winter is very high ($p = 0.75$), the population decays exponentially. If that probability is lower, the moose population can still increase, but one bad year does knock the population down.




## Discrete systems of equations
Another way to extend Equation \@ref(eq:moose) is to account for both adult ($M$) and juvenile ($J$) moose populations with Equation \@ref(eq:moose-juvenile):

\begin{equation}
\begin{split}
J_{t+1} &=f \cdot M_{t} \\
M_{t+1} &= g \cdot J_{t} + p \cdot M_{t}
\end{split} (\#eq:moose-juvenile)
\end{equation}

Equation \@ref(eq:moose-juvenile) is a little different from \@ref(eq:moose) because it includes juvenile and adult moose populations, which have the following parameters:

- $f$: represents the birth rate of new juvenile moose
- $g$: represents the maturation rate of juvenile moose
- $p$: represents the survival probability of adult moose

We can code up this model using `R` in the following way:

```{r}

M0 <- 900 # Initial population of adult moose
J0 <- 100 # Initial population of juvenile moose

N <- 10 # Number of years we run the simulation
moose_two_stage <- function(f, g, p) {

  # f: birth rate of new juvenile moose
  # g: maturation rate of juvenile moose
  # p: survival probability of adult moose

  # Create a data frame of moose to return
  out_moose <- tibble(
    years = 0:N,
    adult = M0,
    juvenile = J0
  )

  # And now the dynamics
  for (i in 1:N) {
    out_moose$juvenile[i + 1] <- f * out_moose$adult[i]
    out_moose$adult[i + 1] <-
      g * out_moose$juvenile[i] + p * out_moose$adult[i]
  }

  return(out_moose)
}
```

To simulate the dynamics we just call the function `moose_two_stage` and plot in Figure \@ref(fig:moose-2-21):

```{r eval=FALSE}

moose_two_stage_rates <- moose_two_stage(
  f = 0.5,
  g = 0.6,
  p = 0.7
)

ggplot(data = moose_two_stage_rates) +
  geom_line(aes(x = years, y = adult), color = "red") +
  geom_line(aes(x = years, y = juvenile), color = "blue") +
  labs(
    x = "Years",
    y = "Moose"
  )
```

```{r moose-2-21,fig.cap="Simulation of a two stage moose population model.",echo=FALSE}

moose_two_stage_rates <- moose_two_stage(
  f = 0.5,
  g = 0.6,
  p = 0.7
) %>%
  pivot_longer(cols = c(-"years")) %>%
  mutate(name = str_to_title(name))

ggplot(data = moose_two_stage_rates) +
  geom_line(aes(x = years, y = value, color = name)) +
  labs(
    x = "Years",
    y = "Moose"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name = "Stage:")
```


Looking at Figure \@ref(fig:moose-2-21), it seems like both populations stabilize after a few years. We could further analyze this model for stable population states (in fact, it would be similar to determining eigenvalues as in Chapter \@ref(eigenvalues-18)). Additional extensions could also incorporate adjustments to the parameters $f$, $g$, and $p$ in snow years (Exercise \@ref(exr:moose-2-snowy)).

As you can see, introducing stochastic or random effects to a model yields some interesting (and perhaps more realistic) results. Next we will examine how computing can further explore stochastic models and how to generate expected patterns from all this randomness. Onward!


## Exercises
<!-- DO SOME MODELS WITH LOONS HERE (SEE PUBS) -->
```{exercise}
Re-run the moose population model with probabilities of adjusting to the deep snowpack at $p = 0, \; 0.1, \; 0.9, \mbox{ and} \;1$. How does adjusting the probability affect the moose population after 10 years?
```



```{exercise}
Modify the function `moose_snow` so that `runif(1) < snowfall_prob)` is changed to `runif(1) > snowfall_prob)`. How does that code change the resulting solution trajectories in Figure \@ref(fig:moose-out-21)?  Why is this not the correct way to code changes in the net birth rate in deep snowpacks?

```



```{exercise}
Modify the two stage moose population model (Equation \@ref(eq:moose-juvenile)) with the following parameters and plot the resulting adult and juvenile populations:
  

a. $f = 0.6$, $g = 0.6$, $p = 0.7$
b. $f = 0.5$, $g = 0.6$, $p = 0.4$
c. $f = 0.3$, $g = 0.6$, $p = 0.5$


You may assume $M_{0} = 900$ and $J_{0}=100$.  
```




```{exercise}
You are playing a casino game. If you win the game you earn \$10. If you lose the game you lose your bet of \$10. The probability of winning or losing is 50-50 (0.50). You decide to play the game 20 times and then cash out your net earnings.

a. Write code that is able to simulate this stochastic process. Plot your results.
b. Run this code five different times. What do you think your long term net earnings would be?
c. Now assume that you have a 40\% chance of winning. Re-run your code to see how that affects your net earnings.

```



```{exercise moose-2-snowy}
Modify the two stage moose population model (Equation 21.5) to account for years with large snowdepths. In normal years, $f=0.5$, $g=0.6$, $p=0.7$. However for snowy years, $f=0.3$, $g=0.6$, $p=0.5$. Generate code that can account for these variable rates (similar to the moose population model). You may assume $M_{0} = 900$, $J_{0}=100$, and $N$ (the number of years) is 30. Plot simulations when the probability of snowy winters is $s=0.05$ $s=0.10$, or $s=0.20$. Comment on the long-term dynamics of the moose for these simulations.
```


```{exercise}
A population grows according the the growth law $x_{t+1}=r_{t}x_{t}$.

a. Determine the general solution to this discrete dynamical system.
b. Plot a sample growth curve with $r_{t}=0.86$ and $r_{t}=1.16$, with $x_{0}=100$. Show your solution for $t=50$ generations.
c. Now consider a model where $r_{t}=0.86$ with probability 1/2 and $r_{t}=1.16$ with probability 1/2. Write a function that will predict the population after $t=50$. Show three or four different realizations of this stochastic process.

```


 <!-- %LW #2 page 317 -->
```{exercise}
(Inspired by @logan_mathematical_2009) A rectangular preserve has area $a$. At one end of the boundary of the preserve (contained within the area), is a small band of land of area ($a_{b}$) from which animals disperse into the wilderness. Only animals at that eged disperse. Let $u_{t}$ be the number of animals in $a$ at any time $t$. The growth rate of all the animals in $a$ is $r$. The rate at which animals disperse from the strip is proportional to the fraction of the animals in the edge band, with proportionality constant $\epsilon$.


a. Draw a picture of the situation described above.
b. Explain why the equation that describes the dynamics is $\displaystyle u_{t+1}=r \, u_{t} - \epsilon \frac{a_{b}}{a} u_{t}$.
c. Determine conditions on the parameter $r$ as a function of the other parameters under which the population is growing.

```

<!--chapter:end:21-stochasticSystems.Rmd-->

# Simulating and Visualizing Randomness {#stoch-sim-22}

In Chapter \@ref(stoch-sys-21) we examined models for stochastic biological systems. These types of models are an introduction to the study of stochastic differential equations (SDEs). A common theme to SDEs is learning how to analyze and visualize randomness, broadly defined. In order to do that we will need to level up our skills to summarize a cohort of simulations over time. Let's get started!

## Ensemble averages
Consider Figure \@ref(fig:kuopio-weather), which shows the weather forecast for Kuopio, a city in Finland [@finnish_meteorological_institute_weather_2021]:^[While I could have picked any city, a lot of this textbook was written while I was on sabbatical in Kuopio. I highly recommend Finland as a country to visit.] 

```{r kuopio-weather, echo=FALSE,out.width = "80%",fig.cap = "Long term weather forecast for Kuopio, a city in Finland, from the Finnish Weather Institute. Accessed 16-Dec 2021."}
knitr::include_graphics("figures/22-simulation/kuopio.png")
```

Figure \@ref(fig:kuopio-weather) shows a great example of what is called an *ensemble average*.\index{ensemble average} The horizontal axis lists the time of day and the vertical axis is the temperature (the bar graph represents precipitation). The forecast temperature at a given point in time can have a range of outcomes, with the median of the distribution as the "temperature probability forecast"[^stochasticsimulation-1]. The red shading states that 80% of the outcomes fall in a given range, so while the median temperature on Monday, December 20 (labeled as Mo 20.12 - dates are represented as DAY-MONTH-YEAR) is  $-10^{\circ}$C, it may range between $-16$ and $-4^{\circ}$C (3 to 24 $^{\circ}$F, brrr!). Based on the legends given, we would say the 80% confidence interval is between $-10$ to $-4^{\circ}$C, or the models have 80% confidence for the temperature to be between that range of temperatures.

[^stochasticsimulation-1]: Notice the meteorologist's temperature forecast on Wednesday, December 22 - sometimes what they predict may diverge from the model outcomes!

Because there may be different factors that alter the weather in a particular spot (e.g. the timing of a low pressure front, clouds, etc.) there are different possibilities for an outcome of the weather forecast. While it may seem like forecasting weather is impossible to do, sometimes these changes lead to small fluctuations in the forecasted weather at a given point. The ensemble average in Figure \@ref(fig:kuopio-weather) becomes more uncertain (wider shading), as unforeseen events may drastically change the weather in the long term.

### Spaghetti plots
Now let's focus on how to construct an ensemble average, but first let's start with a sample dataset. Consider the following data in Table \@ref(tab:simul-table). Notice how all the simulations (`sim1`, `sim2`, `sim3`) share the variable `t` in common, so it makes sense to plot them on the same axis in Figure \@ref(fig:simul-graph-prev). We call a plot of all of the simulations together a *spaghetti plot*, because, well, it can look like a bowl of spaghetti noodles was dumped all over the plotting space.\index{plot!spaghetti}

```{r simul-table, echo=FALSE}
my_table <- tibble(
  t = 1:5,
  sim1 = round(runif(5, min = 0, max = 5), digits = 2),
  sim2 = round(runif(5, min = 0, max = 5), digits = 2),
  sim3 = round(runif(5, min = 0, max = 5), digits = 2)
)

knitr::kable(my_table, caption = "Simulations of a variable at different times.")
```



```{r simul-graph-prev,fig.cap="Spaghetti plot of the three simulations from Table \\@ref(tab:simul-table).",echo=FALSE}

my_table %>%
  pivot_longer(cols = c("sim1":"sim3"), 
               names_to = "name", values_to = "value") %>%
  ggplot(aes(x = t, y = value, group = name)) +
  geom_point() +
  geom_line() +
  labs(x = "Time", y = "Simulation Value") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) 
  


```

While making the spaghetti plot isn't bad when you have three simulations, with (a lot) more simulations this would be a pain! An ensemble average computes *across* the rows in Table \@ref(tab:simul-table) (could be an average or a quantile) to generate a new column in the data. Building an ensemble average is a step by step process that involves a series of processes that transform a dataset where you will need to first pivot the dataset and then group and summarize.

### Pivot
When you have multiple columns of a plot that you want to show together (such as a spaghetti plot) we can use a command called `pivot_longer` that gathers different columns together (also shown in Table \@ref(tab:simul-table-long)).

```{r}
my_table_long <- my_table %>%
  pivot_longer(cols = c("sim1":"sim3"), 
               names_to = "name", values_to = "value")
```

```{r simul-table-long,echo=FALSE}
knitr::kable(my_table_long, caption = "Simulations of a variable at different times, condensed into a long table.")
```

Notice how the command `pivot_longer` takes the different simulations (`sim1`, `sim2`, `sim3`) and reassigns the column names to a new column called `name`, with values in the different columns appropriately assigned to the column `value`. This process called pivoting creates a new data frame, which makes it easier to generate the spaghetti plot (Figure \@ref(fig:simul-graph-prev)). Try the following code out on your own to confirm this:


```{r eval=FALSE}
my_table_long %>%
  ggplot(aes(x = t, y = value, group = name)) +
  geom_point() +
  geom_line() +
  labs(x = "Time", y = "Simulation Value")
```

### Group and summarize
The next step after pivoting is to collect the time points at $t=1$ together, $t=2$ together, and so on. In each of these groups we then compute the mean (average). This process is called *grouping* and then applying a *summarizing* function to all the members in a particular group (which in this case is the `mean`). The code to do this summarizing is shown below, with the results in Table \@ref(tab:table-summary):

```{r}
summarized_table <- my_table_long %>%
  group_by(t) %>%
  summarize(mean_val = mean(value))
```

```{r table-summary,echo=FALSE}
knitr::kable(t(round(summarized_table,2)), caption = "Ensemble averages for the three simulations at each of the times $t$.")
```

Notice how we are using the pipe `%>%` command to help organize the actions that we are doing. Think of the pipe as part of a multistep process - similar to function composition. Here is how you can read the previous code:

To compute the variable `summarized_table` we will:

 - **First** start with the data frame `my_table_long`,
 - **Second** signal to `R` to `group_by` the variable `t` (collect similar values together) in order to,
- **Third** compute the mean.


To explain this code a little more:  

- The command `group_by(t)` means collect similar time points together.
- The next line computes the mean. The command `summarize` means that we are going to create a new data frame column (labeled `mean_val` that is the mean of all the grouped times, from the `value` column.

We can add this mean value to our data (Figure \@ref(fig:simul-graph-prev)), represented with a thick red line. Try the following code out on your own:

```{r eval = FALSE}
my_table_long %>%
  ggplot(aes(x = t, y = value, group = name)) +
  geom_point() +
  geom_line() +
  geom_line(data = summarized_table, 
            aes(x = t, y = mean_val), color = "red", size = 2, 
            inherit.aes = FALSE) +
  geom_point(data = summarized_table, aes(x = t, y = mean_val), 
             color = "red", size = 2, 
             inherit.aes = FALSE) +
  guides(color = "none") +
  labs(x = "Time", y = "Simulation Value")
```

Notice two things:

- We can use the pipe (`%>%`) to the workflow before plotting with `ggplot`. This signals that `my_table_long` is the input data into `ggplot`.
- We included the option `inherit.aes = FALSE` (`inherit.aes` stands for "inherit aesthetics" ) when we plotted `summarized_table` with `geom_point` and `geom_line`. When you add a new data frame to a plot, the initial aesthetics (such as the `color` or the `group`) are passed on to subsequent commands. Setting `inherit.aes = FALSE` allows you to work with a clean slate.


## Repeated iteration
The previous example introduced the concept of pivoting data and computing an ensemble mean. Let's put this into additional practice with examples we have studied previously. Let's work with the logistic differential equation $\displaystyle \frac{dx}{dt} = rx\left(1- \frac{x}{K}\right)$. Our goal is to examine how different (random) initial conditions affect the modeled solution trajectories. The way we will approach this problem is with the following workflow:

> Do once $\rightarrow$ Do several times $\rightarrow$ Summarize $\rightarrow$ Visualize

We will apply this workflow step by step with code and results provided.

### Do once
To investigate the effect of the initial condition on the solution we will choose the initial condition from a uniform distribution between 0 and 20, shown in the following code and plotted in Figure \@ref(fig:logistic-example-rev):

```{r,eval=FALSE}

# Define the rate equation
logistic_eq <- c(dx ~ r * x * (1 - x / K)) 

# Identify any parameters
params <- c(r = .8, K = 100) 

# Random initial condition number 1
init_cond_rand <- c(x = runif(1, min = 0, max = 20)) 

soln_rand <- euler(
  system_eq = logistic_eq,
  initial_condition = init_cond_rand,
  parameters = params,
  deltaT = .05,
  n_steps = 200
)

# Random initial condition number 2
init_cond_rand_two <- c(x = runif(1, min = 0, max = 20)) 

soln_rand_two <- euler(
  system_eq = logistic_eq,
  initial_condition = init_cond_rand_two,
  parameters = params,
  deltaT = .05,
  n_steps = 200
)

# Plot your solutions:
ggplot() +
  geom_line(data = soln_rand, aes(x = t, y = x), color = "black") +
  geom_line(data = soln_rand_two, aes(x = t, y = x), color = "red") +
  labs(
    x = "Time",
    y = "x"
  ) 
```

```{r logistic-example-rev,fig.cap="Two solutions to the logistic differential equation with a random initial condition.",echo=FALSE}

# Define the rate equation
logistic_eq <- c(dx ~ r * x * (1 - x / K)) 

# Identify any parameters
params <- c(r = .8, K = 100) 

# Random initial condition
init_cond_rand <- c(x = runif(1, min = 0, max = 20)) 

soln_rand <- euler(
  system_eq = logistic_eq,
  initial_condition = init_cond_rand,
  parameters = params,
  deltaT = .05,
  n_steps = 200
) %>% mutate(run = 1)

# Random initial condition
init_cond_rand_two <- c(x = runif(1, min = 0, max = 20)) 

soln_rand_two <- euler(
  system_eq = logistic_eq,
  initial_condition = init_cond_rand_two,
  parameters = params,
  deltaT = .05,
  n_steps = 200
) %>% mutate(run = 2)

# Plot your solution:
solutions <- rbind(soln_rand,soln_rand_two)

solutions %>%
ggplot() +
  geom_line(aes(x = t, y = x,color=as.factor(run))) +
  labs(
    x = "Time",
    y = "x"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind() +
  guides(color="none")
```


### Do several times
Running several hundred iterations of this model could quickly grow time consuming. Fortunately we can use iteration here to compute and gather several different solutions. First the code, followed by a deconstruction:

```{r eval=FALSE}
n_sims <- 500 # The number of simulations

# Compute solutions
logistic_sim <- rerun(n_sims, c(x = runif(1, min = 0, max = 20))) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler(
    system_eq = logistic_eq,
    initial_condition = .x,
    parameters = params,
    deltaT = .05,
    n_steps = 200
  )) %>%
  map_dfr(~.x, .id = "simulation")

# Plot these all together
logistic_sim %>%
  ggplot(aes(x = t, y = x)) +
  geom_line(aes(color = simulation)) +
  ggtitle("Random initial conditions") +
  guides(color = "none")

```


```{r logistic-example-many,fig.cap="Spaghetti plot for logistic differential equation with 500 random initial conditions.",echo=FALSE}
n_sims <- 500 # The number of simulations

# Compute solutions
logistic_sim <- rerun(n_sims, c(x = runif(1, min = 0, max = 20))) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler(
    system_eq = logistic_eq,
    initial_condition = .x,
    parameters = params,
    deltaT = .05,
    n_steps = 200
  )) %>%
  map_dfr(~.x, .id = "simulation")

# Plot these all together
logistic_sim %>%
  ggplot(aes(x = t, y = x)) +
  geom_line(aes(color = simulation)) +
  ggtitle("Random initial conditions") +
  guides(color = "none") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )

```


Wow!  This spaghetti plot is really interesting - it should show how even though the initial conditions vary between $x=0$ to $x=20$, eventually all solutions flow to the carrying capacity $K=100$ (which is a stable equilbrium solution). Initial conditions that start closer to $x=0$ take longer, mainly because they are so close to the other equilibrium solution at $x=0$ (which is an unstable equilibrium solution).

Ok, let's deconstruct this code line by line:

-   `rerun(n_sims, c(x=runif(1,min=0,max=20)))` This line does two things: `x=runif(1,min=0,max=20)` makes a random initial condition, and the command `rerun` runs this again for `n_sims` times.
-   `set_names(paste0("sim", 1:n_sims))` This line distinguishes between all the different simulations.
-   `map(~ euler( ... )` You should be familiar with `euler`, but notice the pronoun `.x` that substitutes all the different initial conditions into Euler's method. The `map` function iterates over each of the simulations.
-   `map_dfr(~ .x, .id = "simulation")` This line binds everything up together.

The resulting data frame should have three columns:
- `simulation`: which one of the 500 simulations (`sim1`, `sim2`, etc ...) this corresponds to.
- `t`: the value of the time
- `x`: the output value of the variable x.

This code applies a new concept called *functional programming*. This is a powerful tool that allows you to perform the process of iteration (do the same thing repeatedly) with uncluttered code. We won't delve more into this here, but I encourage you to read about more functional programming concepts in @wickham_r_2017.

### Summarize
Computing the ensemble average requires knowledge of how to use `R` to compute percentiles from a distribution of values. For our purposes here we will use the 95% confidence interval, so that means the 2.5 and 97.5 percentile (in which only 5% of the values will be outside of the specified interval), along with the median value (50th percentile). Let's take a look at the code for how to do that:

```{r,warning=FALSE,message=FALSE}

quantile_vals <- c(0.025, 0.5, 0.975)

logistic_quantile <- logistic_sim %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x,   # x is the column to compute the quantiles
      probs = quantile_vals
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(names_from = "q_name", values_from = "q_val", 
              names_glue = "q{q_name}")

```


While this code is a little more involved, let's break it down piece by piece:

- To make things easier the variable `quantile_vals` computes the different quantiles, expressed between 0 to 1 - so 2.5% is 0.025, 50% is 0.5, and 97.5% is .975.
- As above, we are still grouping by the variable `t` and summarizing our data frame.
- However rather than applying the mean, we are using the command `quantile`, whose value is computed with the new column `q_val`. Like the mean, we define to which columns we apply the quantile function in the column `value`. We use `probs = quantile_vals` to specify the quantiles that we wish to compute.
- We also create a new column called `q_name` the contains the names of the quantile probabilities.
- The command `pivot_wider` takes the values in `q_val` with the associated names in `q_name` and creates new columns associated with each quantile. This process of making a data frame wider is the opposite of making a skinny and tall data frame with `pivot_longer`. A key convention with column names is not to start them with a number, so we *glue* a `q` onto the names of the column using the option `names_glue = "q{q_name}"`.

Wow. This is getting involved. One thing to keep in mind is that the the code as written should be easily adaptable if you need to compute an ensemble average. If you take an introductory data science or data visualization course I bet you will learn more about the role of pivoting data - but for now you can just adapt the above code to your needs


### Visualize
To plot the 95% confidence interval we introduce the plot geom called `geom_ribbon`. Applying `geom_ribbon` requires a few more aesthetics (`ymin` and `ymax`, or the minimum and maximum $y$ values to be plotted). The option `alpha = 0.2` refers to the transparency of the plot. The `fill` aesthetic just provides the shading (in other words the fill) between `ymin` and `ymax`.

```{r eval=FALSE}
logistic_quantile %>%
  ggplot() +
  geom_line(aes(x = t, y = q0.5), 
            color = "red", size = 2, inherit.aes = FALSE
            ) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975),
              alpha = 0.2, fill = "red", inherit.aes = FALSE
              ) +
  guides(color = "none") +
  labs(x = "Time", y = "Ensemble average")
```

```{r logistic-conf,fig.cap="Ensemble average plot for the logistic differential equation with 500 random initial conditions.",echo=FALSE}
logistic_quantile %>%
  ggplot() +
  geom_line(aes(x = t, y = q0.5), 
            color = "red", size = 1, inherit.aes = FALSE
            ) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975),
              alpha = 0.2, fill = "red", inherit.aes = FALSE
              ) +
  guides(color = "none") +
  labs(x = "Time", y = "Ensemble average") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```


Making ensemble averages isn't easy and requires strengthening your computational skills on several levels. Fortunately there are a lot of good tools when doing iteration and looping that easily allow you to adapt (shall I say iterate on?) existing examples to your needs. In future chapters we will move beyond random initial conditions to figuring out how the variables or parameters can be subject to random effects as time goes on.


## Exercises
<!-- EXERCISES - how are the logistic results different with varying k? varying r? what would you expect? - do a stochastic lotka-volterra or something stable (predator-prey) - find an ensemble average plot and interpret it. - do random initial conditions with neutral stability problem (birfurcation). - what if they plotted the ensemble phase plane? HMMM - bonus problem. -->
```{exercise}
Using the code to produce Figure \@ref(fig:logistic-conf):
  
  a. Adjust the `alpha` level to a number between 0 and 1. What does that do to the plot?
  b. Adjust the `fill` level to a color of your choosing. A list of R Colors can be found at the [R Color chart](http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/).

```

```{exercise}
Read the Chapter 12 (tidy data) in @wickham_r_2017. In this chapter you will learn about tidy data. Explain what tidy data is and the potential uses for pivoting data wider or longer.
```

```{exercise}
Look at the documentation for quantile (remember you can type `?quantile` at the command line to see the associated help for this function). Change the ensemble average in `quantile_vals` to compute the 25%, 50%, and 75% percentile for `logistic_sim`. Finally, produce a ensemble average plot of this percentile.
```


```{exercise}
Consider the logistic differential equation: $\displaystyle \frac{dx}{dt} = rx\left(1- \frac{x}{K}\right)$. The function `logistic_mod` below takes the initial value problem $x(0)=3$ and solves the differential equation.

a. Run `logistic_mod(r=0.8,K=100)` and plot its result.
b. Run 500 simulations with varying $r$ chosen from a uniform distribution with minimum value of 0.4 and maximum value of 1.0. Create a spaghetti and ensemble average plot. Set $K=100$.
c. Run 500 simulations with varying $K$ chosen from a uniform distribution with minimum value of 50 and maximum value of 150. Create a spaghetti and ensemble average plot. Set $r=0.8$.
d. Compare your results along with Figures \@ref(fig:logistic-example-many) and \@ref(fig:logistic-conf). How does randomizing the initial condition or the parameters affect the results?

```


```{r,eval=FALSE}
logistic_mod <- function(r,K) {
  logistic_eq <- c(dx ~ r * x * (1 - x / K)) # Define the rate equation
  
  params <- c(r=r,K=K) # Identify any parameters
  
  init_cond <- c(x = 3) # Initial condition
  soln <- euler(
    system_eq = logistic_eq,
    initial_condition = init_cond,
    parameters = params,
    deltaT = .05,
    n_steps = 200
  )
  
  return(soln)
}
```

```{exercise}
Using the data frame `my_table`, compare the following code below. The data frame `table1` is skinny and long, and the second data frame `table2` is called short and wide. Why did we need to make this data frame short and wide for plotting?

```

```{r code-chunk-compare,eval=FALSE}
# First code chunk
table1 <- my_table %>%
  rowwise(t) %>%
  summarise(q_val = quantile(c_across(starts_with("sim")),
                                probs = quantile_vals),
            q_name = quantile_vals)

# Second code chunk
table2 <- my_table %>%
  rowwise(t) %>%
  summarise(q_val = quantile(c_across(starts_with("sim")),
                                probs = quantile_vals),
            q_name = quantile_vals) %>%
  pivot_wider(names_from = "q_name",values_from="q_val",
              names_glue = "q{q_name}")
```


```{exercise}
Consider the following differential equation:

\begin{equation}
\begin{split}
\frac{dx}{dt} =-y-x(x^2+y^2-1) \\
\frac{dy}{dt}=x-y(x^2+y^2-1)
\end{split}
\end{equation}

a. Generate a phase plane for this differential equation. Store this phase plane in a variable called `pp1`. Set your x and y windows to be between $-1$ and $1$.
b. The code below defines a function `limit_cycle_mod` that creates a solution trajectory of the differential equation. Super-impose a few different solution trajectories with random initial conditions onto your phase plane (`pp1`). Use initial conditions `x0` and `y0` between 0 and 1. Be sure to use the plot geom `geom_path`.
c. Modify the code from this chapter to run 50 different simulations with random initial conditions `x0` and `y0` between 0 and 1. *Note:* It may be helpful to include the code `map(~ limit_cycle_mod(runif(1),runif(1)))`.
d. Plot the initial conditions from your simulation onto your phase plane. Isn't the result pretty?
  
```

```{r eval=FALSE}
limit_cycle_mod <- function(x0,y0) {
  limit_cycle_eq <- c(dx ~ -y-x*x*(x^2+y^2-1),
                   dy ~ x-y*(x^2+y^2-1) ) # Define the rate equation

  init_cond = c(x=x0,y=y0)
  soln <- rk4(
    system_eq = limit_cycle_eq,
    initial_condition = init_cond,
    deltaT = .05,
    n_steps = 200
  )
  
  return(soln)
}

```

<!--chapter:end:22-stochasticSimulation.Rmd-->

# Random Walks {#random-walks-23}

Chapters \@ref(stoch-sys-21) and \@ref(stoch-sim-22) introduced the concept of stochastic dynamical systems and ways to compute ensemble averages. In this chapter we will begin to develop some tools to understand stochastic differential equations by studying the concept of _random walks_.\index{random walk} While exploring random walks may seem like a diversion from understanding stochastic differential equations, there are deep connections between the two topics. We will do some interesting computational exercises that may lead to some non-intuitive results. Curious? Let's get started!

## Random walk on a number line
The conceptual idea of a random walk begins on a number line. Let's begin at the origin (so at $t=0$ then $x=0$). Based on this number line we can only move to the left or the right, with equal probability. At a given time we decide to move in a direction based on a random number $r$ drawn between 0 and 1 (in `R` we do this with the command `runif(1)`). Figure \@ref(fig:random-walk) conceptually illustrates this random walk

```{tikz, random-walk,warning=FALSE,message=FALSE,echo=FALSE,fig.align="center",fig.cap="Schematic diagram for one-dimensional random walk."}
\begin{tikzpicture}
    \draw (-3,0) -- (3,0);
    \foreach \i in {-3,-2,...,3} % numbers on line
      \draw (\i,0.1) -- + (0,-0.2) node[below] (\i) {$\i$};
   % \foreach \i in {0.5, 0.7, 0.9}% points on line
    \fill[red] (0,0) circle (1 mm);
   \node[align=center] at (-1,0.5) {$0 \leq r < 0.5$};
    \draw [->,red,thick] (0,0.15) to [out=150,in=30] (-1,0.15);
    \node[align=center] at (1,0.5) {$0.5 \leq r \leq 1$};
    \draw [->,red,thick] (0,0.15) to [out=30,in=150] (1,0.15);
\end{tikzpicture}
```


For each iteration of this process we will draw a random number using `runif(1)`. We can code this process using a `for` loop. Try the following code out on your own:

```{r,eval=FALSE}
# Number of steps our random walk takes
number_steps <- 100

# Set up vector of results
x <- array(0, dim = number_steps)

for (i in 2:number_steps) {
  if (runif(1) < 0.5) {
    x[i] <- x[i - 1] - 1
  } # Move right
  else {
    x[i] <- x[i - 1] + 1
  } # Move left
}

# Let's take a peek at our result:

plot(x, type = "l")

print(mean(x)) # Average position over the time interval
print(sd(x)) # Standard deviation over the time interval
```


Let's remind ourselves what this code does:

- `number_steps <- 100`: The number of times we draw a random number, referred to *steps*.
- `x <- array(0,dim=number_steps)`: We are going to pre-allocate a vector (`array`) of our results. Values in this array are all set at 0 for convenience.
- The for loop starts at the second step and then either adds or subtracts one from the prevoius position `x[i-1]` and updates the result to `x[i]`.
- `plot(x,type='l')` makes a simple line plot of the results.

Now that you have run this code, try running it again. Do you get the same result?  I hope you didn't - because this process is random!  It is interesting to run it several times because there can be a wide variance in our results - for some realizations of the sample path, we end up being strictly positive, other times we go negative, and other times we just hover around the middle line ($x=0$) in the plot.

## Iteration and ensemble averages
We can apply the workflow (Do once $\rightarrow$ Do several times $\rightarrow$ Summarize $\rightarrow$ Visualize) from Chapter \@ref(stoch-sim-22) on this random walk process to investigate what happens when we run additional simulations and compute the ensemble average. Let's apply this workflow:


For the "Do once" step, we will define a function called `random_walk` that has the number of steps as an input:

```{r}
random_walk <- function(number_steps) {

  # Set up vector of results
  x <- array(0, dim = number_steps)

  for (i in 2:number_steps) {
    if (runif(1) < 0.5) {
      x[i] <- x[i - 1] - 1
    } # Move right
    else {
      x[i] <- x[i - 1] + 1
    } # Move left
  }

  out_x <- tibble(t = 0:(number_steps - 1), x)
  return(out_x)
}
```

Next to "Do several times" we will run this random walk function for 100 steps and 500 simulations and display the spaghetti plot in Figure \@ref(fig:spaghetti-rw-23):

```{r eval=FALSE}
number_steps <- 100 # Then number of steps in random walk
n_sims <- 500 # The number of simulations

# Compute solutions
random_walk_sim <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ random_walk(number_steps)) %>%
  map_dfr(~.x, .id = "simulation")

# Plot these all together
ggplot(data = random_walk_sim, aes(x = t, y = x)) +
  geom_line(aes(color = simulation)) +
  ggtitle("Random Walk") +
  guides(color = "none") +
  labs(x="Steps")
```

```{r spaghetti-rw-23,fig.cap="Spaghetti plot of 500 simulations for the random walk.",echo=FALSE,message=FALSE,warning=FALSE}
number_steps <- 100 # Then number of steps in random walk
n_sims <- 500 # The number of simulations

# Compute solutions
random_walk_sim <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ random_walk(number_steps)) %>%
  map_dfr(~.x, .id = "simulation")

# Plot these all together
ggplot(data = random_walk_sim, aes(x = t, y = x)) +
  geom_line(aes(color = simulation)) +
  ggtitle("Random Walk") +
  guides(color = "none") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```


For the "Summarize" and "Visualize" steps we will compute the 95% confidence interval, structuring the code similar to Figure \@ref(fig:logistic-conf) from Chapter \@ref(stoch-sim-22). Try writing this code out on your own, but the results are shown in Figure \@ref(fig:ensemble-ave-23).

```{r ensemble-ave-23,fig.cap="Ensemble average of 500 simulations for the random walk.",echo=FALSE,warning=FALSE,message=FALSE}
# Compute Quantiles
quantile_vals <- c(0.025, 0.5, 0.975)

random_walk_quantile <- random_walk_sim %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x, # x is the column to compute the quantiles
      probs = quantile_vals
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(
    names_from = "q_name", values_from = "q_val",
    names_glue = "q{q_name}"
  )


# Plot Ensemble Average
ggplot(data = random_walk_quantile) +
  geom_line(aes(x = t, y = q0.5),
    color = "red", size = 1, inherit.aes = FALSE
  ) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975),
    alpha = 0.2, fill = "red", inherit.aes = FALSE
  ) +
  guides(color = "none") +
  labs(x = "Steps", y = "x") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  ggtitle("Ensemble average")
```



Two interesting things are occurring in Figures \@ref(fig:spaghetti-rw-23) and \@ref(fig:ensemble-ave-23). First, Figure \@ref(fig:ensemble-ave-23) suggests that on *average* you go nowhere (in other words, the average position is $x=0$), but as the number of steps increases, you are very likely to be *somewhere* (in other words, the confidence interval increases as the number of steps increases). Second, the 95% confidence interval appears to be a square root function $y=a\sqrt{t}$. One way that we could confirm this is by running more realizations and investigating the ensemble average (Exercise \@ref(exr:rw-sim)).


## Random walk mathematics
Another way to corroborate our observations in Figures \@ref(fig:spaghetti-rw-23) and \@ref(fig:ensemble-ave-23) is with mathematics.
First we define some terminology and notation. Call $x^{n}$ the position $x$ at step $n$ in a random walk that increments with step size $\Delta x$ (Equation \@ref(eq:evol-eqn):

\begin{equation}
x^{n}=x^{n-1}+r_{s} \; \Delta x = \sum_{s=1}^{n} \Delta x \; r_{s}, (\#eq:evol-eqn)
\end{equation}

with $\Delta x$ being the jump size (in our example above $\Delta x=1$), and $s$ the particular step in our random walk. We denote $r_{s}$ as a random variable that takes the value of $-1$ or $1$. Another way to write $r_{s}$ is with Equation \@ref(eq:equal-prob):

\begin{equation}
r_{s}=\begin{cases} -1 & p(-1)=0.5 \\
1 & p(1)=0.5 \end{cases} (\#eq:equal-prob)
\end{equation}

The way to interpret Equation \@ref(eq:evol-eqn) is that the variable $r_{s}$ equals $-1$ ($r_{s}=-1$) with probability 0.5 ($p(-1)=0.5$). We can write Equation \@ref(eq:evol-eqn) as a single summation because the position depends on the different values of $-1$ or 1 generated at each step, only as long as we keep track of the sequence of $-1$ or 1 for each step $s$.

When we run multiple simulations we will use the notation $x_{j}^{n}$, which is the position at step $n$ for simulation $j$. 


Let's introduce some terminology to help us out here. The quantity $\displaystyle \big \langle X \big \rangle = \sum_{i=1}^{n} p(X) \cdot X$ is the *expected value* \index{expected value} for a discrete random variable. What we do is weight the value of each possibility by its probability. For the random variable $r_{s}$ we have:

\begin{equation}
\big \langle r_{s} \big \rangle = (1) \cdot 0.5 + (- 1) \cdot 0.5 = 0
\end{equation}

Nice! The expected value is a linear operator^[This means that for random variables $X$ and $Y$ with constants $a$ and $b$: $\displaystyle \big \langle aX+bY \big \rangle = a \cdot \big \langle X \big \rangle + b \cdot \big \langle Y \big \rangle$.]\index{linear operator}.

The next step is to determine the expected value of $x^{n}$. Here we have $J$ different simulations; this will be computed as $\displaystyle \big \langle x^{n} \big \rangle = \frac{1}{J} \sum_{j=1}^{J} p(x_{j}^{n}) \cdot x_{j}^{n}$. We can further compute the expected value using Equation \@ref(eq:evol-eqn):

\begin{equation}
\big \langle x^{n} \big \rangle = \big \langle \sum_{j=1}^{J} \left( \Delta x \; r_{s} \right) \big \rangle = \sum_{j=1}^{J} \left( \big \langle \Delta x \; r_{s} \big \rangle \right) = \sum_{j=1}^{J} \Delta x  \cdot \big \langle \; r_{s} \big \rangle  = 0
\end{equation}

Imagine that! All of this mathematics leads to the conclusion that the expected value of $x^{n}$ is zero! In other words, on average a *random walk goes nowhere*!

### Random walk variance
Now that we have characterized the expected value we will determine the variance^[The definition of the variance of a random variable $X$ is $\big \langle (X-\mu)^{2} \big \rangle$, where $\mu$ is the expected value. For our case $\mu$ equals 0.] \index{variance} of $x^{n}$, or $\langle (x^{n})^{2} \rangle$. This is still a lot of work with summations, but it is worth it! First, multiply out the square of Equation \@ref(eq:evol-eqn) using properties of summation:

\begin{equation}
(x^{n})^{2} = \left( \sum_{s=1}^{n} \left( \Delta x \; r_{s} \right) \right)^{2} = \left( \Delta x \right)^{2}  \sum_{s=1}^{n} \sum_{t=1}^{n}  (r_{s} \cdot r_{t} )
\end{equation}

Now when we compute the expected value for $\displaystyle \big \langle (x^{n})^{2} \big \rangle$ we need to consider the double summation - so this means there are two different indices $s$ and $t$. However because of summation properties we have:

\begin{equation}
\big \langle (x^{n})^{2}  \big \rangle =  \left( \Delta x \right)^{2}  \sum_{s=1}^{n} \sum_{t=1}^{n}  \big \langle r_{s} \cdot r_{t} \big \rangle
\end{equation}


So really we need to consider the term $\displaystyle \big \langle r_{s} \cdot r_{t}  \big \rangle$. While that may seem scary, let's break it down into two cases: when $t=s$ and $t \neq s$:

| - Case $t \neq s$: Here we need to multiply together all the possible combinations for the random variable $r_{s} \cdot r_{t}$. There are four possibilities: $r_{s} \cdot r_{t}=(-1) \cdot (1)$, $r_{s} \cdot r_{t}=(-1) \cdot (-1)$, $r_{s} \cdot r_{t}=(1) \cdot (1)$, $r_{s} \cdot r_{t}=(1) \cdot (-1)$. When we multiply these results and consider the different probabilities associated with them we get the following random variable for $r_{s} \cdot r_{t}$:

\begin{equation}
r_{s} \cdot r_{t} =\begin{cases} -1 & p(-1)=0.5 \\
1 & p(1)=0.5 \end{cases} (\#eq:equal-prob-two)
\end{equation}

| Then the expected value is the following:

\begin{equation}
\big \langle r_{s} \cdot r_{t} \big \rangle = (1) \cdot 0.5 + (-1) \cdot 0.5 = 0
\end{equation}


| - Case $t = s$: This case is a little easier. In both instances (when $r_{t}=1$ or $r_{t}=-1$) the variable $r_{t} \cdot r_{t}$ equals 1, so the expected value $\displaystyle \big \langle r_{s} \cdot r_{t} \big \rangle$ is simply 1! So as a result:

\begin{equation}
\begin{split}
\big \langle (x^{n})^{2} \big \rangle &= \left( \Delta x \right)^{2}  \sum_{s=1}^{n} \big \langle r_{s}^{2} \big \rangle \\
& = \left( \Delta x \right)^{2}  \sum_{s=1}^{n} 1\\
&= n \left( \Delta x \right)^{2}
\end{split} (\#eq:rw-variance-final)
\end{equation}

| Equation \@ref(eq:rw-variance-final) tells us that the variance, or the mean square displacement, is proportional to $n$. Another way to state this is that the standard deviation (the square root of the variance) is equal to $\pm \sqrt{n} \; \Delta x$, where $n$ is the current step. This matches up with our graphs from earlier since $\Delta x =1$!  Informally, the variance tells us that on *average* you go nowhere, but *eventually* you travel everywhere - how cool!

## Continuous random walks and diffusion
On a final note, we can extend the discrete random walk to continuous time. This process is perhaps similar to how you may have seen that the Riemann sum to (discretely) approximate the area underneath a curve and the horizontal axis becomes a definite integral.

Define the variable $t$ such that $t= n \Delta t$. Equivalently $\displaystyle n = \frac{t}{\Delta t}$. With this information we can use Equation \@ref(eq:rw-variance-final) to do the following:

\begin{equation}
\big \langle (x^{n})^{2} \big \rangle = \frac{t}{\Delta t} ( \Delta x)^{2}. (\#eq:rw-diffusion)
\end{equation}

The quantity $\displaystyle D = \frac{( \Delta x)^{2}}{2 \Delta t}$ is known as the *diffusion coefficent*.\index{diffusion}  So then the mean square displacement can be arranged as $\langle (x^{n})^{2} \rangle  = 2Dt$, confirming again that the variance grows proportional to $t$.

To connect this back to our discussion of stochastic differential equations, understanding random walks helps us to understand how demographic and environmental stochasticity may affect a differential equation. An excellent, highly readable book on random walks in biology is @berg_random_1993. Since there is a randomness to solution trajectories, we will repeatedly use ensemble averages (developed in Chapter \@ref(stoch-sim-22)) to understand the expected behaviors of a stochastic differential equation. The next chapters will apply the workflows studied here to investigate the connections between random walks and stochastic differential equations.

## Exercises

```{exercise}
When doing the random walk mathematics, we made the claim that $\displaystyle x^{n} = \sum_{s=1}^n \Delta x \, r_{s}$, where $r_{s}$ takes on the value of $-1$ or $1$. Set $\Delta x = 1$ and do a random walk for 5 steps, keeping track whether the value of $r$ is $-1$ or $1$ at each step. Does the final position equal the sum of all the values of $-1$ or $1$?
```

```{exercise}
Let $r_{s}$ be the random variable defined by Equation \@ref(eq:equal-prob).


a. Multiply out the following summation: $\displaystyle \sum_{s=1}^{2} \sum_{t=1}^{2} (r_{s} r_{t} )$.
b. Use the previous result to compute the expected value $\displaystyle \big \langle \sum_{s=1}^{2} \sum_{t=1}^{2} r_{s} \cdot r_{t} \big \rangle$

```

```{exercise rw-sim}
Re-run the code used to generate Figure \@ref(fig:ensemble-ave-23), but where the number of realizations is set to 1000 and 5000 (this may take some time to compute). Do your results conform to the observation that the expected position is zero, but the uncertainty grows as the number of steps increases? Can you determine the value of $a$ such that $y=a\cdot\sqrt{n}$ that parameterizes the 95% confidence interval as a function of $n$?
```

```{exercise}
Use the fact that the diffusion coefficient is equal to $\displaystyle D = \frac{ (\Delta x)^{2}}{2\Delta t}$ to answer the following questions.  


a. Solve $\displaystyle D = \frac{ (\Delta x)^{2}}{2\Delta t}$ to isolate $\Delta t$ on one side of the expression.
b. The diffusion coefficient for oxygen in water is approximately $10^{-5}$ cm$^{2}$ sec$^{-1}$. Use that value to complete the following table: 

| **Distance ($\Delta x$)** | 1 $\mu$m = 10$^{-6}$ m  | 10 $\mu$m | 1 mm  | 1 cm | 1 m  | 
|:------:|:-----:|:------:|:-----:|:------:|:-----:|
| **Diffusion time ($\Delta t$)** |  |  |   | |  | 

Report the diffusion time in an appropriate unit (seconds, minutes, hours, years) accordingly.

c. Navigate to the following website, which lists [sizes of different cells:](https://en.wikibooks.org/wiki/Cell_Biology/Introduction/Cell_size). For what cells would diffusion be a reasonable process to transport materials?

```

```{exercise}
Consider Equation \@ref(eq:rw-diffusion). Evaluate separately the effect of $\Delta x$ and $\Delta t$ on the variance. How would you characterize the variance if either of them independently is small or large?
```


```{exercise}
Compute $\langle r \rangle$ for the following random variable:
  
\begin{equation} 
r=\begin{cases} -1 & p(-1)=0.52 \\
1 & p(1)=0.48 \end{cases}
\end{equation}

```



```{exercise}
Compute $\langle r \rangle$ for the following random variable:
  
\begin{equation} 
r=\begin{cases} -1 & p(-1)=q \\
1 & p(1)=(1-q) \end{cases}
\end{equation}
```



```{exercise}
Consider the following random variable:
  
\begin{equation}
r_{q}=
\begin{cases}
-1 & p(-1) = 1/3 \\
0 & p(0)= 1/3\\
1 & p(1)=1/3
\end{cases}
\end{equation}


a. Modify the code for the one-dimensional random walk to generate a simulation of this random walk and plot your result. You can do this by applying an `if` `else` statement as shown in the code chunk below.
b. Compute $\displaystyle \langle r_{q} \rangle$ and $\displaystyle \langle r_{q}^{2} \rangle$.
c. Based on your last answer, explain how this random variable introduces a different random walk than the one described in this chapter. In what ways would this random walk change the calculations for the mean and variance of the ensemble simulations?

```


```{r,eval=FALSE}
# Code for random variable r_q:
p <- runif(1)
if (p < 1 / 3) {
  x[i] <- x[i - 1] - 1
} else if (1 / 3 <= p & p < 2 / 3) {
  x[i] <- x[i - 1]
} else {
  x[i] <- x[i - 1] + 1
}
```
 
```{exercise random-2d-23}
In this exercise you will write code and simulate a two-dimensional random walk. In a given step you can either move (1) left, (2) right, (3) up, or (4) down. (You cannot move up and left for example). The random walk starts at $(x,y)=(0,0)$. With $\Delta x = 1$, the random walk at step $n$ can be described by $\displaystyle (x,y)^{n} = \sum_{s=1}^{n} r_{d}$, where $r_{d}$ is one of the four motions, represented as a coordinate pair. (A movement up is $r_{d}=(0,1)$ for example.) 


a. Define a variable $r_{d}$ that models the motion from step to step.
b. Modify the code for the one-dimensional random walk to incorporate this two-dimensional random walk. One way to do this is to create a variable $y$ structured similar to $x$, and to have multiple `if` statements in the `for` loop that moves `y`.
c. Plot a few different realizations of your sample paths.
d. If we were to compute the mean and variance of the ensemble simulations, what do you think they would be?

```

<!--chapter:end:23-randomWalks.Rmd-->

# Diffusion and Brownian Motion {#diffusion-24}

Studying random walks in Chapter \@ref(random-walks-23) led to some surprising results, namely that for an unbiased random walk the mean displacement was zero but the variance increased proportional to the step number. In this chapter we will revisit the random walk problem from another perspective that further strengthens its connection to understanding diffusion. Let's get started!

## Random walk redux
The random walk derivation in Chapter \@ref(random-walks-23) focused on the *position* of a particle on the random walk, based upon prescribed rules of moving to the left and the right.\index{random walk} To revisit this random walk we consider the *probability* (between 0 and 1) that a particle is at position $x$ in time $t$, denoted as $p(x,t)$.\index{probability distribution!stochastic differential equation solution} In other words, rather than focusing on where the particle *is*, we focus on the *chance* that the particle will be at a given spot.

A way to conceptualize a random walk is that any given position $x$, a particle can arrive to that position from either the left or the right (Figure \@ref(fig:random-walk-24)):


```{tikz,random-walk-24,warning=FALSE,message=FALSE,echo=FALSE,fig.cap="Schematic diagram for the one-dimensional random walk."}
\begin{tikzpicture}
    \draw (-3,0) -- (3,0);
    \foreach \i in {-3,-2,...,3} % numbers on line
      \draw (\i,0.1) -- + (0,-0.2) node[below] (\i) {$\i$};
   % \foreach \i in {0.5, 0.7, 0.9}% points on line
    \fill[red] (0,0) circle (1 mm);
     \draw[red] (-1,0) circle (1 mm);
     \draw[red] (1,0) circle (1 mm);
   \node[align=center] at (-1.1,0.5) {$p(-1,t)$};
    \draw [<-,red,thick] (-.05,0.15) to [out=150,in=30] (-1,0.15);
    \node[align=center] at (1.1,0.5) {$p(1,t)$};
       \node[align=center] at (1.9,-1) {$\displaystyle p(0,t+\Delta t)=\frac{1}{2} p(-1,t)+\frac{1}{2} p(1,t)$};

    \draw [<-,red,thick] (0.05,0.15) to [out=30,in=150] (1,0.15);
\end{tikzpicture}
```

We can generalize Figure \@ref(fig:random-walk-24) further where the particle moves in increments $\Delta x$, as defined in Equation \@ref(eq:master-diff):

\begin{equation}
p(x,t+\Delta t) = \frac{1}{2} p(x-\Delta x,t) + \frac{1}{2} p(x+\Delta x,t) (\#eq:master-diff)
\end{equation}

To analyze Equation \@ref(eq:master-diff) we apply Taylor approximations on each side of Equation \@ref(eq:master-diff).\index{Taylor approximation}  First let’s do a locally linear approximation for $p(x,t+\Delta t)$:

\begin{equation}
p(x,t+\Delta t)  \approx p(x,t) + \Delta t \cdot p_{t},
\end{equation}

where we have dropped the shorthand $p_{t}(x,t)$ as $p_{t}$. On the right hand side of Equation \@ref(eq:master-diff) we will compute the 2nd degree (quadratic) Taylor polynomial:

\begin{align*}
\frac{1}{2} p(x-\Delta x,t)  & \approx \frac{1}{2} p(x,t) -   \frac{1}{2} \Delta x \cdot p_{x} + \frac{1}{4} (\Delta x)^{2}\cdot  p_{xx} \\
\frac{1}{2} p(x+\Delta x,t)  & \approx \frac{1}{2} p(x,t) +   \frac{1}{2} \Delta x \cdot p_{x} + \frac{1}{4} (\Delta x)^{2} \cdot p_{xx}
\end{align*}

With these approximations we can re-write Equation \@ref(eq:master-diff) as Equation \@ref(eq:new-master-diff):

\begin{equation}
\Delta t \cdot p_{t} = \frac{1}{2} (\Delta x)^{2} p_{xx} \rightarrow  p_{t} = \frac{1}{2} \frac{(\Delta x)^{2}}{\Delta t} \cdot p_{xx} (\#eq:new-master-diff)
\end{equation}

Equation \@ref(eq:new-master-diff) is called a partial differential equation - what this means is that it is a differential equation with derivatives that depend on two variables ($x$ and $t$ (two derivatives).\index{partial differential equation} As studied in Chapter \@ref(random-walks-23), Equation \@ref(eq:new-master-diff) is called the **diffusion equation**.\index{diffusion} In Equation \@ref(eq:new-master-diff) we can also define $\displaystyle D =  \frac{1}{2} \frac{(\Delta x)^{2}}{\Delta t}$ so $p_{t}=D \cdot p_{xx}$.

The solution to Equation \@ref(eq:new-master-diff) is given by Equation \@ref(eq:diff-eq-soln).^[Determining an exact solution to the diffusion equation requires more study in techniques of partial differential equations (see @keener_biology_2021).]

\begin{equation}
 p(x,t) = \frac{1}{\sqrt{4 \pi Dt} } e^{-x^{2}/(4 D t)} (\#eq:diff-eq-soln)
 \end{equation}



What Equation \@ref(eq:diff-eq-soln) represents is the probability that the particle is at the position $x$ at time $t$. Figure \@ref(fig:diffusion-profile) shows *profiles* for $p(x,t)$ when $D=0.5$ at different values of $t$.


```{r diffusion-profile,echo=FALSE,fig.cap="Profiles of $p(x,t)$ (Equation \\@ref(eq:diff-eq-soln)) for different values of $t$ with $D = 0.5$."}
# sigma = sqrt(2*D*t) set #D = 1/2
x <- seq(-10, 10, length.out = 100)
y1 <- dnorm(x, mean = 0, sd = 1) # t = 1
y2 <- dnorm(x, mean = 0, sd = sqrt(2)) # t = 2
y3 <- dnorm(x, mean = 0, sd = 4) # t = 16
y4 <- dnorm(x, mean = 0, sd = sqrt(32)) # t = 32

data.frame(x, y1, y2, y3, y4) %>%
  gather(key = run, value = y, -1) %>%
  ggplot(aes(x = x, y = y, color = as.factor(run))) +
  geom_line(size = 1) +
  theme(legend.position = "bottom") +
  ylab("p(x,t)") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name = NULL, labels = c("t = 1", "t = 2", "t = 16", "t = 32"))
```

As you can see, as time increases the graph of $p(x,t)$ gets flatter - or more uniform. What this tells you is that the longer $t$ increases it is less likely to find the particle at the origin.

### Verifying the solution to the diffusion equation
Verifying that Equation \@ref(eq:diff-eq-soln) is the solution to Equation \@ref(eq:new-master-diff) is a good review of your multivariable calculus skills!  As a first step to verifying this solution, let's take the partial derivative with respect to $x$ and $t$.

First we will compute the partial derivative of $p$ with respect to the variable $x$ (represented as $p_{x}$):

\begin{align*}
p_{x} &= \frac{\partial }{\partial x} \left( \frac{1}{\sqrt{4 \pi Dt} } e^{-x^{2}/(4 D t)} \right) \\
&= \frac{1}{\sqrt{4 \pi Dt} } e^{-x^{2}/(4 D t)} \cdot \frac{-2x}{4Dt}
\end{align*}

Notice something interesting here:  $\displaystyle p_{x} = p(x,t) \cdot \left( \frac{-x}{2Dt} \right)$.

To compute the second derivative, we have the following expressions by applying the product rule:

\begin{align*}
p_{xx} &= p_{x} \cdot \left( \frac{-x}{2Dt} \right) - p(x,t) \cdot \left( \frac{1}{2Dt} \right) \\
&= p(x,t) \cdot \left( \frac{-x}{2Dt} \right) \cdot \left( \frac{-x}{2Dt} \right)- p(x,t) \cdot \left( \frac{1}{2Dt} \right) \\
&= p(x,t) \left( \left( \frac{-x}{2Dt} \right)^{2} - \left( \frac{1}{2Dt} \right) \right) \\
&= p(x,t) \left( \frac{x^{2}-2Dt}{(2Dt)^{2}}\right).
\end{align*}


So far so good. Now computing $p_{t}$ gets a little tricky because this derivative involves both the product rule with the chain rule in two places (the variable $t$ appears twice in the formula for $p(x,t)$).\index{chain rule}\index{product rule}  To aid in computing the derivative we identify two functions $\displaystyle f(t) = (4 \pi D t)^{-1/2}$ and $\displaystyle g(t) = -x^{2} \cdot (4Dt)^{-1}$. This changes $p(x,t)$ into $p(x,t) = f(t) \cdot e^{g(t)}$. In this way $p_{t} = f'(t) \cdot e^{g(t)} + f(t) \cdot e^{g(t)} \cdot g'(t)$. Now we can focus on computing the individual derivatives $f'(t)$ and $g'(t)$ (after simplification - be sure to verify these on your own!):

\begin{align*}
f'(t) &= -\frac{1}{2} (4 \pi D t)^{-3/2} \cdot 4 \pi D = -2\pi D \; (4 \pi D t)^{-3/2}  \\
g'(t) &= x^{2}\; (4Dt)^{-2} 4D = \frac{x^{2}}{4Dt^{2}}
\end{align*}

Assembling these results together, we have the following:

\begin{align*}
p_{t} &= f'(t) \cdot e^{g(t)} + f(t) \cdot e^{g(t)} \cdot g'(t) \\
&= -2\pi D \; (4 \pi D t)^{-3/2} \cdot e^{-x^{2}/(4 D t)}  + \frac{1}{\sqrt{4 \pi Dt} } \cdot e^{-x^{2}/(4 D t)} \cdot \frac{x^{2}}{4Dt^{2}} \\
&= \frac{1}{\sqrt{4 \pi Dt} } \cdot e^{-x^{2}/(4 D t)} \left( -2 \pi D \; (4 \pi D t)^{-1} +  \frac{x^{2}}{4Dt^{2}} \right) \\
&= \frac{1}{\sqrt{4 \pi Dt} } \cdot e^{-x^{2}/(4 D t)} \left( -\frac{1}{2t} +  \frac{x^{2}}{4Dt^{2}} \right) \\
&= p(x,t)  \left( -\frac{1}{2t} +  \frac{x^{2}}{4Dt^{2}} \right)
\end{align*}


Wow. Verifying that Equation \@ref(eq:diff-eq-soln) is a solution to the diffusion equation is getting complicated, but also notice that through algebraic simplification, $\displaystyle p_{t} = p(x,t)  \left(\frac{x^{2}-2Dt}{4Dt^{2}} \right)$. When we compare $p_{t}$ to $D p_{xx}$, they are equal!


 The connections between diffusion and probability are so strong. Equation \@ref(eq:diff-eq-soln) is related to the formula for a normal probability density function (Equation \@ref(eq:normal-09) from Chapter \@ref(likelihood-09))! In this case, the standard deviation in Equation \@ref(eq:diff-eq-soln) equals $\sqrt{2Dt}$ and is time dependent (see Exercise \@ref(exr:normal-compare)). Even though we approached the random walk differently here compared to Chapter \@ref(random-walks-23), we also saw that the variance grew proportional to the time spent, so there is some consistency.

## Simulating Brownian motion
Another name for the process of a particle undergoing small random movements is *Brownian Motion*.\index{Brownian motion}  We can simulate Brownian motion similar to the random walk as discussed in Chapter \@ref(random-walks-23). Brownian motion is connected to the diffusion equation (Equation \@ref(eq:new-master-diff)) and its solution (Equation \@ref(eq:diff-eq-soln)). These connections are helpful when simulating stochastic differential equations. To simulate Brownian motion we will also apply the workflow from Chapter \@ref(stoch-sim-22) (Do once $\rightarrow$ Do several times $\rightarrow$ Summarize $\rightarrow$ Visualize).

### Do once
First we define a function called `brownian_motion` that will compute a sample path given the:

- number of steps to run the stochastic process;
- diffusion coefficient $D$;
- timestep $\Delta t$;

a sample path will be computed (see Figure \@ref(fig:sample-brownian)).

```{r sample-brownian, fig.cap="Sample trajectory for a realization of Brownian motion. The horizontal axis represents a step of the random walk."}
brownian_motion <- function(number_steps, D, deltaT) {
  # D: diffusion coefficient
  # deltaT: timestep length
  ### Set up vector of results
  x <- array(0, dim = number_steps)

  for (i in 2:number_steps) {
    x[i] <- x[i - 1] + sqrt(2 * D * deltaT) * rnorm(1)
  }

  out_x <- tibble(t = 0:(number_steps - 1), x)
  return(out_x)
}

# Run a sample trajectory and plot
try1 <- brownian_motion(100, 0.5, 0.1)

plot(try1, type = "l")
```

### Do several times
Once we have the function for Brownian motion defined we can then run this process several times and plot the spaghetti plot (try the following code out on your own):

```{r eval = FALSE}

number_steps <- 200 # Then number of steps in random walk
D <- 0.5 # The value of the diffusion coefficient
dt <- 0.1 # The timestep length

n_sims <- 500 # The number of simulations

# Compute solutions
brownian_motion_sim <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ brownian_motion(number_steps, D, dt)) %>%
  map_dfr(~.x, .id = "simulation")

# Plot these all together
ggplot(data = brownian_motion_sim, aes(x = t, y = x)) +
  geom_line(aes(color = simulation)) +
  ggtitle("Random Walk") +
  guides(color = "none")
```

### Summarize and visualize
Finally, the 95% confidence interval is computed and plotted in Figure \@ref(fig:ensemble-ave-24), using similar code from Chapter \@ref(stoch-sim-22) to compute the ensemble average. Note that the horizontal axis is time so each step is scaled by `dt`.


```{r ensemble-ave-24, warning=FALSE,message=FALSE,fig.cap="Ensemble average of 500 simulations for the random walk. Each step on the horizontal axis is scaled by `dt`.",echo=FALSE}
number_steps <- 200 # Then number of steps in random walk
D <- 0.5 # The value of the diffusion coefficient
dt <- 0.1 # The timestep length

n_sims <- 500 # The number of simulations

# Compute solutions
brownian_motion_sim <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ brownian_motion(number_steps, D, dt)) %>%
  map_dfr(~.x, .id = "simulation")

# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)

brownian_motion_quantile <- brownian_motion_sim %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x, # x is the column to compute the quantiles
      probs = quantile_vals
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(
    names_from = "q_name", values_from = "q_val",
    names_glue = "q{q_name}"
  )

# Plot Ensemble Average
ggplot(data = brownian_motion_quantile) +
  geom_line(aes(x = t*dt, y = q0.5),
    color = "red", size = 1, inherit.aes = FALSE
  ) +
  geom_ribbon(aes(x = t*dt, ymin = q0.025, ymax = q0.975),
    alpha = 0.2, fill = "red", inherit.aes = FALSE
  ) +
  guides(color = "none") +
  labs(x = "Time", y = "Ensemble average") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )
```

I sure hope the results are very similar to ones generated in Chapter \@ref(random-walks-23) (especially Figure \@ref(fig:ensemble-ave-23)) - this is no coincidence! With the ideas of a random walk developed here and in Chapter \@ref(random-walks-23), we will now be able to understand and simulate how small changes in a variable or parameter affect the solutions to a differential equation. Looking ahead to Chapters \@ref(sdes-25) and \@ref(simul-stoch-26), we will simulate stochastic processes using numerical methods (Euler's method in Chapter \@ref(euler-04)) with Brownian motion. Onward!



## Exercises
```{exercise}
Through direct computation, verify the following calculations:

a. When $\displaystyle f(t)=\frac{1}{\sqrt{4 \pi Dt} }$, then $\displaystyle f'(t)=-2\pi D (4 \pi D t)^{-3/2}$
b. When $\displaystyle g(t)=\frac{-x^{2}}{4Dt}$, then $\displaystyle g'(t)=\frac{x^{2}}{4Dt^{2}}$
c. Verify that $\displaystyle \left( -\frac{1}{2t} +  \frac{x^{2}}{4Dt^{2}} \right)= \left( \frac{x^{2}-2Dt}{(2Dt)^{2}}\right)$

```

```{exercise normal-compare}
The equation for the normal distribution is $\displaystyle f(x)=\frac{1}{\sqrt{2 \pi} \sigma } e^{-(x-\mu)^{2}/(2 \sigma^{2})}$, with mean $\mu$ and variance $\sigma^{2}$. Examine the formula for the diffusion equation (Equation \@ref(eq:diff-eq-soln)) and compare it to the formula for the normal distribution. If Equation \@ref(eq:diff-eq-soln) represents a normal distribution, what do $\mu$ and $\sigma^{2}$ equal?
```


```{exercise}
For this problem you will investigate $p(x,t)$ (Equation \@ref(eq:diff-eq-soln)) with $\displaystyle D=\frac{1}{2}$.

a. Evaluate $\displaystyle \int_{-1}^{1} p(x,10) \; dx$. Write a one sentence description of what this quantity represents.
b. Using desmos or some other numerical integrator, complete the following table:

| **Equation** | **Result**  |
|:------:|:-----:|
|$\displaystyle \int_{-1}^{1} p(x,10) \; dx=$ |  |
|$\displaystyle \int_{-1}^{1} p(x,5) \; dx=$ |  |
|$\displaystyle \int_{-1}^{1} p(x,2.5) \; dx=$ |  |
|$\displaystyle \int_{-1}^{1} p(x,1) \; dx=$ |  |
|$\displaystyle \int_{-1}^{1} p(x,0.1) \; dx=$ |  |
|$\displaystyle \int_{-1}^{1} p(x,0.01) \; dx=$ |  |
|$\displaystyle \int_{-1}^{1} p(x,0.001) \; dx=$ |  |

c. Based on the evidence from your table, what would you say is the value of $\displaystyle \lim_{t \rightarrow 0^{+}} \int_{-1}^{1} p(x,t) \; dx$?
d. Now make graphs of $p(x,t)$ at each of the values of $t$ in your table. What would you say is occuring in the graph as $\displaystyle \lim_{t \rightarrow 0^{+}} p(x,t)$? Does anything surprise you? (The results you computed here lead to the foundation of what is called the Dirac delta function.)

```



```{exercise}
Consider the function $\displaystyle  p(x,t) = \frac{1}{\sqrt{4 \pi D t}}  e^{-x^{2}/(4 D t)}$. Let $x=1$.


a. Explain in your own words what the graph $p(1,t)$ represents as a function of $t$.
b. Graph several profiles of $p(1,t)$ when $D = 1$, $2$, and $0.1$. How does the value of $D$ affect the profile?

```

```{exercise}
In statistics an approximation for the 95% confidence interval is twice the standard deviation. Confirm this by adding the curve $y=2\sqrt{2Dt}$ to the ensemble average plot in Figure 24.4. Recall that $D$ was equal to $0.5$ and $\Delta t = 0.1$, so the horizontal axis will need to be scaled appropriately.
```

```{exercise}
Consider the function $\displaystyle  p(x,t) = \frac{1}{\sqrt{\pi t}} e^{-x^{2}/t}$:


a. Using your differentiation skills compute the partial derivatives $p_{t}$, $p_{x}$, and $p_{xx}$.
b. Verify $p(x,t)$ is consistent with the diffusion equation $\displaystyle p_{t}=\frac{1}{4} p_{xx}$.

```

```{exercise}
Modify the code used to generate Figure \@ref(fig:ensemble-ave-24) with $D=10, \; 1, \; 0.1, \; 0.01$. Generally speaking, what happens to the resulting ensemble average when $D$ is small or large? In which scenarios are stochastic effects more prevalent?
```

```{exercise}
For the one-dimensional random walk we discussed where there was an equal chance of moving to the left or the right. Here is a variation on this problem.

Let's assume there is a chance $v$ that it moves to the left (position $x - \Delta x$), and therefore a chance is $1-v$ that the particle remains at position $x$. The basic equation that describes the particle's position at position $x$ and time $t + \Delta t$ is:

\begin{equation}
p(x,t + \Delta t) = (1-v) \cdot p(x,t) + v \cdot p(x- \Delta x,t)
\end{equation}


Apply the techniques of local linearization in $x$ and $t$ to show that this random walk is used to derive the following partial differential equation, called the *advection equation*:

\begin{equation}
p_{t} = - \left( v \cdot \frac{ \Delta x}{\Delta t} \right) \cdot p_{x}
\end{equation}

*Note: you only need to expand this equation to first order*
```

```{exercise}
Complete Exercise \@ref(exr:random-2d-23) if you haven't already. If Equation \@ref(eq:diff-eq-soln) (a normal distribution) is the solution to the one-dimensional diffusion equation, what do you think the solution would be in the bivariate case?
```

<!--chapter:end:24-diffusion.Rmd-->

# Simulating Stochastic Differential Equations {#sdes-25}

In this chapter we will begin to combine our knowledge of random walks to numerically simulate _stochastic differential equations_, or SDEs for short. Here is the good news: our previous work comes into focus. This chapter returns to a specific model you are familiar with (the logistic differential equation) and examines it stochastically. Hopefully this specific example will allow you to see how the methods developed here work in other contexts. Let's get started!

## The stochastic logistic model
Equation \@ref(eq:logistic-de-25) begins with the logistic differential equation, but written a little differently by multiplying the differential $dt$ to the right hand side:

\begin{equation}
dx = rx \left(1 - \frac{x}{K} \right) \; dt  (\#eq:logistic-de-25)
\end{equation}

One way to interpret Equation \@ref(eq:logistic-de-25) is that a small change is the variable $x$ (denoted is $dx$), which is equal to the rate $\displaystyle rx \left(1 - \frac{x}{K} \right)$ multiplied by $dt$.

A direct way to incorporate stochastics is to modify Equation \@ref(eq:logistic-de-25) by incorporating aspects of Brownian motion, as shown with Equation \@ref(eq:logistic-sde-25):\index{Brownian motion}\index{differential equation!stochastic!logistic model}

\begin{equation}
dx = \underbrace{rx \left(1 - \frac{x}{K} \right) \; dt}_{\text{Deterministic part}} + \underbrace{\sqrt{2D \, dt} \, \mathcal{N}(0,1)}_{\text{Stochastic part}}  (\#eq:logistic-sde-25)
\end{equation}

In Equation \@ref(eq:logistic-sde-25), $D$ represents the diffusion coefficient and $\mathcal{N}(0,1)$ signifies the normal distribution with mean zero and variance one.^[As a reminder, in `R` the command `rnorm(1)` draws a random number from the standard unit normal distribution.]

It may seem odd to express Equation \@ref(eq:logistic-sde-25) in this form (i.e. $dx = ...$ versus $\displaystyle \frac{dx}{dt} = ...$). However a good way to think of this stochastic differential equation is that a small change in the variable $x$ (represented by the term $dx$) is computed in two ways:

\begin{equation}
\begin{split}
\mbox{Deterministic part: } & rx \left(1 - \frac{x}{K} \right) \; dt \\
\mbox{Stochastic part: } & \sqrt{2D \, dt} \, \mathcal{N}(0,1)
\end{split} (\#eq:logistic-sde-split-25)
\end{equation}


To simplify things somewhat we will represent $\sqrt{2D \, dt} \, \mathcal{N}(0,1)$ in Equation \@ref(eq:logistic-sde-25) with $dW(t)$, so that $dW(t)=\sqrt{2D \, dt} \, \mathcal{N}(0,1)$. The term $dW(t)$ can be thought of as similar to a stochastic differential equation $\displaystyle \frac{dW}{dt} = \sqrt{2D \, dt}$, with $W(0)=0$. The solution $W(t)$ is also representative of a *Weiner process*.\index{Weiner process} See @logan_mathematical_2009 For more information. In most cases a Weiner process does not include the term $\sqrt{2D}$ (effectively $D=\frac{1}{2}$). It is helpful to keep the term $D$ as a control parameter for simulating the SDE (see Exercises \@ref(exr:control-d-1) - \@ref(exr:control-d-3)).


How does the stochastic part of this differential equation change the solution trajectory? It turns out that the "exact" solutions to problems like these are difficult (we will study a sample of exact solutions to SDEs in Chapter \@ref(solvingSDEs-27)). Rather than focus on exact solution techniques we will apply the workflow developed in Chapter \@ref(stoch-sim-22) (Do once $\rightarrow$ Do several times $\rightarrow$ Summarize $\rightarrow$ Visualize) by simulating several solution trajectories and then taking the ensemble average at each of the time points.


## The Euler-Maruyama method
One way to numerically solve a stochastic differential equation begins with a variation of Euler's method. The Euler-Maruyama method accounts for stochasticity and implements the random walk (Brownian motion).\index{random walk}\index{Brownian motion}\index{Euler-Maruyama method} We will build this method up step by step.

Like Euler's method, the Euler-Maruyama method begins by writing the differential $dx$ as a difference: $dx = x_{n+1}-x_{n}$, where $n$ is the current step of the method. Likewise $dW(t) = W_{n+1} - W_{n}$, which represents one step of the random walk, but we approximate this difference by $\sqrt{2D \Delta t} \mathcal{N}(0,1)$, where $\Delta t$ is the timestep length. Given $\Delta t$, diffusion coefficient $D$, and starting value $x_{0}$, we can define the following method.

- From the initial condition $x_{0}$, compute the value at the next time step ($x_{0}$), which for Equation \@ref(eq:logistic-sde-25) is:

\begin{equation*}
x_{1} = x_{0} + rx_{0} \left(1 - \frac{x_{0}}{K} \right) \; \Delta t + \sqrt{2D \, \Delta t} \, \mathcal{N}(0,1)
\end{equation*}

- Repeat this iteration to step $n$, where $\mathcal{N}(0,1)$ is re-computed at each timestep:

\begin{equation*}
x_{n} = x_{n-1} + rx_{n-1} \left(1 - \frac{x_{n-1}}{K} \right) \; \Delta t + \sqrt{2D \Delta t} \, \mathcal{N}(0,1)
\end{equation*}


That is it!  We can apply this numerical method for as many steps as we want. In the `demodelr` package the function `euler_stochastic` will apply the Euler-Maruyama method to a stochastic differential equation. Just like the functions `euler` or `rk4` there are some things that need to be set first:
        
- The size ($\Delta t$) of your timestep.
- The value of the diffusion coefficient $D$ (we will discuss this later).
- The number of timesteps you wish to run the method. More timesteps means more computational time. If $N$ is the number of timesteps, $\Delta t \cdot N$ is the total time.
- A function for our deterministic dynamics. For Equation \@ref(eq:logistic-de-25) this equals $\displaystyle rx \left(1 - \frac{x}{K} \right)$.
- A function for our stochastic dynamics. For Equation \@ref(eq:logistic-de-25) this equals 1.
- The values of the vector of parameters $\vec{\alpha}$. For the logistic differential equation we will take $r=0.8$ and $K=100$.


      
Sample code for this stochastic differential equation is shown below, with the resulting trajectory of the solution in Figure \@ref(fig:log-sde).

```{r eval=FALSE}

# Identify the deterministic and stochastic parts of the DE:
deterministic_logistic <- c(dx ~ r*x*(1-x/K))
stochastic_logistic <-  c(dx ~ 1)

# Identify the initial condition and any parameters
init_logistic <- c(x=3) 
logistic_parameters <- c(r=0.8, K=100)   # parameters: a named vector

# Identify how long we run the simulation
deltaT_logistic <- .05    # timestep length
timesteps_logistic <- 200   # must be a number greater than 1

# Identify the standard deviation of the stochastic noise
D_logistic <- 1

# Do one simulation of this differential equation
logistic_out <- euler_stochastic(
  deterministic_rate = deterministic_logistic,
  stochastic_rate = stochastic_logistic,
  initial_condition = init_logistic,
  parameters = logistic_parameters,
  deltaT = deltaT_logistic,
  n_steps = timesteps_logistic,
  D = D_logistic
  )

# Plot out the solution
ggplot(data = logistic_out) +
  geom_line(aes(x=t,y=x))
```

```{r log-sde,echo=FALSE,fig.cap="One realization of Equation \\@ref(eq:logistic-sde-25)."}

# Identify the deterministic and stochastic parts of the DE:
deterministic_logistic <- c(dx ~ r*x*(1-x/K))
stochastic_logistic <-  c(dx ~ 1)

# Identify the initial condition and any parameters
init_logistic <- c(x=3) 
logistic_parameters <- c(r=0.8, K=100)   # parameters: a named vector

# Identify how long we run the simulation
deltaT_logistic <- .05    # timestep length
timesteps_logistic <- 200   # must be a number greater than 1

# Identify the standard deviation of the stochastic noise
D_logistic <- 1

# Do one simulation of this differential equation
logistic_out <- euler_stochastic(
  deterministic_rate = deterministic_logistic,
  stochastic_rate = stochastic_logistic,
  initial_condition = init_logistic,
  parameters = logistic_parameters,
  deltaT = deltaT_logistic,
  n_steps = timesteps_logistic,
  D = D_logistic
  )

# Plot out the solution
ggplot(data = logistic_out) +
  geom_line(aes(x=t,y=x)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind()
```

Let's break the code down to generate Figure \@ref(fig:log-sde) step by step:

- We identify the deterministic and stochastic parts to our differential equation with the variables `deterministic_logistic` and `stochastic_logistic`. The same structure is used for Euler's method from Chapter \@ref(euler-04).
- Similar to Euler's method we need to identify the initial conditions (`init_logistic`), parameters (`logistic_parameters`), $\Delta t$ (`deltaT_logistic`), and number of timesteps (`timesteps_logistic`).
- The diffusion coefficient for the stochastic process ($D$) is represented with `D_logistic`.
- The command `euler_stochastic` does one realization of the Euler-Maruyama method.


### Do several times
Figure \@ref(fig:log-sde) shows one sample trajectory of our solution, but there is benefit to running several simulations and then plotting out all the solution trajectories together. The code presented below accomplishes that task and makes a plot of all the solution trajectories (Figure \@ref(fig:spaghetti)). This code is similar to code presented in Chapter \@ref(stoch-sim-22). As you may recall, the main engine of the code is contained in the `map( ~ euler_stochastic ... )` which re-runs the codes for the number of times specified in `n_sims`.

```{r eval = FALSE}
# Many solutions
n_sims <- 100  # The number of simulations

# Compute solutions
logistic_run <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler_stochastic(deterministic_rate = deterministic_logistic,
                             stochastic_rate = stochastic_logistic,
                             initial_condition = init_logistic,
                             parameters = logistic_parameters,
                             deltaT = deltaT_logistic,
                             n_steps = timesteps_logistic,
                             D = D_log)) %>%
  map_dfr(~ .x, .id = "simulation")


# Plot these all together
ggplot(data = logistic_run) +
  geom_line(aes(x=t, y=x, color = simulation)) +
  ggtitle("Spaghetti plot for the logistic SDE") +
  guides(color="none")


```

```{r spaghetti,fig.cap="Several different realizations of Equation \\@ref(eq:logistic-sde-25).",echo=FALSE}
# Many solutions
n_sims <- 100  # The number of simulations

# Compute solutions
logistic_run <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler_stochastic(deterministic_rate = deterministic_logistic,
                             stochastic_rate = stochastic_logistic,
                             initial_condition = init_logistic,
                             parameters = logistic_parameters,
                             deltaT = deltaT_logistic,
                             n_steps = timesteps_logistic,
                             D = D_logistic)) %>%
  map_dfr(~ .x, .id = "simulation")


# Plot these all together
ggplot(data = logistic_run) +
  geom_line(aes(x=t, y=x, color = simulation)) +
  ggtitle("Spaghetti plot for the logistic SDE") +
  guides(color="none") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  )


```




### Summarize and Visualize
The code used to generate the ensemble average for the different simulations is shown below (try running this out on your own). This code is similar to ones presented in Chapter \@ref(stoch-sim-22). The variable `summarized_logistic` first groups the simulations by the variable `t` in order to compute the quantiles across each of the simulations.

```{r eval=FALSE}
# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)


### Summarize the variables
summarized_logistic <- logistic_run %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x,   # x is the column to compute the quantiles
                     probs = quantile_vals
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(names_from = "q_name", values_from = "q_val", 
              names_glue = "q{q_name}")

### Make the plot
ggplot(data = summarized_logistic) +
  geom_line(aes(x = t, y = q0.5)) +
  geom_ribbon(aes(x=t,ymin=q0.025,ymax=q0.975),alpha=0.2) +
  ggtitle("Ensemble average plot for the logistic SDE")

```



## Adding stochasticity to parameters

A second approach for modeling SDEs is to assume that the parameters are stochastic. For example, let's say that the growth rate $r$ in the logistic differential equation is subject to stochastic effects. How we would implement this is by replacing $r$ with $r + \mbox{ Noise }$:

\begin{equation}
dx = (r + \mbox{ Noise} ) \; x \left(1 - \frac{x}{K} \right) \; dt
\end{equation}

Now what we do is separate out the terms that are multiplied by "Noise" - they will form the stochastic part of the differential equation. The terms that aren't multipled by "Noise" form the deterministic part of the differential equation:

\begin{equation}
dx = r  x \left(1 - \frac{x}{K} \right) \; dt + x \left(1 - \frac{x}{K} \right) \mbox{ Noise } \; dt (\#eq:logistic-de-25-r)
\end{equation}

When we write $\mbox{ Noise } \; dt = dW(t)$, then the deterministic and stochastic parts to Equation \@ref(eq:logistic-de-25-r) are easily identified:

\begin{equation}
\begin{split}
\mbox{Deterministic part: } & rx \left(1 - \frac{x}{K} \right) \; dt \\
\mbox{Stochastic part: } & x \left(1 - \frac{x}{K} \right) dW(t)
\end{split} (\#eq:logistic-sde-split-r-25)
\end{equation}

There are a few things to notice with Equation \@ref(eq:logistic-sde-split-r-25). First, the deterministic part of the differential equation is what we would expect without incorporating the "Noise" term. Second, notice how the stochastic part of Equation \@ref(eq:logistic-sde-split-r-25) changed compared to Equation \@ref(eq:logistic-sde-split-25). Given these differences, let's see what happens when we simulate this SDE!


### Do once 
The following code will produce one realization of Equation \@ref(eq:logistic-de-25-r), denoting the deterministic and stochastic parts as `deterministic_logistic_r` and `stochastic_logistic_r` respectively. We will also change the value of the diffusion coefficient $D$ to equal 0.1. I encourage you to try this code on your own.


```{r,eval=FALSE}

# Identify the deterministic and stochastic parts of the DE:
deterministic_logistic_r <- c(dx ~ r*x*(1-x/K))
stochastic_logistic_r <-  c(dx ~ x*(1-x/K))

# Identify the initial condition and any parameters
init_logistic <- c(x=3)
logistic_parameters <- c(K=100,r=0.8)   # parameters: a named vector

# Identify how long we run the simulation
deltaT_logistic <- .05    # timestep length
timesteps_logistic <- 200   # must be a number greater than 1

# Identify the standard deviation of the stochastic noise
D_logistic <- .1

# Do one simulation of this differential equation
logistic_out_r <- euler_stochastic(
  deterministic_rate = deterministic_logistic_r,
  stochastic_rate = stochastic_logistic_r,
  initial_condition = init_logistic,
  parameters = logistic_parameters,
  deltaT = deltaT_logistic,
  n_steps = timesteps_logistic,
  D = D_logistic
  )

# Plot out the solution
ggplot(data = logistic_out_r) +
  geom_line(aes(x=t,y=x))
```


### Do several times
As we did before, we can run multiple iterations of Equation \@ref(eq:logistic-de-25-r), which you can also try on your own. When you do this, does the resulting spaghetti plot look the same as or different from Figure \@ref(fig:spaghetti)? What could be some possible reasons for any differences?


```{r eval=FALSE}
# Many solutions
n_sims <- 100  # The number of simulations

# Compute solutions
logistic_run_r <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler_stochastic(deterministic_rate = deterministic_logistic_r,
                             stochastic_rate = stochastic_logistic_r,
                             initial_condition = init_logistic,
                             parameters = logistic_parameters,
                             deltaT = deltaT_logistic,
                             n_steps = timesteps_logistic,
                             D = D_logistic)
) %>%
  map_dfr(~ .x, .id = "simulation")


# Plot these all together
  ggplot(data = logistic_run_r) +
  geom_line(aes(x=t, y=x, color = simulation)) +
  ggtitle("Spaghetti plot for the logistic SDE") +
  guides(color="none")
  
```



### Summarize and Visualize 
Now after running several different simulations we can plot the ensemble average, shown in Figure \@ref(fig:ensemble-r):
```{r eval = FALSE}
  
# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)

# Summarize the variables
summarized_logistic <- logistic_run %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x,   # x is the column to compute the quantiles
                     probs = quantile_vals
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(names_from = "q_name", values_from = "q_val", 
              names_glue = "q{q_name}")

# Make the plot
ggplot(data = summarized_logistic) +
  geom_line(aes(x = t, y = q0.5)) +
  geom_ribbon(aes(x=t,ymin=q0.025,ymax=q0.975),alpha=0.2) +
  ggtitle("Ensemble average plot for the logistic SDE")
```

```{r ensemble-r,fig.cap="Ensemble average plot for Equation \\@ref(eq:logistic-de-25-r).",warning=FALSE,echo=FALSE,message=FALSE}
# Identify the deterministic and stochastic parts of the DE:
deterministic_logistic_r <- c(dx ~ r*x*(1-x/K))
stochastic_logistic_r <-  c(dx ~ x*(1-x/K))

# Identify the initial condition and any parameters
init_logistic <- c(x=3)
logistic_parameters <- c(K=100,r=0.8)   # parameters: a named vector

# Identify how long we run the simulation
deltaT_logistic <- .05    # timestep length
timesteps_logistic <- 200   # must be a number greater than 1

# Identify the standard deviation of the stochastic noise
D_logistic <- .1

# Many solutions
n_sims <- 100  # The number of simulations

# Compute solutions
logistic_run_r <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler_stochastic(deterministic_rate = deterministic_logistic_r,
                             stochastic_rate = stochastic_logistic_r,
                             initial_condition = init_logistic,
                             parameters = logistic_parameters,
                             deltaT = deltaT_logistic,
                             n_steps = timesteps_logistic,
                             D = D_logistic)
) %>%
  map_dfr(~ .x, .id = "simulation")


  
# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)


### Summarize the variables
summarized_logistic <- logistic_run_r %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x,   # x is the column to compute the quantiles
                     probs = quantile_vals
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(names_from = "q_name", values_from = "q_val", 
              names_glue = "q{q_name}")

### Make the plot
ggplot(data = summarized_logistic) +
  geom_line(aes(x = t, y = q0.5),
    color = "red", size = 1, inherit.aes = FALSE
  ) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975),
    alpha = 0.2, fill = "red") +
  ggtitle("Ensemble average plot for the logistic SDE") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) + ylab("x")
  

```


The results you obtain in Figure \@ref(fig:ensemble-r) might look similar to the ensemble average from simulations of Equation \@ref(eq:logistic-sde-25), but perhaps with more variability (represented in the shading for the 95% confidence interval). Letting $r$ be a stochastic parameter affects how quickly the solution $x$ increases to the carrying capacity at $x=100$.

### When you may have odd results
Let's discuss failure and other oddities when simulating SDEs. Figure \@ref(fig:poor-sde) shows one odd realization of Equation \@ref(eq:logistic-de-25-r):

```{r poor-sde, echo=FALSE,out.width = "80%",fig.cap = "One odd realization of Equation \\ref{eq:logistic-de-25-r}."}
knitr::include_graphics("figures/25-sdes/poor-sde.png")
```


Notice in Figure \@ref(fig:poor-sde) how the variable $x$ has values that are negative. Is this a problem? ... Maybe. As with analyzing (non-stochastic) differential equations, unanticipated results may be a signal of the following:

- You may have an error in the coding of the SDE (always double-check your work!)
- You chose too large of a timestep $\Delta t$. For Brownian motion, the variance grows proportional to $\Delta t$, so larger timesteps mean more variability when you simulate a random variable, which leads to larger stochastic jumps. (To be fair, large values of $\Delta t$ are also a shortcoming of Euler's method.)
- Similar to the last point, the value of $D$ may be too large. Recall that $D$ is the diffusion coefficient and affects the rate of spread. Try setting $D=0$ and seeing if that produces a result in line with your expectation from the (non-stochastic) differential equation.
- The Euler-Maruyama may cause the variable to move across an equilibrium solution, thereby undergoing a change in the long-term behavior.^[Dynamically, a variable cannot cross an equilibrium solution. If a variable $x$ reaches an equilibrium solution at a finite time $t$, then it would remain at that equilibrium solution (see Exercise \@ref(exr:cross-eq)).] For the logistic equation, we know that $x=0$ is a unstable equilibrium solution. If for this instance $x$ becomes negative in Figure \@ref(fig:poor-sde), it will move (quickly) away from $x=0$ in the negative direction.
- Sometimes you may get NaN values in your simulations. You may still be able to compute the ensemble average, but you will need add to the following `na.rm = TRUE` in your `quantile` command (using the variable `logistic_run` as an example):

```{r eval=FALSE}
# Summarize the variables
summarized_logistic <- logistic_run %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x,   # x is the column to compute the quantiles
                     probs = quantile_vals
    na.rm = TRUE),  # Note the include of na.rm = TRUE
    q_name = quantile_vals
  ) %>%
  pivot_wider(names_from = "q_name", values_from = "q_val", 
              names_glue = "q{q_name}")
```

The last point is not a fault of the numerical method, but rather a feature of the differential equation. It is *always* helpful to understand your underlying dynamics before you start to implement a stochastic process!


## Systems of stochastic differential equations
To incorporate stochastic effects with a system of differential equations the process is similar to above, with a few changes. First, we need to keep track of the deterministic and stochastic parts *for each variable*. Second, when we summarize our results in computing the ensemble averages we also need to group by each of the variables. Let's take a look at an example.

Let's revisit the tourism model from Chapter \@ref(mcmc-13) [@sinay_simple_2006]. This model described in Equation \@ref(eq:tourism-13-redux) relies on two non-dimensonal scaled variables: $R$, which is the amount of the resource (as a percentage), and $V$, the percentage of visitors that could visit (also as a percentage). 

\begin{equation}
\begin{split}
\frac{dR}{dt}&=R\cdot (1-R)-aV \\ 
\frac{dV}{dt}&=b\cdot V \cdot (R-V)
\end{split} (\#eq:tourism-13-redux)
\end{equation}

Equation \@ref(eq:tourism-13-redux) has two parameters $a$ and $b$, which relate to how the resource is used up as visitors come ($a$) and how as the visitors increase, word of mouth leads to a negative effect of it being too crowded ($b$). @sinay_simple_2006 reported $a=0.15$ and $b=0.3316$. Let's assume that $b$ is stochastic, leading to the following deterministic and stochastic parts to Equation \@ref(eq:tourism-13-redux):


- Deterministic part for $\displaystyle \frac{dR}{dt}$: $R\cdot (1-R)-aV$
- Stochastic part for $\displaystyle \frac{dR}{dt}$: 0
- Deterministic part for $\displaystyle \frac{dV}{dt}$: $b\cdot V \cdot (R-V)$
- Stochastic part for $\displaystyle \frac{dV}{dt}$: $V \cdot (R-V)$



Now we will apply the established workflow. The code to generate one realization of this stochastic process (the "Do once" step) is shown below (try this out on your own):

```{r eval = FALSE}
# Identify the deterministic and stochastic parts of the DE:
deterministic_tourism<- c(dr ~ R*(1-R)-a*V, dv ~ b*V*(R-V))
stochastic_tourism <-  c(dr ~ 0, dv ~ V*(R-V))

# Identify the initial condition and any parameters
init_tourism <- c(R = 0.995, V = 0.00167)
tourism_parameters <- c(a = 0.15, b = 0.3316)   #

deltaT_tourism <- .5 # timestep length
timeSteps_tourism <- 200 # must be a number greater than 1

# Identify the diffusion coefficient
D_tourism <- .05

# Do one simulation of this differential equation
tourism_out <- euler_stochastic(
  deterministic_rate = deterministic_tourism,
  stochastic_rate = stochastic_tourism,
  initial_condition = init_tourism,
  parameters = tourism_parameters,
  deltaT = deltaT_tourism,
  n_steps = timeSteps_tourism,
  D = D_tourism
  )

# We will pivot the data to ease in plotting:
tourism_revised <- tourism_out %>%
  pivot_longer(cols=c("R","V"))

# Plot out the solution
ggplot(data = tourism_revised) +
  geom_line(aes(x=t,y=value)) +
  facet_grid(.~name)
```

Note the additional creation of the variable `tourism_revised` and the command `facet_grid` in the plotting. Let's break this down:

- The data frame `tourism_out` has three variables: $t$, $R$, and $V$. Because we have more than one variable, we need to do an additional step in pivoting longer. (`pivot_longer(cols=c("r","v"))`) The data frame `tourism_revised` has more rows, but the second column (called `name`) contains the name of each variable), and the third column (`value`) has the associated value of each variable at a given point in time.
- In order to plot these variables in a multipanel plot we use `facet_grid`. Think of `facet_grid(.~name)` as a row by column display. We want the columns to be the instances of the variable `name`, with one row (`.`).

The "Do several times" step follows a similar process as previous examples. The code to generate and plot multiple realizations of this stochastic process is below and also displayed with Figure \@ref(fig:tourism-many).

```{r eval=FALSE}

# Many solutions
n_sims <- 100  # The number of simulations

# Compute solutions
tourism_run <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler_stochastic(
    deterministic_rate = deterministic_tourism,
    stochastic_rate = stochastic_tourism,
    initial_condition = init_tourism,
    parameters = tourism_parameters,
    deltaT = deltaT_tourism,
    n_steps = timeSteps_tourism,
    D = D_tourism)
    ) %>%
  map_dfr(~ .x, .id = "simulation")

# We will pivot the data to ease in plotting and computing:
tourism_run_revised <- tourism_run %>%
  pivot_longer(cols=c("R","V"))

# Plot these all together
ggplot(data = tourism_run_revised) +
  geom_line(aes(x=t, y=value, color = simulation)) +
  facet_grid(.~name) +
  guides(color="none")


```

```{r tourism-many,fig.cap="Spaghetti plot for many realizations of Equation \\@ref(eq:tourism-13-redux).",warning=FALSE,echo=FALSE}
# Identify the deterministic and stochastic parts of the DE:
deterministic_tourism<- c(dr ~ R*(1-R)-a*V, dv ~ b*V*(R-V))
stochastic_tourism <-  c(dr ~ 0, dv ~ V*(R-V))

# Identify the initial condition and any parameters
init_tourism <- c(R = 0.995, V = 0.00167)
tourism_parameters <- c(a = 0.15, b = 0.3316)   #

deltaT_tourism <- .5 # timestep length
timeSteps_tourism <- 200 # must be a number greater than 1

# Identify the diffusion coefficient
D_tourism <- .05


# Many solutions
n_sims <- 100  # The number of simulations

# Compute solutions
tourism_run <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler_stochastic(
    deterministic_rate = deterministic_tourism,
    stochastic_rate = stochastic_tourism,
    initial_condition = init_tourism,
    parameters = tourism_parameters,
    deltaT = deltaT_tourism,
    n_steps = timeSteps_tourism,
    D = D_tourism)
    ) %>%
  map_dfr(~ .x, .id = "simulation")

# We will pivot the data to ease in plotting and computing:
tourism_run_revised <- tourism_run %>%
  pivot_longer(cols=c("R","V"))

# Plot these all together
ggplot(data = tourism_run_revised) +
  geom_line(aes(x=t, y=value, color = simulation)) +
  facet_grid(.~name) +
  guides(color="none") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) + ylab("Value")

```

Notice how the different realizations in Figure \@ref(fig:tourism-many) show eventually the variables $R$ and $V$ approaching a steady-state value, but when $b$ is stochastic it affects how quickly the steady state is approached. The variability in $R$ and $V$ (especially during the intervals $0 \leq t \leq 50$) may be important for how quickly this resource gets utilized.

The final steps (Summarize and Visualize) can be down together and as you may have suspected, have a similar process to the previous examples. However when computing the ensemble average plot, there are three changes because we pivoted our results in the variable `tourism_run_revised`:

- When we group by our variables to compute the ensemble average we use `group_by(t,name)` so that we organize by each time `t` and the variables in our system (gathered in the column called `name`).
- When computing the ensemble average, we will need to specify that we are computing the quantile of the variable `value` in our pivoted data frame (`tourism_run_revised`).
- Finally, as in Figure \@ref(fig:tourism-many), we need to apply the `facet_grid` command.

The code to generate this ensemble average plot is shown below. Try this code out on your own to generate an ensemble average plot.
```{r eval = FALSE}
  
# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)

# Summarize the variables
summarized_tourism <- tourism_run_revised %>%
  group_by(t,name) %>%  # We also include grouping by each variable.
  summarize(
    q_val = quantile(value, # the column to compute quantiles
                     probs = quantile_vals,
                     na.rm = TRUE  # Remove NA vlaues
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(names_from = "q_name", values_from = "q_val", 
              names_glue = "q{q_name}")

# Make the plot
ggplot(data = summarized_tourism) +
  geom_line(aes(x = t, y = q0.5)) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975),alpha = 0.2) +
  facet_grid(.~name) +
  ggtitle("Ensemble average plot")

```




## Concluding thoughts
Whew. This chapter provided some concrete examples to model and visualize results for stochastic differential equations. To summarize, when you begin with a given differential equation and

1. if you want to add stochastic effects to a *variable*, include a term $\sqrt{2D \, \Delta t} \, \mathcal{N}(0,1)$ in the differential equation to model Brownian motion.
2. if want to add stochastic effects to a *parameter*, here are the following steps:

    - Replace instances of the parameter with a "parameter + Noise" term (i.e $a \rightarrow a + \mbox{ Noise }$).
    - Collect terms that are multiplied by Noise - they will form the stochastic part of the differential equation.
    - The deterministic part of the differential equation should be your original (deterministic) differential equation.

The most general form of the stochastic differential equation is: $\displaystyle d\vec{y} = f(\vec{y},\vec{\alpha},t) \; dt + g(\vec{y},\vec{\alpha},t) \; dW(t)$, where $\vec{y}$ is the vector of variables, $\vec{\alpha}$ is your vector of parameters, and $dW(t)$ is the stochastic noise from the random walk.

The workflow (Do once $\rightarrow$ Do several times $\rightarrow$ Summarize $\rightarrow$ Visualize) should provide structure to break down the different parts of simulation of SDEs. The general framework presented here allows you to computationally explore several different types of SDEs. If you are interested in further study of SDEs, see @gardiner_handbook_2004, @keener_biology_2021, or @logan_mathematical_2009 to learn more. In the next two chapters we will investigate additional properties of SDEs. Onward!

## Exercises

```{exercise control-d-1}
Consider the logistic differential equation (Equation \@ref(eq:logistic-de-25)). In this chapter we set $D = 0.1$. Re-run the code to generate one simulation with $D = 0.01, \; 2, \; 10$. In each case, how does changing $D$ affect the resulting solution trajectories?
```



```{exercise control-d-2}
Consider the logistic differential equation (Equation \@ref(eq:logistic-de-25)). For this example we set $D = 1$. Re-run the code to generate 100 simulations with $D = 0.01, \; 2, \; 10$, and then compute the ensemble average. In each case, how does changing $D$ affect the ensemble average?
```



```{exercise control-d-3}
Return to Equation \@ref(eq:logistic-de-25-r). Use the code provided in this chapter to generate a spaghetti and ensemble average plot where $D = 0.1$. What happens to the resulting spaghetti and ensemble plots when $D = 0.01, \; 1, \; 10$?
```

```{exercise}
When $a=0.15$ and $b=0.3316$, determine the equilibrium solutions for Equation \@ref(eq:tourism-13-redux) and using the Jacobian, evaluate the stability of the equilibrium solutions.
```

```{exercise}
Consider Equation \@ref(eq:tourism-13-redux) to answer the following questions:

a. Determine the equilibrium solutions and the Jacobian matrix for general $a$ and $b$.
b. Set $a=0.15$ but let $b$ be a free parameter ($b>0$). Evaluate the stability of the equilibrium solutions as a function of $b$.


```


<!-- LW pg 341 #2 -->
```{exercise}
(Inspired by @logan_mathematical_2009) Consider the logistic differential equation: $\displaystyle \frac{dx}{dt} = r x \left( 1-\frac{x}{K} \right)$. Assume there is stochasticity in the inverse carrying capacity $1/K$ (so this means you will consider $1/K + \mbox{ Noise }$).


a. Identify the deterministic and stochastic parts of each of the differential equation.
b. Assume that $x(0)=3$,  $r=0.8$, $K=100$, $\Delta t = 0.05$, and the number of timesteps is 200. Set $D=0$. Do you get a value consistent with the deterministic solution to the logistic differential equation?
c. Run several simulations where you slowly increase the value of $D$. At what point does $D$ get too large to produce meaningful results?
d. With your chosen value of $D$, do 500 simulations of this stochastic process and compute the ensemble average.
e. Contrast your results to when we added stochasticity to the parameter $r$ in the logistic model.

```




```{tikz,sis-stochastics,echo=FALSE,warning=FALSE,message=FALSE,warning=FALSE,fig.cap="The $SIS$ model."}


\tikzstyle{vspecies}=[rectangle,minimum size=0.5cm,draw=black]
\begin{tikzpicture}[auto, outer sep=1pt, node distance=2cm]

\node [vspecies] (S) {$S$} ;
\node [vspecies, right of = S] (I) {$I$} ;
\draw [->] ([yshift=3pt]S.east) --  node[above] {\small{$b$}} ([yshift=3pt]I.west) ;
\draw [<-] ([yshift=-3pt]S.east) --  node[below] {\small{$r$}} ([yshift=-3pt]I.west) ;
\end{tikzpicture}



```


<!-- Adapted from LW pg 346 #4 -->
```{exercise}
(Inspired by @logan_mathematical_2009) An $SIS$ model is one where susceptibles $S$ become infected $I$, and then after recovering from an illness, become susceptible again. The schematic representing this is shown in Figure \@ref(fig:sis-stochastics). While you can write this as a system of differential equations, assuming the population size is constant $N$, this simplifies to the following differential equation:
  
\begin{equation}
\frac{dI}{dt} = b(N-I) I - r I
\end{equation}

a. Determine the equilibrium solutions for this model. As a bonus, analyze the stability of the equilibrium solutions. You will need to assume that all parameters are positive and $bN-r > 0$.
b. Assuming $N=1000$, $r=0.01$, and $b=0.005$, $I(0)=1$, apply Euler's method to simulate this differential equation over two weeks with $\Delta t = 0.1$ days. Show the plot of your result.
c. Assume the transmission rate $b$ is stochastic. Write down this stochastic differential equation. Do 500 simulations of this stochastic process with with $D = 1 \cdot 10^{-6}$. Generate a spaghetti plot of your results. Contrast this result to the deterministic solution.
d. Assume the recovery rate $r$ is stochastic. Write down this stochastic differential equation. Do 500 simulations of this stochastic process with $D = 1 \cdot 10^{-3}$. Generate a spaghetti plot of your results. Contrast this result to the deterministic solution.

```

 <!-- Van den Berg page 19 -->
```{exercise}
Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of $S$ (as a percent) in the blood is the following:
  
$$\frac{dS}{dt} = I + p \cdot (W - S), $$

where the parameter $I$ represents the active uptake of salt (% / hour), $p$ is the permeability of the skin (hour$^{-1}$), and $W$ is the salinity in the water (as a percent). Use this information to answer the following questions:
  
  a. When $S(0)=S_{0}$, apply techniques from Chapter \@ref(exact-solns-07) to determine an exact solution for this initial value problem.
b. Set $I = 0.1$, $p = 0.05$, $W = 0.4$, $S_{0}=0.6$. Make a plot of your solution for $0 \leq t \leq 20$.
c. What is a corresponding stochastic differential equation when the parameter $p$ is stochastic?
  d. Set $I = 0.1$, $p = 0.05$, $W = 0.4$, $S_{0}=0.6$. With $\Delta t = 0.05$ and for 400 timesteps, with $D = 0.1$, simulate this stochastic process. With an ensemble of 200 realizations compare the generated ensemble average to your deterministic solution.

```

 
```{exercise}
(Inspired by @munz_when_2009) Consider the following model for zombie population dynamics:

\begin{equation}
\begin{split}  
\frac{dS}{dt} &=-\beta S Z - \delta S  \\
\frac{dZ}{dt} &= \beta S Z + \xi R - \alpha SZ \\
\frac{dR}{dt} &= \delta S+ \alpha SZ  - \xi R
\end{split}
\end{equation}


a. Let's assume the transmission rate $\beta$ is a stochastic parameter. With this assumption, group each differential equation into two parts: terms not involving noise (the deterministic part) and terms that are multiplied by noise (the stochastic part)

- Deterministic part for $\displaystyle \frac{dS}{dt}$:
- Stochastic part for $\displaystyle \frac{dS}{dt}$:
- Deterministic part for $\displaystyle \frac{dZ}{dt}$:
- Stochastic part for $\displaystyle\frac{dZ}{dt}$:
- Deterministic part for $\displaystyle \frac{dR}{dt}$:
- Stochastic part for $\displaystyle \frac{dR}{dt}$:

b. Apply the Euler-Maruyama method to generate an ensemble avearage plot with the following values:

- $D = 5 \cdot 10^{-6}$
- $\Delta t= 0.05$.
- Timesteps: 1000.
- $\beta = 0.0095$, $\delta = 0.0001$ ,$\xi =  0.1$, $\alpha = 0.005$.
- Initial condition: $S(0)=499$, $Z(0)=1$, $R(0)=0$.
- Set the number of simulations to be 100.

c. How does making $\beta$ stochastic affect the disease transmission?

```


```{exercise}
Evaluate results from stochastic simulation of Equation \@ref(eq:tourism-13-redux) when the parameter $a$ is stochastic. You will need to determine an appropriate value of $D$ with the values of the parameters and timestep given. Contrast your findings with the results presented in Figure \@ref(fig:tourism-many).
```


<!--chapter:end:25-sdes.Rmd-->

# Statistics of a Stochastic Differential Equation {#simul-stoch-26}

In Chapter \@ref(sdes-25) you explored ways to incorporate stochastic processes into a differential equation. The workflow  (Do once $\rightarrow$ Do several times $\rightarrow$ Summarize $\rightarrow$ Visualize) generated ensemble averages of several different realizations of a stochastic process, which allowed you to visually characterize the solution.

The ensemble average indicates that the solution to a stochastic process is a probability distribution (similar to what we studied in Chapter \@ref(likelihood-09)). More importantly, this probability distribution may evolve and change in time. This chapter examines how this distribution changes in time by computing statistics from these stochastic processes. Additionally we will also investigate stochastic processes with tools from Markov modeling (such as the random walk mathematics from Chapter \@ref(random-walks-23). Let's get started!

## Expected value of a stochastic process
Chapter \@ref(sdes-25) introduced a general form of a stochastic differential equation for a single variable $X$:

\begin{equation}
dX = a(X,t) \; dt + b(X,t) \, dW(t) (\#eq:general-sde-26)
\end{equation}

We will now look at a few idealized examples of Equation \@ref(eq:general-sde-26) to understand the behavior of simulated solutions.

### A purely stochastic process
The first example is when $a(X,t)=0$ and $b(X,t)=1$ (a constant) in Equation \@ref(eq:general-sde-26), so $\displaystyle dX =  dW(t)$. We call this example a purely stochastic process. Let's code this up using the function `sde` to examine one realization (Figure \@ref(fig:b-once)) with $\Delta t =0.2$ and $X(0)=0$.

```{r eval=FALSE}

sde <- function(number_steps, dt) {
  x0 <- 0 # The initial condition

  # Set up vector of results
  x <- array(x0, dim = number_steps)

  # Iterate through this random process.
  for (i in 2:number_steps) {
    x[i] <- x[i - 1] + rnorm(1) * sqrt(dt)
  }

  # Create the time vector
  t <- seq(0, length.out = number_steps, by = dt)
  out_x <- tibble(t, x)
  return(out_x)
}

out <- sde(1000, .2)

ggplot(data = out) +
  geom_line(aes(x = t, y = x))
```


```{r b-once,fig.cap="One realization of the stochastic differential equation $dX = 2 \\; dW(t)$.",echo=FALSE}

sde <- function(number_steps, dt) {
  x0 <- 0 # The initial condition

  # Set up vector of results
  x <- array(x0, dim = number_steps)

  # Iterate through this random process.
  for (i in 2:number_steps) {
    x[i] <- x[i - 1] + rnorm(1) * sqrt(dt)
  }

  # Create the time vector
  t <- seq(0, length.out = number_steps, by = dt)
  out_x <- tibble(t, x)
  return(out_x)
}

out <- sde(1000, .2)

ggplot(data = out) +
  geom_line(aes(x = t, y = x)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) + ylab('X')
```

Let's highlight two parts from the function `sde`:

1. Notice the line `x[i] <- x[i-1] + rnorm(1)*sqrt(dt)` where we iterate through the random process with the term $\sqrt{dt}$, similar to what we did in Chapter \@ref(sdes-25).
2. To produce the correct time intervals in Figure \@ref(fig:b-once) we defined a time vector:
`t <- seq(0,length.out=number_steps,by=dt)`. 


When we repeat this process several times and plot of the results, we have the following ensemble average in Figure \@ref(fig:b-many):

```{r eval = FALSE}

# Many solutions
n_sims <- 1000 # The number of simulations

# Compute solutions
sde_run <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ sde(1000, .2)) %>%
  map_dfr(~.x, .id = "simulation")


# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)


# Summarize the variables
summarized_sde <- sde_run %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x, # x is the column to compute the quantiles
      probs = quantile_vals,
      na.rm = TRUE # remove NA values
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(
    names_from = "q_name", values_from = "q_val",
    names_glue = "q{q_name}"
  )

# Make the plot
ggplot(data = summarized_sde) +
  geom_line(aes(x = t, y = q0.5)) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2)
```



```{r b-many,fig.cap="Ensemble average of 1000 realizations of the stochastic differential equation $dX = dW(t)$." ,warning=FALSE,message=FALSE,echo=FALSE}

# Many solutions
n_sims <- 1000 # The number of simulations

# Compute solutions
sde_run <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ sde(1000, .2)) %>%
  map_dfr(~.x, .id = "simulation")


# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)


### Summarize the variables
summarized_sde <- sde_run %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x, # x is the column to compute the quantiles
      probs = quantile_vals,
      na.rm = TRUE # remove NA values
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(
    names_from = "q_name", values_from = "q_val",
    names_glue = "q{q_name}"
  )

### Make the plot
ggplot(data = summarized_sde) +
  geom_line(aes(x = t, y = q0.5), color = "red", size = 1) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2, fill = "red") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  ylab("X")
```

I sure hope (again!) the results are very similar to ones generated in Chapters \@ref(random-walks-23) and Chapter \@ref(diffusion-24) (especially Figures \@ref(fig:ensemble-ave-23) and \@ref(fig:ensemble-ave-24)) - this is no coincidence! Figure \@ref(fig:ensemble-ave-24) is another simulation of Brownian motion with $D = 1/2$.

### Stochastics with drift
The second example is a modification where $a(X,t)=2$ and $b(X,t)=1$ in Equation \@ref(eq:general-sde-26), yielding $\displaystyle dX = 0.2 \; dt + dW(t)$. To simulate this stochastic process you can easily modify this approach by modifying the function `sde`, which I will call `sde_v2`. The code to simulate the stochastic process and visualize the ensemble average (Figure \@ref(fig:a-ensemble)) is shown below:

```{r eval = FALSE}
sde_v2 <- function(number_steps, dt) {
  a <- 2
  b <- 1 # The value of b
  x0 <- 0 # The initial condition

  ### Set up vector of results
  x <- array(x0, dim = number_steps)

  for (i in 2:number_steps) {
    x[i] <- x[i - 1] + a * dt + b * rnorm(1) * sqrt(dt)
  }

  # Create the time vectror
  t <- seq(0, length.out = number_steps, by = dt)
  out_x <- tibble(t, x)
  return(out_x)
}


# Many solutions
n_sims <- 1000 # The number of simulations

# Compute solutions
sde_v2_run <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ sde_v2(1000, .2)) %>%
  map_dfr(~.x, .id = "simulation")


# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)


### Summarize the variables
summarized_sde_v2 <- sde_v2_run %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x, # x is the column to compute the quantiles
      probs = quantile_vals,
      na.rm = TRUE # Remove NA values
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(
    names_from = "q_name", values_from = "q_val",
    names_glue = "q{q_name}"
  )

### Make the plot
ggplot(data = summarized_sde_v2) +
  geom_line(aes(x = t, y = q0.5)) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2)
```

```{r,a-ensemble,warning=FALSE,echo=FALSE,message=FALSE,fig.cap="Ensemble average of 1000 realizations of the stochastic process $dX = 2 \\; dt + dW(t)$."}
sde_v2 <- function(number_steps, dt) {
  a <- 2
  b <- 1 # The value of b
  x0 <- 0 # The initial condition

  ### Set up vector of results
  x <- array(x0, dim = number_steps)

  for (i in 2:number_steps) {
    x[i] <- x[i - 1] + a * dt + b * rnorm(1) * sqrt(dt)
  }

  # Create the time vectror
  t <- seq(0, length.out = number_steps, by = dt)
  out_x <- tibble(t, x)
  return(out_x)
}


# Many solutions
n_sims <- 1000 # The number of simulations

# Compute solutions
sde_v2_run <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ sde_v2(1000, .2)) %>%
  map_dfr(~.x, .id = "simulation")


# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)


### Summarize the variables
summarized_sde_v2 <- sde_v2_run %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x, # x is the column to compute the quantiles
      probs = quantile_vals,
      na.rm = TRUE # Remove NA values
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(
    names_from = "q_name", values_from = "q_val",
    names_glue = "q{q_name}"
  )

### Make the plot
ggplot(data = summarized_sde_v2) +
  geom_line(aes(x = t, y = q0.5), color = "red", size = 1) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2, fill = "red") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  ylab("X")
```

In Figure \@ref(fig:a-ensemble), the median seems to follow a straight line. Approximating the final position of the median at $t=200$ and $X=400$, the slope of this line $\displaystyle \frac{\Delta X}{\Delta t} \approx \frac{400}{200} = 2$. So we could say that the median of this distribution follows the line $X=2t$. I recognize the ensemble average computes the *median* which is not the *average*. Because we are simulating this process with random variables drawn from a normal distribution we assume the median approximates the average. 

Let's generalize the lessons learned from these examples. The solution to the SDE $\displaystyle dX = a(X,t) \; dt + b(X,t) \, dW(t)$ is probability distribution $p(X,t)$. The expected value, $\mu$, is denoted as $E[p(X,t)]$. The solution to the differential equation $\displaystyle dX = a(X,t) \; dt$ equals the expected value $\mu$. This result should make some intuitive sense: stochastic differential equations should - on the average - converge to the deterministic solution.

The variance is a little harder to compute as the definition of the variance of a random variable $Y$ is $\displaystyle \sigma^{2} = E[(Y - \mu)^{2}] = \sigma^{2}= E[Y^{2}] - (E[Y] )^{2}$. However, what we *can* say is that the variance should be proportional (in some way) to $b(X,t) \; dt$. In both our cases studied here $b(X,t)=1$, so we can infer that the variance grows in time.


## Birth-death processes
In Chapter \@ref(stoch-sys-21) we examined a model of moose population dynamics with stochastic population fluctuations. In many biological contexts this makes sense: rarely do biological processes follow a deterministic curve. On the other hand, models usually start with a differential equation. What are we to do?

To reconcile this, another way to generate a stochastic process is through consideration of the differential equation itself. Let’s go back to the logistic population model but re-written in a specific way (Equation \@ref(eq:log-sde-26)).

\begin{equation}
\frac{dx}{dt} = r x \left( 1 - \frac{x}{K} \right) = r x - \frac{rx^{2}}{K} (\#eq:log-sde-26)
\end{equation}

From Equation \@ref(eq:log-sde-26), we can obtain a change in the variable $x$ (denoted as $\Delta x$) over $\Delta t$ units by re-writing the differential equation in differential form:

\begin{equation}
\Delta x = r x \Delta t - \frac{rx^{2}}{K} \Delta t (\#eq:log-sde-diff-26)
\end{equation}

Equation \@ref(eq:log-sde-diff-26) is separated into two terms - one that increases the variable $x$ (represented by $r x \Delta t$, the same units as $x$) and one that decreases the variable (represented by $\displaystyle \frac{rx^{2}}{K} \Delta t$, the same units as $x$). We will consider these changes in $x$ with the associated Markov random variable $\Delta X$ organized in Table \@ref(tab:probability-tab-26). (It is okay and understandable if you envison $\Delta x$ and $\Delta X$ as conceptually similar.)

Table: (\#tab:probability-tab-26) Unit increment changes for the Markov variable $\Delta x$, based on Equation \@ref(eq:log-sde-diff-26).

Outcome | Probability | 
------------- | ------------- | 
$\Delta X = 1$ (population change by 1) | $r X \; \Delta t$ |
$\Delta X = -1$ (population change by $-1$) | $\displaystyle \frac{rX^{2}}{K} \; \Delta t$ |
$\Delta X = 0$ (no population change) | $\displaystyle 1 - rX \; \Delta t - \frac{rX^{2}}{K} \; \Delta t$ |

It also may be helpful to think of these changes on the following number line (Figure \@ref(fig:random-walk-stoch-26)), similar to how we examined the random walk number line (or Markov process) from Chapter \@ref(random-walks-23).

```{tikz,random-walk-stoch-26,warning=FALSE,message=FALSE,echo=FALSE,fig.cap="A random walk for Equation \\@ref(eq:log-sde-diff-26)."}
\begin{tikzpicture}
    \draw (-3,0) -- (3,0);
    \foreach \i in {-3,-2,...,3} % numbers on line
      \draw (\i,0.1) -- + (0,-0.2) node[below] (\i) {$\i$};
   % \foreach \i in {0.5, 0.7, 0.9}% points on line
    \fill[red] (0,0) circle (1 mm);
   \node[align=center] at (-1,0.5) {$\displaystyle \frac{rX^{2}}{K} \Delta t$};
\node[align=center] at (-3.5,0.1) {$\displaystyle \Delta X$};
    \draw [->,red,thick] (0,0.15) to [out=150,in=30] (-1,0.15);
    \node[align=center] at (1,0.5) {$r X \Delta t$};
    \draw [->,red,thick] (0,0.15) to [out=30,in=150] (1,0.15);
  \end{tikzpicture}
```


It may seem odd to think of the different outcomes ($\Delta X$ equals 1, $-1$, or 1) as probabilities. Assuming that the time interval $\Delta t$ is small enough, this won't be a numerical difficulty. Let's compute $E[\Delta X]$ and the variance $\sigma^{2}$ as we did in Chapter \@ref(random-walks-23). The computation of $\mu$ is shown in Equation \@ref(eq:log-sde-mu-26).

\begin{equation}
\begin{split}
\mu = E[\Delta X] &= (1) \cdot \mbox{Pr}(\Delta X = 1) + (- 1) \cdot \mbox{Pr}(\Delta X = -1)  + (0) \cdot \mbox{Pr}(\Delta X = 0) \\
&= (1) \cdot \left( r X \; \Delta t \right) + (-1)  \frac{rX^{2}}{K} \; \Delta t \\
&= r X \; \Delta t - \frac{rX^{2}}{K} \; \Delta t
\end{split} (\#eq:log-sde-mu-26)
\end{equation}

Observe that the right hand side of Equation \@ref(eq:log-sde-mu-26) is similar to the right hand side of the original differential equation (Equation \@ref(eq:log-sde-diff-26)))!

Next let's also calculate the variance of $\Delta X$ (Equation \@ref(eq:log-sde-sigma-26)).

\begin{equation}
\begin{split}
\sigma^{2} &= E[(\Delta X)^{2}] - (E[\Delta X] )^{2} \\
&= (1)^{2} \cdot \mbox{Pr}(\Delta X = 1) + (- 1)^{2} \cdot \mbox{Pr}(\Delta X = -1)  + (0)^{2} \cdot \mbox{Pr}(\Delta X = 0) - (E[\Delta X] )^{2} \\
&= (1) \cdot \left( r X \; \Delta t \right) + (1)  \frac{rX^{2}}{K} \; \Delta t  - \left( r X \; \Delta t - \frac{rX^{2}}{K} \; \Delta t \right)^{2}
\end{split} (\#eq:log-sde-sigma-26)
\end{equation}

Notice the term $\displaystyle \left( r X \; \Delta t - \frac{rX^{2}}{K} \; \Delta t \right)^{2}$ in Equation \@ref(eq:log-sde-sigma-26). While this may seem complicated, we are going to simplify things by only computing the variance to the first order in $\Delta t$. Because of that, we are going to assume that in $\sigma^{2}$ any terms involving $(\Delta t)^{2}$ are small, or in effect negligible. While this is a huge simplifying assumption for the variance, it is useful!

The combination of the mean (Equation \@ref(eq:log-sde-mu-26)) and variance (Equation \@ref(eq:log-sde-sigma-26)) yields the stochastic process in Equation \@ref(eq:log-markov-26).

\begin{equation}
\begin{split}
dX &= \mu + \sigma \; dW(t) \\
 &= \left( r X - \frac{rX^{2}}{K} \right) \; dt + \sqrt{\left(  r X  +  \frac{rX^{2}}{K}  \right)} \; dW(t)
\end{split} (\#eq:log-markov-26)
\end{equation}

To simulate this stochastic process we will modify `euler_stochastic` from Chapter \@ref(sdes-25), appending `_log` to denote "logistic" for each of the parts and applying the workflow (Do once $\rightarrow$ Do several times $\rightarrow$ Summarize $\rightarrow$ Visualize). One realization of this stochastic process is shown in Figure \@ref(fig:log-sde-26).

```{r eval=FALSE}

# Identify the birth and death parts of the DE:
deterministic_rate_log <- c(dx ~ r * x - r * x^2 / K)
stochastic_rate_log <- c(dx ~ sqrt(r * x + r * x^2 / K))

# Identify the initial condition and any parameters
init_log <- c(x = 3)
parameters_log <- c(r = 0.8, K = 100)

# Identify how long we run the simulation
deltaT_log <- .05 # timestep length
time_steps_log <- 200 # must be a number greater than 1

# Identify the diffusion coefficient
D_log <- 1

# Do one simulation of this differential equation
out_log <- euler_stochastic(
  deterministic_rate = deterministic_rate_log,
  stochastic_rate = stochastic_rate_log,
  initial_condition = init_log,
  parameters = parameters_log,
  deltaT = deltaT_log,
  n_steps = time_steps_log,
  D = D_log
)

# Plot out the solution
ggplot(data = out_log) +
  geom_line(aes(x = t, y = x))
```


```{r log-sde-26,fig.cap="One realization of Equation \\@ref(eq:log-markov-26).",echo=FALSE}

# Identify the birth and death parts of the DE:
deterministic_rate_log <- c(dx ~ r * x - r * x^2 / K)
stochastic_rate_log <- c(dx ~ sqrt(r * x + r * x^2 / K))

# Identify the initial condition and any parameters
init_log <- c(x = 3)
parameters_log <- c(r = 0.8, K = 100)

# Identify how long we run the simulation
deltaT_log <- .05 # timestep length
time_steps_log <- 200 # must be a number greater than 1

# Identify the diffusion coefficient
D_log <- 1

# Do one simulation of this differential equation
out_log <- euler_stochastic(
  deterministic_rate = deterministic_rate_log,
  stochastic_rate = stochastic_rate_log,
  initial_condition = init_log,
  parameters = parameters_log,
  deltaT = deltaT_log,
  n_steps = time_steps_log,
  D = D_log
)

# Plot out the solution
ggplot(data = out_log) +
  geom_line(aes(x = t, y = x)) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) + ylab("X")
```

The following code will produce a spaghetti plot from 100 different simulations. Try this code out on your own.

```{r eval=FALSE}
# Many solutions
n_sims <- 100 # The number of simulations

# Compute solutions
logistic_sim_r <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler_stochastic(
    deterministic_rate = deterministic_rate_log,
    stochastic_rate = stochastic_rate_log,
    initial_condition = init_log,
    parameters = parameters_log,
    deltaT = deltaT_log,
    n_steps = time_steps_log,
    D = D_log
  )) %>%
  map_dfr(~.x, .id = "simulation")


# Plot these all together
ggplot(data = logistic_sim_r) +
  geom_line(aes(x = t, y = x, color = simulation)) +
  guides(color = "none")
```


```{r echo=FALSE,warning=FALSE,message=FALSE}
# Many solutions
n_sims <- 100 # The number of simulations

# Compute solutions
logistic_sim_r <- rerun(n_sims) %>%
  set_names(paste0("sim", 1:n_sims)) %>%
  map(~ euler_stochastic(
    deterministic_rate = deterministic_rate_log,
    stochastic_rate = stochastic_rate_log,
    initial_condition = init_log,
    parameters = parameters_log,
    deltaT = deltaT_log,
    n_steps = time_steps_log,
    D = D_log
  )) %>%
  map_dfr(~.x, .id = "simulation")
```
  
Finally, Figure \@ref(fig:ensemble-r-26) displays the ensemble average plot from all the simulations. Figure \@ref(fig:ensemble-r-26) also includes the solution to the the logistic differential equation for comparison.

```{r eval=FALSE}
# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)


# Summarize the variables
summarized_logistic <- logistic_sim_r %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x, # x is the column to compute the quantiles
      probs = quantile_vals,
      na.rm = TRUE
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(
    names_from = "q_name", values_from = "q_val",
    names_glue = "q{q_name}"
  )

logistic_solution <- tibble(
  t = seq(0, 10, length.out = 100),
  x = 100 / (1 + 97 / 3 * exp(-0.8 * t))
)


# Make the plot
ggplot(data = summarized_logistic) +
  geom_line(aes(x = t, y = q0.5), 
            color = "red", size = 1) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), 
              alpha = 0.2, fill = "red") +
  geom_line(data = logistic_solution, aes(x = t, y = x), 
            size = 1, linetype = "dashed")

```




```{r ensemble-r-26,echo=FALSE,message=FALSE,warning=FALSE,fig.cap="Ensemble average of 100 simulations of Equation \\@ref(eq:log-markov-26) (in red). The red line represents the median with the shading the 95% confidence interval. For comparison, the deterministic solution to the logistic differential equation is the dashed black curve."}
# Compute Quantiles and summarize
quantile_vals <- c(0.025, 0.5, 0.975)


# Summarize the variables
summarized_logistic <- logistic_sim_r %>%
  group_by(t) %>%
  summarize(
    q_val = quantile(x, # x is the column to compute the quantiles
      probs = quantile_vals,
      na.rm = TRUE
    ),
    q_name = quantile_vals
  ) %>%
  pivot_wider(
    names_from = "q_name", values_from = "q_val",
    names_glue = "q{q_name}"
  )

logistic_solution <- tibble(
  t = seq(0, 10, length.out = 100),
  x = 100 / (1 + 97 / 3 * exp(-0.8 * t))
)


# Make the plot
ggplot(data = summarized_logistic) +
  geom_line(aes(x = t, y = q0.5), color = "red", size = 1) +
  geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2, fill = "red") +
  geom_line(data = logistic_solution, aes(x = t, y = x), size = 1, linetype = "dashed") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  ylab("X")
```


Awesome! Notice how the median in the ensemble average plot in Figure \@ref(fig:ensemble-r-26) closely tracks the deterministic solution. 

The types of stochastic processes we are describing in this chapter are called "birth-death" processes.\index{birth-death process} Here is another way to think about Equation \@ref(eq:log-sde-diff-26):

\begin{equation}
\begin{split}
r x \; \Delta t   &= \alpha(x) \\
\frac{rx^{2}}{K} \; \Delta t &= \delta(x)
\end{split} (\#eq:birth-death-log-26)
\end{equation}

In this way, we think of $\alpha(x)$ in Equation \@ref(eq:birth-death-log-26) as a "birth process" and $\delta(x)$ as a "death process". When we computed the mean $\mu$ and variance $\sigma^{2}$ for Equation \@ref(eq:log-sde-diff-26) we had the mean $\mu = \alpha(x)-\delta(x)$ and variance $\sigma^{2}=\alpha(x)+\delta(x)$. (You should double check to make sure this is the case!). Here is an interesting fact: $\mu = \alpha(x)-\delta(x)$ and $\sigma^{2}=\alpha(x)+\delta(x)$ hold up for *any* differential equation where we have identified a birth ($\alpha(x)$) or death ($\delta(x)$) process.

Generalizing birth-death processes for a system of differential equations is a little more involved as there are more possibilities involved when computing $\mu$ and $\sigma$. We don't tackle this complexity here, but some considerations are thinking through *all* possible combinations of increase or decrease in the variables. For example, one variable could increase where the other decreases, both could increase, or one variable decreases and the other remains the same. In addition, the standard deviation is computed by the square root of the variance, and for a linear system of equations the matrix square root involves matrix decompositions. If you are curious to know more, see @logan_mathematical_2009 for more information on implementing birth-death processes for a system of differential equations.

## Wrapping up
In the exercises for this chapter you will solve problems involving applications of stochastic processes in biological contexts. The nice part is that the framework presented here generalizes to several other contexts where a stochastic process can be identified. This chapter contained several key points about statistics for the SDE $dX = a(X,t) \; dt + b(X,t) \; dW(t)$ and its solution $p(X,t)$, summarized below:

- The solution of the deterministic equation $dX = a(X,t) \; dt$ equals $E[p(X,t)]$. The variance is proportional (but not equal) to the solution of $dX = b(X,t) \; dt$.
- The SDE can be simulated by Brownian motion, so $dX \approx \Delta X = a(X,t) \; \Delta t + b(X,t)\; \sqrt{2D \, \Delta t} \, Z$, where $Z$ is a random variable from a unit normal distribution (so in `R` we would use `rnorm(1)`).
- Since $\Delta x = x_{n+1}-x_{n}$, then we have $x_{n+1} = x_{n} + \mu + a(X,t) \; \Delta t + b(X,t)\; \sqrt{2D \, \Delta t} \, Z$ (the Euler-Maruyama method).
- Several realizations of the SDE and subsequent computation of the ensemble average approximately characterize the distribution $p(X,t)$.

Chapter \@ref(solvingSDEs-27) will revisit some of the examples studied in this chapter, but apply tools to further characterize the distribution $p(X,t)$ with partial differential equations. There are still more mathematics to investigate, so onward!

## Exercises

```{exercise}
Simulate the stochastic process $dX = 2 \; dW(t)$ with the following values:
  
- $D = 1$
- $\Delta t= 0.05$
- Timesteps: 200
- Initial condition: $X(0)=1$.
- Set the number of simulations to be 100.
  
Generate a spaghetti plot and ensemble average of your simluation results.
```


```{exercise}
Return to the simulation of the logistic differential equation in this chapter. To generate Figure \@ref(fig:ensemble-r-26) we set $D = 1$. What happens to the resulting ensemble average plots when $D =0, \;  0.01, \; 0.1, \; 1, \; 10$? You may use the following values:

- Set the number of simulations to be 100. 
- Initial condition: $x(0)=3$
- Parameters: $r=0.8$, $K=100$.
- $\Delta t = 0.05$ for 200 time steps.

```



```{exercise}
For the logistic differential equation consider the following splitting of $\alpha(x)$ and $\delta(x)$ as a birth-death process:

\begin{equation}
\begin{split}
\alpha(x) &= rx - \frac{rx^{2}}{2K} \\
\delta(x) &=  \frac{rx^{2}}{2K}
\end{split}
\end{equation}

Simulate this SDE with the following values:
 
- Initial condition: $x(0)=3$
- Parameters: $r=0.8$, $K=100$.
- $\Delta t = 0.05$ for 200 time steps.
- $D=1$.

Generate an ensemble average plot. How does this SDE compare to Figure \@ref(fig:ensemble-r-26)?
```

<!-- Based off LW pg 346 #3 -->
```{exercise}
Let $S(t)$ denote the cumulative snowfall at a location at time $t$, which we will assume to be a random process. Assume that probability of the change in the cumulative amount of snow from day $t$ to day $t+\Delta t$ is the following:

  | change | probability  | 
|:------:|:-----:|
| $\Delta S = \sigma$ | $\lambda \Delta t$ |
| $\Delta S = 0$ | $1- \lambda \Delta t$ |

The parameter $\lambda$ represents the frequency of snowfall and $\sigma$ the amount of the snowfall in inches. For example, during January in Minneapolis, Minnesota, the probability $\lambda$ of it snowing 4 inches or more is 0.016, with $\sigma=4$. (This assumes a Poisson process with rate = 0.5/31, according to the [Minnesota DNR.](https://www.dnr.state.mn.us/climate/twin_cities/snow_event_counts.html)). The stochastic differential equation generated by this process is $dS = \lambda \sigma \; dt + \sqrt{\lambda \sigma^{2}} \; dW(t) = .064 \; dt + .506 \; dW(t)$$ .

a. With this information, what is $E[\Delta S]$ and the variance of $\Delta S$?
b. Simulate and summarize this stochastic process. Use $S(0)=0$ and run 500 simulations of this stochastic process. Simulate this process for a month, using $\Delta t = 0.1$ for 300 timesteps and with $D=1$. Show the resulting spaghetti plot and interpret your results.

```


<!-- Ornstein uhlenbeck process -->
```{exercise}
Consider the stochastic differential equation $\displaystyle dS = \left( 1 - S \right) \; dt + \sigma \; dW(t)$, where $\sigma$ controls the amount of stochastic noise. For this stochastic differential equation what is $E[S]$ and Var$(S)$?
```




```{exercise}
Consider the equation

\begin{equation*}
\Delta x = \alpha(x) \; \Delta t - \delta(x) \; \Delta t
\end{equation*}

If we consider $\Delta x$ to be a random variable, show that the expected value $\mu$ equals $\alpha(x) \; \Delta t - \delta(x) \; \Delta t$ and the variance $\sigma^{2}$, to first order, equals $\alpha(x) \; \Delta t + \delta(x) \; \Delta t$.
```

<!--chapter:end:26-birthDeath.Rmd-->

# Solutions to Stochastic Differential Equations {#solvingSDEs-27}

Chapters \@ref(sdes-25) and \@ref(simul-stoch-26) simulated SDEs. Visualizing the ensemble average of solution trajectories revealed a *distribution* of solutions that evolves in time. This final chapter examines methods to characterize exact solutions to stochastic differential equations by developing formulas to characterize the evolution of the distribution of solutions in time. There are a lot of mathematics here that can be a stepping stone for further study in the field of stochastics. Let's (for one last time) get started!

## Meet the Fokker-Planck equation
Let's start with a general way to express a stochastic differential equation:

\begin{equation}
dx = a(x,t) \; dt + b(x,t) \; dW(t) (\#eq:generic-sde-27)
\end{equation}

The "solution" to this SDE will be a probability density function $p(x,t)$, describing the evolution of $p(x,t)$ in both time and space. Based on our work with Brownian motion in Chapters \@ref(sdes-25) and \@ref(simul-stoch-26), the probability density function $p(x,t)$ should have the following properties:

- $E[p(x,t)]$ is the deterministic solution to $\displaystyle \frac{dx}{dt} = a(x,t)$.
- The variance $\sigma^{2}$ incorporates the function $b(x,t)$.


A way to determine the probability density function is by solving the following partial differential equation, termed the **Fokker-Planck Equation** (Equation \@ref(eq:fpe-27)).\index{Fokker-Planck equation}

\begin{equation}
\frac{\partial p}{\partial t} = - \frac{\partial}{\partial x} \left(p(x,t) \cdot a(x,t) \right) + \frac{1}{2}\frac{\partial^{2} }{\partial x^{2}} \left(\; p(x,t) \cdot (b(x,t))^{2}  \;\right) (\#eq:fpe-27)
\end{equation}

We can write Equation \@ref(eq:fpe-27) in shorthand, dropping the dependence of $x$ and $t$ for $p(x,t)$, $a(x,t)$ and $b(x,t)$: $\displaystyle p_{t} = -  (p \cdot a)_{x} + \frac{1}{2} (p \cdot b^{2})_{xx}$. We do not include the derivation of the Fokker-Planck Equation here, as the proof requires concepts from  advanced calculus (see @logan_mathematical_2009 for more discussion and derivation of the Fokker-Planck equation). However, let's build up understanding of Equation \@ref(eq:fpe-27) through some examples.

### Diffusion (again)
Consider the SDE $dx = dW(t)$ with $x(0)=0$ and apply the Fokker-Planck equation to characterize the solution $p(x,t)$. We know from Chapter \@ref(diffusion-24) that SDE $dx = dW(t)$ characterizes Brownian motion. When we compare this SDE to the Fokker-Planck equation and Equation \@ref(eq:generic-sde-27), we have $a(x,t)=0$ and $b(x,t)=1$, yielding Equation \@ref(eq:fpe-diff-27):

\begin{equation*}
p_{t} = \frac{1}{2} p_{xx}. (\#eq:fpe-diff-27)
\end{equation*}

This equation should look familiar - it is the partial differential equation for diffusion (Equation \@ref(eq:new-master-diff))!^[To remind you, the solution to $p_{t} = D p_{xx}$ is $\displaystyle p(x,t) = \frac{1}{\sqrt{4 \pi Dt} } e^{-x^{2}/(4 D t)}$. So in this case $D = 1/2$.]  The solution to this SDE is given by Equation \@ref(eq:diffuse-sde-27). Figure \@ref(fig:diracdelta-27) shows the evolution of $p(x,t)$ in time.

\begin{equation}
p(x,t) = \frac{1}{\sqrt{2 \pi t}} e^{-x^{2}/(2 t)} (\#eq:diffuse-sde-27)
\end{equation}

One way to describe Equation \@ref(eq:diffuse-sde-27) is a normally distributed random variable, with $E[p(x,t)]=0$ and $\sigma^{2}$ (the variance) equal to $t$. Notice how the mean and variance for Equation \@ref(eq:diffuse-sde-27) connect back to our previous work with random walks and diffusion in Chapters \@ref(random-walks-23) and \@ref(diffusion-24). Namely, simulations and random walk mathematics showed that the expected value of a random walk or Brownian motion was zero and the variance grew in time, which is the same for this SDE.


```{r diracdelta-27,echo=FALSE,fig.cap="Profiles for the solution to SDE $dx = dW(t)$ (given by Equation \\@ref(eq:diffuse-sde-27)) for different values of $t$."}
# sigma = sqrt(2*D*t) set #D = 1/2
x <- seq(-10,10,length.out=100)
y1 <- dnorm(x,mean=0,sd=1) # t = 1 
y2 <- dnorm(x,mean=0,sd=sqrt(2)) # t = 2
y3 <- dnorm(x,mean=0,sd=4) # t = 16
y4 <- dnorm(x,mean=0,sd=sqrt(32)) # t = 32 

data.frame(x,y1,y2,y3,y4) %>%
  gather(key=run,value=y,-1) %>%
  ggplot(aes(x=x,y=y,color=as.factor(run))) + geom_line(size=1) +
  ylab('p(x,t)') +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name=NULL,labels=c("t = 1", "t = 2","t = 16","t = 32"))


```


Let's discuss the initial condition for Equation \@ref(eq:diffuse-sde-27). Our SDE had the initial condition $x(0)=0$. How this initial condition translates to Equation \@ref(eq:diffuse-sde-27) is that $x(0)$ is the same as $p(x,0)$. For this example the initial condition $p(x,0)$ is a special function called the [Dirac delta function](https://en.wikipedia.org/wiki/Dirac_delta_function), written as $p(x,0)=\delta(x)$.\index{Dirac delta function} The function $\delta(x)$ is a special type of probability density function, which you may study in a course that explores the theory of functions. Applications of the Dirac delta function include modeling an impulse (such as the release of a particle from a specific point) and watching its evolution in time. 

### Diffusion with drift
Now that we have a handle on the SDE $dx = dW(t)$, let's extend this next example a little more. Consider the SDE $dx = r \; dt + \sigma \; dW(t)$, where $r$ and $\sigma$ are constants. As a first step, let's examine the deterministic equation:  $dx = r \; dt$, which has a linear function $x(t) = rt + x_{0}$ as its solution.^[This example is similar to the SDE $\displaystyle dX = 0.2 \; dt + dW(t)$ in Chapter \@ref(simul-stoch-26), where $r=0.2$ and $\sigma = 1$. In that example we found that the variance $\sigma^{2}$ grew linearly in time.] As before, the initial condition for this SDE is $p(x,0)=\delta(x)$.

Applying Equation \@ref(eq:fpe-27) to this SDE we obtain Equation \@ref(eq:diffuse-advect-sde-27):

\begin{equation}
p_{t} = -r \, p_{x}+ \frac{\sigma^{2}}{2} \, p_{xx} (\#eq:diffuse-advect-sde-27)
\end{equation}

Equation \@ref(eq:diffuse-advect-sde-27) is an example of a *diffusion-advection equation*.\index{diffusion-advection equation}   Amazingly through a change of variables we will reduce Equation \@ref(eq:diffuse-advect-sde-27) to an example of Equation \@ref(eq:diffuse-sde-27). Here's how to do this:

- First, let $x=z+r \, \tau$ and $t=\tau$. This change of variables may seem odd, but our goal here is to write $p(x,t)=p(z,\tau)$ and to develop expressions for $p_{\tau}$ $p_{z}$, and $p_{zz}$ from this change of variables. But in order to do that, we will need to apply the *multivariable chain rule* (see Figure \@ref(fig:chain-rule)).\index{chain rule!multivariable}


```{tikz,chain-rule,warning=FALSE,message=FALSE,echo=FALSE,fig.cap='Multivariable chain rule.',fig.width=4,fig.height=3}

\begin{tikzpicture}[auto, outer sep=1pt, node distance=1.5cm]
\node (A) {$p$};
\node [below of = A, left of = A] (B) {$x$} ;
\node [below of = A, right of = A] (C) {$t$} ;
%\node [below of = C] (D) {$t$};
\node [below of = B, left of = B] (E) {$z$} ;
\node [below of = B, right of = B] (F) {$\tau$} ;

\draw [-] (A) -- (B);
\draw [-] (B) -- (E);
\draw [-] (B) -- (F);
\draw [-] (A) -- (C);
\draw [-] (C) -- (F);
%\draw [-] (A) -- node[left, yshift=10pt] {$\displaystyle \frac{\partial z}{\partial x}$} (B);

%\draw [-] (A) -- node[right, yshift=10pt] {$\displaystyle \frac{\partial z}{\partial t}$} (C);


\end{tikzpicture}

```

- By the multivariable chain rule we can develop expressions for $p_{\tau}$ and $p_{z}$: 

\begin{equation}
\begin{split}
\frac{\partial p}{\partial \tau} & = \frac{\partial p}{\partial x} \cdot \frac{ \partial x}{\partial \tau} + \frac{\partial p}{\partial t} \cdot \frac{ \partial t}{\partial \tau}  \\
\frac{\partial p}{\partial z} & = \frac{\partial p}{\partial x} \cdot \frac{ \partial x}{\partial z}
\end{split}
\end{equation}


- Now let's consider the partial derivatives $\displaystyle \frac{\partial x}{ \partial \tau}$, $\displaystyle \frac{\partial x}{ \partial z}$, and $\displaystyle \frac{\partial t}{ \partial z}$. Remember that $x=z+r \, \tau$. By direct differentiation $x_{\tau} = r$ and $x_{z} = 1$. Also since $t=\tau$ then $\tau_{t}=1$. With these substitutions, we can now re-write $p_{\tau}$ and $p_{z}$: 

\begin{equation}
\begin{split}
\frac{\partial p}{\partial \tau} & = \frac{\partial p}{\partial x} \cdot \frac{ \partial x}{\partial \tau} + \frac{\partial p}{\partial t} \cdot \frac{ \partial t}{\partial z}  = \frac{\partial p}{\partial x} \cdot r + \frac{\partial p}{\partial t} \cdot 1 \\
\frac{\partial p}{\partial z} & = \frac{\partial p}{\partial x} \cdot \frac{ \partial x}{\partial z} = \frac{\partial p}{\partial x} \cdot 1 \rightarrow
\frac{\partial^{2} p}{\partial z^{2}} = \frac{\partial^{2} p}{\partial x^{2}} 
\end{split} (\#eq:cov-27)
\end{equation}


- Finally if re-write Equation \@ref(eq:diffuse-advect-sde-27) with the variables $z$, $\tau$, and the change of variables (Equation \@ref(eq:cov-27)) we obtain Equation \@ref(eq:diffuse-advect-cov-sde-27).

\begin{equation}
\begin{split}
\underbrace{p_{t} + r p_{x}} &=\frac{\sigma^{2}}{2} p_{xx} \\
p_{\tau} &=  \frac{\sigma^{2}}{2} p_{zz}
\end{split} (\#eq:diffuse-advect-cov-sde-27)
\end{equation}

All this work and transformation of variables to obtain Equation \@ref(eq:diffuse-advect-cov-sde-27) yielded a diffusion equation in the variables $z$ and $\tau$!  Applying Equation \@ref(eq:diffuse-sde-27) with the variables $z$ and $\tau$, we have $\displaystyle p(z, \tau) = \frac{1}{\sqrt{2 \pi \sigma^{2} \tau}} e^{-z^{2}/(2 \sigma^{2} \tau)}$, which we then transform back into the original variables $x$ and $t$ (Equation \@ref(eq:diffuse-advect-soln-27)):

\begin{equation}
p(x, t) = \frac{1}{\sqrt{2 \pi \sigma^{2} t}} e^{-(x-rt)^{2}/(2 \sigma^{2} t)} (\#eq:diffuse-advect-soln-27)
\end{equation}

Now that we have an equation, next let's visualize the solution. Let's take a look at some representative plots in Figure \@ref(fig:diff-advec):


```{r diff-advec,echo=FALSE,fig.cap="Profiles for the solution to SDE $dx = r \\; dt + \\sigma \\; dW(t)$ (given with Equation \\@ref(eq:diffuse-advect-soln-27)) for different values of $t$."}
# sigma = sqrt(2*D*t) set #D = 1/2
r <- 0.1
x <- seq(-10,10,length.out=100)
y1 <- dnorm(x,mean=r,sd=1) # t = 1 
y2 <- dnorm(x,mean=r*2,sd=sqrt(2)) # t = 2
y3 <- dnorm(x,mean=r*16,sd=4) # t = 16
y4 <- dnorm(x,mean=r*32,sd=sqrt(32)) # t = 32 

data.frame(x,y1,y2,y3,y4) %>%
  gather(key=run,value=y,-1) %>%
  ggplot(aes(x=x,y=y,color=as.factor(run))) + geom_line(size=1) +
  ylab('p(x,t)') +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 14),
    axis.title.x = element_text(size = 14),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 14)
  ) +
  scale_color_colorblind(name=NULL,labels=c("t = 1", "t = 2","t = 16","t = 32"))


```

Based on what we know of this distribution, it should look like a normal distribution with mean (expected value) equal to $rt$ and variance equal to $\sigma^{2}t$. What the mean and variance tell us is that the mean is shifting and growing more diffuse as time increases. Remember that our solution to the deterministic equation was linear, and the mean of our distribution grows linearly as well! Also notice in Figure \@ref(fig:diff-advec) as $t$ increases the solution drifts ("advects") to the right.



## Deterministically the end
The examples in this chapter scratch the surface of developing a deeper understanding of stochastic processes and differential equations. Even though we looked at a few test cases, there is a lot of power in understanding them (and integration across much of the mathematics you may have learned). Stochastic differential equations and stochastic processes are a fascinating field of study with a lot of interesting mathematics - I hope what you learned here will make you want to study it further!


## Exercises

```{exercise}
Consider the SDE $dx = dW(t)$ with initial condition $x(0)=2$. Using the results from this chapter, what is the solution $p(x,t)$ for this SDE?
```

```{exercise}
What is the solution to the SDE $\displaystyle dX = 0.2 \; dt + dW(t)$ with initial condition $X(0)=0$? Plot a few sample profiles (in $X$) of $p(X,t)$ at different times. Using your solution, what is E$[p(X,t)]$ (expected value) and $\sigma^{2}$ (variance)?
```


<!-- Based off LW pg 346 #3 -->
```{exercise}
Let $S(t)$ denote the cumulative snowfall at a location at time $t$, which we will assume to be a random process. Assume that probability of the change in the cumulative amount of snow from day $t$ to day $t+\Delta t$ is the following:

  | change | probability  | 
|:------:|:-----:|
| $\Delta S = \sigma$ | $\lambda \Delta t$ |
| $\Delta S = 0$ | $1- \lambda \Delta t$ |

The parameter $\lambda$ represents the frequency of snowfall and $\sigma$ the amount of the snowfall in inches. For example, during January in Minneapolis, Minnesota, the probability $\lambda$ of it snowing 4 inches or more is 0.016, with $\sigma=4$. (This assumes a Poisson process with rate = 0.5/31, according to the [Minnesota DNR.](https://www.dnr.state.mn.us/climate/twin_cities/snow_event_counts.html)) The stochastic differential equation generated by this process is $dS = \lambda \sigma \; dt + \sqrt{\lambda \sigma^{2}} \; dW(t) = .064 \; dt + .506 \; dW(t)$.


a. What is the Fokker-Planck equation for the probability distribution $p(S,t)$?
b. What is the solution $p(S,t)$ for the Fokker-Planck equation?
c. What are $E[p(S,t)]$ and the variance of $p(S,t)$?
d. Generate representative plots of the solution as it evolves over time.

```

<!-- Gardinier, pg 126 -->
```{exercise}
A particle is moving in a gravitational field but is still allowed to diffuse randomly. In this case the stochastic differential equation is $dx = -g \; dt + \sqrt{D} \; dW(t)$, where $g$ and $D$ are constants.


a. What is the Fokker-Planck partial differential equation for the probability distribution $p(x,t)$?
b. Based on the work done in this chapter, what is the equation for the probability distribution $p(x,t)$?

```

<!-- Ornstein uhlenbeck process -->
```{exercise}
Consider the stochastic differential equation $\displaystyle dS = \left( 1 - S \right) \; dt + \sigma \; dW(t)$, where $\sigma$ controls the amount of stochastic noise. 


a. First let $\sigma = 0$ so the equation is entirely deterministic. Classify the stability of the equilibrium solutions for this differential equation.
b. Still let $\sigma = 0$. Apply separation of variables to solve this differential equation.
c. Now let $\sigma = 0.1$. Do 100 realizations of this stochastic process, with initial condition $S(0)=0.5$. What do you notice?
d. Now try different values of $\sigma$ larger and smaller than 0.1. What do you notice?
e. What is the Fokker-Planck partial differential equation for the probability distribution $p(S,t)$?

```

<!--chapter:end:27-solvingSDEs.Rmd-->

# References {-}
<div id="refs"></div>

<!--chapter:end:references.Rmd-->

