# Diffusion

By studying random walks you how randomness led to some surprising results: that a one-dimensional random walk was not biased (meaning the mean displacement was zero) but the variance increased proportional to the step size.  In this section we will revisit the random walk problem from another perspective that applies the concept of diffusion.

## Random walk redux
In our random walk derivation we focused on the position of a particle on the random walk, based upon prescribed rules.  In this scenario we are going to consider the *probability* that a particle is at position $x$ in time $t$, so rather than focusing on where the particle is we focusing on the chance that the particle is at a given spot.

How we think about this is at a position, a particle can arrive at location $x$ from either the left or the right, illustrated in the following number line:

```{r,engine='tikz',warning=FALSE,message=FALSE,echo=FALSE}
\begin{center}
\begin{tikzpicture}
    \draw (-3,0) -- (3,0);
    \foreach \i in {-3,-2,...,3} % numbers on line
      \draw (\i,0.1) -- + (0,-0.2) node[below] (\i) {$\i$};
   % \foreach \i in {0.5, 0.7, 0.9}% points on line
    \fill[red] (0,0) circle (1 mm);
     \draw[red] (-1,0) circle (1 mm);
     \draw[red] (1,0) circle (1 mm);
   \node[align=center] at (-1.1,0.5) {$p(-1,t)$};
    \draw [<-,red,thick] (-.05,0.15) to [out=150,in=30] (-1,0.15);
    \node[align=center] at (1.1,0.5) {$p(1,t)$};
       \node[align=center] at (1.9,-1) {$\displaystyle p(0,t+\Delta t)=\frac{1}{2} p(-1,t)+\frac{1}{2} p(1,t)$};    
        
        
    \draw [<-,red,thick] (0.05,0.15) to [out=30,in=150] (1,0.15);
  \end{tikzpicture}
\end{center}
```

The equation that defines the position $x$ at any time $t$ is the following:

\begin{equation}
p(x,t+\Delta t) = \frac{1}{2} p(x-\Delta x,t) + \frac{1}{2} p(x+\Delta x,t)
\end{equation}

To analyze this expression we do Taylor approximations on each side of the equation.  First letâ€™s do a locally linear approximation for $p(x,t+\Delta t)$:

\begin{equation}
p(x,t+\Delta t)  \approx p(x,t) + \Delta t \cdot p_{t},
\end{equation}

where we have dropped the shorthand $p_{t}(x,t)$ as $p_{t}$.  On the right hand side of the equation we will compute the 2nd degree (quadratic) Taylor polynomial:

\begin{align}
\frac{1}{2} p(x-\Delta x,t)  & \approx \frac{1}{2} p(x,t) -   \frac{1}{2} \Delta x \cdot p_{x} + \frac{1}{4} (\Delta x)^{2}\cdot  p_{xx} \\
\frac{1}{2} p(x+\Delta x,t)  & \approx \frac{1}{2} p(x,t) +   \frac{1}{2} \Delta x \cdot p_{x} + \frac{1}{4} (\Delta x)^{2} \cdot p_{xx}
\end{align}

With these approximations we can re-write the probability equation:

\begin{equation}\label{eq:diffusion}
\Delta t \cdot p_{t} = \frac{1}{2} (\Delta x)^{2} p_{xx} \rightarrow  p_{t} = \frac{1}{2} \frac{(\Delta x)^{2}}{\Delta t} \cdot p_{xx}
\end{equation}

If we define $$ D =  \frac{1}{2} \frac{(\Delta x)^{2}}{\Delta t} $$ and we have what is called a partial differential equation - a differential equation with derivatives that depend on two variables.  This particular equation is the **diffusion equation**. Cool!

Other derivations of the diffusion equation let $\Delta t \rightarrow 0$ and $\Delta x \rightarrow 0$ in the limit, but to continue the physical aspect of this problem and connects back to our work on the random walk.

Determining an exact solution to the diffusion equation requires more study in techniques of partial differential equations, so we will leave that for another time.  However, the solution to this equation is the following:
\begin{equation}
 p(x,t) = \frac{1}{\sqrt{4 \pi Dt} } e^{-x^{2}/(4 D t)} \end{equation}
 
Verifying this is the solution to Equation~\ref{eq:diffusion} is a good review of your multivariable calculus skills!

Let's examine some plots of this solution:


```{r,echo=FALSE,fig.width=4,fig.height=3,fig.align='center'}
# sigma = sqrt(2*D*t) set #D = 1/2
x <- seq(-10,10,length.out=100)
y1 <- dnorm(x,mean=0,sd=1) # t = 1 
y2 <- dnorm(x,mean=0,sd=sqrt(2)) # t = 2
y3 <- dnorm(x,mean=0,sd=4) # t = 16
y4 <- dnorm(x,mean=0,sd=sqrt(32)) # t = 32 

data.frame(x,y1,y2,y3,y4) %>%
  gather(key=run,value=y,-1) %>%
  ggplot(aes(x=x,y=y,color=as.factor(run))) + geom_line(size=2) +
  theme(legend.position="bottom") +
  scale_color_discrete(name=NULL,labels=c("t = 1", "t = 2","t = 16","t = 32")) +
  ylab('p(x,t)')


```
 
 As you can see that as time increases the graph of this solution gets flatter.  In fact, the form of the solution of the diffusion equation is exactly equal to the formula for a normal probability density function (you might want to refer back Chapter 9)!
 
 The connections between diffusion and probability are so strong.  And in fact, this formula for the solution is a normal distribution with mean 0 and standard deviation $\sqrt{2Dt}$.  This is consistent with our simulations of the random walk - the variance grows proportional to time.  Cool!
 
## Concluding Thoughts

You may be wondering how this discussion of random walks connects back into stochastic differential equations.  With this notion of a random walk we can now understand how small changes in a variable or parameter affect the solutions.  We know that changes in parameters can affect stability, but if in the *average* ensemble of solutions behave similarly to what we would expect for the deterministic solution, then we are in business.

For example if we consider the following logistic differential equation $\displaystyle \frac{dx}{dt} = rx \left(1-\frac{x}{K} \right)$, a naive way to add stochasticity is to add an additional term (which we call "Noise")

\begin{equation}
\frac{dx}{dt} = rx \left(1-\frac{x}{K} \right) + \mbox{ Noise }.
\end{equation}

One way that we examine this is by multiplying the $dt$ term over to the right hand side:

\begin{equation}
dx = rx \left(1-\frac{x}{K} \right) \; dt + \mbox{ Noise } \; dt
\end{equation}

The first term ($\displaystyle rx \left(1-\frac{x}{K} \right) \; dt$) is called the "deterministic part" of the equation.  The second term ($\mbox{ Noise } \; dt$) is the "stochastic part".  We are going to model that with the white noise (Weiner process) $dW(t)=\mbox{ Noise } \; dt$.  This white noise has the following characateristics:

- $W(t)$ is continuous
- $W(0)=0$
- $W(t)-W(s)$ independent  (they call this independent increments)
- $W(t)-W(s)$ is normally distributed with mean 0 and standard deviation $$\sqrt{t-s}$$.

Building this process into our simulation models will be the focus of the final sections.

\newpage

## Exercises

```{exercise}
Compute the partial derivatives $p_{t}$, $p_{x}$, and $p_{xx}$ for the function $\displaystyle p(x,t) = \frac{1}{\sqrt{4 \pi Dt} } e^{-x^{2}/(4 D t)}$.  Then verify $p(x,t)$ is consistent with the diffusion equation.
```

&nbsp;

```{exercise}
For the one-dimensional random walk we discussed where there was an equal chance of moving to the left or the right.  Here is a variation on this problem.

Let's assume there is a chance $v$ that it moves to the left (position $x - \Delta x$), and therefore a chance is $1-v$ that the particle remains at position $x$. The basic equation that describes the particle's position at position $x$ and time $t + \Delta t$ is: 
\begin{equation}
p(x,t + \Delta t) = (1-v) p(x,t) + v \; p(x- \Delta x,t)
\end{equation}


Apply the techniques of local linearization in $x$ and $t$ to show that this random walk to derive the following partial differential equation, called the \emph{advection equation}:
\begin{equation}
p_{t} = - \left( v \frac{ \Delta x}{\Delta t} \right) p_{x}
\end{equation}

\emph{Note: you only need to expand this equation to first order}
```

&nbsp;

```{exercise}
Using Desmos, plot several solution curves of  $\displaystyle p(x,t) = \frac{1}{\sqrt{4 \pi Dt} } e^{-x^{2}/(4 D t)}$ for $t =0$, $t=5$, $t=10$ when $\displaystyle D=\frac{1}{2}$.  (\emph{Note: this means that your plots will be $p$ as a function of $x$.})  Finally, for $\displaystyle D= \frac{1}{2}$, evaluate $\displaystyle \int_{1}^{2} p(x,10) \; dx$.  Write a one sentence description what this quantity represents.
```


&nbsp;

```{exercise}
Using Desmos, complete the following table:
  
  \begin{tabular}{p{0.20\linewidth}|p{0.2\linewidth}}
    \textbf{Equation} & \textbf{Result} \\ \hline
  & \\
   $\displaystyle \int_{1}^{2} p(x,10) \; dx=$ &  \\
 & \\ \hline
   & \\
   $\displaystyle \int_{1}^{2} p(x,5) \; dx=$ &  \\
 & \\ \hline
   & \\
   $\displaystyle \int_{1}^{2} p(x,2.5) \; dx=$ &  \\
 & \\ \hline
   & \\
   $\displaystyle \int_{1}^{2} p(x,1) \; dx=$ &  \\
 & \\ \hline
   & \\
   $\displaystyle \int_{1}^{2} p(x,0.1) \; dx=$ &  \\
 & \\ \hline
   & \\
   $\displaystyle \int_{1}^{2} p(x,0.01) \; dx=$ &  \\
 & \\ \hline
   & \\
   $\displaystyle \int_{1}^{2} p(x,0.001) \; dx=$ &  \\
 & \\ \hline
%        $y=a+bx+cx^{2}$ &  \texttt{y $\sim$ 1 + x + I(x$^{2}$)} \\
  %              $y=a+bx+cx^{2}+dx^{3}$ &  \texttt{y $\sim$ 1 + x + I(x$^{2}$)+ I(x$^{3}$)} \\
    \end{tabular}

Based on the evidence in the table, what is the value of $\displaystyle \lim_{t \rightarrow 0^{+}} \int_{1}^{2} p(x,t) \; dx?$.  This result leads to the foundation of the [Dirac delta Function](https://en.wikipedia.org/wiki/Dirac_delta_function).
```





