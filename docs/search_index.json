[["index.html", "Exploring modeling with data and differential equations using R Welcome", " Exploring modeling with data and differential equations using R John M. Zobitz Version 2.0.0 Welcome This book is written for you: the student learning about modeling and differential equations. Perhaps you first encountered models, differential equations, and better yet, building plausible models from data in your Calculus course. This book sits “at the intersection” of several different mathematics courses: differential equations, linear algebra, statistics, calculus, data science - as well as the partner disciplines of biology, chemsitry, physics, business, and economics. An important idea is one of transference where a differential equation model applied in once context can also be applied (perhaps with different variable names) in a separate context. I intentionally emphasize models from biology and the environmental sciences, but throughout the text you can find examples from the other disciplines. I hope you see the connections of this content to your own intended major. This book is divided into 4 parts: Modeling with differential equations and data. Model parameter estimation. Stability analysis for differential equations. Modeling with stochastic differential equations. This may seem like a different order than traditionally presented. This is a “modeling first” paradigm that first introduces models, and equally important, how to estimate parameters for a model using data. This conversation between models and data are important to help build plausibility. Stability analysis helps to solidify the connection between models and parameters (which may change the underlying dynamical stability). Finally the notion of randomness is extended with the introduction of stochastic differential equations. "],["computational-code.html", "Computational code", " Computational code This book makes heavy use of the R programming language, and unabashedly develops programming principles using the tidyverse syntax and programming approach. This is intentional to facilitate direct connections to courses in introductory data science or data visualization. Throughout the years learning (and teaching) different programming languages I have found R to be the most versatile and adaptable. The tidyverse syntax has also been transformational for me in my own work and as my students - the barrier to compute and write code is lowered. There is a companion R package available to run programs and functions in the text. Instructions to do so are given in Section 2. The minimum version of R Version 4.0.2 (2020-06-22) and RStudio is Version 1.4.1106. "],["acknowledgments.html", "Acknowledgments", " Acknowledgments This book has been developed over the course of several years and has been written across different continents. Augsburg University: You have been my professional home for over 14 years and given me the space to be intellectually creative in my teaching. I have great colleagues to work with. Augsburg University students: Thank you for your interest in this topic, providing honest and insightful feedback about the course. This has been a work in progress (albeit bumpy at times). My family: Shannon, Colin, Grant, and Phoebe for humoring me while this project has been completed. Waterparks, coffee shops, soccer practices: Many times this was written “in the spaces” between work and home, and especially during downtimes when my kids could play. Turns out my kids love waterparks. Who knew? "],["copyright.html", "Copyright", " Copyright This work is distributed under the Creative Commons, Attribution-Non Commercial-No Derivatives 4.0 License. You may copy, distribute, display and perform the work and make derivative works and remixes based on it only if they give the author (Zobitz) attribute and use it for non-commerical purposes. You may copy, distribute, display and perform only verbatim copies of the work, not derivative works and remixes based on it. "],["intro-01.html", "Chapter 1 Models of rates with data ", " Chapter 1 Models of rates with data "],["rates-of-change-in-the-world-a-model-is-born.html", "1.1 Rates of change in the world: a model is born", " 1.1 Rates of change in the world: a model is born The focus of this textbook is understanding rates of change and how you can apply them to model real-world phenomena. In addition this textbook focuses on using equations with data, building both your competence and confidence to construct a mathematical model from data and a context. Perhaps you analyzed rates of change in calculus course when answering the following types of questions: If \\(y = xe^{-x}\\), what is the derivative function \\(f&#39;(x)\\)? What is the equation of the tangent line to \\(y=x^{3}-x\\) at \\(a=1\\)? Where is the graph of \\(\\sin(x)\\) increasing at an increasing rate? What is the largest area that can be enclosed with 100 feet of fencing to enclose, with one side being along a wall? If you release a ball from the top of a skyscraper 500 meters above the ground, what is its speed when it impacts the ground? The first three questions do not appear to be connected in a real-world context - but the last two questions do. For the fencing problem, perhaps a person raises chickens and wants to care for their well-being. Perhaps a rectangular pen is more aesthetically pleasing than a circular pen. In the last example the ball falling off the skyscraper assumes that acceleration of the ball is constant. In both of these last two cases the context may reveal underlying assumptions or physical principles, which are the starting point to build a mathematical model. For the chicken coop problem the next step is to use the assumed geometry (rectangle) with the 100 feet of fencing to develop a function for the area. For the ball problem, the velocity (or the antiderivative of acceleration) can be found, from which the position function can be calculated through antidifferentiation. Here is another approach. Let’s say we have observational data and several different (perhaps conflicting) assumptions. These assumptions describe models that involve rates of change. Which model is the best one to approximate the data? The short answer: it depends. To understand why, let’s take a look at a problem in context. "],["modeling-in-context-the-spread-of-a-disease.html", "1.2 Modeling in context: the spread of a disease", " 1.2 Modeling in context: the spread of a disease Consider the data in Figure ??, which come from an Ebola outbreak in Sierra Leone in 2014. Ebola is a fatal disease so we can also consider the vertical axis in Figure ?? to represent total infections due to Ebola. Figure 1.1: An Ebola outbreak in Sierra Leone Constructing a model from disease dynamics is part of the field of mathematical epidemiology. How we construct a mathematical model of the spread of this outbreak largely depends on the assumptions underlying the dynamics of the disease, such as considering the rate of spread of Ebola. For our purposes here we focus on the population level (person to person) spread of Ebola. Other types of models could focus on the immune response in a single person - perhaps with a goal to design effective types of treatments to reduce the severity of infection. Here are three initial assumptions one can make regarding the spread of Ebola: The infection rate is proportional to the number of people infected. The infection rate is proportional to the number of people not infected. The infection rate is proportional to the number of infected people coming into contact with those not infected. Let’s see what each of these mathematical models would look like if we wrote down a mathematical equation. Since we are discussing rates of infection, this means we will need a rate of change or derivative. Let’s use the letter \\(I\\) to represent the number of people that are infected. 1.2.1 Model 1: Infection rate proportional to number infected. The first assumption states that the infection rate is proportional to the number of people infected. Translated into an equation this would be the following: \\[\\begin{equation} \\frac{dI}{dt} = kI \\tag{1.1} \\end{equation}\\] In Equation (1.1) \\(k\\) can be thought of as a proportionality constant, with units of time\\(^{-1}\\) for consistency. Equation (1.1) is an example of a differential equation, which is just a mathematical equation with rates of change. The solution to a differential equation is a function \\(I(t)\\).1 When we ``solve’’ a differential equation we determine the family of functions consistent with our rate equation. There are a lot of techniques we can use to do that, and we will examine a few later. Back to this proportionality constant \\(k\\) - another term for it is a parameter. We can always try to solve an equation without specifying the parameter - and then if we wanted to plot a solution the parameter would also be specified. In some situations we may not be as concerned with the particular value of the parameter but rather its influence on the long-term behavior of the system (this is one aspect of bifurcation theory). Otherwise we can use the collected data shown above with the given model to determine the value for \\(k\\). This combination of a mathematical model with data is called data assimilation or model-data fusion. Before we think about possible solutions let’s try to reason out if the first model would be plausible. This model states that the rate of change (the amount of increase) gets larger the more people that are sick. While this may seem reasonable initially, but perhaps grows quickly unreasonable. In the case of Ebola or any other infectious disease, stringent public health measures would be enacted if the number of people infected become too large2. Following public health measures we would expect that the rate of infection would decrease and the number of deaths to slow. So perhaps the second model might be a little more plausible. At some point the number of people who are not sick will reach zero, making the rate of infection be zero (or no increase). 1.2.2 Model 2: Infection rate proportional to number NOT infected. In this description notice how we are talking about people who are sick (which we have denoted as \\(I\\)) and people who are not sick. This looks like we might need to introduce another variable for the ``not sick’’ people, which we will call \\(S\\), or susceptible. So the differential equation we would write down would be: \\[\\begin{equation} \\frac{dI}{dt} = kS \\tag{1.2} \\end{equation}\\] We are still using the parameter \\(k\\) as with the previous model. Also note we introduced the second variable \\(S\\) is in Equation (1.2). Because we have introduced another variable \\(S\\) we should also include a differential equation for how \\(S\\) changes as well. One way that we can do this is by considering our entire population as consisting of two groups of people: \\(S\\) and \\(I\\). Infection brings someone over from \\(S\\) to \\(I\\), which we have in Figure 1.2: Figure 1.2: Schematic diagram for Model 1 There are three reasons why I like to use diagrams like Figure 1.2: (1) they help organize my thinking about a mathematical model (2) any assumed parameters are listed, and (3) they help me to see that rates can be conserved. If I enter into the box for \\(I\\), then someone is leaving \\(S\\). So then the rate of change equation for \\(S\\) is \\(\\displaystyle \\frac{dS}{dt} = -kS\\). Together with the differentail equation for \\(I\\) I have the following: \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;= -kS \\\\ \\frac{dI}{dt} &amp;= kS \\end{split} \\tag{1.3} \\end{equation}\\] Equation (1.3) is an example of a coupled differential equation. In order to ``solve’’ the system we need to determine functions for \\(S\\) and \\(I\\). This coupled set of equations looks a little clunky, but there is something interest going on if we add the rates \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) together. Algebriacally we have: \\[\\begin{equation} \\frac{dS}{dt} + \\frac{dI}{dt} = \\frac{d(S+I)}{dt} = 0 \\end{equation}\\] Recall from calculus that if a rate of change equals zero then the function is constant. In this case, the variable \\(S+I\\) is constant, or we can also call \\(S+I=N\\), the number of people in the population. This means that \\(S=N-I\\), so we can re-write Equation (1.3) with a single equation: \\[\\begin{equation} \\frac{dI}{dt} = k(N-I) \\tag{1.4} \\end{equation}\\] This second model does have some limiting behavior to this model as well. As the number of infected people reaches \\(N\\) (the total population size), the values of \\(\\displaystyle \\frac{dI}{dt}\\) approaches zero, meaning \\(I\\) doesn’t change. There is one caveat to this - if there are no infected people around (\\(I=0\\)) the disease can still be transmitted, which might make not good biological sense. 1.2.3 Model 3: Infection rate proportional to infected meeting not infected. The third model rectifies some of the shortcomings of the second model (which rectified the shortcomings of the first model). This model states that the rate of infection is due to those who are sick, actually infecting those who are not sick. This would sort of scenario would also make some sense, as it focused on that transmission of the disease are between susceptibles and infected people. So if nobody is sick (\\(I=0\\)) then the disease is not spread. Likewise if there are no susceptibles (\\(S=0\\)), the disease is not spread as well. In this case the diagram outlining this approach looks something like this: Figure 1.3: Schematic diagram for Model 3 Notice how in Figure 1.3 there is an additional \\(S\\) associated with \\(k\\) to show how the rate of infection depends on \\(S\\). The differential equations that describe the scenario outlined in Figure 1.3 are the following: \\[\\begin{align*} \\frac{dS}{dt} &amp;= -kSI \\\\ \\frac{dI}{dt} &amp;= kSI \\end{align*}\\] Just like before for Model 2 we can combine the two equations to yield a single differential equation: \\[\\begin{equation} \\frac{dI}{dt} = k\\cdot I \\cdot (N-I) \\end{equation}\\] Look’s pretty similar to model 2, doesn’t it? In this case notice the variable \\(I\\) outside the expression. Notice this seems to be appropriate - if \\(I=0\\), then there is no increase in infection. If \\(I=N\\) (the total population size) then there is no increase in the infection. Let’s compare these two different models graphically. For both models let’s plot \\(\\displaystyle \\frac{dI}{dt}\\) versus \\(I\\), and just so we can plot let’s \\(k=1\\) and \\(N=10\\) respectively. Plots of these functions are shown in Figure 1.4. Figure 1.4: Comparing rates of change for three models Figure 1.4 has a lot to unpack, but we can use some of our understanding of rates of change in calculus to compare the three models. Notice how the sign of \\(\\displaystyle \\frac{dI}{dt}\\) is always positive for Model 1, indicating that the solution (\\(I\\)) is always increasing. For Models 2 and 3, \\(\\displaystyle \\frac{dI}{dt}\\) equals zero when \\(I=10\\), which also is the value for \\(N\\) After that case, \\(\\displaystyle \\frac{dI}{dt}\\) turns negative, meaning that \\(I\\) is decreasing. In summary, examining the graphs of the rates can tell a lot about the qualitative behavior of a solution to a differential equation even without the solution. You may be used to working with algebraic equations (e.g. solve \\(x^{2}-4=0\\) for \\(x\\)). In that case the solution can be points (for our example, \\(x=\\pm2\\)).↩︎ The COVID-19 pandemic that began in 2020 is an example of the heroic efforts of public health officials.↩︎ "],["model-solutions.html", "1.3 Model solutions", " 1.3 Model solutions Let’s return back to possible solutions (in this case formulas for \\(I(t)\\)) for our models. Usually a differential equation also has a starting or an initial value (typically at \\(t=0\\)) that actualizes the solution. When we state a differential equation with a starting value we have an initial value problem. We will represent that initial value as \\(I(0)=I_{0}\\), where could be considered another parameter. With that assumption, we can (and will solve later!) the following solutions for these models: \\[\\begin{align*} \\mbox{ Model 1 (Exponential): } &amp; I(t) = I_{0}e^{kt} \\\\ \\mbox{Model 2 (Saturating): } &amp; I(t) = N-(N-I_{0})e^{-kt} \\\\ \\mbox{Model 3 (Logistic): } &amp; I(t) = \\frac{N \\cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}} \\end{align*}\\] Notice how I assigned the names to each model (Exponential, Saturating, and Logistic). That may not mean much at the moment, but Figure 1.5 plots the three functions \\(I(t)\\) together when \\(I_{0}=5\\), \\(k=0.03\\), and \\(N=4000\\). Figure 1.5: Three models compared Notice how in Figure 1.5 Model 1 increases quickly - it actually grows without bound off the chart! Model 2 and Model 3 have saturating behavior, but it looks like Model 3 might be the one that actually captures the trend of the data. "],["which-model-is-best.html", "1.4 Which model is best?", " 1.4 Which model is best? All three of these scenarios describe different modeling scenarios. With the saturating and logistic models (Models 2 and 3) we have some limiting behavior the possibility that the the rate of infecion slows. Of the two models, which one is the best one? Here could be some possible criteria we could evaluate: Do the model outputs match the data? (For timeseries data) are the trends accurately represented? Is the model easy to use? How will model outputs compare with newly collected measurements? Here is a list of several things that go into a model: The model complexity - how many equations do we have? The number of parameters - a few or many? We will also address that question later on in this textbook when we discuss model selection (Chapter ??). Model selection is one key part of the modeling hypothesis - where we investigate the implications of a particular model analyzed. If we don’t do this, we don’t have an opportunity to test out what is plausible and what is believeable in our models. "],["start-here.html", "1.5 Start here", " 1.5 Start here In summary, it turns out that even with some initial assumptions we can very quickly build up a mathematical model to explain data. We have a lot more to uncover: How would you determine the parameters \\(k\\) and \\(N\\) with the collected data? Are there other more complicated models? What techniques are used to determine the formulas \\(I(t)\\)? Are there other numerical techniques to approximate the solution \\(I(t)\\)? What happens to our solutions when the parameters \\(k\\) and \\(N\\) change? What happens to our solutions when the number of infected people change randomly for some reason? We will study answers to these questions and more. Let’s get started! "],["exercises.html", "1.6 Exercises", " 1.6 Exercises Exercise 1.1 Solutions to an outbreak model of the flu are the following: \\[\\begin{align*} \\mbox{Saturating model: } &amp; I(t) = 3000-(2990)e^{-.1t} \\\\ \\mbox{Logistic model: } &amp; I(t) = \\frac{30000 }{10+(2990)e^{-.15t}}, \\end{align*}\\] where \\(t\\) is in days. Use these two functions to answer the following questions: Figure 1.6: An Ebola outbreak in Liberia in 2014 Exercise 1.2 Figure 1.6 shows the Ebola outbreak for the country of Liberia in 2014. If we were to apply the logistic model (Model 3) based on this graphic what would be your estimate for \\(N\\)? Exercise 1.3 The general solution for the saturating and the logistic models are: \\[\\begin{align*} \\mbox{Saturating model: } &amp; I(t) = N-(N-I_{0})e^{-kt} \\\\ \\mbox{Logistic model: } &amp; I(t) = \\frac{N \\cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}}, \\end{align*}\\] where \\(I_{0}\\) is the initial number of people infected and \\(N\\) is the overall population size. Using the functions from Exercise 1.1 for both models, what are \\(N\\) and \\(I_{0}\\)? Exercise 1.4 The general solution for the saturating and the logistic models are: \\[\\begin{align*} \\mbox{Saturating model: } &amp; I(t) = N-(N-I_{0})e^{-kt} \\\\ \\mbox{Logistic model: } &amp; I(t) = \\frac{N \\cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}}, \\end{align*}\\] where \\(I_{0}\\) is the initial number of people infected and \\(N\\) is the overall population size. For both models carefully evaluate the limits to show \\(\\lim_{t \\rightarrow \\infty} I(t)=N\\). How do your limiting values these compare to the steady-state values you found for Models 2 and 3 in Figure 1.5, where \\(N=4000\\)? Exercise 1.5 A model that describes the growth of sales of a product in response to advertising is the following: \\[\\frac{dS}{dt} = .55\\sqrt{1-S}-S, \\] where \\(S\\) is the product’s share of the market (scaled between 0 and 1). Use this information to answer the following questions: Exercise 1.6 A more general form of the advertising model is \\[\\begin{equation} \\frac{dS}{dt} = r\\sqrt{1-S}-S, \\end{equation}\\] where \\(S\\) is the product’s share of the market (scaled between 0 and 1). The parameter \\(r\\) is related to the effectiveness of the advertising (between 0 and 1). Exercise 1.7 A common saying is ``You are what you eat.’’ This saying is mostly true and can be related in a mathematical model! Here’s how: an equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\[\\begin{equation} y = c x^{1/\\theta}, \\end{equation}\\] where \\(\\theta \\geq 1\\) and \\(c\\) are both constants is a constant. Units on \\(x\\) and \\(y\\) are expressed as a proportion of a given nutrient (such as nitrogen or carbon). Let’s start with an example when \\(c=1\\) and \\(\\theta = 1\\). Our function then is \\(y=x\\). In this case the point \\((0.05,0.05)\\) would say that if an animal ate food that was 5% nitrogen, their body composition would be 5% as well. Exercise 1.8 A model for the outbreak of a cold virus assumes that the rate people get infected is proportional to infected people contacting susceptible people, as with Model 3 (the Logistic model). However people who are infected can also recover and become susceptible again with rate \\(\\alpha\\). Construct a diagram similar Model 3 for this scenario and also write down what you think the system of differential equations would be. Exercise 1.9 A model for the outbreak of the flu assumes that the rate people get infected is proportional to infected people contacting susceptible people, as in Model 3. However people also account for recovering from the flu, denoted with the variable \\(R\\). Assume that the rate of recovery is proportional to the number of infected people with parameter \\(\\beta\\). Construct a diagram like Model 3 for this scenario and also write down what you think the system of differential equations would be. Exercise 1.10 Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of \\(S\\) in the blood is the following: \\[\\frac{dS}{dt} = I + p \\cdot (W - S), \\] where the parameter \\(I\\) represents the active uptake of salt, \\(p\\) is the permeability of the skin, and \\(W\\) is the salinity in the water. Use this information to answer the following questions: Exercise 1.11 The immigration rate of bird species (species per time) from a mainland to an offshore island is \\(I_{m} \\cdot (1-S/P)\\), where \\(I_{m}\\) is the maximum immigration rate, \\(P\\) is the size of the source pool of species on the mainland, and \\(S\\) is the number of species already occupying the island. Further, the extinction rate is \\(E \\cdot S / P\\), where \\(E\\) is the maximum extinction rate. The growth rate of the number of species on the island is the immigration rate minus the extinction rate. Exercise 1.12 This problem relates to animal size and volume. Assume that an animal assimilates nutrients at a rate \\(R\\) proportional to its surface area. Also assume that it uses nutrients at a rate proportional to its volume. You may assume that the size of the animal is implicitly a function of the nutrient intake and usage. Determine the size of the animal if its intake and use rates were in balance (meaning \\(R\\) is set to zero), assuming the animal is the following shapes: Hint: For both of these problems your goal is to determine a numeric value of \\(r\\) and \\(l\\). "],["r-intro-02.html", "Chapter 2 Introduction to R", " Chapter 2 Introduction to R The primary tool we have to analyze models will be R and RStudio, which are commonly used for scientific and statistical computations. This is an exciting program and powerful program to learn! Admittedly learning a software requires a learning curve, however I think it is worth it. With R you will have enormous flexibility to efficiently utilize data, design effective visualizations, and process statistical models. "],["r-and-rstudio.html", "2.1 R and RStudio", " 2.1 R and RStudio We will use a lot with RStudio, which is an Integrated Development Environment for R. These are two separate downloads and files, which can be found here: R: (https://cran.r-project.org/mirrors.html)[Link to download mirror] (you need to select a location to download from; choose any one that is geographically close to you.) RStudio: (https://www.rstudio.com/products/rstudio/download/)[Choose RStudio desktop - the free version]. 2.1.1 Why do we have two programs? Think of R as your basic program - this is the engine that does the computation. RStudio was developed as an Integrated Development Environment - meaning a place where you can see everything you are working on in one place. Figure 2.1 shows an example of an RStudio pane that I have: Figure 2.1: A sample RStudio pane from one of my projects. There are 4 key panels that I work with, clockwise from the top: The source window is in the upper left - notice how those have different tabs associated with them. You can have multiple source files that you can toggle between. For the moment think of these as commands that you will want to send to R. The environment and history pane - these tables allow you to see what variables are stored locally in your environment, or the history of commands. The files and plots command (a simple plot I was working on is shown currently), but you can toggle between the tabs. The files tab shows the files in the current Rstudio project directory. Finally, the console pane is the place where R works and runs commands. You can type in there directly, otherwise we will also just ``send’’ commands from the source down to the console. Now we are ready to work with R and RStudio! "],["first-steps-getting-acquainted-with-r.html", "2.2 First steps: getting acquainted with R", " 2.2 First steps: getting acquainted with R Let’s get started! Open up RStudio. Task one will be to create a project. A project is a central place to organize your materials for this course. You may do this already, but R can be picky about its working directory - and navigating to it. I found creating a project file is an easy way to avoid some of that fussiness. Let me describe steps in how to do this. In RStudio select “File” then “New Project” Next select the first option “New Directory” in the window - this will create a new folder on your computer. At the next window choose New Directory or Existing Directory - it depends on where you want to place this project. Name the project as you like. Click the “Create Project” button. 2.2.1 “Working” with R Our next step: where do we get R to do something? For example if we wanted to compute of 4+9 (yeah, it is 13, but this is an illustrative example), we could type this command in the R console (lower left) window. Let’s try this now. In the console type 4+9 Then hit enter (or return) Is the result 13? Success! Now let me show you another way that works well if you have multiple lines of code to evaluate or save. Working with a script (.R file) is better. This will utilize the upper left hand corner of your RStudio window. (You may not have anything there when you start working on a project, so let’s create one.) In RStudio select “File” then “New File” Next select the first option “New Script” A new window called “UntitledX” should appear, where X is a number. You are set to go! I like to use this window as a file to type stuff in and then evaluate it, which we will do next. Pro tip: There are shortcuts to creating a new file: Ctrl+Shift+N (Windows and Linux) or Command+Shift+N (Mac) 2.2.2 Sending commands down to console. Now we want to type and evaluate a command for R to do something. Click anywhere in the source file that you created and type the following: 4+9 ## [1] 13 You have several options: Copying and pasting the command to the window. Shortcuts are Ctrl+C / Command+C for copying and Ctrl+V / Command+C for Windows / Mac. Run the line. This means that your cursor is at the line in your source file, then clicking the ‘Run’ button in the upper right hand side of the source window. Shortcuts are Ctrl+Enter / Command+Enter. Figure 2.2: Sending a command to the console. You can also source the whole file, which means runs all the lines from top to bottom. You do this by clicking the source button, or with shortcuts Ctrl+Shift+Enter / Cmd+Shift+Enter (Windows / Mac). You can imagine this makes things easier when you have SEVERAL lines of commands to evaluate. Why do I like working with a script file? Well if I run some commands that have an error then it is much easier to just fix a quick mistake and then re-run the code. It is also helpful to annotate your code with comments (#), which appear as green text in RStudio. 2.2.3 Saving your work The neat part about a source file is that it allows you to save the file (Ctrl+S / Cmd+S). The first time you do this you may need to give this a name. The location where this file will be saved is in the same directory as your .Rproj project file. Now you have a file that you can come back to! In general I try to use descriptive names of files to I can refer back to them later. "],["increasing-functionality-with-packages.html", "2.3 Increasing functionality with packages", " 2.3 Increasing functionality with packages One awesome versatility with R is the ability to add packages - these packages extend the functionality of R with contributed, specialized code. These are similar to apps on your phone. You can get packages from a few different places: CRAN, which stands for Comprehensive R Archive Network. This is the clearing house for many contributed packages - and allows for easy cross platform functonality. One key package is tidyverse, which is actually a collection of packages. If you take an introductory data science course you will most likely be learning more about this package, but to install this at the command line you type the following: install.packages(&#39;tidyverse&#39;) Typing this line will connect to the CRAN download mirrors and install this set of packages locally to your computer. It make take some time, but be patient. Another package you should install is devtools: install.packages(&#39;devtools&#39;) Sometimes when you are installing packages you may be prompted to install additional packages. In this case just say yes. Github. This is another place where people can share code and packages. The code here has not been vetted through CRAN for compatibility, but if you trust the person sharing the code, it should work. In order to do this we will need to first load up the devtools library: library(devtools) install_github(&quot;jmzobitz/demodelr&quot;,build_vignettes= TRUE) What this command will do is pull in the package structure from my github page and install it locally. Here is the good news: you only need to install a package once before using it! To load the package up into your workspace you use the command library: library(tidyverse) library(demodelr) You need to load up your these libraries each time you restart your R session. This is part of the benefit of a script file - at the start I always declare the libraries that I will need at the start of the script file, as shown in the following figure: Figure 2.3: A sample R script. Also notice here that the first few lines of the script file I used comments # to denote the basic purpose of the file, who wrote it, and the date it was last revised. This type of information is good programming practice at the start. "],["working-with-r-variables-data-frames-and-datasets.html", "2.4 Working with R: variables, data frames, and datasets", " 2.4 Working with R: variables, data frames, and datasets 2.4.1 Creating variables The next thing we will want to do is to define variables that are stored locally. This is pretty easy to do: my_result &lt;- 4+9 The symbol &lt;- is assignment (you can use equals (=), but it is good coding practice to use the arrow for assignment). Notice how I named the variable called my_result. Generally I prefer using descriptive names for variables for the context at hand (In other words, x would be an odd choice - too ambiguous.) I also used snake case to string together multiple words. In practice you can use snake case, or alphabetic cases (myResult) or even my.result (although that may not be preferred practice in the long run). You can’t use my-result because it looks like subtraction. Here is the good news, then we can compute with the new variable, so for example 10*my_result should yield 130. Cool, no? As an example, let’s define a sequence, spaced from 0 to 5 with spacing of 0.05. Store this in a variable called my_sequence. To do this we use the seq command and requires the starting value, ending value, and step size: my_sequence &lt;- seq(from=0,to=5,by=0.05) The format for the function seq is seq(from=start,to=end,by=step_size). The seq command is a pretty flexible - there are alternative ways you can generate a sequence by specifying the starting and the end values along with the number of points. If you want to know more about seq you can always use ? followed by the command - that will bring up the help values: ?seq Once you get more comfortable with syntax in R, you will see that seq(0,5,0.5) gives the same result as seq(from=0,to=5,by=0.05), but it is helpful to write your code so that you can understand what it does. 2.4.2 Data frames A key structure in R is that of a data frame, which allows different types of data to be collected together. A data frame is like a spreadsheet where each column is a value and each row a value (much like you would find in a spreadsheet): Table 2.1: A data frame mpg disp Mazda RX4 21.0 160 Mazda RX4 Wag 21.0 160 Datsun 710 22.8 108 Hornet 4 Drive 21.4 258 Hornet Sportabout 18.7 360 Table 2.1 shows the miles per gallon in one column (the variable mpg and the engine size (the variable disp) for different types of cars. The row names (Mazda RX4) just tell you the type of the car. Sometimes row names are not shown. Another data frame may list solutions to a differential equation, like we did with our three infection models in Chapter 1: Table 2.2: Model solutions time model_1 model_2 model_3 0.000000 5.000000 5.0000 5.000000 6.060606 5.996981 669.1571 5.995486 12.121212 7.192755 1222.9000 7.188814 18.181818 8.626962 1684.5848 8.619147 24.242424 10.347145 2069.5158 10.333332 Data frames are an example of tidy data, where each row is an observation, each column a variable (which can be quantitative or categorical). There are several different ways to define a data frame in R. I am going to rely on the approach utilized by the tidyverse, which calls data frames tibbles. So for example, here is I am going to define a data frame that computes the quadratic function \\(y=3x^2-2x\\) for \\(-5 \\leq x \\leq 2\\). x &lt;- seq(from=-5,to=2,by=0.05) y &lt;- 3*x^2-2x my_data &lt;- tibble(x=x, y=y) # Notice I sam specifically defining x and y Notice that the data frame my_data uses the column (variable) names of x and y. You could have also used tibble(x,y), but it is helpful to name the columns in the way that you would like them to be named. 2.4.3 Reading in datasets R has a lot of built in datasets! In fact to see all the datasets, type data() at the console. This will popup a new window in RStudio with the names. Cool! If you want to see the datasets for a specific package (such as demodelr) you type data(package = \"demodelr\") at the console. Perhaps what is most important is being able to read in datasets provided to you. Data come in several different types of formats, but one of the more versatile ones are csv (comma separated values). What you need to do is the following: Where you have your .Rproj file located, create a folder called data or datasets Save the file locally on your computer. Take note where you have it saved on your computer, and drag the file to your data folder. To read in the file you will use the command read_csv, which has the following structure: in_data &lt;- read_csv(FILENAME) The data gets assigned to the variable in_data (You can call this variable what you want.) For example I have the following csv file of ebola data, which I read in via the following: ebola &lt;- read_csv(&quot;data/ebola.csv&quot;) Notice the quotes around the FILENAME. Pro tip: If you have the data files in the data folder, in RStudio you can type “data” and it may start to autocomplete - this is hand (you can also use tab.) "],["visualization-with-r.html", "2.5 Visualization with R", " 2.5 Visualization with R Now we are ready to begin visualizing data frames. Two types of plots that we will need to make will be a scatter plot and a line plot. We are going to consider both of these separately, with examples that you should be able to customize. 2.5.1 Making a scatterplot One dataset we have is the mass of a dog over time, adapted from here. We have two variables here: \\(D=\\) the age of the dog in days and \\(W=\\) the weight of the dog in pounds. I have the data loaded into the demodelr package, which you can investigate by typing the following at the command line (I display it below as well): glimpse(wilson) (Notice that I have assumed you have the demodelr library loaded.) You can also explore the documentation for this dataset by typing ?wilson at the console. Table 2.3: Weight of a dog over time days mass 31 6.25 62 10.00 93 20.00 99 23.00 107 26.00 113 27.60 121 29.80 127 31.60 148 37.20 161 41.20 180 48.70 214 54.00 221 54.00 307 63.00 452 66.00 482 72.00 923 72.20 955 76.00 1308 75.00 Notice that this data frame has two variables: days and mass To make a scatter plot of these data we are going to use the command ggplot: ggplot(data = wilson) + geom_point(aes(x = days, y = mass)) + labs(x=&#39;Days since birth&#39;, y=&#39;Weight (pounds)&#39;) Wow! This looks complicated. Let’s break this down step by step: ggplot(data = wilson) + sets up the graphics structure and identifies the name of the data frame we are include. geom_point(aes(x = days, y = mass)) defines the type of plot we are going to be making. geom_point() defines the type of plot geometry (or geom) we are using here - in this case, a point plot. aes(x = days, y = mass) determines the aesthetics of the plot. On the x axis is the days variable, on the y axis is the mass variable. The statement beginning with labs(x=...) defines the labels on the x and y axes. I know this seems like a lot to write for a plot, but this structure is actually used for some more advanced data visualization. Trust me - learning how to make informative plots can be a useful skill! 2.5.2 Making a line plot Using the same wilson data, later on we will discover that the function \\(\\displaystyle W =f(D)= \\frac{70}{1+e^{2.46-0.017D}}\\). represents these data. In order to make a plot of this function we can use need to first build a data frame, plotFunction, which has a data frame as inputs: days &lt;- seq(0,1500,by=1) # Choose spacing that is &quot;smooth enough&quot; mass &lt;- 70/(1+exp(2.46-0.017*days)) wilson_model &lt;- tibble(days = days, mass = mass) ggplot(data = wilson_model) + geom_line(aes(x=days,y=mass)) + labs(x=&#39;Days since birth&#39;, y=&#39;Weight (pounds)&#39;) Notice that once we have the data frame set up, the structure is very similar to the scatter plot - but this time we are calling using geom_line() than geom_point. 2.5.3 Changing options Want a different color? Thicker line? That is fairly easy to do. For example if we wanted to make either our points or line a different color, we can just choose the following: ggplot(data = wilson) + geom_point(aes(x = days, y = mass), color=&#39;red&#39;,size=2) labs(x=&#39;Days since birth&#39;, y=&#39;Weight (pounds)&#39;) Notice how the command color='red' was applied outside of the aes - which means it gets mapped to each of the points in the data frame. size=2 refers to the size (in millimeters) of the points. I’ve linked more options about the colors and sizes you can use here: Named colors in R: LINK Scroll down to “Picking one color in R” - you can see the list of options! More colors: LINK. More information about working with colors. Using hexadecimal colors: LINK (You specify these by the code so \"#FF3300\" is a red color.) Changing sizes of lines and points: LINK 2.5.4 Combining scatter and line plots. This is actually easy to do, especially since we are combining both the plot geoms together. Try running the following code (I am still using the data frame wilson_model as defined above: ggplot(data = wilson) + geom_point(aes(x = days, y = mass),color=&#39;red&#39;) + geom_line(data = wilson_model, aes(x=days,y=mass)) + labs(x=&#39;Days since birth&#39;, y=&#39;Weight (pounds)&#39;) Notice in the above code a subtle difference when I added in the dataset wilson_model with geom_line: you need to name the data bringing in a new data frame to a plot geom. While it may be useful to have a legend to the plot, for this course we will make plots where this the context will be more apparent. Additional reading on legends can be found here. "],["defining-functions.html", "2.6 Defining functions", " 2.6 Defining functions We will study lots of other built-in functions for this course, but you may also be wondering how you define your own function (let’s say \\(y=x^{3}\\)). We need the following construct: function_name &lt;- function(inputs) { # Code return(outputs) } Here function_name serves as what you call the function, inputs are what you need in order to run the function, and outputs are what gets returned. So if we are doing \\(y=x^{3}\\) then we will call that function cubic: cubic &lt;- function(x) { y=x^3 return(y) } So now if we want to evaluate \\(y(2)=2^{3}\\) we type cubic(2). Neat! Now let’s make a plot of the graph \\(y=x^{3}\\) using the function defined as cubic. Here is the R code that will accomplish this: x &lt;- seq(from = 0, to = 2, by = 0.05) y &lt;- cubic(x) my_data &lt;- tibble(x=x,y=y) ggplot(data = my_data) + geom_line(aes(x=x,y=y)) + labs(x=&#39;x&#39;, y=&#39;y&#39;) 2.6.1 Functions with inputs Sometimes you may want to define a function with different input parameters, so for example the function \\(y=x^{3}+c\\). To define that, we can modify the function to have input variables: cubic_revised &lt;- function(x,c) { y=x^3+c return(y) } So if we want to plot what happens for different values of c we have the following: x &lt;- seq(from = 0, to = 2, by = 0.05) my_data_revised &lt;- tibble(x=x, c_zero=cubic_revised(x,0), c_pos1=cubic_revised(x,1), c_pos2=cubic_revised(x,2), c_neg1=cubic_revised(x,-1)) ggplot(data = my_data_revised) + geom_line(aes(x=x,y=c_zero)) + geom_line(aes(x=x,y=c_pos1)) + geom_line(aes(x=x,y=c_pos2)) + geom_line(aes(x=x,y=c_neg1)) + labs(x=&#39;x&#39;, y=&#39;y&#39;) Notice how I defined multiple columns of the data frame my_data_revised in the tibble command, and then used mutiple geom_line commands to plot the data. Since we had combined the different values of c in a single data frame we didn’t need to define the data with each instance of geom_line. "],["concluding-thoughts.html", "2.7 Concluding thoughts", " 2.7 Concluding thoughts This is not meant to be a self-contained chapter in R but rather one to get you - those miles have been trod by others, and here are few of my favorites that I turn to: R Graphics. This is a go to resource for making graphics. (I also use google a lot too.) The Pirates Guide to R. This book promises to build your R knowledge from the ground up. R for Reproducible Scientific Analysis. This set of guided tutorials can help you build your programming skills in R. R for Data Science this is a useful book to take your R knowledge to the next level. The best piece of advice: DON’T PANIC! Patience and persistence are your friend. Reach out for help, and recognize that like with any new endeavor, practice makes progress. x "],["exercises-1.html", "2.8 Exercises", " 2.8 Exercises Exercise 2.1 Create a folder on your computer and a project file where you will store all your R work for this textbook. Exercise 2.2 Install the packages devtools, tidyverse to your R installation. Once that is done, then install the package demodelr from my github page. Exercise 2.3 What are the variables listed in the dataset phosphorous in the demodelr library? (Hint: try the command ?phosphorous.) Exercise 2.4 Make a scatterplot (geom_point()) of the dataset phosphorous in the demodelr library. Be sure to label the axes. Exercise 2.5 Change the line plot of Wilson’s weight over time so the line is blue and the size is 4. Exercise 2.6 Change the color of the scatterplot of Wilson’s weight over time to a either a hexadecimal color or a named color of your choice. Exercise 2.7 For this exercise you will do some plotting: Exercise 2.8 An equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\(\\displaystyle y = c x^{1/\\theta}\\), where \\(\\theta \\geq 1\\) and \\(c\\) are both constants is a constant. Let’s just assume that \\(c=1\\) and the \\(0 \\leq x \\leq 1\\). Write a function called nutrient that will make a sequence of y values for an input x and theta (\\(theta\\)). Then use that code to make a make a line plot (geom_line()) for five different values of \\(\\theta&gt;1\\), appropriately labeling all axes. Exercise 2.9 Researchers measured the phosphorous content of Daphnia and its primary food source algae. This is the dataset phosphorous in the demodelr library. Researchers believe that Daphnia has strict homeostatic regulation of the phosphorous in algae, and as such want to determine the value of \\(\\theta\\) in the equation \\(y= \\displaystyle y = c x^{1/\\theta}\\). They have already determined that the value of \\(c=1.737\\). Exercise 2.10 For this exercise you will investigate some built-in functions. Remember you can learn more about a function by typing ?FUNCTION, where FUNCTION is the name. Exercise 2.11 For this exercise you write a sample function file. Exercise 2.12 The Ebola outbreak in Africa in 2014 severely affected the country of Sierra Leone. A model for the number of deaths \\(D\\) due to ebola is given by the following equation: \\[ D(t) = \\frac{K \\cdot N_{0} }{N_{0} + (K-N_{0}) \\exp(-rt)}, \\] where \\(K = 3980\\), \\(N_{0}=5\\) and \\(r = 0.0234\\). The variable \\(t\\) is in days. Use geom_line() to visualize this curve from \\(0 \\leq t \\leq 700\\). Exercise 2.13 Consider the following piecewise function: \\[\\begin{equation} y = \\begin{cases} x^2 &amp; \\text{ for } 0 \\leq x &lt; 1,\\\\ 2-x &amp;\\text{ for } 1 \\leq x \\leq 2 \\\\ \\end{cases} \\end{equation}\\] Exercise 2.14 An insect’s development rate \\(r\\) depends on temperature \\(T\\) (degrees Celsius) according to the following equation: \\[\\begin{equation} r = \\begin{cases} 0.1 &amp; \\text{ for } 17 \\leq T &lt; 27,\\\\ 0 &amp;\\text{ otherwise.} \\end{cases} \\end{equation}\\] "],["modeling-rates-03.html", "Chapter 3 Modeling with rates of change", " Chapter 3 Modeling with rates of change So far we have looked at some examples for how we can apply rates of change to develop a mathematical model, and also learned a little bit about the ways we can apply computational software such as R. In this section we are going to look some additional examples of how we can translate equations with rates of change to understand phenomena. The focus here will be on writing differential equations from a contextual description. Oftentimes when we construct differential equations from a contextual description we bring our own understanding and knowledge to this situation. How you may write down the differential equation may be different from someone else - do not worry! This is the fun part of modeling: models can be considered testable hypotheses that can be refined when confronted with data. In this section I work through a few well-known examples from mathematical biology and you will apply that knowledge to develop models from context. "],["lynx-and-hares.html", "3.1 Lynx and Hares", " 3.1 Lynx and Hares Our first example is a system of differential equations. The context is between the snowshoe hare and the Canadian lynx. Figure 3.1 shows a picture of them below from link Figure 3.1: The lynx and hare - aren’t they beautiful? Figure 3.2 timeseries of their population is shown with this figure from (stenseth_population_1997?). Figure 3.2: A timeseries of the combined lynx and hare system. Notice how the populations are coupled with each other. Notice how in Figure 3.2 both populations seem to fluctuate periodically. One plausible reason is that the lynx prey on the snowshoe hares, which causes the population to initially decline. Once the snowshoe hare population declines, then there is less food for the lynx to survive, so their population declines. The decline in the lynx population causes the hare population to increase, and so on it goes … In summary it is safe to say that the two populations are coupled to one another. But in order to understand how they are coupled together, first let’s consider the two populations separately. The hares grow much more quickly than then lynx - in fact some hares have been known to reproduce several times a year. A reasonable assumption for large hare populations is that rate of change of the hares is proportional to the hare population. Based on this assumption Equation (3.1) describes the rate of change of the hare population, with \\(H\\) is the population of the hares: \\[\\begin{equation} \\frac{dH}{dt} = r H \\tag{3.1} \\end{equation}\\] In this case we know that the growth rate \\(r\\) is positive, so then the rate of change (\\(H&#39;\\)) will be positive as well, and \\(H\\) will be increasing. Typical values given for \\(r\\) in (stenseth_population_1997?) are between 1.8 - 2.0 year\\(^{-1}\\). You may be thinking that the units on \\(r\\) seem odd - (year\\(^{-1}\\)). Another way to think about \\(r\\) is to take its inverse: \\(r^{-1} \\approx\\) 0.5 - 0.55 years. Then \\(r^{-1}\\) represents the amount of time that passes before the hare population increases (pretty short!) Let’s consider the lynx now. A approach is to assume their population declines exponentially, or changes at the rate proportional to the current population. Let’s consider \\(L\\) to be the lynx population, with the following differential equation (Equation (3.2)): \\[\\begin{equation} \\frac{dL}{dt} = -dL \\tag{3.2} \\end{equation}\\] We assume the death rate \\(d\\) in Equation (3.2) is positive, leading to a negative rate of change for the Lynx population (and a decreasing value for \\(L\\)). Typical values of \\(d\\) are 0.9 - 2.4 year\\(^{-1}\\). Similar to \\(r\\), another way \\(d\\) is - like \\(b\\) - to take its inverse (about 0.4 - 1.1 years), which represents the amount of time that passes before the lynx population decreases by one. The next part to consider is how they interact. Since the hares are prey for the lynx, when the lynx hunt, the hare population. We can represent this with the following adjustment to our hare equation: \\[\\begin{equation} \\frac{dH}{dt} = r H - b HL \\end{equation}\\] So the parameter \\(b\\) represents the hunting rate. Notice how we have the term \\(HL\\) for this interaction. This term injects a sense of realism: if the lynx are not present (\\(L=0\\)), then the hare population can’t decrease due to hunting. We say that the interaction between the hares and the lynx with multiplication. Typical values for \\(b\\) are 480 - 870 hares \\(\\cdot\\) lynx\\(^{-1}\\) year\\(^{-1}\\). It is okay if that unit seems a little odd to you - it should be! Here is one way to think about it. The quantity \\(\\displaystyle \\frac{dH}{dt}\\) represents the rate of change of the hares, so it should have units of hares per year. Since the term \\(bHL\\) has both lynx and hare, the units for \\(b\\) need to account for this. How does hunting affect the lynx population? One possibility is that it increases the lynx population: \\[\\begin{equation} \\frac{dL}{dt} =bHL -dL \\end{equation}\\] Notice the symmetry between the rate of change for the hares and the lynx equations. In many cases this makes sense - if you subtract a rate from one population, then that rate should be added to the receiving population. You could also argue that there is some efficiency loss in converting the hares to lynx - not all of the hare is converted int the lynx biomass. In this situation we sometimes like to adjust the lynx equation with another parameter \\(e\\), representing the efficiency that hares are converted into lynx: \\[\\begin{equation} \\frac{dL}{dt} =e\\cdot bHL -dL \\end{equation}\\] (sometimes people just make a new parameter \\(c=e \\cdot b\\), but for now we will just leave it as is). Equation (3.3) shows the coupled system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= r H - b HL \\\\ \\frac{dL}{dt} &amp;=ebHL -dL \\end{split} \\tag{3.3} \\end{equation}\\] The schematic diagram representing these interactions is the following is shown in Figure 3.3: Figure 3.3: Schematic diagram Lynx-Hare system. Equation (3.3) is a classical model in mathematical biology and differential equations - it is called the predator prey model, also known as the Lotka-Volterra equations. There is a lot of interesting mathematics from this system of equations that we will study later in this textbook. In later sections we will graphically and numerically analyze these equations and their solutions. "],["the-law-of-mass-action.html", "3.2 The Law of Mass Action", " 3.2 The Law of Mass Action Notice in the previous section that the interaction between the lynx and the hare was of the form bHL - meaning you needed both positive values of H and L for the interaction to continue. This law states that the rate of a change is directly proportional to the product of the populations. This assumption of the law of mass action is also commonly used in chemical reactions - especially in modeling enzyme dynamics. For example let’s say you have a substrate A that reactions with enzyme B to form a product S. Perhaps you might have seen this represented as a reaction equation: \\[\\begin{equation} A+B \\rightarrow S \\end{equation}\\] How we would write the product of formation, or \\(\\displaystyle \\frac{dS}{dt}\\) is the following: \\[\\begin{equation} \\frac{dS}{dt}= kAB, \\end{equation}\\] where \\(k\\) is the proportionality constant or the rate constant associated with the reaction. If we wanted to represent this as a schematic we would have the following diagram (Figure 3.4): Figure 3.4: Schematic diagram of the law of mass action. We could also consider if there was a constant decay of the substrate, which we might revise Figure 3.4 to Figure 3.5: Figure 3.5: Revised schematic diagram of the law of mass action with decay. For this case, the rate of change of \\(S\\) would then be: \\[\\begin{equation} \\frac{dS}{dt}= kAB - dS, \\tag{3.4} \\end{equation}\\] You may be wondering about rates for \\(A\\) and \\(B\\). When \\(S\\) is formed \\(A\\) and \\(B\\) are catalyzed, so the rate of formation for \\(S\\) (the positive term in Equation (3.4)) will be a loss for \\(A\\) and \\(B\\) (Equation (3.5)): \\[\\begin{equation} \\begin{split} \\frac{dA}{dt} &amp;= -kAB \\\\ \\frac{dB}{dt} &amp;= -kAB \\end{split} \\tag{3.5} \\end{equation}\\] Both equations are similar in this case, not pending any additional inputs or outputs. "],["establishing-species.html", "3.3 Establishing species", " 3.3 Establishing species Let’s look at another example where from we will determine a differential equation model from a context: An newly introduced plant species is introduced to a region. It competes with another established species for nutrients (and is a better competitor). However, the growth rate of the new species is proportional to the difference between the current number of established species and the number of new species. You may assume that the number of established species is a constant E. For this problem we will start by naming our variables. Let \\(N\\) represent number of new species and \\(E\\) the number of established species. We will break this down accordingly: “the growth rate of the new species” means \\(\\displaystyle \\frac{dN}{dt}\\). “is proportional to the difference between the current number of established species and the number of new species” means \\(\\displaystyle k \\cdot (E-N)\\), where \\(k\\) is the proportionality constant. Including this parameter helps to avoid assuming we have a 1:1 correspondence between the growth rate of the new species and the population difference. “and is a better competitor” helps to explain why the term is \\(\\displaystyle k \\cdot (E-N)\\) insteady of \\(\\displaystyle k \\cdot (N-E)\\). We know that the newly established species will start out in much smaller numbers than \\(N\\). But since it is a better competitor, we would expect its rate to increase initially. So \\(\\displaystyle \\frac{dN}{dt}\\) should be positive rather than negative. Assuming \\(N &lt; E\\), then \\(E-N &gt; 0\\), which guarantees that the new species will grow. So this description could be modeled with Equation in summary we have the following model: \\[\\begin{equation} \\frac{dN}{dt} = k(E-N) \\end{equation}\\] Does this equation seem familiar to you? It is similar to the second model in Chapter 1 for the spread of Ebola! While this may seem surprising, it is often the case that similar equations appear in different contexts. It is far more advantageous to learn how to analyze models qualitatively rather than memorize several different types of models and not see the connections between them. An interesting solution to a differential equation is the steady state or equilibrium solution. We find this where the rate equals zero. Let’s take a look how to do that for our establishing plant model Example 3.1 What is the steady state value for the differential equation \\(\\displaystyle \\frac{dE}{dt} = k(E-N)\\)? (That is solve for \\(E\\) when \\(\\displaystyle \\frac{dE}{dt} = 0\\).) Solution. Let’s solve \\(\\displaystyle \\frac{dE}{dt} = k(E-N) = 0\\). For this equation we want to express the right hand side in terms of \\(E\\). The parameter \\(k\\) is a constant \\(k &gt; 0\\), so really the steady state occurs when \\(E-N = 0\\), or when \\(N = E\\). What this model tells us that eventually the new species will overtake the established species \\(E\\). "],["other-types-of-functional-responses.html", "3.4 Other types of functional responses", " 3.4 Other types of functional responses In several examples we have seen a rate of change proportional to the current population, as in the rate of growth of the hare population is \\(rH\\). This is one example of what we would call a functional response. Another type of functional response assumes that the rate reaches a limiting value proportional to the population size, so \\(\\displaystyle \\frac{dH}{dt} = \\frac{rH}{1+arH}\\). This is an example of a type II functional response. Finally the type II response has also been generalized (a type III functional response) \\(\\displaystyle \\frac{dH}{dt} = \\frac{rH^{2}}{1+arH^{2}}\\). Figure 3.6 shows all three functional responses together: Figure 3.6: Comparison between Type I - Type III functional responses. Notice the limiting behavior in the Type II and Type III functional responses. These responses are commonly used in ecology and predator-prey dynamics and in problems of how animals search for food. "],["exercises-2.html", "3.5 Exercises", " 3.5 Exercises Exercise 3.1 Consider the following type of functional responses: \\[\\begin{align} \\mbox{ Type I: } \\frac{dP}{dt} &amp;= 0.1 P \\\\ \\mbox{ Type II: } \\frac{dP}{dt} &amp;= \\frac{0.1P}{1+.03P} \\\\ \\mbox{ Type III: } \\frac{dP}{dt} &amp;= \\frac{0.1P^{2}}{1+.05P^{2}} \\end{align}\\] For each of the functional responses evaluate \\(\\displaystyle \\lim_{P \\rightarrow \\infty} \\frac{dP}{dt}\\). Since these functional responses represent a rate of change of a population, what are some examples (hypothetical or actual) would each of these responses be appropriate? Exercise 3.2 A population grows according to the equation \\(\\displaystyle \\frac{dP}{dt} = \\frac{0.1P}{1+.05P} -P\\). Exercise 3.3 A population grows according to the equation \\(\\displaystyle \\frac{dP}{dt} = 2P - \\frac{4P^{2}}{1+P^{2}}\\). Exercise 3.4 A population grows according to the equation \\(\\displaystyle \\frac{dP}{dt} = \\frac{aP}{1+abP} - dP\\), where \\(a\\), \\(b\\) and \\(d\\) are parameters. Determine the two steady state values of \\(P\\), that is solve \\(\\displaystyle \\frac{dP}{dt}=0\\) for \\(P\\). Exercise 3.5 For a chemical reaction takes two chemicals \\(X\\) and \\(Y\\) to form a substrate \\(Z\\) through the law of mass action. However the substrate can also disassociate. The reaction schematic is the following: \\[\\begin{equation} X + Y \\rightleftharpoons Z, \\end{equation}\\] where the proportionality constant \\(k_+\\) is associated with the formation of the substrate \\(Z\\) and \\(k_-\\) the disassociation (\\(Z\\) decays back to \\(X\\) and \\(Y\\)). Write down a differential equation that represents the rate of reaction \\(\\displaystyle \\frac{dZ}{dt}\\). Exercise 3.6 For each of the following exercises consider the following contextual situations modeling rates of change. Name variables for each situation and write down a differential equation describing the context. Be sure to identify and briefly describe any parameters you need for your model. For each problem you will need to: Figure 3.7: Reaction schemes. Exercise 3.7 You are tasked with the job of investigating the effect of a pesticide on water quality, in terms of its effects on the health of the plants and fish in the ecosystem. Different models can be created that investigate the effect of the pesticide. Different types of reaction schemes for this system are shown in Figure 3.7, where \\(F\\) represents the amount of pesticide in the fish, \\(W\\) the amount of pesticide in the water, and \\(S\\) the amount of pesticide in the soil. The prime (e.g. \\(F&#39;\\), \\(W&#39;\\), and \\(S&#39;\\) represent other bound forms of the respective state). In all seven different models can be derived. For each of the model schematics, apply the Law of Mass Action to write down a system of differential equations. "],["euler-04.html", "Chapter 4 Euler’s method", " Chapter 4 Euler’s method The focus of this section is on approximation of solutions to a differential equation via a numerical method. Typically a first numerical methods one might learn to tackle this problem is Euler’s method, which is so fundamental it was popularized in the movie Hidden Figures. The way we are going to do this is through expansion of the idea of a locally linear approximation to the tangent line. Let’s start with an example. Example 4.1 (The flu bug) The rate of change of the flu through a population is given by the number of people infected \\(t\\) days after infection is, \\[\\displaystyle \\frac{dI}{dt} = 3e^{-.025t}. \\] Assuming that \\(I(0)=10\\), what is a locally linear approximation to this infection? Second, using your linear approximation, what would you predict is the value after one day (\\(I(1)\\))? Remark. In order to solve this problem, we know that the locally linear approximation is to \\(I(t)\\) at \\(t=0\\) is \\(L(t) = I(0) + I&#39;(0) \\cdot (t-0)\\). Here, \\(I(0)=10\\) and \\(I&#39;(0)=3\\), so \\[L(t) = 10 +3t\\]. Using \\(L(t) \\approx I(t)\\), we have \\(L(1)=10 + 3 = 13\\). So our model predicts there will be 13 people sick. Notice in Example 4.1 we used two pieces of information: the (given) value of the function at \\(t=0\\) and the estimate of the derivative from the rate of change. It may be helpful to compare our prediction from \\(L(1)\\) to the actual value. The solution to the differential equation in Example 4.1 is \\(I(t) = 130-120e^{-.025t}\\) (you should verify this is the case by differentation). Let’s compare in the following table: \\(t\\) Linear approximation Actual Solution 0 10 10 1 13 12.96 Not too bad, huh? Our approximation at \\(L(1)\\) is an overestimate, mainly because the actual solution is concave down, but it isn’t that far off. Let’s build this solution out a little more by computing the rate of change at \\(t=1\\), assuming that thirteen people is a pretty close estimate of the \\(I\\) at \\(t=1\\). What we could do is to build another linear approximation using the differential equation. So the locally linear approximation is to \\(I(t)\\) at \\(t=1\\) is \\[ L(t) = I(1) + I&#39;(1) \\cdot (t-1) \\]. Here, \\(I(1)=13\\) and \\(I&#39;(1)=2.92\\), and \\(L(t) = 13 +2.92(t-1).\\) Assuming that \\(L(t) \\approx I(t)\\), we can evaluate \\(L(t)\\) at \\(t=2\\) as an approximation for \\(I(2)\\): have \\(L(2)=13 + 2.92 = 15.92\\). Comparing this to the actual solution at \\(t=2\\), we have \\(I(2)=15.85\\). Again, not too bad of a solution. We can continue to build out the solution from there. Figure @ref(fig:eulers_ver1) shows what we would have for a solution if we continued to build out this approach: (#fig:eulers_ver1)Approximation of a solution using local linearity When you plot them they do look indistinguishable from each other by eye. It looks like we are onto something here! "],["defining-an-algorithm.html", "4.1 Defining an Algorithm", " 4.1 Defining an Algorithm Here would be an algorithm that would describe our process to determine a solution to a differential equation: Determine the locally linear approximation at a given point. Forecast out to another time value. Repeat the locally linear approximation. If we continue on in this way, let’s take a look at how our approximation would do after several days: \\(t\\) Approximate Solution Actual Solution 90 118.4 117 95 119.9 118.6 Now it seems that our approximation isn’t so accurate as time goes on. What if we updated the infection rate every half day? I know this means that we would be doing additional work (more iterations), but taking smaller timesteps goes hand in hand with more accurate solutions. Let’s start out smaller with the first few timesteps: \\(t\\) \\(I\\) \\(\\displaystyle \\frac{dI}{dt}\\) \\(\\displaystyle \\frac{dI}{dt} \\cdot \\Delta t\\) 0 10 3 1.5 0.5 = 10 + 1.5 = 11.5 2.96 1.48 1 = 11.5 + 1.48 = 12.98 2.92 1.46 1.5 = 12.92 + 1.46 = 14.38 2.88 1.44 2 = 14.38 + 1.44 = 15.82 Notice how we have started to build up a way to organize how to compute the solution. Each column is a ``step’’ of the method, computing the solution at a new timestep based on our step size \\(\\Delta t\\). The third column just computes the value of the derivative for a particular time and \\(I\\), and then the fourth column is the increment size, or the amount we are forecasting the solution will grow by to the next timestep. (There are other ways to think about this, but if you have a rate of change multiplied by a time increment this will give you an approximation to the net change in a function.) This idea of approximate, forecast, repeat is the heart of many numerical methods that approximate solutions to differential equations. The particular method that we have developed here is called Euler’s Method. We display the results from additional steps in Figure 4.1. Figure 4.1: Approximation of a solution using local linearity You may notice that the approximation in Figure @ref(fig:eulers_ver2) compares very favorably to the actual solution function. At the end, we have the following comparisons: \\(t\\) Euler’s Method (\\(\\Delta t = 1\\)) Euler’s Method (\\(\\Delta t = 0.5\\)) Actual Solution 190 130.5 129.7 129 195 130.6 129.8 129.1 200 130.7 129.9 129.2 There is a tradeoff here - the smaller stepsizes you have the more work it will take to compute you solution. You may have seen a similar tradeoff in Calculus when you explored numerical integration and Riemann sums. "],["building-an-iterative-method.html", "4.2 Building an iterative method", " 4.2 Building an iterative method Now that we have worked on an example, let’s carefully formulate Euler’s method with another example. Consider the following differential equation: Consider the following equation that describes the rate of change of the spread of a disease (such as Ebola, as we covered in the first section): \\[ \\frac{dI}{dt} = 0.003 I \\cdot (4000-I) \\] Let’s call the function \\(f(I) = 0.03 I\\cdot (4000-I)\\). In order to numerically approximate the solution, we will need to recall some concepts from calculus. One way that we can approximate the derivative is through a difference function: \\[ \\frac{dI}{dt} = \\lim_{\\Delta t \\rightarrow 0} \\frac{I(t+\\Delta t) - I(t)}{\\Delta t} \\] As long as we consider the quantity \\(\\Delta t\\) to be small (say for this problem 0.1 days if you would like to have units attached to this), we can approximate the derivative with difference function on the right hand side. With this information, we have a reasonable way to organize the problem: \\[\\begin{align*} \\frac{I(t+\\Delta t) - I(t)}{\\Delta t} &amp;= 0.003 I \\cdot (4000-I) \\\\ I(t+\\Delta t) - I(t) &amp;= 0.003 I \\cdot (4000-I) \\cdot \\Delta t \\\\ I(t+\\Delta t) &amp;= I(t) + 0.003 I \\cdot (4000-I) \\cdot \\Delta t \\end{align*}\\] The last equation \\(I(t+\\Delta t) = I(t) + 0.03 I \\cdot (4000-I) \\cdot \\Delta t = f(I) \\cdot \\Delta t\\) is a reasonable way to define an iterative system, especially if we have a spreadsheet program. Here is some code in R that can define a for loop to do this in an iterative way and then plot the solution with plotData: # Define your timestep and time vector deltaT &lt;- 0.1 t &lt;- seq(0,2,by=deltaT) # Define the number of steps we take. This is equal to 10 / dt (why?) N &lt;- length(t) # Define the initial condition i_approx &lt;- 10 # Define a vector for your solution:the derivative equation for(i in 2:N) { # We start this at 2 because the first value is 10 didt &lt;- .003 * i_approx[i-1] * (4000-i_approx[i-1]) i_approx[i] &lt;- i_approx[i-1] + didt*deltaT } # Define your data for the solution into a tibble: solution_data &lt;- tibble(time =t, infected = i_approx) # Now plot your solution: ggplot(data = solution_data) + geom_line(aes(x=time,y=infected)) Figure 4.2: An iterative method labs(x=&#39;Time&#39;, y=&#39;Infected&#39;) ## $x ## [1] &quot;Time&quot; ## ## $y ## [1] &quot;Infected&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;labels&quot; Ok, let’s discuss the different aspects of this code: deltaT &lt;- 0.1 and t &lt;- seq(0,2,by=deltaT) define the timesteps (\\(\\Delta t\\)) and the output time vector t. We also define N &lt;- length(t) so we know how many steps we take. i_approx &lt;- 10 defines the initial condition to the system, in other words \\(I(0)=10\\). The for loop goes through this system - first computing the value of \\(\\displaystyle \\frac{dI}{dt}\\) and then forecasing out the next timestep \\(I(t+\\Delta t) = f(I) \\cdot \\Delta t\\) The remaining code plots the dataframe, like we learned in Section 2. Let’s recap what we’ve learned to summarize Euler’s method. The most general form of a differential equation is: \\[ \\displaystyle \\frac{d\\vec{y}}{dt} = f(\\vec{y},\\vec{\\alpha},t), \\] where \\(\\vec{y}\\) is the vector of state variables you want to solve for, and \\(\\vec{\\alpha}\\) is your vector of parameters. At a given initial condition, Euler’s method applies locally linear approximations to forecast the solution forward \\(\\Delta t\\) time units: \\[ \\vec{y}_{n+1} = y_{n} + f(\\vec{y}_{n},\\vec{\\alpha},t_{n}) \\cdot \\Delta t \\] To generate Figure 4.2 we created the solution directly in R - but you don’t want to copy and paste the code. I’ve created a function called euler that does the same process to generate the output solution: # Define the rate equation: system_eq &lt;- c(didt ~ .003 * i * (4000-i)) # Define the initial condition (as a named vector) init_cond &lt;- c(i=10) # Define deltaT and the time steps: deltaT = 0.2 n_steps &lt;- 10 # Compute the solution via Euler&#39;s method: out_solution &lt;- euler(system_eq,initial_condition=init_cond,deltaT=deltaT,n_steps = n_steps) # Now plot your solution: ggplot(data = out_solution) + geom_line(aes(x=t,y=i)) Figure 4.3: Euler's method solution labs(x=&#39;Time&#39;, y=&#39;Infected&#39;) ## $x ## [1] &quot;Time&quot; ## ## $y ## [1] &quot;Infected&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;labels&quot; Let’s talk through the steps of this code as well: The line system_eq &lt;- c(didt ~ .003 * i * (4000-i)) represents the differential equation, written in formula notation. So \\(\\displaystyle \\frac{dI}{dt} \\rightarrow\\) didt and \\(f(I) \\rightarrow\\) .003 * i * (4000-i)), with the variable i. The initial condition \\(I(0)=10\\) is written as a named vector: init_cond &lt;- c(i=10). Make sure the name of the variable is consistent with your differential equation. As before we need to identify \\(\\Delta t\\) and the number of steps \\(N\\). The command euler then computes the solution applying Euler’s method, returning a data frame so we can plot the results. Note the columns of the data frame are the variables \\(t\\) and \\(i\\) that have been named in our equations. "],["eulers-method-applied-to-systems.html", "4.3 Euler’s method applied to systems", " 4.3 Euler’s method applied to systems Here is a sample code that shows the dynamics for the Lotka-Volterra equations, as studied in Section 3: \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= r H - bHL \\\\ \\tag{4.1} \\frac{dL}{dt} &amp;= e b H L - dL \\end{split} \\end{equation}\\] We are going to use Euler’s method to solve this differential equation. Similar to the previous example we will need to determine the f # Define the rate equation: system_eq &lt;- c(dHdt ~ r*H-b*H*L, dLdt ~ e*b*H*L-d*L) # Define the parameters (as a named vector) lynx_hare_params &lt;- c(r = 2, b = 0.5, e = 0.1, d = 1) # parameters: a named vector # Define the initial condition (as a named vector) init_cond &lt;- c(H=1, L=3) # Define deltaT and the time steps: deltaT &lt;- 0.05 # timestep length timeSteps &lt;- 200 # must be a number greater than 1 # Compute the solution via Euler&#39;s method: out_solution &lt;- euler(system_eq, parameters = lynx_hare_params, initial_condition=init_cond, deltaT=deltaT, n_steps = n_steps) # Make a plot of the solution, using different colors for lynx or hares. ggplot(data = out_solution) + geom_line(aes(x=t,y=H),color=&#39;red&#39;) + geom_line(aes(x=t,y=L),color=&#39;blue&#39;) + labs(x=&#39;Time&#39;, y=&#39;Lynx (red) or Hares (blue)&#39;) Figure 4.4: Euler's method solution for Lynx-Hare system This example is structured similarly as a single variable differential equation, with some key changes: The variable system_eq is now a vector, with each entry one of the rate equations. We need to identify both variables in their initial condition. Most importantly, Equation (4.1) has parameters, which we define as a named vector lynx_hare_params &lt;- c(r = 2, b = 0.5, e = 0.1, d = 1) that we pass through to the command euler with the option parameters. If your equation does not have any parameters you do not need to worry about it. We plot both solutions together at the end, or you can make two separate plots. Remember that you can choose the color in your plot. Thankfully the code is pretty easy to adapt for systems of equations! "],["more-refined-numerical-solvers.html", "4.4 More refined numerical solvers", " 4.4 More refined numerical solvers Perhaps in the course of working with Euler’s method you encounter a differential equation that produces some nonsensible results. Take for example the following which is the implementation of our quarantine model: system_eq &lt;- c(dSdt ~ -k*S*I, dIdt ~ k*S*I-beta*I) deltaT &lt;- .1 # timestep length timeSteps &lt;- 15 # must be a number greater than 1 quarantine_parameters &lt;- c(k=.05, beta=.2) # parameters: a named vector init_cond &lt;- c(S=300, I=1) # Be sure you have enough conditions as you do variables. # Compute the solution via Euler&#39;s method: out_solution &lt;- euler(system_eq, parameters = quarantine_parameters, initial_condition=init_cond, deltaT=deltaT, n_steps = n_steps) ggplot(data = out_solution) + geom_line(aes(x=t,y=S),color=&#39;red&#39;) + geom_line(aes(x=t,y=I),color=&#39;blue&#39;) + labs(x=&#39;Time&#39;, y=&#39;Susceptible (red) or Infected (blue)&#39;) Figure 4.5: Surprising results with Euler’s method. You may notice the solution for \\(S\\) wiggles around \\(t=0.75\\) and is negative. This is concerning because we know there can’t be negative people! This requires a little more investigation. If we take a look at \\(t=0.75\\) the value for \\(S \\approx 1\\) and the value for \\(I \\approx 280\\). If we let \\(k=0.05\\) and \\(\\beta=0.2\\), this means that \\(\\displaystyle \\frac{dS}{dt}=-14\\) and \\(\\displaystyle \\frac{dI}{dt}=-42\\). The values of \\(S\\) and \\(I\\) are both decreasing! We know that our Euler’s method update is one where the new value is the old value plus any change. So the new value for \\(S = 1 -14\\cdot 0.1 = -0.4\\). Mathematically Euler’s method is doing nothing incorrect, but we know realistically this cannot happen. In turns out that this can easily be overcome. While Euler’s method is useful, it does quite poorly in cases where the solution is changing rapidly - or we might need to make some smaller step sizes. How we remedy this is to use a higher order solver, and one such method is called the Runge-Kutta Method. If you take a course in numerical analysis you might study these, but for the moment you see the difference between the twoa Runge-Kutta solver implemented in the demodelr package, which by all intents and purposes is replaces the command euler with rk4: system_eq &lt;- c(dSdt ~ -k*S*I, dIdt ~ k*S*I-beta*I) deltaT &lt;- .1 # timestep length timeSteps &lt;- 15 # must be a number greater than 1 quarantine_parameters &lt;- c(k=.05, beta=.2) # parameters: a named vector init_cond &lt;- c(S=300, I=1) # Be sure you have enough conditions as you do variables. # Compute the solution via a Runge-Kutta method: out_solution &lt;- rk4(system_eq, parameters = quarantine_parameters, initial_condition=init_cond, deltaT=deltaT, n_steps = n_steps) ggplot(data = out_solution) + geom_line(aes(x=t,y=S),color=&#39;red&#39;) + geom_line(aes(x=t,y=I),color=&#39;blue&#39;) + labs(x=&#39;Time&#39;, y=&#39;Susceptible (red) or Infected (blue)&#39;) Figure 4.6: Better results with the Runge-Kutta method. So what is going on here? Briefly, the differences between the two methods have to do with the error in the numerical method. The error is quantified as the difference between the actual solution and the numerical solution. Euler’s method has an error on the order of the stepsize \\(\\Delta t\\), whereas the Runge-Kutta method has an error of \\((\\Delta t)^4\\). For this example \\(\\Delta t = .1\\), so \\((\\Delta t)^{4} =.0001\\) - that is a noticeable difference! We can improve Euler’s method by taking a smaller timestep - BUT that means we need a larger number of steps \\(N\\) - which may take more computational time. Does this discussion sound familiar? Perhaps you examined similar when you took calculus and studied Riemann sums to approximate the area underneath a curve (left sum, right sum, trapezoid, midpoint). It turns out that these two problems are closely related. Numerical analysis is a great field of study to examine these topics and others! Moving ahead, I might switch between Euler’s method or the Runge-Kutta method when solving a differential equation. Thankfully the bulk of the work is “under the hood” - the setup will be the same for both! "],["exercises-3.html", "4.5 Exercises", " 4.5 Exercises Exercise 4.1 Verify that \\(I(t) = 130-120e^{-0.25t}\\) is a solution to the differential equation \\[\\displaystyle \\frac{dI}{dt} = 130-0.025I \\] with \\(I(0)=10\\). Exercise 4.2 Apply the rk4 solver with \\(\\Delta t = 0.1\\) with \\(N=10\\) to the initial value problem \\(\\displaystyle \\frac{dI}{dt} = 0.003 I \\cdot (4000-I) \\; I(0)=10\\). Compare your graph to Figure 4.2. What differences do you observe? Which solution method (euler or rk4) is better (and why)? Exercise 4.3 The following exercise will help you explore the relationships between stepsize, ending points, and number of steps needed. You may assume that we will start at \\(t=0\\) in all parts. Exercise 4.4 To get a rough approximation between error and step size, let’s say for a particular differential equation that we are starting at \\(t=0\\) and going to \\(t=2\\), with \\(\\Delta t = 0.2\\). We know that the Runge-Kutta error will be on the order of \\((\\Delta t)^{4} =0.0016\\). If we want to use Euler’s method with the same order of error, we could say \\(\\Delta t = .0001\\). For that case, how many steps will we need to take? Exercise 4.5 For each of the following differential equations, apply Euler’s method to generate a numerical solution to the differential equation and plot your solution. The stepsize (\\(\\Delta t\\)) and number of iterations (\\(N\\)) are listed. Exercise 4.6 For each of the following differential equations, apply the Runge-Kutta method method to generate a numerical solution to the differential equation and plot your solution. The stepsize (\\(\\Delta t\\)) and number of iterations (\\(N\\)) are listed. Contrast your answers with Exercise 4.5. Exercise 4.7 Let’s do some more work with Euler’s method for \\(\\displaystyle \\frac{dS}{dt} = 0.8 \\cdot S \\cdot (10-S)\\). This time set \\(S(0)=15\\), \\(\\Delta t = 0.1\\), \\(N = 10\\). When you examine your solution, what is incorrect about the Euler’s method solution based on your qualitative knowledge of the underlying dynamics? Now calculate Euler’s method for the same differential equation for the following conditions: \\(S(0)=15\\), \\(\\Delta t = 0.01\\), \\(N = 100\\). What has changed in your solution? Exercise 4.8 Let’s do some more work with Euler’s method for \\(\\displaystyle \\frac{dS}{dt} =\\frac{1}{1-S}\\). This time set \\(S(0)=1.5\\), \\(\\Delta t = 0.1\\), \\(N = 10\\) and also \\(S(0)=1.5\\), \\(\\Delta t = 0.01\\), \\(N = 100\\). Between these two solutions, what has changed? Do you think it is numerically possible to calculate a reasonable solution for Euler’s method near \\(S=1\\)? (note: this differential equation is an example of finite time blow up) Exercise 4.9 Similar to Exercise 4.8, let’s apply the rk4 method for \\(\\displaystyle \\frac{dS}{dt} =\\frac{1}{1-S}\\). This time set \\(S(0)=1.5\\), \\(\\Delta t = 0.1\\), \\(N = 10\\) and also \\(S(0)=1.5\\), \\(\\Delta t = 0.01\\), \\(N = 100\\). Between these two solutions, what has changed? Does this numerical solver do a better job in computing solutions compared to the Euler method? (note: this differential equation is an example of finite time blow up) Exercise 4.10 One way to model the growth rate hares is with \\(\\displaystyle f(H) = \\frac{r H}{1+kH}\\), where \\(r\\) and \\(k\\) are parameters. This is in constrast to exponential growth, which assumes \\(f(H) = rH\\). Exercise 4.11 In the lynx hare example we can also consider an alternative system where the growth of the hare is not exponential: \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= \\frac{2 H}{1+kH} - 0.5HL \\\\ \\frac{dL}{dt} &amp;= 0.05 H L - dL \\end{split} \\end{equation}\\] Set the number of timesteps to be 2000. Apply Euler’s method to numerically solve this system of equations when \\(k=0.1\\) and \\(k=1\\). Plot your simulation results. For the following differential equations use the code euler to generate a numerical solution to the differential equation and plot your solution. The stepsize (\\(\\Delta t\\)) and number of iterations (\\(N\\)) are listed. After your Euler’s method solution is determined, explain why you would expect the behavior in the solution that you see. Exercise 4.12 Consider the following: Exercise 4.13 Consider the following: Exercise 4.14 Consider the differential equation \\(\\displaystyle \\frac{dS}{dt} = \\frac{1}{1-S}\\). Notice that at \\(S=1\\) the rate \\(\\displaystyle \\frac{dS}{dt}\\) is not defined. "],["phase-05.html", "Chapter 5 Phase lines and equilibrium solutions", " Chapter 5 Phase lines and equilibrium solutions In modeling with differential equations, we want to understand how a system develops both qualitatively and quantitatively. Euler’s method (and other associated numerical methods for solving differential equations) illustrate solution behavior numerically. One key thing about the qualitative analysis is we are interested in the motion or the “flow” of the solution at a given point. Is the solution increasing, decreasing, or staying the same? For this section we will discuss qualitative aspects of a differential equation. We are going to focus on differential equations in one variable. Section 6 will build your understanding of the same idea with coupled systems of equations. "],["equilibrium-solutions.html", "5.1 Equilibrium solutions", " 5.1 Equilibrium solutions A great place to start is where the rate of change for a differential equation is zero, or in other words there is no flow. Borrowing ideas from calculus, this occurs when the rate of change is zero. We solve this by setting the left hand side of \\(\\displaystyle \\frac{dy}{dt}=f(y)\\) equal to zero and solving for \\(y\\) (or whatever dependent variable describes the problem). Example 5.1 What are the equilibrium solutions to \\(\\displaystyle \\frac{dy}{dt}=- y\\)? Solution. For this example we know that when the rate of change is zero, this means that \\(\\displaystyle \\frac{dy}{dt} = 0\\), or when \\(0 = -y\\). So \\(y=0\\) is the equilibrium solution. The general solution to the differential equation \\(\\displaystyle \\frac{dy}{dt}=- y\\) is when \\(y(t)=Ce^{-t}\\), where \\(C\\) is an arbitrary constant. Figure 5.1 plots different solution curves, with the equilibrium solution shown as a horizontal line: Figure 5.1: Exponential solution curves Notice that as \\(t\\) increases, all solutions approach the equilbrium solution \\(y=0\\) as \\(t\\) no matter if the initial condition is positive or negative. You can also evaluate the following limit for the solution: \\(\\displaystyle \\lim_{t\\rightarrow \\infty} Ce^{-t} = 0\\) to verify this is the case too. Example 5.2 What are the equilibrium solutions to \\(\\displaystyle \\frac{dN}{dt} = N \\cdot(1-N)\\)? Solution. In this case the equilibrium solutions occur when \\(N \\cdot(1-N) = 0\\), or when \\(N=0\\) or \\(N=1\\). The generic solution to this differential equation is \\[ \\displaystyle N(t)= \\frac{N_0}{N_0 +(1-N_0) e^{-t}}.\\] Figure 5.2 displays several different solution curves. Figure 5.2: Solution curves for \\(N&#39;=N(1-N)\\) In Figure 5.2 notice how all the solutions tend towards \\(N=1\\), but even solutions that start close to \\(N=0\\) seem to move away from this equilibrium solution. This example brings us to understanding classifying the stability of the equilibrium solutions. "],["phase-lines-for-differentail-equations.html", "5.2 Phase lines for differentail equations", " 5.2 Phase lines for differentail equations While it is one thing to determine where the equilibrium solutions are, we are also interested in classifying the stability of the equilibrium solutions. To do this investigate the behavior of the differential around the equilibrium solutions, using facts from calculus: If \\(\\displaystyle \\frac{dy}{dt}&lt;0\\), the function is decreasing. If \\(\\displaystyle \\frac{dy}{dt}&gt;0\\), the function is increasing. We say that the solution \\(y=0\\) is a stable equilibrium solution in this case. Let’s apply this logic to our differential equation \\(\\displaystyle \\frac{dy}{dt}=- y\\). We know that if \\(y=3\\), \\(\\displaystyle \\frac{dy}{dt}=- 3 &lt;0\\), so we say the function is decreasing to \\(y=0\\). If \\(y=-2\\), \\(\\displaystyle \\frac{dy}{dt}=- (-2) = 2 &gt;0\\), so we say the function is increasing to \\(y=0\\). This can be represented neatly in the phase line diagram for Figure 5.3: Figure 5.3: Phase line to \\(y&#39;=-y\\). Because the solution is increasing to \\(y=0\\) when \\(y &lt;0\\), and decreasing to \\(y=0\\) when \\(y &gt;0\\), we say that the equilibrium solution is stable, which is also confirmed by the solutions we plotted above. Example 5.3 Classify the stability of the equilibrium solutions to \\(\\displaystyle \\frac{dy}{dt} = k \\cdot y\\), where \\(k\\) is a parameter. Solution. In this case the equilibrium solution is still \\(y=0\\). We will need to consider two different cases for the stability depending on the value of \\(k\\) (\\(k&gt;0\\), \\(k&lt;0\\), and \\(k=0\\)): When \\(k&lt;0\\), the phase line will be similar Figure 5.3. When \\(k&gt;0\\) the phase line will be is shown in the Figure 5.4. We say in this case that the equilibrium solution is unstable, as all solutions flow away from the equilibrium. Several different solutions are shown in Figure 5.5 When \\(k=0\\) we have the differential equation \\(\\displaystyle \\frac{dy}{dt}=0\\), which has \\(y=C\\) as a general solution. For this special case the equilibrium solution is neither stable or unstable3. Figure 5.4: Phaseline for \\(y&#39;=ky\\), with \\(k&gt;0\\). Figure 5.5: Solution curves for \\(y&#39;=ky\\), with \\(k&gt;0\\). Example 5.4 Let’s investigate the phase line for the differential equation \\(\\displaystyle \\frac{dN}{dt} = N \\cdot(1-N)\\) and classify stability of the equilibrium solutions. Solution. This differential equation has equilbrium solutions when \\(N(1-N)=0\\), or \\(N=0\\) or \\(N=1\\). We evaluate the stability of the solutions in the following table: Test point Sign of \\(N&#39;\\) Tendency of solution \\(N=-1\\) Negative Decreasing \\(N=0\\) Zero Equilibrium solution \\(N=0.5\\) Positive Increasing \\(N=1\\) Zero Equilibrium solution \\(N=2\\) Negative Decreasing Notice how the selected test points in the first column are either the the left or the right of the equilbrium solution. We can also represent the information in the table using a phase line diagram (Figure 5.6), but in this case we need to include two equilibrium solutions. The table and Figure 5.6 confirms that \\(N\\) is moving away from \\(N=0\\) (either decreasing when \\(N\\) is less than 0 and increasing when \\(N\\) is greater than 0) and moving towards \\(N=1\\) (either increasing when \\(N\\) is between 0 and 1 and decreasing when \\(N\\) is greater than one. These results suggest that equilibrium solution at \\(N=0\\) to be unstable and at \\(N=1\\) to be stable. Figure 5.6: Phase line diagram for the differential equation \\(N&#39;=N(1-N)\\). Other than writing the words in the phase line diagram, we also use arrows to signify increasing or decreasing in the solutions. By all intents and purposes this is a different differential equation than \\(\\displaystyle \\frac{dy}{dt}=k\\cdot y\\); something peculiar is going on here - which we come back to when discuss bifurcations in Section ??.↩︎ "],["a-stability-test-for-equilibrium-solutions.html", "5.3 A stability test for equilibrium solutions", " 5.3 A stability test for equilibrium solutions Notice how when constructing the phase line diagram we relied on the behavior of solutions around the equilibrium solution to classify the stability. As an alternative we can also use the point at the equilibrium solution itself. To do this we are going to consider the general differential equation \\(\\displaystyle \\frac{dy}{dt}=f(y)\\). We are going to assume that we have an equilibrium solution at \\(y=y_{*}\\). We use local linearization to construct a locally linear approximation to \\(L(y)\\) to \\(f(y)\\) at \\(y=y_{*}\\): \\[L(y) = f(y_{*}) + f&#39;(y_{*}) \\cdot (y-y_{*}) \\] We will use \\(L(y)\\) as an approximation to \\(f(y)\\). There are two key things here. First, because we have an equilibrium solution, \\(f(y_{*}) =0\\). The other key thing is that if we define the variable \\(P = y-y_{*}\\), then the differential equation translates to \\[\\begin{equation} \\frac{dP}{dt} = f&#39;(y_{*}) \\cdot P \\tag{5.1} \\end{equation}\\] Does Equation (5.1) look familiar? I - it should! This is similar to the example where we classified the stability of \\(\\displaystyle \\frac{dy}{dt} = k \\cdot y\\) – cool! So let’s use what we learned in Example 5.3 above to classify the stability: Local linearization stability test for equilibrium solutions: For a differential equation \\(\\displaystyle \\frac{dy}{dt} = f(y)\\) with equilibrium solution \\(y_{*}\\), we can classify the stability of the equilibrium solution through the following: If \\(f&#39;(y_{*})’&gt;0\\) at an equilibrium solution, the equilibrium solution \\(y=y_{*}\\) will be unstable. If \\(f&#39;(y_{*}) &lt;0\\) at an equilibrium solution, the equilibrium solution \\(y=y_{*}\\) will be stable. If \\(f&#39;(y_{*}) = 0\\), we cannot conclude anything about the stability of \\(y=y_{*}\\). Example 5.5 Apply local linearization to classify the stability of the equilibrium solutions of \\(\\displaystyle \\frac{dN}{dt} = N \\cdot(1-N)\\) Solution. The locally linear approximation is \\(L(N) = 1-2N\\). We have \\(L(0)=1&gt;0\\), so \\(N=0\\) is unstable. Similarly \\(L(1)=-1\\), so \\(N=1\\) is stable. "],["exercises-4.html", "5.4 Exercises", " 5.4 Exercises Exercise 5.1 What are the equilibrium solutions to the following differential equations? Then classify the stability of the equilbrium solutions using the local linearization stability test. Exercise 5.2 Using your results from Exercise 5.1, construct a phase line for each of the differential equations and classify the stability of the equilibrium solutions. Exercise 5.3 A population grows according to the equation \\(\\displaystyle \\frac{dP}{dt} = \\frac{P}{1+2P} - 0.2P\\). Exercise 5.4 A cell with radius \\(r\\) assimilates nutrients at a rate proportional to its surface area, but uses nutrients proportional to its volume, according to the following differential equation: \\[ \\frac{dr}{dt} = 4 \\pi r^{2} - \\frac{4}{3} \\pi r^{3}. \\] Exercise 5.5 The Chanter equation of growth is the following, where \\(W\\) is the weight of an object: \\[\\begin{equation} \\frac{dW}{dt} = W(3-W)e^{-Dt}, \\end{equation}\\] Use this differential equation to answer the following questions. Exercise 5.6 Red blood cells are formed from stem cells in the bone marrow. The red blood cell density \\(r\\) satisfies an equation of the form \\[\\begin{equation} \\frac{dr}{dt} = \\frac{br}{1+r^{n}} - c r, \\end{equation}\\] where \\(n&gt;1\\) and \\(b&gt;1\\) and \\(c&gt;0\\). Find all the equilibrium solutions \\(r_{*}\\) to this differential equation. can you factor an \\(r\\) from your equation first? Exercise 5.7 Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of \\(S\\) in the blood is the following: \\[ \\frac{dS}{dt} = I + p \\cdot (W - S) \\] Where the parameter \\(I\\) represents the active uptake of salt, \\(p\\) is the permeability of the skin, and \\(W\\) is the salinity in the water. Exercise 5.8 The immigration rate of bird species (species per time) from a mainland to an offshore island is \\(I_{m} \\cdot (1-S/P)\\), where \\(I_{m}\\) is the maximum immigration rate, \\(P\\) is the size of the source pool of species on the mainland, and \\(S\\) is the number of species already occupying the island. Further, the extinction rate is \\(E \\cdot S / P\\), where \\(E\\) is the maximum extinction rate. The growth rate of the number of species on the island is the immigration rate minus the extinction rate, given by the following differential equation: \\[\\begin{equation} \\frac{dS}{dt} = I_{m} \\left(1-\\frac{S}{P} \\right) - \\frac{ES}{P} \\end{equation}\\] Exercise 5.9 A colony of bacteria growing in a nutrient-rich medium deplete the nutrient as they grow. As a result, the nutrient concentration \\(x(t)\\) is steadily decreasing. The equation describing this decrease is the following: \\[ \\displaystyle \\frac{dx}{dt} = - \\mu \\frac{x \\cdot (\\xi- x)}{\\kappa + x}, \\] where \\(\\mu\\), \\(\\kappa\\), and \\(\\xi\\) are all parameters greater than zero. Exercise 5.10 Can a solution curve cross an equilibrium solution of a differential equation? "],["coupled-06.html", "Chapter 6 Coupled systems of equations", " Chapter 6 Coupled systems of equations In this section we will learn how to qualitatively understand systems of differential equations. When analyzing a single differential equation we used the idea of a phase line to understand if a solution was stable or unstable. Here we extend that to equations of more than one variable and investigate what we will call the phase plane. "],["model-redux-flu-with-quarantine.html", "6.1 Model redux: flu with quarantine", " 6.1 Model redux: flu with quarantine In Section 1 we studied the following model for the flu as a coupled system of equations: \\[\\begin{align*} \\frac{dS}{dt} &amp;= -kSI \\\\ \\frac{dI}{dt} &amp;= kSI \\end{align*}\\] In this scenario we are also going to consider that those who are infected are quarantined, proportional to the number infected, according to the following schematic: Which gives us the following system of equations: \\[\\begin{equation} \\begin{split}\\tag{6.1} \\frac{dS}{dt} &amp;= -kSI \\\\ \\frac{dI}{dt} &amp;= kSI - \\beta I \\end{split} \\end{equation}\\] To find the equilibrium solutions we want to find values of \\(S\\) and \\(I\\) where the rates \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) are both zero. This can be done by algebraically solving the system of equations: \\[\\begin{align*} 0 &amp;= -kSI \\\\ 0 &amp;= kSI - \\beta I \\end{align*}\\] Let’s examine the first equation (\\(0 = -kSI\\)), which we can see is consistent when either \\(S=0\\) and \\(I=0\\). These give us two options, which we then use in the second equation (\\(0 = kSI - \\beta I\\)). When \\(S=0\\), then \\(0 = k\\cdot 0 \\cdot I - \\beta I \\rightarrow 0 = -\\beta I\\), which is consistent when \\(I=0\\). So \\((S_{*},I_{*}) = (0,0)\\) is one equilibrium solution. (In fact, if \\(I=0\\), then any value of \\(S\\) would be an equilibrium solution. Can you explain why?) This is an interesting example. We call the equations \\(S=0\\) and \\(I=0\\) when \\(\\displaystyle \\frac{dS}{dt}=0\\) as nullclines for \\(S\\). In a similar manner, the equations in \\(S\\) and \\(I\\) when \\(\\displaystyle \\frac{dI}{dt}=0\\) are called nullclines for \\(I\\). Let’s try to determine formulas for these equations: \\[\\begin{align*} 0 &amp;= kSI-\\beta I \\\\ 0 &amp;= I\\cdot (kS - \\beta ) \\end{align*}\\] Because the last equation is factored as a product, nullclines for \\(I\\) are either \\(I=0\\) or \\(\\displaystyle S = \\frac{\\beta}{k}\\). Nullclines are not equilibrium solutions by themselves - it is the intersection of two different nullclines that determine equilibrium solutions. Figure 6.1 shows the nullclines in the \\(S-I\\) plane (since we have two equations), with \\(S\\) on the horizontal axis and \\(I\\) on the vertical axis. Figure 6.1: Nullclines for Equation (6.1). To generate the plot we assumed \\(\\beta=1\\) and \\(k=1\\) ## &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; ## aesthetics: colour ## axis_order: function ## break_info: function ## break_positions: function ## breaks: S&#39; = 0 I&#39; = 0 ## call: call ## clone: function ## dimension: function ## drop: TRUE ## expand: waiver ## get_breaks: function ## get_breaks_minor: function ## get_labels: function ## get_limits: function ## guide: legend ## is_discrete: function ## is_empty: function ## labels: waiver ## limits: NULL ## make_sec_title: function ## make_title: function ## map: function ## map_df: function ## n.breaks.cache: NULL ## na.translate: TRUE ## na.value: grey50 ## name: Nullclines ## palette: function ## palette.cache: NULL ## position: left ## range: &lt;ggproto object: Class RangeDiscrete, Range, gg&gt; ## range: NULL ## reset: function ## train: function ## super: &lt;ggproto object: Class RangeDiscrete, Range, gg&gt; ## rescale: function ## reset: function ## scale_name: hue ## train: function ## train_df: function ## transform: function ## transform_df: function ## super: &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; A key thing to note is that where two different nullclines cross is an equilibrium solution to the system of equations. This means that both \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) are zero at this point. Examining Figure 6.1, there are three possibilities: At \\(S=0\\) and \\(I=0\\) (otherwise known as the origin). This equilibrium solution makes biological sense: if there is nobody susceptible or infected (everyone is perfectly healthy - yay!) there are no flu cases. Building on the last equilibrium solution, the entire horizontal axis is an equilibrium solution because the nullclines for \\(S\\) and \\(I\\) are the (\\(I=0\\)). There is a practical interpretation of this nullcline - whenever \\(I=0\\), meaning there are no infected people around, the solution is at an equilibrium. There is also a third possibility where the vertical line at \\(S=1\\) crosses the horizontal axis (\\(S=1\\), \\(I=0\\)), but that also falls under the second equilibrium solution. Now that we have identified our nullclines and equilibrium solutions, we will add additional context with the flow of the solution. 6.1.1 Adding context to our phase plane: slope fields Let’s go back to the idea of a phase plane, but this time we are going to add more context to our nullcline graph by evaluating different values of \\(S\\) and \\(I\\) into our system of equations and plot the slope field. First let’s evaluate the derivatives \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) in Equation (6.1) for different values of \\(S\\) and \\(I\\): Table 6.1: Values of \\(\\frac{dS}{dt}\\) and \\(\\frac{dI}{dt}\\) for the flu with quarantine model. S I dS_dt dI_dt 0.0000000 0.0000000 0.0000000 0.0000000 0.4444444 0.4444444 -0.1975309 -0.2469136 0.8888889 0.8888889 -0.7901235 -0.0987654 1.3333333 1.3333333 -1.7777778 0.4444444 1.7777778 1.7777778 -3.1604938 1.3827160 2.2222222 2.2222222 -4.9382716 2.7160494 2.6666667 2.6666667 -7.1111111 4.4444444 3.1111111 3.1111111 -9.6790123 6.5679012 3.5555556 3.5555556 -12.6419753 9.0864198 4.0000000 4.0000000 -16.0000000 12.0000000 Notice how the different values of \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) at each of the \\(S\\) and \\(I\\) values. We can plot each of the coordinate pairs of \\(\\displaystyle \\left( \\frac{dS}{dt}, \\frac{dI}{dt} \\right)\\) with a vector in the \\((S,I)\\) plane. We associate \\(\\displaystyle \\frac{dS}{dt}\\) with left-right motion, so positive \\(\\displaystyle \\frac{dS}{dt}\\) means pointing to the right. Likewise, we associate \\(\\displaystyle \\frac{dI}{dt}\\) with up-down motion, so positive \\(\\displaystyle \\frac{dI}{dt}\\) means the vector points up. At the point \\((S,I)=(1,1)\\), we have an arrow that points directly to the west because and \\(\\displaystyle \\frac{dI}{dt} &lt; 0\\) and \\(\\displaystyle \\frac{dI}{dt} =0\\). If we sequentially sample points in the \\((S,I)\\) plane we get a vector field plot, superimposed with the nullclines: Figure 6.2: Phaseplane for Equation (6.1). To generate the plot we assumed \\(\\beta=1\\) and \\(k=1\\) 6.1.2 Motion around the nullclines We can also extend the motion around the nullclines to investigate the stability. With a one dimensional differential equation we used a number line to quantify values where the solution is increasing / decreasing. The problem with several differential equations is that the notion of “increasing” or “decreasing”\" becomes difficult to understand - as there is an additional degree of freedom! Simply put, in a plane you can move left/right or up/down. The benefit for having nullclines is that they isolate the motion in one direction. In general for a two dimensional system: - When a horizontal axis variable has a nullcline, the only allowed motion is up/down. - When a vertical axis variable has a nullcline, the only motion is up/down. Applying this knowledge to Equation (6.1), if we choose points where \\(I&#39;=0\\) then we know that the only motion is to the left and the right because \\(S\\) can still change along that curve. If we choose points where \\(S&#39;=0\\) then we know that the only motion is to the up/down because \\(I\\) can still change along that curve (Figure 6.3). Figure 6.3: Nullclines for Equation (6.1) with context on the direction of the motion. "],["determining-stability.html", "6.2 Determining stability", " 6.2 Determining stability The picture of the phase plane with the nullcline qualitatively tells us about the stability of an equilibrium point. Once of the equilibrium solutions is at the origin \\((S,I)=(0,0)\\). As before we want to investigate if the equilibrium solution is stable or unstable. Figure ?? zooms in the phaseplane at the equilibrium solution at \\(S=0\\), \\(I=0\\): As you can see the arrows appear to be pointing into and towards the equilibrium solution. So we would classify this equilbrium solution as stable. "],["generating-a-phase-plane-in-r.html", "6.3 Generating a phase plane in R", " 6.3 Generating a phase plane in R Let’s take what we learned from the case study of the flu model with quarantine to qualitatively analyze a system of differential equations: We determine nullclines by setting the derivatives equal to zero. Equilibrium solutions occur where nullclines for the two different equations intersect. The arrows in the phase plane help us characterize the stability of the equilibrium solution. To determine the phaseplane diagram demodelr package has some basic functionality to generate a phase plane. Consider the following system of differential equations (Equation (6.2)): \\[\\begin{equation} \\begin{split}\\tag{6.2} \\frac{dx}{dt} &amp;= x-y \\\\ \\frac{dy}{dt} &amp;= -x+y \\end{split} \\end{equation}\\] In order to generate a phaseplane diagram for Equation (6.2) we need to define functions for \\(x&#39;\\) and \\(y&#39;\\), which I will annotate as \\(dx\\) and \\(dy\\) respectively. We are going to collect these equations in one vector called system_eq, using the tilde (~) as a replacement for the equals sign: system_eq &lt;- c(dx ~ x-y, dy ~ x+y) Then what we do is apply the command phaseplane, which will generate a vector field over a domain: phaseplane(system_eq,&#39;x&#39;,&#39;y&#39;) # The values in quotes are the labels for the axes and to identify the variables - they are needed! Figure 6.4: Phaseplane diagram for Equation (6.2) The command phaseplane has an option called eq_soln that will if there are any equilibrium solutions to be found and report them to the console. For example try running phaseplane(system_eq,'x','y',eq_soln=TRUE) and see what gets output to console. While this option lists equilibrium solutions, you should confirm them with the differential equation through direct solving. 6.3.1 Generating a phase line in R: From Section 5 we discussed how to construct phaselines by hand. It turns out that the command phaseplane can also plot phase lines. Let’s take a look at an example first and then discuss how that it works. Example 6.1 A colony of bacteria growing in a nutrient-rich medium deplete the nutrient as they grow. As a result, the nutrient concentration \\(x(t)\\) is steadily decreasing. Determine the phaseline for the following differential equation: The R code to generate this phaseline is the following: # Define the windows where we make the plots t_window &lt;- c(0,3) x_window &lt;- c(0,5) # Define the differential equation system_eq &lt;- c(dt ~ 1, dx ~ -0.7 * x*(3-x)/(1+x)) phaseplane(system_eq,&quot;t&quot;,&quot;x&quot;,t_window,x_window) Notice how we have the equation \\(dt = 1\\). What we are doing is re-writing the differential equation with a new variable \\(s\\) (Equation (6.4)): \\[\\begin{equation} \\begin{split}\\tag{6.4} \\frac{dt}{ds} &amp;= 1 \\\\ \\frac{dx}{ds} &amp;= - 0.7 \\cdot \\frac{x \\cdot (3- x)}{1 + x} \\end{split} \\end{equation}\\] The differential equation \\(\\displaystyle \\frac{dt}{ds} = 1\\) has solution \\(s=t\\), so in essence is the same as Equation (6.3) (perhaps a little more complicated). However re-writing this system was a quick and handy workaround to re-use code. "],["exercises-5.html", "6.4 Exercises", " 6.4 Exercises Exercise 6.1 Determine equilibrium solutions for Equation (6.2). Exercise 6.2 This problem considers the following system of differential equations: \\[\\begin{align*} \\frac{dx}{dt} &amp;= y \\\\ \\frac{dy}{dt} &amp;= -x \\end{align*}\\] Exercise 6.3 Considers the following system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y \\\\ \\frac{dy}{dt} &amp;= 3x^{2}-1 \\end{split} \\end{equation}\\] Exercise 6.4 A plant grows propritional to its current length \\(L\\). Assume this proportionality constant is \\(\\mu\\), whose rate also decreases proportional to its current value. The system of equations that models this plant growth is the following: \\[\\begin{equation} \\begin{split} \\frac{dL}{dt} &amp;= \\mu L \\\\ \\frac{d\\mu}{dt} &amp;= -0.1 \\mu \\\\ \\end{split} \\end{equation}\\] Exercise 6.5 Red blood cells are formed from stem cells in the bone marrow. The red blood cell density \\(r\\) satisfies an equation of the form \\[\\begin{equation} \\frac{dr}{dt} = \\frac{0.2r}{1+r^{2}} - 0.1 r, \\end{equation}\\] Exercise 6.6 Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of \\(S\\) in the blood is the following: \\[ \\frac{dS}{dt} = 1 + 0.3 \\cdot (3 - S) \\] Exercise 6.7 The core body temperature (\\(T\\)) of a mammal is coupled to the heat production (scaled by heat capacity \\(Q\\)) with the following system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dT}{dt} &amp;= Q + 0.5 \\cdot (20-T) \\\\ \\frac{dQ}{dt} &amp;= 0.1 \\cdot (38-T), \\end{split} \\end{equation}\\] Exercise 6.8 Consider the following system of differential equations for the lynx-hare model: \\[\\begin{align} \\frac{dH}{dt} &amp;= r H - b HL \\\\ \\frac{dL}{dt} &amp;=ebHL -dL \\end{align}\\] Figure 6.5: An example of a chemostat. Exercise 6.9 A chemostat is a tank used to study microbes and ecology, where microbes grow under controlled conditions. Think of this like a large tank with nutrient-rich water being continuously cycled through, as shown in the following Figure (Source: Wikipedia). Equations that describe the microbial biomass \\(W\\) and the nutrient concentration \\(C\\) (in the culture) are the following: \\[\\begin{align} \\frac{dW}{dt} &amp;= \\mu W - F \\frac{W}{V} \\\\ \\frac{dC}{dt} &amp;= D \\cdot (C_{R}-C) - S \\mu \\frac{W}{V}, \\end{align}\\] where we have the following parameters: \\(\\mu\\) is the per capita reproduction rate, \\(F\\) is the flow rate, \\(V\\) is the volume of the culture solution, \\(D\\) is the dilution rate, \\(C_{R}\\) is the concentration of nutrients entering the culture, and \\(S\\) is a stoichiometric conversion of nutrients to biomass. Figure 6.6: Population results from two yeast species growing in competition. From Gause (1932) Exercise 6.10 A classical paper by Gause (1932) examined two different species of yeast growing in competition with each other. The differential equations given for two species in competition are: \\[\\begin{align*} \\frac{dy_{1}}{dt} &amp;= -b_{1} y_{1} \\frac{(K_{1}-(y_{1}+\\alpha y_{2}) )}{K_{1}} \\\\ \\frac{dy_{2}}{dt} &amp;= -b_{2} y_{2} \\frac{(K_{2}-(y_{2}+\\beta y_{1}) )}{K_{2}}, \\\\ \\end{align*}\\] where \\(y_{1}\\) and \\(y_{2}\\) are the two species of yeast with the parameters \\(b_{1}, \\; b_{2}, \\; K_{1}, \\; K_{2}, \\; \\alpha, \\; \\beta\\) describing the characteristics of the yeast species. "],["exact-solns-07.html", "Chapter 7 Exact Solutions to Differential Equations", " Chapter 7 Exact Solutions to Differential Equations We have already discussed some tools that can analyze differential equations numerically (Section 4) and qualitatively (Sections 5 and 6). The phase plane allowed us to evaluate equilibrium solutions and their stability. Beyond this graphical approach, it is also helpful to know the exact solution to an equation. In this section we will study a few techniques to find exact solutions to differential equation. We will apply some of the tools you may have learned from calculus. "],["separable-differential-equations.html", "7.1 Separable Differential Equations", " 7.1 Separable Differential Equations One technique to solve differential equations is the method of separation of variables. Let’s look at an example: What is the general solution to \\(\\displaystyle \\frac{dy}{dx} = yx^{2}\\)? To solve this expression we collect the variables involving \\(x\\) and one side of the equation, and the variables involving \\(y\\) on the other: \\[\\begin{equation*} \\frac{1}{y} dy = x^{2} dx. \\end{equation*}\\] Now the next step is to determine the antiderivative of both sides of expression: \\[\\begin{equation*} \\begin{split} \\int \\frac{1}{y} dy = \\ln(y) + C. \\\\ \\int x^{2} dx = \\frac{1}{3} x^{3} + C. \\end{split} \\end{equation*}\\] Finally since both sides are equal we can solve for the dependent variable \\(y\\). One thing to note: usually for antiderivatives we always include a \\(+C\\). For solving differential equations it is okay just to keep only one \\(+C\\), which usually is best on the side of the independent variable: \\[\\begin{equation*} \\ln(y) =\\frac{1}{3} x^{3} + C \\rightarrow e^{\\ln(y)} = e^{\\frac{1}{3} x^{3} + C} \\rightarrow y = Ce^{\\frac{1}{3} x^{3}}. \\end{equation*}\\] We are in business! So here is a general technique approach to solving a differential equation via separation of variables: Separate the variables on one side of the equation. Integrate both sides individually. Solve for the dependent variable. If we solve this equation using separation of variables we havUsing your work above as a guide, solve this differential equation to determine a solution \\(y(x)\\). "],["integrating-factors.html", "7.2 Integrating factors", " 7.2 Integrating factors One model that we have looked at is the the \\(SI\\) model where the spread of the disease is proportional to the number infected: \\[\\begin{equation*} \\frac{dI}{dt} = .03(1000-I) = 30 - .03I \\end{equation*}\\] While this differential equation can be solved via separation of variables, let’s try a different approach as an illustration of another useful technique. First let’s write the terms involving \\(I\\) on one side of the equation: \\[\\begin{equation*} \\frac{dI}{dt} + .03I = 30. \\end{equation*}\\] What we are going to do is multiply both sides of this equation by \\(e^{.03t}\\) (I’ll explain more about that later): \\[\\begin{equation*} \\frac{dI}{dt} \\cdot e^{.03t} + .03I \\cdot e^{.03t} = 30 \\cdot e^{.03t} \\end{equation*}\\] Hmmm - this seems like we are making our equation harder to solve, doesn’t it? However the left hand side is actually the derivative of the expression \\(I \\cdot e^{kt}\\)! Let’s take a look: \\[\\begin{equation*} \\frac{d}{dt} \\left( I \\cdot e^{.03t} \\right) = \\frac{dI}{dt} \\cdot e^{.03t} + I \\cdot .03 e^{.03t} \\end{equation*}\\] This derivative is courtesy of the product rule from calculus. Ok, so what does this do to the differential equation? Well, by re-writing the differential equation as a derivative and integrating: \\[\\begin{equation} \\begin{split} \\frac{d}{dt} \\left( I \\cdot e^{.03t} \\right) &amp;= 30 \\cdot e^{.03t} \\rightarrow \\\\ \\int \\frac{d}{dt} \\left( I \\cdot e^{.03t} \\right) \\; dt &amp;= \\int 30 \\cdot e^{.03t} \\; dt \\rightarrow \\\\ I \\cdot e^{.03t} &amp;= 30 \\cdot e^{.03t} + C \\end{split} \\end{equation}\\] Notice how by writing the left hand side in terms of the product rule and integrating we could find the solution. We added the \\(+C\\) to the right hand side. All that is left to do is to solve in terms of \\(I(t)\\) by dividing by \\(e^{kt}\\). We will label this solution \\(I_{1}(t)\\): \\[\\begin{equation} I_{1}(t) = 1000 + Ce^{-.03t} \\tag{7.1} \\end{equation}\\] Cool! The function \\(e^{.03t}\\) is called an integrating factor. To see what is meant by that, let’s try one more example. In this case, we are going to assume that the rate of infection is time dependent, or \\(k(t) = .03t\\). How this would work in practice is that initially (at \\(t=0\\)) there is no infection, but the infection rate increases as time goes on. Our differential equation in this case is: \\[\\begin{equation*} \\frac{dI}{dt} + .03 t \\cdot I = 30 t. \\end{equation*}\\] So if we want to write the left hand side as a product, what we will do is multiply the entire differential equation by \\(\\displaystyle e^{\\int .03t \\; dt} = e^{ 0.015 t^{2}}\\) This term is called the integrating factor: \\[\\begin{equation*} \\frac{dI}{dt} \\cdot e^{ 0.015 t^{2}} + .03 t \\cdot I \\cdot e^{0.015 t^{2}} = 30 t \\cdot e^{0.015 t^{2}} \\end{equation*}\\] First we rewrite the left hand side using the product rule: \\[\\begin{equation*} \\frac{dI}{dt} \\cdot e^{ 0.015 t^{2}} + .03 t \\cdot I \\cdot e^{0.015 t^{2}} = \\frac{d}{dt} \\left( I \\cdot e^{0.015 t^{2}} \\right). \\end{equation*}\\] Now we can integrate this equation by: \\[\\begin{equation} \\begin{split} \\frac{d}{dt} \\left( I \\cdot e^{0.015 t^{2}} \\right) &amp;= 30 t \\cdot e^{0.015 t^{2}} \\rightarrow \\\\ \\int \\frac{d}{dt} \\left( I \\cdot e^{0.015 t^{2}} \\right) \\; dt &amp;= \\int 30t \\cdot e^{0.015 t^{2}} \\; dt \\rightarrow \\\\ I \\cdot e^{0.015 t^{2}} &amp;= N \\cdot e^{0.015 t^{2}} + C \\end{split} \\end{equation}\\] All right! So the last step is to write the equation in terms of \\(I(t)\\), which we will label \\(I_{2}(t)\\): \\[\\begin{equation} I_{2}(t) = 1000 + C e^{-0.015 t^{2}} \\tag{7.2} \\end{equation}\\] Figure 7.1 compares solutions when \\(I(0)=10\\). Figure 7.1: Comparison of two integrating factor solutions, Equation (7.1) in red and Equation (7.2) in blue. Ok, let’s summarize this integrating factor approach for differential equations that can be written in the form \\[\\frac{dy}{dt} + f(t) \\cdot y = g(t)\\] Calculate the integrating factor \\(\\displaystyle e^{\\int f(t) \\; dt}\\). Hopefully the integral \\(\\displaystyle \\int f(t) \\; dt\\) is easy to compute! Next multiply the integrating factor across your equation to rewrite the differential equation as \\(\\displaystyle \\frac{d}{dt} \\left( y \\cdot e^{\\int f(t) \\; dt} \\right) = g(t) \\cdot e^{\\int f(t) \\; dt}\\). Then compute the integral \\(\\displaystyle H(t) = \\int g(t) \\cdot e^{\\int f(t) \\; dt} \\; dt\\). This looks intimidating - but hopefully is manageable to compute! Don’t forget the \\(+C\\)! Then solve for \\(y(t)\\): \\(\\displaystyle y(t) = H(t) \\cdot e^{-\\int f(t) \\; dt} + C e^{-\\int f(t) \\; dt}\\). This technique is a handy way to work with equations that aren’t easily separable. "],["guess-and-check.html", "7.3 Guess and Check", " 7.3 Guess and Check A final approach is called the guess and check method. Say for example we have the following equation that describes the rate of change above: The first approach if a function is a solution to a differential equation is the guess and check method, or by direct substitution. \\[ \\frac{dS}{dt} = 0.7 S \\] We know that we can apply separation of variables, but instead let’s try to see if the function \\(\\tilde{S}(t) = 5 e^{0.7t}\\) is a solution in order to do that, we need to differentiate \\(\\tilde{S}(t)\\), which using our knowledge of calculus is \\(0.7 \\cdot 5 e^{-0.7t}\\). If we note that \\(\\displaystyle \\frac{d\\tilde{S}}{dt} = 0.7 \\tilde{S} = 0.7 e^{0.7t}\\) than the function \\(\\tilde{S}\\) does solve the differential equation. Super cool! Let’s try an example: Example 7.1 Verify the following functions are solutions to the differential equation \\(\\displaystyle \\frac{dS}{dt} = 0.7 S\\): \\(\\tilde{R}(t) = 10e^{0.7t}\\) \\(\\tilde{P}(t) = e^{0.7t}\\) \\(\\tilde{Q}(t) = 5e^{0.7t}\\) \\(\\tilde{F}(t)=3\\) \\(\\tilde{G}(t)=0\\) Remark. Let’s apply direct differentiation to each of these functions: \\(\\tilde{R}(t) = 10e^{0.7t} \\rightarrow \\tilde{R}&#39;(t) = 7e^{0.7t}\\) \\(\\tilde{P}(t) = e^{0.7t} \\rightarrow \\tilde{P}&#39;(t) = e^{0.7t}\\) \\(\\tilde{Q}(t) = 5e^{0.7t} \\rightarrow \\tilde{Q}&#39;(t) = 3.5e^{0.7t}\\) \\(\\tilde{F}(t)=3 \\rightarrow \\tilde{F}&#39;(t) = 0\\) \\(\\tilde{G}(t)=0 \\rightarrow \\tilde{G}&#39;(t) = 0\\) Now we will compare each of these solutions to the right hand side: \\(0.7\\tilde{R}(t) = 0.7 \\cdot 10e^{0.7t} \\rightarrow 7e^{0.7t}\\) \\(0.7\\tilde{P}(t) = 0.7 e^{0.7t}\\) \\(0.7\\tilde{Q}(t) = 0.7 \\cdot 5e^{0.7t} \\rightarrow = 3.5e^{0.7t}\\) \\(0.7 \\tilde{F}(t)=0.7 \\cdot 3 \\rightarrow 2.1\\) \\(0.7 \\tilde{G}(t)=0.7 \\cdot 0 \\rightarrow 0\\) Notice how the right hand sides of each equation equals the left hand sides. When that is the case, our candidate functions are indeed solutions to the differential equation! "],["superposition-of-solutions.html", "7.4 Superposition of solutions", " 7.4 Superposition of solutions Related to the Guess and Check method is this concept called superposition of solutions. Here how this works: if you have two known solutions to a differential equation, then the sum (or difference) is a solution as well. Let’s look at an example: Example 7.2 Show that \\(\\tilde{S}(t) = 5e^{0.7t} + e^{0.7t}\\) is a solution to the differential equation \\(\\displaystyle \\frac{dS}{dt} = 0.7 S\\) Remark. By direct differentiation, \\(\\tilde{S}&#39;(t) = 3.5e^{0.7t} + 0.7e^{0.7t}\\). Also we have that \\(0.7 \\cdot \\tilde{S}(t) = 0.7 \\cdot (5e^{0.7t} + e^{0.7t}) = 3.5 e^{0.7t} + 0.7 e^{0.7t}\\), which equals \\(\\tilde{S}\\). What this example illustrates is the principle that if you have two solutions to a differential equation, they can be added together and produce a new solution. This is an example of a linear combinations of solutions, and we can state this more formally: If \\(x(t)\\) and \\(y(t)\\) are solutions to the differential equation \\(z&#39; = f(t,z)\\), then \\(c(t) = a \\cdot x(t) + b \\cdot y(t)\\) are also solutions, where \\(a\\) and \\(b\\) are constants. Hopefully you were able to verify that \\(\\tilde{R}\\) and \\(\\tilde{Q}\\) and \\(\\tilde{G}\\) all were solutions to the differential equation, and that \\(\\tilde{R} + \\tilde{Q}\\) was a solution as well. The most general solution to this differential equation is \\(S(t)=Ce^{0.7t}\\), where the initial condition would determine the value of \\(C\\). "],["applying-guess-and-check-more-broadly.html", "7.5 Applying guess and check more broadly", " 7.5 Applying guess and check more broadly As noted earlier, the guess and check method may seem to be trivial - if you have a differential equation, and solution, why verify it? Well, this method helps to introduce a useful solution technique to a differential equation, and one that we can build up through directverification. We are going to revisit the lynx hare model, but simplified a little bit. Here we are going to assume that lynx and hares both decline at a rate proportional to the population size, but the lynx population increases according to the rate of hare decline: \\[\\begin{align} \\frac{dH}{dt} &amp;= -b H \\\\ \\frac{dL}{dt} &amp; = b H - d L \\end{align}\\] Based on these simplified assumptions a good approach is to assume a solution that is exponential for both \\(H\\) and \\(L\\): \\[\\begin{align} \\tilde{H}(t) &amp;= C_{1} e^{\\lambda t} \\\\ \\tilde{L}(t) &amp;= C_{2} e^{\\lambda t} \\end{align}\\] The form of this solution has three unknowns: \\(C_{1}\\), \\(C_{2}\\), and \\(\\lambda\\). If you have had Linear Algebra, you may recognize that we are assuming the solution is a vector of the form \\(\\vec{v} = \\vec{C} e^{\\lambda t}\\) . Let’s apply Guess and Check to solve these equations. By differentiation, we have the following: \\[\\begin{align} \\frac{d\\tilde{H}}{dt} &amp;= \\lambda C_{1} e^{\\lambda t} \\\\ \\frac{d\\tilde{L}}{dt} &amp;= \\lambda C_{2} e^{\\lambda t}. \\end{align}\\] Comparing to our differential equation we can show that \\[\\begin{align} \\lambda C_{1} e^{\\lambda t} &amp;= - b C_{1} e^{\\lambda t} \\rightarrow (\\lambda +\\delta) C_{1} e^{\\lambda t} = 0\\\\ \\lambda C_{2} e^{\\lambda t} &amp;= b C_{1} e^{\\lambda t} - d C_{2} e^{\\lambda t} \\end{align}\\] Let’s rearrange this expression a little bit: \\[\\begin{align} (\\lambda + b) C_{1} e^{\\lambda t} &amp;= 0\\\\ (\\lambda + d) C_{2} e^{\\lambda t} &amp;=b C_{1} e^{\\lambda t} \\end{align}\\] Notice that for the second equation we can solve for \\(C_{1} e^{\\lambda t}\\), or \\(\\displaystyle C_{1} e^{\\lambda t} = \\frac{(\\lambda + d)}{b} C_{2} e^{\\lambda t}\\). This allows for something neat to happen. We can substitute this expression for \\(C_{1} e^{\\lambda t}\\) into the first equation: \\[\\begin{equation} (\\lambda + b) \\frac{(\\lambda + d)}{b} C_{2} e^{\\lambda t} = 0 \\end{equation}\\] If we assume that \\(b \\neq 0\\), then we have the following simplified expression: \\[\\begin{equation} (\\lambda +b) (\\lambda +d) C_{2} e^{\\lambda t} = 0 \\end{equation}\\] Because the exponential function never equals zero, with this new equation, the only possibility is that \\((\\lambda + b)(\\lambda + d)=0\\), or that \\(\\lambda = -b\\) or \\(\\lambda = -d\\).Remember: if expressions multiply to zero, then the only possibility is that at least one of them is zero. This process finds the eigenvalues and eigenvectors of a system of equations. We will study this later in the course. We now need to determine values of \\(C_{1}\\) and \\(C_{2}\\). We can do this by going back to the equation \\((\\lambda + d) C_{2} e^{\\lambda t} =b C_{1} e^{\\lambda t}\\), or \\((\\lambda + d) C_{2} e^{\\lambda t} -b C_{1} e^{\\lambda t}=0\\) rearranged. Let’s analyze this equation for each of the values of \\(\\lambda\\): 7.5.1 Case \\(\\lambda = -d\\) For this situation, we have \\[(-d +d) C_{2} e^{-d t} - b C_{1} e^{-d t} =0 \\rightarrow -b C_{1} e^{-d t} =0.\\] The only way for this equation to be consistent and remain zero is if \\(C_{1}=0\\). We don’t have any restrictions on \\(C_{2}\\), so the general solution will be \\[\\begin{align} \\tilde{H}(t) &amp;=0 \\\\ \\tilde{L}(t) &amp;= C_{2} e^{-d t}. \\end{align}\\] 7.5.2 Case \\(\\lambda = -d\\) For this situation, we have \\((-d +b) C_{2} e^{-d t} - d C_{1} e^{-d t} =0\\) which leads to the following equation: \\[\\begin{equation} \\left( (-d +b) C_{2} - d C_{1} \\right) e^{-d t} =0 \\end{equation}\\] The only way for this equation to be consistent and remain zero is if \\(\\left( (-d +b) C_{2} - d C_{1} \\right)=0\\), or if \\(\\displaystyle C_{2} = \\left( \\frac{d}{-d + b} \\right) C_{1}\\). In this case, the general solution will be \\[\\begin{align} \\tilde{H}(t) &amp;= C_{1} e^{-d t} \\\\ \\tilde{L}(t) &amp;= \\left( \\frac{d}{-d + b} \\right) C_{1} e^{-d t}, \\end{align}\\] The parameter \\(C_{2}\\) can be determined by the initial condition. Notice that we need to have \\(d \\neq b\\) or our solution will be undefined. Now we can write down a general solution to the system by combining our two solutions together. Here we can you the fact that two solutions can be added together (superposition) to generate a solution. \\[\\begin{align} H(t) &amp;= C_{1} e^{-d t} \\\\ L(t) &amp;= \\left( \\frac{d}{-d + b} \\right) C_{1} e^{-d t} + C_{2} e^{-b t} \\end{align}\\] This method only works on linear differential equations (i.e. it wouldn’t work if there was a term such as \\(kHL\\) in our dynamics. Later on in the course we will look a more systematic method (i.e eigenvalues) to determine solutions to linear systems of equations. "],["exercises-6.html", "7.6 Exercises", " 7.6 Exercises Exercise 7.1 Determine the value of \\(C\\) when \\(I(0)=10\\) for the two equations: \\[\\begin{equation} \\begin{split} I_{1}(t) = 1000 + Ce^{-.03t} \\\\ I_{2}(t) = 1000 + C e^{-0.015 t^{2}} \\end{split} \\end{equation}\\] Exercise 7.2 Verify that \\(I_{2}(t) = N + C e^{-0.5 k t^{2}}\\) is the solution to the differential equation \\(\\displaystyle \\frac{dI}{dt} = kt (N-I)\\). Plot your dolution for various values of \\(k\\) ranging from .001 to .1. What effect does \\(k\\) have on the solution? Exercise 7.3 A chemical reaction \\(2A \\rightarrow C + D\\) can be modeled with the following differential equation (scholz_first-order_2014?): \\[\\begin{equation} \\frac{dA}{dt} = -2 k A^{2} \\end{equation}\\] Apply the method of separation of variables to determine a general solution for this differential equation. Exercise 7.4 Which of the following differential equations be solved via separation of variables? Once you have identified which ones can be solved via separation of variables, apply that technique to solve each differential equation. Exercise 7.5 Solve the following differential equations by separation of variables: Exercise 7.6 Consider the following differential equation \\(\\displaystyle \\frac{dP}{dt} = - \\delta P\\), \\(P(0)=P_{0}\\), where \\(\\delta\\) is a constant parameter. Exercise 7.7 Here we return to the problem of how animals consume food. A differential equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\[\\begin{equation} \\frac{dy}{dx} = \\frac{1}{\\theta} \\frac{y}{x}, \\end{equation}\\] where \\(\\theta \\geq 1\\) is a constant. Apply separation of variables to determine the general solution to this differential equation. Exercise 7.8 Apply separation of variables to determine general solutions to the following systems of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= x \\\\ \\frac{dy}{dt} &amp;= y \\end{split} \\end{equation}\\] (This system is an example of an uncoupled system of equations.) Exercise 7.9 A plant grows propritional to its current length \\(L\\). Assume this proportionality constant is \\(\\mu\\), whose rate also decreases proportional to its current value. The system of equations that models this plant growth is the following: \\[\\begin{align} \\frac{dL}{dt} = \\mu L \\\\ \\frac{d\\mu}{dt} = -k \\mu \\\\ \\mbox{($k$ is a constant parameter)} \\end{align}\\] Apply separation of variables to determine the general solutions to this system of equations. Exercise 7.10 Use the method developed in this section determine the general solution to the following system of differential equations: \\[\\begin{align} \\frac{dx}{dt} &amp;= x-y \\\\ \\frac{dy}{dt} &amp; = 2y \\end{align}\\] Exercise 7.11 Apply the method of integrating factors to determine the solution to the differential equation \\(\\displaystyle \\frac{dI}{dt} = (N-I) = kN - kI\\), where \\(k\\) and \\(N\\) are parameters. Exercise 7.12 For each of the following differential equations: Exercise 7.13 Consider the following differential equation, where \\(M\\) represents a population of mayflies and \\(t\\) is time (given in months), and \\(\\delta\\) is a mortality rate (units % mayflies / month): \\[\\begin{equation} \\frac{dM}{dt} = - \\delta M \\end{equation}\\] Determine the general solution to this differential equation and plot a few different solution curves with different values of \\(\\delta\\). Assume that \\(M(0) = 10,000\\). Also identify the equilibrium solution to the differential equation and classify the stability of the equilibrium solution based on your solution curves. Exercise 7.14 An alternative model of mayfly mortality is the following: \\[\\begin{equation} \\displaystyle \\frac{dM}{dt} = - \\delta(t) M, \\end{equation}\\] where \\(\\delta(t)\\) is a time dependent mortality function. Determine a solution and plot a solution curve (assuming \\(M(0)=10,000\\) and over the interval from \\(0 \\leq t \\leq 1\\)) for this differential equation when \\(\\delta(t)\\) has the following forms: Provide a reasonable biological explanation justifying the use of this alternative mayfly model. "],["linear-regression-08.html", "Chapter 8 Linear Regression &amp; Curve Fitting ", " Chapter 8 Linear Regression &amp; Curve Fitting "],["what-is-parameter-estimation.html", "8.1 What is parameter estimation?", " 8.1 What is parameter estimation? Over the next several sections we will examine aspects of parameter estimation, which can be generally stated as the following process: Parameter estimation is the process of determining values of parameters \\(\\vec{\\alpha}\\) for a function \\(f(\\vec{x}, \\vec{\\alpha})\\). Usually these parameters are determinined by minimizing the square difference between data \\(\\vec{y}\\) and the output of the function \\(f(\\vec{x}, \\vec{\\alpha})\\). Example 8.1 The function \\(f(x)=ax+b\\) has parameters \\(a\\) and \\(b\\). In our notation above, \\(\\displaystyle \\vec{\\alpha} = [a \\; b]^{T}\\). Usually these parameters can be determined through a set of measurements \\((\\vec{x},\\vec{y})\\) (in other words a scatterplot). In Example 8.1, is an example of a linear parameter estimation problem. I did use matrix notation to denote the \\(\\vec{\\alpha}\\) - although I will say matrix notation might be a little more formal for our purposes to start. To solve this problem we can address it from several different mathematical areas: calculus (optimization), statistics (likelihood functions), and linear algebra (systems of linear equations). In this section I will show you how to apply R to determine the unknown parameters and interpret the results. A few section later we will explore how to approach the parameter estimation problem with likelihood and cost functions. We will R a lot in this section to make plots - so please visit Section 2 if you need some reminders on plotting in R. "],["fitting-temperature-data.html", "8.2 Fitting temperature data", " 8.2 Fitting temperature data Let’s take a look at a specific example. Consider the following dataset of average global temperature over time: Table 8.1: Global Temperature from 1880 yearSince1880 globalTemp 0 13.50 1 13.53 2 13.62 3 13.60 4 13.32 5 13.47 6 13.33 7 13.26 8 13.49 9 13.74 10 13.39 11 13.38 12 13.48 13 13.45 14 13.55 15 13.63 16 13.68 17 13.76 18 13.60 19 13.68 (This dataset can be found in the demodelr package with the name global_temperature.) To name our variables let \\(Y=\\mbox{ Year since 1880 }\\) and \\(T= \\mbox{ Temperature }\\). First let’s visualize the data, with time on the horizontal axis and temperature on the vertical axis: ggplot(data = global_temperature,) + geom_point(aes(x=yearSince1880,y=globalTemp), color=&#39;red&#39;, size=2) + labs(x=&#39;Y&#39;, y=&#39;T&#39;) Figure 8.1: Scatterplot of global temperature data. The variable \\(Y\\) represents the Year since 1880 and \\(T\\) the temperature in degrees Celsius. We will be working with these data to fit a function \\(f(Y,\\vec{\\alpha})=T\\). In order to fit a function in R we need three essential elements: We need data for the formula to interpret. This needs to be a two column data table, such as global_temperature. The regression formula we will use for the fit is given in the text regressionFormula &lt;- y ~ 1 + x. We adopt the convention that \\(y\\) signifies the “dependent variable” and \\(x\\) signifies the “independent variable,” but are named columns in a data frame. For the global_temperature dataset we would write the regression_formula as regression_formula &lt;- globalTemp ~ 1 + yearSince1880 Said differently, what this regression formula “does a linear regression where the factors are a constant term and one proportional to the independent variable.” The command lm stands for linear model. This is the main function that does our fitting procedure, where we need to specify the dataset we are using. That’s it! So if we need to do a linear regression of global temperature against year since 1880 the code to do this is the following: regression_formula &lt;- globalTemp ~ 1 + yearSince1880 linear_fit &lt;- lm(regression_formula,data=global_temperature) summary(linear_fit) ## ## Call: ## lm(formula = regression_formula, data = global_temperature) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.45013 -0.11632 -0.00849 0.11326 0.36865 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.358442 0.028832 463.32 &lt;2e-16 *** ## yearSince1880 0.009601 0.000372 25.81 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1684 on 133 degrees of freedom ## Multiple R-squared: 0.8336, Adjusted R-squared: 0.8323 ## F-statistic: 666.2 on 1 and 133 DF, p-value: &lt; 2.2e-16 What is printed on the console is the summary of the fit results. This summary contains a of interesting things that you would study in advanced courses in statistics, but here is what we will focus on: The estimated coefficients of the linear regression. The column Estimate lists the constants in front of our regression formula \\(y=a+bx\\). What follows is the error on that estimate by formulas from statistics. The other additional things are statistical tests that show significance of the estimate. One helpful thing to look at is the Residual standard error, which represents the overall, total effect of the differences between the model predicted values of \\(\\vec{y}\\) and the measured values of \\(\\vec{y}\\). The goal of linear regression is to minimize this model-data difference. To plot the fitted equation with the regression coefficients we are going to borrow from another package called broom, which helps produce model output in what is called “tidy” data format. You can read more about broom here. Since we are only going to use one or two functions from this package, I am going to refer to the functions I need with the syntax PACKAGE_NAME::FUNCTION. First we will make a data frame with the predicted coefficients from our linear model: global_temperature_model &lt;- broom::augment(linear_fit, data = global_temperature) glimpse(global_temperature_model) ## Rows: 135 ## Columns: 8 ## $ yearSince1880 &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16… ## $ globalTemp &lt;dbl&gt; 13.50, 13.53, 13.62, 13.60, 13.32, 13.47, 13.33, 13.26, … ## $ .fitted &lt;dbl&gt; 13.35844, 13.36804, 13.37764, 13.38725, 13.39685, 13.406… ## $ .resid &lt;dbl&gt; 0.141557734, 0.161956817, 0.242355900, 0.212754983, -0.0… ## $ .hat &lt;dbl&gt; 0.02930283, 0.02865412, 0.02801515, 0.02738595, 0.026766… ## $ .sigma &lt;dbl&gt; 0.1686042, 0.1684612, 0.1677079, 0.1680214, 0.1689313, 0… ## $ .cooksd &lt;dbl&gt; 1.098342e-02, 1.403997e-02, 3.069797e-02, 2.309589e-02, … ## $ .std.resid &lt;dbl&gt; 0.85304308, 0.97564433, 1.45949662, 1.28082181, -0.46247… Notice how the augment command takes the results from linear_fit with the data global_temperature. I like appending _model to the original name of the data frame to signify that there are modeled components here to work with. There is a lot to unpack with this new data frame, but the important ones are the columns yearSince1880 (the independent variable) and .fitted, which represents the fitted coefficients. Now we are ready to graph the data along with the fitted regression line. ggplot(data = global_temperature) + geom_point(aes(x=yearSince1880,y=globalTemp), color=&#39;red&#39;, size=2) + geom_line(data = global_temperature_model, aes(x=yearSince1880,y=.fitted)) + labs(x=&#39;Year Since 1880&#39;, y=&#39;Temperature (Celsius)&#39;) "],["moving-beyond-linear-models.html", "8.3 Moving beyond linear models", " 8.3 Moving beyond linear models We can also fit additional polynomial models such as the equation \\(y = a + bx + cx^{2} + dx^{3} ...\\) (estimated parameters \\(a\\), \\(b\\), \\(c\\), \\(d\\), …). There is a key distinction here: the equation is nonlinear in the variable \\(x\\), but linear with respect to the parameters. How we do that in R is pretty simple, it just depends on how we enter in the regression formula. Here are few templates: Equation Regression Formula \\(y=a+bx\\) y ~ 1 + x \\(y=a\\) y ~ 1 \\(y=bx\\) y ~ -1+x \\(y=a+bx+cx^{2}\\) y ~ 1 + x + I(x^2) \\(y=a+bx+cx^{2}+dx^{3}\\) y~ 1 + x + I(x^2) + I(x^3) Note: the structure I(..) is needed for R to signify a factor of the form \\(x^{n}\\). "],["can-you-linearize-your-model.html", "8.4 Can you linearize your model?", " 8.4 Can you linearize your model? We can also plot nonlinear models, or models that can be made linear. While the equation \\(y=ae^{bx}\\) non linear with respect to the parameters, it can be made linear by a logarithmic transformation of the data: \\[\\begin{equation} \\ln(y) = \\ln(ae^{bx}) = \\ln(a) + \\ln (e^{bx}) = \\ln(a) + bx \\end{equation}\\] The advantage to this approach is that the growth rate parameter is easily identifiable from the data, and the value of \\(a\\) is found by exponentiation of the fitted intercept value. The disadvantage is that you need to interpret the do a log transform of the \\(y\\) variable first before doing any fits. Example 8.2 A common equation in enzyme kinetics is the Michaelis-Menten law, which states that the rate of the uptake of a substrate \\(V\\) is given by the equation: \\[\\begin{equation} V = \\frac{V_{max} s}{s+K_{m}}, \\end{equation}\\] where \\(s\\) is the amount of substrate, \\(K_{m}\\) is half-saturation constant, and \\(V_{max}\\) the maximum reaction rate. (Typically \\(V\\) is used to signify the “velocity” of the reaction.) Say you have the following data: s (mM) V (mM / s) 0.1 0.04 0.2 0.08 0.5 0.17 1.0 0.24 2.0 0.32 3.5 0.39 5.0 0.42 Let’s explore these data in R. Solution. First thing that we will need to do is to define a data frame (tibble) of these data: enzyme_data &lt;- tibble( s = c(0.1,0.2,0.5,1.0,2.0,3.5,5.0), V = c(0.04,0.08,0.17,0.24,0.32,0.39,0.42) ) Next let’s do some exploratory data analysis: ggplot(data = enzyme_data) + geom_point(aes(x=s,y=V), color=&#39;red&#39;, size=2) + labs(x = &#39;s (mM)&#39;, y= &#39;V (mM / s)&#39;) Definitely looks non-linear. But take a look at what happens if we plot the reciprocal of \\(s\\) and the reciprocal of \\(V\\): ggplot(data = enzyme_data) + geom_point(aes(x=1/s,y=1/V), color=&#39;red&#39;, size=2) + labs(x = &#39;1/s (1/mM)&#39;, y= &#39;1/V (s / mM)&#39;) That looks really linear! Notice how easy it was to do that data transformation in our plot command. In order to do a linear fit to the transformed data we will use the regression formulas defined above and the handy structure I(VARIABLE): enzyme_fit &lt;- lm(I(1/V) ~ 1+ I(1/s), data = enzyme_data) summary(enzyme_fit) ## ## Call: ## lm(formula = I(1/V) ~ 1 + I(1/s), data = enzyme_data) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 0.3913 -0.6764 -0.4347 0.1360 0.2376 0.1667 0.1795 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.74417 0.21009 8.302 0.000414 *** ## I(1/s) 2.28645 0.04868 46.968 8.26e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4323 on 5 degrees of freedom ## Multiple R-squared: 0.9977, Adjusted R-squared: 0.9973 ## F-statistic: 2206 on 1 and 5 DF, p-value: 8.263e-08 enzyme_data_model &lt;- broom::augment(enzyme_fit,data=enzyme_data) ggplot(data = enzyme_data) + geom_point(aes(x=1/s,y=1/V), color=&#39;red&#39;, size=2) + geom_line(data = enzyme_data_model, aes(x=1/s,y=.fitted)) + labs(x = &#39;1/s (1/mM)&#39;, y= &#39;1/V (s / mM)&#39;) Notice when plotting the fitted model we didn’t need to take the reciprocal of .fitted because the linear model already did the inverse. However if we wanted to plot the model with the original data, then we need to take the reciprocal (confusing - I know!) ggplot(data = enzyme_data) + geom_point(aes(x=s,y=V), color=&#39;red&#39;, size=2) + geom_line(data = enzyme_data_model, aes(x=s,y=1/.fitted)) + labs(x = &#39;s (mM)&#39;, y= &#39;V (mM / s)&#39;) "],["nonlinear-models.html", "8.5 Nonlinear models", " 8.5 Nonlinear models Many cases you will not be able to write your model in a linear format. You can still do a non-linear curve fit using the function nlm, however you will need to specify the function along with the formula you are using. For example if we try to fit the weight of the dog Wilson over time to the logistic equation we would have the following: \\[\\begin{equation} W =f(D,a,b,c)= a - be^{ct}, \\end{equation}\\] where we have the parameters \\(a\\), \\(b\\), and \\(c\\). Notice how \\(W\\) is a function of \\(D\\) and the parameters. nonlinear_fit &lt;- nls(mass ~ a-b*exp(c*days), data = wilson, start = list(a = 75, b = 30,c = -0.01)) summary(nonlinear_fit) ## ## Formula: mass ~ a - b * exp(c * days) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## a 75.2349873 1.5726835 47.84 &lt; 2e-16 *** ## b 90.7696994 3.3999731 26.70 1.07e-14 *** ## c -0.0060311 0.0004324 -13.95 2.26e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.961 on 16 degrees of freedom ## ## Number of iterations to convergence: 11 ## Achieved convergence tolerance: 6.9e-06 The tricky part for a nonlinear model is that you need a starting value for the parameters (it is an iterative method). This can be tricky and takes some trial and error. However once you have your fitted model, you can still plot the fitted values with the coefficients: wilson_model &lt;- broom::augment(nonlinear_fit, data = wilson) ggplot(data = wilson) + geom_point(aes(x=days,y=mass), color=&#39;red&#39;, size=2) + geom_line(data = wilson_model, aes(x=days,y=.fitted)) + labs(x=&#39;Days since birth&#39;, y=&#39;Weight (pounds)&#39;) We will revisit these data later when we are making likelihood functions. "],["exercises-7.html", "8.6 Exercises", " 8.6 Exercises Exercise 8.1 Determine if the following equations are linear with respect to the parameters. For the purposes of this problem we assume that \\(y\\) is a function of \\(x\\). Exercise 8.2 Each of the following equations can be written as linear with respect to the parameters, through applying some elementary transformations to the data. Write each equation as a linear function with respect to the parameters. Exercise 8.3 Use the dataset global_temperature and the function lm to answer the following questions: Exercise 8.4 An equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\(\\displaystyle y = c x^{1/\\theta},\\) where \\(\\theta \\geq 1\\) and \\(c\\) are both constants is a constant. Exercise 8.5 Following on from the last exercise, do a non-linear least squares fit for the dataset phosphorous to the equation \\(\\displaystyle y = c x^{1/\\theta},\\) where \\(\\theta \\geq 1\\) and \\(c\\) are both constants is a constant. For a starting point, you may use the values of \\(c\\) and \\(\\theta\\) from the previous exercise. Finally make a plot of the original phosophorous data and the fitted model. Exercise 8.6 A common equation in enzyme kinetics is the Michaelis-Menten law, which states that the rate of the uptake of a substrate \\(V\\) is given by the equation: \\[\\begin{equation} V = \\frac{V_{max} s}{s+K_{m}}, \\end{equation}\\] where \\(s\\) is the amount of substrate, \\(K_{m}\\) is half-saturation constant, and \\(V_{max}\\) the maximum reaction rate. (Typically \\(V\\) is used to signify the “velocity” of the reaction.) Say you have the following data: s (mM) V (mM / s) 0.1 0.04 0.2 0.08 0.5 0.17 1.0 0.24 2.0 0.32 3.5 0.39 5.0 0.42 Note: The process outlined here is a Lineweaver-Burk plot. Exercise 8.7 Following on from the last exercise, let’s do a nonlinear least squares fit of the enzyme data to the equation: \\[\\begin{equation} V = \\frac{V_{max} s}{s+K_{m}}, \\end{equation}\\] where \\(s\\) is the amount of substrate, \\(K_{m}\\) is half-saturation constant, and \\(V_{max}\\) the maximum reaction rate. Exercise 8.8 Consider the following data which represents the temperature over the course of a day: "],["likelihood-09.html", "Chapter 9 Probability and Likelihood Functions", " Chapter 9 Probability and Likelihood Functions The problem we examined in the last chapter was the following: Determine the set of parameters \\(\\vec{\\alpha}\\) that minimize the difference between data \\(\\vec{y}\\) and the output of the function \\(f(\\vec{x}, \\vec{\\alpha})\\) and measured error \\(\\vec{\\sigma}\\). We are going to examine the linear regression problem again using a smaller dataset. Ohe to the linear regression problem is through likelihood functions, which is a topic from probability and statistics. This section will introduce likelihood functions but also discuss some interesting visualization techniques of multivariable functions and contour plots. We will also see a technique to evaluate a continuous function. We are starting to build out some R skills and techniques that you can apply in other context. Let’s get started! "],["linear-regression-part-2.html", "9.1 Linear regression, part 2", " 9.1 Linear regression, part 2 Assume we have the following (limited) number of points where we wish to fit a function of the form \\(y=bx\\). x y 1 3 2 5 4 4 4 10 For this example we are forcing the intercept term \\(a\\) to equal zero - for most cases you will just fit the linear equation. See Exercise 9.7. Figure 9.1 displays a quick scatterplot of these data: Figure 9.1: A scatterplot of a small dataset. The goal here is to work to determine the value of \\(b\\) that is most likely (in other words, consistent) with the data. In order to do this, we need to take a quick excursion into probability distributions. Let’s go! "],["probability.html", "9.2 Probability", " 9.2 Probability In order to understand likelihood functions, first I am going to review very essential information about probability and probability distributions. Probability is the association of a set of observable events to a quantitative scale between 0 to 1. (Zero means that event is not possible, 1 means that it definitely can happen). This definition could be refined somewhat (devore_modern_2021?). You can consider discrete events (think counting or combinatorial problems) or continuous events. Here we are only going to consider continuous events, specifically in this case the probability of a parameter obtaining a particular value. Consider this graphic, which may be familiar to you as the normal distribution or the bell curve: Figure 9.2: The normal distribution We tend to think of the plot and the associated function \\(f(x)\\) as something with input and output (such as \\(f(0)=\\) 0.3989). However because it is a probability density function, the area between two points gives yields the probability of an event to fall within two values: Figure 9.3: The area between two values, normally distributed In this case, the shaded area tells us the probability that our measurement is between \\(x=-0.1\\) and \\(x=0.1\\). The value of the area, or the probability is 0.07966. When you took calculus the area was expressed as a definite integral: \\(\\displaystyle \\int_{-0.1}^{0.1} f(x) \\; dx=\\) 0.07966, where \\(f(x)\\) is the formula for the probability density function for the normal distribution. The basic idea is that we can assign values to an outcomes as a way of displaying our belief (confidence) in the result. With this intuition we can summarize key facts about probability density functions: \\(f(x) \\geq 0\\) (this means that probability density functions are positive values) Area integrates to one (in probability, this means we have accounted for all of our outcomes) The formula for the normal distribution is \\[\\begin{equation} f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma } e^{-(x-\\mu)^{2}/(2 \\sigma^{2})} \\end{equation}\\] Where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation 9.2.1 Other probability distributions Beyond the normal distribution some of the more common ones we utilize in the parameter estimation are the following: Uniform: For this distribution we must specify between a minimum value \\(a\\) and maximum value \\(b\\). Figure 9.4: The uniform distribution The formula for the uniform distribution is \\[\\begin{equation} f(x)=\\frac{1}{b-a} \\mbox{ for } a \\leq x \\leq b \\end{equation}\\] Exponential: For this distribution we must specify between a rate parameter \\(\\lambda\\). Figure 9.5: The exponential distribution The formula for the exponential distribution is \\[\\begin{equation} f(x)=\\lambda e^{-\\lambda x} \\mbox{ for } x \\geq 0 \\end{equation}\\] where \\(\\lambda\\) is the rate parameter Lognormal: This distirbution is for positive values, with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Figure 9.6: The lognormal distribution The formula for the lognormal distribution is \\[\\begin{equation} f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma x } e^{-(\\ln(x)-\\mu)^{2}/(2 \\sigma^{2})} \\mbox{ for } x \\geq 0 \\end{equation}\\] 9.2.2 Computing probabilities in R Here is the good news with R: the commands to generate densities and cumulative distributions are already included! There are a variety of implementations: both for the density, cumulative distribution, random number generation, and lognormal distributions from these. For the moment, Table summarizes some common probability distributions in R. To make the graphs of these density functions in R we use the prefix d + the name (norm, exp) etc of the distribution we wish to specify, including any of the key parameters. If we don’t include any of the parameters then it will just use the defaults (which you can see by typing ?NAME where NAME is the name of the command (i.e. ?dnorm). Here is some sample code to plot the lognormal distribution: Example 9.1 Make a graph of the lognormal density function with \\(\\mu=0\\) and \\(\\sigma=1\\) from \\(0 \\leq x \\leq 5\\). Remark. For this case we are using the defaults of the lognormal distribution. The code is the following, and plotted in Figure 9.7. x &lt;- seq(0,5,length=200) y &lt;- dlnorm(x) # Just use the mean defaults lognormal_data &lt;- tibble(x,y) ggplot() + geom_line(data = lognormal_data, aes(x=x,y=y)) + labs(x = &#39;x&#39;, y= &#39;Lognormal density&#39;) Figure 9.7: Code to plot the lognormal distribution To find the area between two values in a density function we use the prefix p. Example 9.2 Use R to evaluate \\(\\displaystyle \\int_{1}^{2} e^{-x} \\; dx\\). Remark. The function \\(e^{-x}\\) is the exponential probability distribution with \\(\\lambda=1\\). For this example if we wanted to find the area between two values in the exponential density in the shaded graph we would type pexp(2)-pexp(1) at the R console, which would give the value of r round(pexp(2)-pexp(1),digits=3). A visual representation of this area is shown in Figure 9.8. Figure 9.8: The area for the exponential distribution "],["connecting-to-linear-regression.html", "9.3 Connecting to linear regression", " 9.3 Connecting to linear regression Now that we have made that small excursion into probablity, let’s start to return back to the linear regression problem. Another way to phrase this the linear regression problem studied in the last chapter is to examine the probability distribution of the model-data residual \\(\\epsilon\\): \\[\\begin{equation} \\epsilon_{i} = y_{i} - f(x_{i},\\vec{\\alpha} ). \\end{equation}\\] The approach with likelihood functions assumes a particular probability distribution on each residual. One common assumption is that the residual distribution is normal with mean \\(\\mu=0\\) and standard deviation \\(\\sigma\\) (which could be specified as measurement error, etc). \\[\\begin{equation} L(\\epsilon_{i}) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\epsilon_{i}^{2} / 2 \\sigma^{2} }, \\end{equation}\\] To extend this further across all measurement, we use the idea of independent, identically distributed measurements so the joint likelihood of all the residuals is the product of the likelihoods \\[\\begin{equation} L(\\vec{\\epsilon}) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\epsilon_{i}^{2} / 2 \\sigma^{2} }, \\end{equation}\\] We are making progress here, but in order to fully characterize the solution we need to specify the parameters \\(\\vec{\\alpha}\\). A simple redefining of the likelihood function where we specify the measurements (\\(x\\) and \\(y\\)) and parameters (\\(\\vec{\\alpha}\\)) is all we need: \\[\\begin{equation} L(\\vec{\\alpha} | \\vec{x},\\vec{y} )= \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp(-(y_{i} - f(x_{i},\\vec{\\alpha} ))^{2} / 2 \\sigma^{2} ) \\end{equation}\\] Now we have a function where the best parameter estimate is the one that optimizes the likelihood. To get back to our original linear regression problem. As a reminder we wanted to fit the function \\(y=bx\\) to the following set of points: x y 1 3 2 5 4 4 4 10 The likelihood \\(L(\\epsilon_{i}) ~ N(0,\\sigma)\\) characterizing these data are the following: \\[\\begin{equation} L(b) = \\left( \\frac{1}{\\sqrt{2 \\pi} \\sigma}\\right)^{4} e^{-\\frac{(3-b)^{2}}{2\\sigma}} \\cdot e^{-\\frac{(5-2b)^{2}}{2\\sigma}} \\cdot e^{-\\frac{(4-4b)^{2}}{2\\sigma}} \\cdot e^{-\\frac{(10-4b)^{2}}{2\\sigma}} \\tag{9.1} \\end{equation}\\] For the purposes of our argument here, we will assume \\(\\sigma=1\\). Let’s also make a plot of this likelihood function: Figure 9.9: The likelihood function for the small dataset Note that although the \\(y\\) values of the likelihood function are really small, the likelihood function is maximized at \\(b=1.86\\). Notice how in the plot of our likelihood function we had really small values of \\(L(b)\\). It is also common to use the log likelihood (Equation (9.2): \\[\\begin{equation} \\begin{split} \\ln(L(\\vec{\\alpha} | \\vec{x},\\vec{y} )) &amp;= N \\ln \\left( \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\right) - \\sum_{i=1}^{N} \\frac{ (y_{i} - f(x_{i},\\vec{\\alpha} )^{2}}{ 2 \\sigma^{2}} \\\\ &amp; = - \\frac{N}{2} \\ln (2) - \\frac{N}{2} \\ln(\\pi) - N \\ln( \\sigma) - \\sum_{i=1}^{N} \\frac{ (y_{i} - f(x_{i},\\vec{\\alpha} )^{2}}{ 2 \\sigma^{2}} \\end{split} \\tag{9.2} \\end{equation}\\] In the homework you will be working on how to transform the likelihood function \\(L(b)\\) to the log-likelihood \\(\\ln(L(b))\\). For our purposes here we are going to assume independent, identical, normally distributed errors with a mean 0 for the likelihood functions. This is a commonly used assumption for this approach, but you should always think about the errors in more advanced applications. "],["plotting-likelihood-surfaces.html", "9.4 Plotting likelihood surfaces", " 9.4 Plotting likelihood surfaces Ok, we are going to examine a second example from (gause_experimental_1932?) which modeled the growing of yeast in solution. This classic paper examines the biological principal of competitive exclusion, how one species can out compete another one for resources. For our purposes here we are going to examine a model for one species growing without competition. First let’s make a quick plot of the function: ### Make a quick ggplot of the data ggplot() + geom_point(data = yeast, aes(x=time,y=volume), color=&#39;red&#39;, size=2) + labs(x = &#39;Time&#39;, y= &#39;Volume&#39;) We are going to assume the population of yeast (represented with the measurement of volume) over time changes according to the equation: \\[\\begin{equation} \\frac{dy}{dt} = -by \\frac{(K-y)}{K}, \\end{equation}\\] where \\(y\\) is the population of the yeast and \\(b\\) represents the growth rate and \\(K\\) is the carrying capacity of the population. It can be shown that the solution to this differential equation is \\(\\displaystyle y = \\frac{K}{1+e^{a-bt}}\\), where the additional parameter \\(a\\) can be found through application of the initial condition \\(y_{0}\\). In (gause_experimental_1932?) the value of \\(a\\) was determined by solving the initial value problem \\(y(0)=0.45\\). In Exercise 9.1 you will show that \\(\\displaystyle a = \\ln \\left( \\frac{K}{0.45} - 1 \\right)\\). Here we are going to explore the likelihood function to try to determine the best set of values for the two parameters \\(K\\) and \\(b\\) using the function compute_likelihood. Inputs to the compute_likelihood function are the following: A function \\(y=f(x,\\vec{\\alpha})\\) A dataset \\((x,y)\\) Ranges of your parameters \\(\\vec{\\alpha}\\). The compute_likelihood function also has an optional input that allows you to specify if you want to compute the likelihood or the log likelihood. We are going to plot the likelihood equation surface for the yeast dataset, contained in the data frame yeast. Recall the function that we fit to determine is \\(\\displaystyle y = \\frac{K}{1+e^{a-bt}}\\). Next we will define the equation used to compare our model in the likelihood. As with the functions euler or systems we need to define this function: library(demodelr) # Gause model equation gause_model &lt;- volume ~ k/(1+exp(log(k/0.45-1)-b*time)) # Identify the ranges of the parameters that we wish to investigate kParam &lt;- seq(5,20,length.out=100) bParam &lt;- seq(0,1,length.out=100) # Allow for all the possible combinations of parameters gause_parameters &lt;- expand.grid(k=kParam,b=bParam) gause_likelihood &lt;- compute_likelihood(model = gause_model, data = yeast, parameters = gause_parameters, logLikely=FALSE) Ok, let’s break this code down step by step: The line gause_model &lt;- volume ~ k/(1+exp(log(k/0.45-1)-b*time)) identifies the formula that relates the variables time to volume. We define the ranges (minimum and maximum values) for our parameters by defining a sequence. Because we want to look at all possible combinations of these parameters we use the command expand.grid. The input logLikely to compute_likelihood reports back log likelihood values. Some care is needed in defining the number of points that we want to evaluate - we will have \\(100^{2}\\) different combinations of \\(k\\) and \\(b\\), which do take time to evaluate. The output to compute_likelihood is a list - this is a flexible data structure. You can think of this as a collection of items. In this case, what gets returned are two data frames: likelihood, which is a data frame of likelihood values for each of the parameters and opt_value, which reports back the values of the parameters that optimize the likelihood function. Note that the optimum value is an approximation, as it is just the optimum from the input values. Let’s take a look at the reported optimum values, which we can do with the syntax LIST_NAME$VARIABLE_NAME, where the dollar sign ($) helps identify which variable from the list you are investigating. gause_likelihood$opt_value ## # A tibble: 1 x 4 ## k b l_hood log_lik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 12.7 0.242 0.000348 FALSE It is also important to visualize this likelihood function. For this dataset we have the two parameters \\(k\\) and \\(b\\), so the likelihood function will be a likelihood surface. The code to generate the plot looks a little different: # Define the likelihood values my_likelihood &lt;- gause_likelihood$likelihood # Make a contour plot ggplot(data = my_likelihood) + geom_tile(aes(x = k,y = b,fill = l_hood)) + stat_contour(aes(x = k, y = b, z = l_hood)) Figure 9.10: Likelihood surface and contour lines for the Gause dataset. Similar to before, let’s take this step by step: The command my_likelihood just puts the likelihood values in a data frame. The ggplot command is similar as before. We use geom_tile to visualize the likelihood surface. There are three required inputs from the my_likelihood data frame: the x and y axis data values and the fill value, which represents the height of the likelihood function. The command stat_contour draws the contour lines, or places where the likelihood function is the same. Notice how we used z = l_hood rather than fill here. I chose some broad parameter ranges at first, so let’s make a likelihood plot, exploring parameters closer to the last optimum value: # Gause model equation gause_model &lt;- volume ~ k/(1+exp(log(k/0.45-1)-b*time)) # Identify the (new) ranges of the parameters that we wish to investigate kParam &lt;- seq(11,14,length.out=100) bParam &lt;- seq(0.1,0.3,length.out=100) # Allow for all the possible combinations of parameters gause_parameters_rev &lt;- expand.grid(k=kParam,b=bParam) gause_likelihood_rev &lt;- compute_likelihood(model = gause_model, data = yeast, parameters = gause_parameters_rev, logLikely=FALSE) # Report out the optimum values opt_value_rev &lt;- gause_likelihood_rev$opt_value opt_value_rev ## # A tibble: 1 x 4 ## k b l_hood log_lik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 12.8 0.241 0.000349 FALSE # Define the likelihood values my_likelihood_rev &lt;- gause_likelihood_rev$likelihood # Make a contour plot ggplot(data = my_likelihood_rev) + geom_tile(aes(x = k,y = b,fill = l_hood)) + stat_contour(aes(x = k, y = b, z = l_hood)) + geom_point(data=opt_value_rev,aes(x=k,y=b),color=&#39;red&#39;) Figure 9.11: Revised likelihood surface. The computed location of the optimum value is shown as a red point. The reported values for \\(k\\) and \\(b\\) may be close to what was reported from the first time. Notice that I also added in the location of the optimum point with the code geom_point(data=opt_value_rev,aes(x=k,y=b),color='red'). Finally we can use the optimized parameters to compare the function against the data: # Define the parameters and the times that you will evaluate the equation my_params &lt;- gause_likelihood_rev$opt_value time=seq(0,60,length.out=100) # Get the right hand side of your equations new_eq &lt;- gause_model %&gt;% formula.tools::rhs() # This collects the parameters and data into a list in_list &lt;- c(my_params,time) %&gt;% as.list() # The eval command evaluates your model out_model &lt;- eval(new_eq,envir=in_list) # Now collect everything into a dataframe: my_prediction &lt;- tibble(time = time, volume = out_model) ggplot() + geom_point(data = yeast, aes(x=time,y=volume), color=&#39;red&#39;, size=2) + geom_line(data = my_prediction, aes(x=time,y=volume)) + labs(x = &#39;Time&#39;, y= &#39;Volume&#39;) Figure 9.12: Model and data comparison from maximum likelihood estimation. All right, this code block has some new commands and techniques that need explaining. Once we have the parameter estimates we need to compute the modeled values. First we define the params and the time we wish to evaluate with our model. We need to evaluate the right hand side of \\(\\displaystyle y = \\frac{K}{1+e^{a+bt}}\\), so the definition of new_eq helps to do that, using the package formula.tools. The %&gt;% is the tidyverse https://r4ds.had.co.nz/pipes.html#pipes. This is a very useful command to help make code more readable! in_list &lt;- c(params,my_time) %&gt;% as.list() collects the parameters and input times in one list to evaluate the model with out_model &lt;- eval(new_eq,envir=in_list) In order to plot we make a data frame my_prediction And the rest of the plotting commands you should be used to. 9.4.1 Single / more than two parameter parameter Are you able use the compute_likelihood function if you have one or more than two parameters? Why yes you can! For a single parameter likelihood function then you will just use geom_line rather than geom_tile to plot the surface. "],["exercises-8.html", "9.5 Exercises", " 9.5 Exercises Exercise 9.1 Algebraically solve the equation \\(\\displaystyle 0.45 = \\frac{K}{1+e^{a}}\\) for \\(K\\). Exercise 9.2 Evaluate \\(\\displaystyle \\int_{0}^{5} 2 e^{-2x} \\; dx\\) by hand. Then Use R to compute the value of \\(\\int_{0}^{5} 2 e^{-2x} \\; dx\\). Does your computed answer match with what you found in R? Exercise 9.3 Make a plot of the normal density distribution with \\(mu=2\\) and \\(\\sigma=0.1\\) for \\(0 \\leq x \\leq 4\\). Then use R to compute the following integral: \\(\\displaystyle f(x)=\\frac{1}{\\sqrt{2 \\pi} 0.1} e^{-(x-2)^{2}/(2\\cdot 0.1^{2})}\\) Exercise 9.4 Visualize the likelihood function for the yeast dataset, but in this case report out and visualize the loglikelihood. (This means that you are setting the option logLikely = TRUE in the compute_likelihood function.) Compare the loglikelihood surface to Figure 9.11. Exercise 9.5 When we generated our plot of the likelihood function in Figure 9.9 we assumed that \\(\\sigma=1\\) in Equation (9.1). For this exercise you will explore what happens in Equation (9.1) as \\(\\sigma\\) increases or decreases. Exercise 9.6 Using Equation (9.1) with \\(\\sigma = 1\\): Exercise 9.7 Consider the linear model \\(y=a+bx\\) for the following dataset: x y 1 3 2 5 4 4 4 10 Exercise 9.8 For the function \\(\\displaystyle P(t)=\\frac{K}{1+e^{a+bt}}\\), with \\(P(0)=P_{0}\\), determine an expression for the parameter \\(a\\) in terms of \\(K\\), \\(b\\), and \\(P_{0}\\). Exercise 9.9 The values of returned by the maximum likelihood estimate were a little different from those reported in (gause_experimental_1932?): Parameter Maximum Likelihood Estimate (gause_experimental_1932?) \\(K\\) 12.7 13.0 \\(b\\) 0.24242 0.21827 Make of plot of the function \\(\\displaystyle y = \\frac{K}{1+e^{a-bt}}\\) with \\(\\displaystyle a = \\ln \\left( \\frac{K}{0.45} - 1 \\right)\\) for both parameter values, along with the yeast data. to generate plots with the yeast data with the curves with parameters from both the Maximum Likelihood estimate and from (gause_experimental_1932?). Which approach does a better job representing the data? Exercise 9.10 An equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\(\\displaystyle y = c x^{1/\\theta}\\), where \\(\\theta \\geq 1\\) and \\(c\\) are both constants. Exercise 9.11 A dog’s weight \\(W\\) (pounds) changes over \\(D\\) days according to the following function: \\[\\begin{equation} W =f(D,p_{1},p_{2})= \\frac{p_{1}}{1+e^{2.462-p_{2}D}} \\end{equation}\\] where we have the parameters \\(p_{1}\\) and \\(p_{2}\\). The dataset wilson shows how the weight of a dog (named Wilson changes adapted from here. Exercise 9.12 Consider the following data which represents the temperature over the course of a day: A function that describes these data is \\(\\displaystyle T = A + B \\sin \\left( \\frac{\\pi}{12} \\cdot H \\right) + C \\cos \\left( \\frac{\\pi}{12} \\cdot H \\right)\\), where \\(H\\) is the hour and \\(T\\) is the temperature. Use the function compute_likelihood to determine maximum likelihood parameter estimates for \\(A\\), \\(B\\), and \\(C\\). "],["cost-fns-10.html", "Chapter 10 Cost Functions &amp; Bayes’ Rule", " Chapter 10 Cost Functions &amp; Bayes’ Rule In Section 9 we examined likelihood functions, which were needed when combining a model with data using probability density functions. In this section we will study this idea of parameter estimation using cost functions, which is another approach to the parameter estimation problem. "],["cost-functions-likelihood-functions-in-disguise.html", "10.1 Cost functions: likelihood functions in disguise", " 10.1 Cost functions: likelihood functions in disguise So far we have seen the idea of Another approach that can be incorporated into parameter estimation is the idea of a cost function. Let’s start with this problem from the last few sections: Assume we have the following (limited) data set of points that we wish to fit a function of the form \\(y=bx\\) (note, we are forcing the intercept term to be zero). \\(x\\) \\(y\\) 1 3 2 5 4 4 4 10 One way that we can do this is by saying the estimate of \\(b\\) is determined by the one that minimizes the difference between the measured \\(y\\) values. We do this by computing the residual, or the expression \\(y-bx\\). Let’s extend out this table a little more: \\(x\\) \\(y\\) \\(bx\\) \\(y-bx\\) 1 3 \\(b\\) \\(3-b\\) 2 5 \\(2b\\) \\(5-2b\\) 4 4 \\(4b\\) \\(4-4b\\) 4 10 \\(4b\\) \\(10-4b\\) You can see how this residual changes \\(y-bx\\) for different values of \\(b\\): \\(y-bx\\) \\(b=1\\) \\(b=3\\) \\(b=-1\\) \\(3-b\\) 2 0 4 \\(5-2b\\) 3 -1 7 \\(4-4b\\) 0 -8 8 \\(10-4b\\) 6 -2 14 Notice that the values of the residual at each \\((x,y)\\) pair change as \\(b\\) changes - some of the residuals can be negative and some can be positive. If we were to assess the overall residuals as a function of the value of \\(b\\), we need to take into account not just the value of the residual (positive or negative), but rather the magnitude of the residual. How we do that is if we take the sum of the square difference (or the residual), we have: \\[\\begin{equation} S(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 \\end{equation}\\] Figure 10.1: The square residual \\(S(b)\\). The vertical line denotes the minimum value at \\(b=1.561\\). This looks like a function of one variable. Let’s make a plot of \\(S(b)\\) in Figure 10.1. Notice how the plot of \\(S(b)\\) looks like a really nice quadratic function, with a minimum at \\(b=1.86\\). Did you notice that this value for \\(b\\) is the same value for the minimum that we found in the likelihood function from Section 9? In fact, if we multiplied out \\(S(b)\\) and collected terms, this would be a quadratic function - which has a well defined optimum value that you can find using calculus. Let’s compare the value of \\(b\\) to the best fit line in Figure 10.2. Figure 10.2: Data with the best fit line The cost function approach described above seems to be working out well in that we have a value for \\(b\\), but it also looks like a lot of the data lies above the best fit line. We can address that later, but one approach is to include the uncertainty on each of the measured values in the cost function. The uncertainty may be the same (\\(\\sigma\\)) for all measurements or it could vary from measurement to measurement. In both cases we divide each of the components of the cost function by the given uncertainty. We can represent this cost function more generally using \\(\\sum\\) notation: \\[\\begin{equation} S(\\vec{\\alpha}) = \\sum_{i=1}^{N} \\frac{(y_{i}-f(x,\\vec{\\alpha}))^{2}}{\\sigma^{2}} \\end{equation}\\] "],["connection-to-likelihood-functions.html", "10.2 Connection to likelihood functions", " 10.2 Connection to likelihood functions We call the function \\(S(b)\\) the cost function. There is something big going on here. In Section 9 we also defined the log likelihood function (Equation (9.2)): \\[\\begin{equation} \\ln(L(\\vec{\\alpha} | \\vec{x},\\vec{y} )) = -2 \\ln(2) - 2 \\ln (\\pi) -(3-b)^{2}-(5-2b)^{2}-(4-4b)^{2}-(10-4b)^{2} \\end{equation}\\] If we compare this log likelihood equation with \\(N=4\\), \\(\\sigma = 1/\\sqrt{2}\\), and \\(f(x_{i},\\vec{\\alpha} ) =bx\\) (\\(\\vec{\\alpha}=b\\)), then we have the function \\(S(b)\\), modulo the terms \\(\\displaystyle - \\frac{N}{2} \\ln (2) - \\frac{N}{2} \\ln(\\pi) - N \\ln( \\sigma)\\). This is no coincidence. In fact, when you study probability and statistics you may encounter likelihood functions (or log-likelihood functions - notice how \\(S(b) \\approx - \\ln(L)\\)!). Likelihood functions are similar in nature to cost functions. You may be thinking: but the log likelihood function contains the extra factors of \\(\\displaystyle - \\frac{N}{2} \\ln (2) - \\frac{N}{2} \\ln(\\pi) - N \\ln( \\sigma)\\) - but you need not worry. Here is why: our goal is to optimize a cost or log-likelihood function. What these extras terms do (for constant \\(\\sigma\\) is shift the graph of the log-likelihood function vertically but not horizontally. Vertically shifting a function doesn’t not change the location of an optimum value (Why? Think back to derivatives from Calculus I). Recognizing the connection between cost and likelihood functions and their goal of optimization leads to a key observation: A quadratic cost function yields the same results as likelihood function assuming the residuals are normally distributed. "],["extending-the-cost-function.html", "10.3 Extending the cost function", " 10.3 Extending the cost function We may be wondering if this can be extended additionally to incorporate other types of data. For example, if we knew there was a given range of values that would make sense (say \\(b\\) is near 1.3 with a standard deviation of 0.1), we should be able to incorporate this information into the cost function. A naive approach would be just to add in some additional squared term: \\[\\begin{equation} \\tilde{S}(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 + \\frac{(b-1.3)^2}{0.1^2} \\end{equation}\\] ## Warning: Removed 191 row(s) containing missing values (geom_path). Figure 10.3: Comparing two cost functions \\(S(b)\\) (black) and \\(\\tilde{S}(b)\\) (black dashed line) Aha! Hopefully that shows how the revised cost function \\(\\tilde{S}(b)\\) changes the optimum value. Numerically this works out to be \\(\\tilde{b}=\\) 1.45. In a homework problem you verify this new minimum value and compare the fitted value to the value of \\(b=1.86\\). Adding this prior information seems like an effective approach - and perhaps is applicable for problems in the sciences. Many times a scientific study wants to build upon the existing body of literature and to take that into account. This approach of including prior information into the cost function uses elements of Bayesian statistics - so let’s have a digression into what that means. "],["conditional-probabilities-and-bayes-rule.html", "10.4 Conditional Probabilities and Bayes’ Rule", " 10.4 Conditional Probabilities and Bayes’ Rule In order to understand Bayesian statistics we first need to understand Bayes’ rule and conditional probability. So let’s look at an example. Example 10.1 The following table shows results from a survey of people’s views on the economy and whether or not they voted for the President in the last election. Percentages are reported as decimals. Probability Optimistic view on economy Pessimistic view on economy Total Voted for the president 0.20 0.20 0.40 Did not vote for president 0.15 0.45 0.60 Total 0.35 0.65 1.00 Compute the probability of having an optimistic view on the economy. Remark. Probability tables are a clever way to organize information with conditional probability. We define the following probabilities: The probability you voted for the President and have an optimistic view on the economy is 0.20 The probability you did not vote for the President and have an optimistic view on the economy is 0.15 The probability you voted for the President and have an pessimistic view on the economy is 0.20 The probability you did not vote for the President and have an pessimistic view on the economy is 0.45 We calculate the probability of having an Optimistic view on the economy by adding the probabilities with an optimistic view, whether or not they voted for the president. For this example, this probability sums to 0.20 + 0.15 = 0.35. On the other hand, the probability you have a pessimistic view on the economy is 0.20 + 0.45 = 0.65. Notice how the two of these together (probability of optimistic and pessimistic views of the economy is 1, or 100% of the outcomes.) 10.4.1 Conditional probabilities A conditional probability is the probability of an outcome given some previous outcome, or \\(\\mbox{Pr} (A | B)\\), where Pr means “probability of an outcome” and \\(A\\) and \\(B\\) are two different outcomes or events. In probability theory you might study the following law of conditional probability: \\[\\begin{equation} \\begin{split}\\tag{10.1} \\mbox{Pr}(A \\mbox { and } B) &amp;= \\mbox{Pr} (A \\mbox{ given } B) \\cdot \\mbox{Pr}(B) \\\\ &amp;= \\mbox{Pr} (A | B) \\cdot \\mbox{Pr}(B) \\\\ &amp;= \\mbox{Pr} (B | A) \\cdot \\mbox{Pr}(A) \\end{split} \\end{equation}\\] Typically in the conditional probability equation we remove “and” and write \\(P(A \\mbox{ and } B) = P(AB)\\) and “given” as \\(P(A \\mbox{ given } B) = P(A|B)\\). Example 10.2 Sometimes people believe that your views of the economy influence if you are going to vote for the President, so use the information from the table in Example 10.1 to compute the probability you voted for the president given you have an optimistic view of the economy. Remark. To compute the probability you voted for the president given you have an optimistic view of the economy is a rearrangement of Equation (10.1): \\[\\begin{equation} \\begin{split}\\tag{10.2} \\mbox{Pr(Voted for President | Optimistic View on Economy)} = \\\\ \\frac{\\mbox{Pr(Voted for President and Optimistic View on Economy)}}{\\mbox{Pr(Optimistic View on Economy)}} = \\\\ \\frac{0.20}{0.35} = 0.57 \\end{split} \\end{equation}\\] So the probability you compute in Equation (10.2) seems telling. Contrast this percentage to that of the probability you voted for the President, which is 0.4. Perhaps your view of the economy does indeed influences whether or not you would vote to re-elect the President. 10.4.2 Bayes’ Rule: Application of Conditional Probabilities How could we systematically incorporate prior information into a parameter estimation problem? We are going to introduce Bayes’ Rule, which is a rearrangment of the rule for conditional probability: \\[\\begin{equation} \\mbox{Pr} (A | B) = \\frac{ \\mbox{Pr} (B | A) \\cdot \\mbox{Pr}(A)}{\\mbox{Pr}(B) } \\end{equation}\\] It turns out Bayes’ Rule is a really helpful way to understand how we can systematically incorporate this prior information into the likelihood function (and by association the cost function). For data assimilation problems our goal is to estimate parameters, given data. So we can think of Bayes’ rule in terms of parameters and data: \\[\\begin{equation} \\mbox{Pr}( \\mbox{ parameters } | \\mbox{ data }) = \\frac{\\mbox{Pr}( \\mbox{ data } | \\mbox{ parameters }) \\cdot \\mbox{ Pr}( \\mbox{ parameters }) }{\\mbox{Pr}(\\mbox{ data }) }. \\end{equation}\\] Here are a few observations from that last equation: The term \\(\\mbox{Pr}( \\mbox{ data } | \\mbox{ parameters })\\) is similar to the model data residual, or the standard likelihood function. If we think of the term \\(\\mbox{Pr}( \\mbox{ parameters })\\), then prior information is a multiplicative effect on the likelihood function - this is good news! You will demonstrate in the homework that the log likelihood is related to the cost function - so when we added that additional term to form \\(\\tilde{S}(b)\\), we accounted for the prior information correctly. The expression \\(\\mbox{Pr}( \\mbox{ parameters } | \\mbox{ data })\\) is the start of a framework for a probability density function, which should integrate to unity. (You will explore this more if you study probability theory.) In many cases we select parameters that optimize a likelihood or cost function. So the expression in the denominator (\\(\\mbox{Pr}(\\mbox{ data })\\) ) does not change the location of the optimum values. And in fact, many people consider the denominator term to be a normalizing constant. In the following sections we will explore Bayes’ Rule in action and how to utilize it for different types of cost functions, but wow - we made some significant progress in our conceptual understanding of how to incorporate models and data. "],["bayes-rule-and-linear-regression.html", "10.5 Bayes’ Rule and Linear Regression", " 10.5 Bayes’ Rule and Linear Regression Returning back to our linear regression problem (\\(y=bx\\)). We have the following assumptions: The data are independent, identically distributed. We can then write the likelihood function as the following: \\[\\begin{equation} \\mbox{Pr}(\\vec{y} | b) = \\left( \\frac{1}{\\sqrt{2 \\pi}}\\right)^{4} e^{-\\frac{(3-b)^{2}}{\\sigma}} \\cdot e^{-\\frac{(5-2b)^{2}}{\\sigma}} \\cdot e^{-\\frac{(4-4b)^{2}}{\\sigma}} \\cdot e^{-\\frac{(10-4b)^{2}}{\\sigma}} \\end{equation}\\] Prior knowledge expects us to say that \\(b\\) is normally distributed with mean 1.3 and standard deviation 0.1. Incorporating this information allows us to write the following: \\[\\begin{equation} \\mbox{Pr}(b) =\\frac{1}{\\sqrt{2 \\pi} \\cdot 0.1} e^{-\\frac{(b-1.3)^{2}}{2 \\cdot 0.1^{2}}} \\end{equation}\\] So when we combine the two pieces of information, the probability of the \\(b\\), given the data \\(\\vec{y}\\) is the following: \\[\\begin{equation} \\mbox{Pr}(b | \\vec{y}) \\approx e^{-\\frac{(3-b)^{2}}{2}} \\cdot e^{-\\frac{(5-2b)^{2}}{2}} \\cdot e^{-\\frac{(4-4b)^{2}}{2}} \\cdot e^{-\\frac{(10-4b)^{2}}{2}} \\cdot e^{-\\frac{(b-1.3)^{2}}{2 \\cdot 0.1^{2}}} \\end{equation}\\] Notice we are ignoring the terms \\(\\displaystyle \\left( \\frac{1}{\\sqrt{2 \\pi}}\\right)^{4}\\) and \\(\\displaystyle \\frac{1}{\\sqrt{2 \\pi} \\cdot 0.1}\\), because per our discussion above not including them the does not change the location of the optimum value, only the value of the likelihood function. as stated above, hence the approximately equals (\\(\\approx\\)) in the last expression. The plot of \\(\\mbox{Pr}(b | \\vec{y})\\), assuming \\(\\sigma = 1\\) is shown in Figure 10.4: Figure 10.4: Posterior Probilities with Bayes Rule It looks like the value that optimizies our posterior probability is \\(b=\\) 1.45. This is very close to the value of \\(\\tilde{b}\\) from the cost function approach. Again, this is no coincidence. Adding in prior information to the cost function or using Bayes’ Rule are equivalent approaches. Now that we have seen the usefulness of cost functions and Bayes’ Rule we can begin to apply this to larger problems involving more equations and data. In order to do that we need to explore some computational methods to scale this problem up - which we will do so in the next sections. "],["exercises-9.html", "10.6 Exercises", " 10.6 Exercises Exercise 10.1 The following problem works with fitting \\(y=bx\\) as in this section: Exercise 10.2 Consider the nutrient equation \\(\\displaystyle y = c x^{1/\\theta}\\) using the dataset phosphorous. Exercise 10.3 From the previous exercise, use the cost function \\(S(1.737,\\theta)\\): Exercise 10.4 Navigate to this desmos file, which you will use to answer the following questions: Exercise 10.5 One way to generalize the notion of prior information using cost functions is to include a term that represents the degree of uncertainty in the prior information, such as \\(\\sigma\\). For the problem \\(y=bx\\) this leads to the following cost function: \\(\\displaystyle \\tilde{S}_{revised}(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 + \\frac{(b-1.3)^2}{\\sigma^{2}}\\). Use calculus to determine the optimum value for \\(\\tilde{S}_{revised}(b)\\), expressed in terms of \\(\\tilde{b}_{revised} = f(\\sigma)\\) (your optimum value will be a function of \\(\\sigma\\)). What happens to \\(\\tilde{b}_{revised}\\) as \\(\\sigma \\rightarrow \\infty\\)? Exercise 10.6 For this problem you will minimize some generic functions. Exercise 10.7 This problem continues the re-election of the President and viewpoint on the economy. Determine the following conditional probabilities: Determine the probability that you voted for the president given that you have a pessimistic view on the economy. Determine the probability that you did not vote for the president given that you have an pessimistic view on the economy. Determine the probability that you did not vote for the president given that you have an optimistic view on the economy. Exercise 10.8 Incumbents have an advantage in re-election due to wider name recognition, which may boost their re-election chances. Complete the following table, estimating the following probabilities. Please report percentages as decimals. Probability Being elected for office Not being elected for office Total Having name recognition 0.55 0.25 0.80 Not having name recognition 0.05 0.15 0.20 Total 0.60 0.40 1.00 Use Bayes’ Rule to determine the probability of being elected, given that you have name recognition. Exercise 10.9 Show how you can derive Bayes’ Rule from the law of conditional probability. "],["bootstrap-11.html", "Chapter 11 The bootstrap method", " Chapter 11 The bootstrap method In Sections and 9 and 10 we saw how the parameter estimation problem is related to optimizing the likelihood or cost function. Figure 11.1 shows the cost function for the nutrient equation \\(\\displaystyle y = c x^{1/\\theta}\\) using the dataset phosphorous: # Can we do this with compute likelihood? my_model &lt;- daphnia ~ 1.737*algae^(1/theta) # This allows for all the possible combinations of parameters parameters &lt;- tibble(theta = seq(1,25,length.out=200)) out_values &lt;- compute_likelihood(my_model, phosphorous, parameters,logLikely = TRUE) out_values$likelihood %&gt;% ggplot(aes(theta,y=l_hood)) + geom_line() + labs(x=expression(theta),y=expression(S(theta))) Figure 11.1: Nonlinear cost function plot for phosphorous data set. While Figure 11.1 shows a clearly defined minimum around \\(\\theta \\approx 6\\), the shape of the cost function is not quadratic (like Figure 10.3). For cases like these direct optimization of the cost function using techniques learned in calculus are not computationally feasible. We need to efficiently examine different combinations of parameters, their model output, and to get a result of the sample. In this section we are going to explore another way to approach this problem. An alternative approach uses the idea of random number generation and sampling, which can efficiently determine the the minimum through direct evaluation. To understand the idea of sampling we will study an approach called bootstrapping. "],["plotting-histograms-in-r.html", "11.1 Plotting histograms in R", " 11.1 Plotting histograms in R In Chapter ?? we discussed probability distributions. Here we are going to discuss them a little more, but now we will first discuss plotting histograms in R. A quick recap of a histogram: this is a binned plot of data, where there are some predefined bins and we count the number of observations in each bin. One way we can do this for (smaller) data is a dot plot, where this is a dot in each observation. Consider the dataset of snowfall observations from weatherstations in Minnesota shown in Table ??, with the following table. knitr::kable(snowfall, caption = &quot;Weather station data from a Minnesota snowstorm.&quot;) Table 11.1: Weather station data from a Minnesota snowstorm. date time station_id station_name snowfall 4/16/18 5:00 AM MN-HN-78 Richfield 1.9 WNW 22.0 4/16/18 7:00 AM MN-HN-9 Minneapolis 3.0 NNW 19.0 4/16/18 7:00 AM MN-HN-14 Minnetrista 1.5 SSE 12.5 4/16/18 7:00 AM MN-HN-30 Plymouth 2.4 ENE 18.5 4/16/18 7:00 AM MN-HN-58 Champlin 1.5 ESE (118) 20.0 4/16/18 7:00 AM MN-HN-89 Edina 1.7 N 11.0 4/16/18 7:00 AM MN-HN-110 Edina 1.9 SSE 15.5 4/16/18 7:00 AM MN-HN-134 Brooklyn Center 1.1 E 13.5 4/16/18 7:00 AM MN-HN-150 Maple Grove 1.8 NE 22.0 4/16/18 8:00 AM MN-HN-17 Eden Prairie 3.3 WSW 16.0 4/16/18 8:00 AM MN-HN-72 Maple Grove 2.9 NE 13.0 4/16/18 8:00 AM MN-HN-175 Bloomington 2.0 SE 13.1 4/16/18 8:30 AM MN-HN-19 Edina 1.3 SW 11.0 4/16/18 8:30 AM MN-HN-31 Maple Grove 1.0 NNE 19.5 4/16/18 6:00 PM MN-HN-215 Richfield 1.4 W 18.0 4/16/18 10:30 PM MN-HN-5 New Hope 1.9 S 13.0 A histogram is an easy way to view the distribution of measurements. Doing a histogram in R is easy to do: ggplot(data = snowfall) + geom_histogram(aes(x = snowfall),) + labs(x=&#39;Snowfall amount&#39;, y=&#39;Number of observations&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This code introduces geom_histogram. Notice , which has two key inputs: The code aes(x = snowfall) is computing the histogram for the snowfall column in the dataset snowfall. You may have received a warning about the bins stat_bin()` using `bins = 30`. Pick better value with `binwidth, so let’s adjust the number of bins to 4: ggplot() + geom_histogram(data = snowfall, aes(x = snowfall),bins = 4) + labs(x=&#39;Snowfall amount&#39;, y=&#39;Number of observations&#39;) Your code may look blockier, but that is ok. "],["statistical-theory-sampling-distributions.html", "11.2 Statistical theory: Sampling distributions", " 11.2 Statistical theory: Sampling distributions To understand the process of sampling, let’s take a look at an example. Figure ?? is a histogram of 50 random samples of the standard normal distribution (\\(\\mu=0\\) and \\(\\sigma = 1\\)): my_data &lt;- tibble(samples = rnorm(50)) ggplot() + geom_histogram(data = my_data, aes(x = samples),bins = 10) + labs(x=&#39;Samples&#39;, y=&#39;N&#39;) This histogram looks like a normal distribution, but since we only have 50 samples that may not be enough data to adequately justify that conclusion, especially in a statistical sense. Now let’s see what happens if we increase the number of samples by factors of 10: Figure 11.2: More random samples of the normal distirubtion. Figure 11.2 also shows the true distribution shaded in red - clearly as the number of data points increases the more the random sample approaches the true distribution. This is the concept underlying statistical inference: process of drawing conclusions about the entire population based on a sample. When we discuss likelihood and cost functions, we assume that the parameters follow some underlying probability distribution (such as normal, uniform, etc). However sometimes if the model is extremely nonlinear, the cost function ends up being non-linear. However by sampling the distribution we can characterize the distribution. "],["sampling-the-empirical-distirbution.html", "11.3 Sampling the empirical distirbution", " 11.3 Sampling the empirical distirbution We can apply this same principle to measurements. The distribution of measurements can be called an empirical distribution. To examine data, we will use weather and precipitation today, which is collected in a cooperative network of local observers. These stations come from the Twin Cities of Minneapolis and St. Paul and surrounding suburbs: date time station_id station_name precip 9/21/2018 05:00:00 MN-HN-52 Champlin 1.8 ESE 2.73 9/21/2018 05:00:00 MN-HN-78 Richfield 1.9 WNW 4.27 9/21/2018 05:00:00 MN-HN-154 Dayton 3.2 SE 2.25 9/21/2018 05:00:00 MN-HN-156 Minnetonka 1.8 SW 3.41 9/21/2018 05:15:00 MN-HN-148 Maple Grove 4.0 SW 2.21 9/21/2018 06:00:00 MN-HN-37 Plymouth 0.8 SSE 2.70 9/21/2018 06:00:00 MN-HN-39 Rockford 0.5 NE 2.55 9/21/2018 06:00:00 MN-HN-49 Maple Grove 2.4 E 2.56 9/21/2018 06:00:00 MN-HN-185 Long Lake 1.7 NNW 2.05 9/21/2018 06:03:00 MN-HN-220 Minnetrista 3.9 SW 1.73 While it is great to have only one value is reported for precipitation amounts, so the question is: What could be a representative number for the average amount of rainfall received for this storm? Let \\(R\\) be the distribution of rainfall in the Twin Cities. The measurements are samples of this distribution. One way to say is that the average of the precipitation data (2.8872727 inches) is good enough, but what we don’t know is how well that average value approximates the expected value of the distribution for \\(R\\). As an exploratory data analysis approach, one way we can do this is by a histogram of the precipitation measurements: ggplot() + geom_histogram(data = precipitation, aes(x = precip),bins = 4) + labs(x=&#39;Precipitation amount&#39;, y=&#39;Number of observations&#39;) ## Warning: Removed 1 rows containing non-finite values (stat_bin). The precipitation measurement illustrates the difference between a population (the true distribution of measurements) and a sample (what people observe). The histogram shown is called the empirical distribution of these data. For the purposes we get here, we are after what is called the sample statistic, which is usually the mean or standard deviation. This distribution looks bimodal with a lot of variability. How could we account for the representative value of the distribution \\(R\\)? Each of the entries in the precipitation data frame represents a measurement made by a particular observer. To get the true distribution we would need to add more observers (such as when we sampled the normal distribution), but that isn’t realistic as the event has already passed. The way around this is a bootstrap sample, which is a sample of the original dataset with replacement. Sampling with replacement is the process of remaking a dataset, but you get to reselect from the entire dataset at the same time. This is easily done with the command slice_sample: p_new &lt;- slice_sample(precipitation,prop=1, replace = TRUE) What this code does is sample the precipitation data frame with replacement (replace = TRUE). Here is how replacement works: say you have each of the precipitation measurements written on a piece of paper in a hat. You draw one slip of paper, record the measurement, and then replace that slip of paper back in the hat to draw again, until you have as many measurements as the original data frame (in this case this is 56. The command prop=1 means that we are sampling 100% of the precipitation data frame. One thing that we can do is compute the mean (average) and the standard deviation of the sample: slice_sample(precipitation,prop=1, replace = TRUE) %&gt;% summarize(mean = mean(precip,na.rm=TRUE), sd = sd(precip,na.rm=TRUE)) ## # A tibble: 1 x 2 ## mean sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.90 0.794 Let’s break this code down: The command summarize is collapsing the precipitation data frame and computes the mean and the standard deviation sd of the column precip. We have the command na.rm=TRUE to remove any NA values that may affect the computation. How this code runs is first to do the sampling, and then the summary. If we want to run this multiple times we need some more powerful functionality here. The purrr package has the wonderful command map, which allows you to quickly iterate through a list quickly. map_df(1:10 , ~( slice_sample(precipitation,prop=1, replace = TRUE) %&gt;% summarize(mean = mean(precip,na.rm=TRUE), sd = sd(precip,na.rm=TRUE)) ) # Close off the tilde ~ () ) # Close off the map_df ## # A tibble: 10 x 2 ## mean sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.67 0.660 ## 2 2.81 0.702 ## 3 2.74 0.699 ## 4 2.83 0.735 ## 5 2.84 0.735 ## 6 2.86 0.723 ## 7 2.98 0.715 ## 8 2.95 0.716 ## 9 3.03 0.775 ## 10 3.01 0.736 What should be returned is a dataframe with columns mean and sd that represents the mean and standard deviation of each bootstrap sample. The process of randomly sampling a dataset is called bootstraping. Let’s review this code bit by bit. Notice that I’ve written this on multiple lines to aid in reading. The basic structure is map_df(1:N,~(COMMANDS)), where N is the number of times you want to run your code (in this case N=10). The second part ~(COMMANDS) lists the different commands we want to re-run (in this case it is our mean and standard deviation of the data frame). I can appreciate this programming might be a little tricky to understand and follow - don’t worry - the goal is to give you a tool that you can easily adapt to a situation. What I would do in a use-case scenario is to first get a working example (where you compute the mean and standard deviation), and then use the map_df to return your result. The final step would be to visualize the mean and the standard deviation. Let’s re-run our example and then plot: bootstrap_samples &lt;- map_df(1:1000 , ~( slice_sample(precipitation,prop=1, replace = TRUE) %&gt;% summarize(mean = mean(precip,na.rm=TRUE), sd = sd(precip,na.rm=TRUE)) ) # Close off the tilde ~ () ) # Close off the map_df # Now make the histogram plots for the mean and standard deviation: ggplot(bootstrap_samples) + geom_histogram(aes(x=mean)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(bootstrap_samples) + geom_histogram(aes(x=sd)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Excellent! This is shaping up nicely. Once we have sampled as much we want, then investigate the distribution of the computed sample statistics (we call this the sampling distribution). It turns out that sample statistics (such as the mean or the standard deviation) will in the long run - approximate the true distribution statistic (such as the mean or standard deviation). This is an example of a non-parametric bootstrap - meaning we are not trying to force a priori a distribution onto the data. One last task: summarize the distribution of bootstrap means and standard deviations. We will do this using a confidence interval, which comes from the percentiles of the distribution. Let’s take a look at the distribution of bootstrap means. The 2.5 percentile is approximately 2.7 inches. This means 2.5% of the distribution is at 2.7 inches or less. The median (50th percentile) is 2.9, so half of the distribution for is 2.9 inches or less. The 97.5 percentile is approximately 3.1, so 97.5% of the distribution is 3.1 inches or less. If we take the difference between 2.5% and 97.5% that is 95%, so 95% of the distribution is contained between 2.7 and 3.1 inches. If we are using the bootstrap mean, we would report that the median rainfall is 2.9 with a 95% confidence interval of 2.7 to 3.1. The confidence interval to give some indication of the uncertainty in the measurements. Here is how we would compute these different statistics using the quantile command, which we need to do separately for the mean and standard deviation: quantile(bootstrap_samples$mean,probs=c(0.025,0.5,.975)) ## 2.5% 50% 97.5% ## 2.701441 2.891429 3.080629 quantile(bootstrap_samples$sd,probs=c(0.025,0.5,.975)) ## 2.5% 50% 97.5% ## 0.6174288 0.7288691 0.8228321 Notice how we using the probs=c(0.025,0.5,.975) command to compute the different quantiles - they need to be scaled between 0 and 1. This code works through the bootstrap for 100 samples. "],["bootstapping-with-linear-regression.html", "11.4 Bootstapping with linear regression", " 11.4 Bootstapping with linear regression Hopefully you can see how much more useful the bootstrap is in terms calculating sample statistics. Another neat application of the bootstrap is determining and expands the provenance of the data. A key goal is to get at the population level parameters, rather than the data level parameters. Can we use this as a modeling tool? - YES! In this case, the “population” represent the distribution on possibilities of parameters. A set of measurements is a sample of these parameters. What we can do is a bootstrap sample for the temperature data, fit a quadratic function to each of the new datasets, and then look at the distribution in parameter values. We are going to return to our example of the global temperature dataset from Section 8, and do the following steps: Do a bootstrap sample of the data 100 times. With each sample, fit a quadratic function. Following each fit, examine the histogram of each fitting coefficient. Here is how we can do this in with the command bootstrap from the modelr pacakge: # Define the regression formula regression_formula &lt;- globalTemp ~ 1 + yearSince1880 + I(yearSince1880^2) # Generate the bootstrap samples boot &lt;- modelr::bootstrap(global_temperature, n=100) # You might not need to modify this code as much models &lt;- map(boot$strap, ~ lm( regression_formula, data = .)) tidied &lt;- map_df(models, broom::tidy, .id = &quot;id&quot;) # Make the histogram. We will make a facetted plot ggplot(data=tidied) + geom_histogram(aes(x=estimate)) + facet_wrap(~term,scales=&quot;free&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. There several new elements of this code, so let’s break this down bit by bit: The code boot &lt;- modelr::bootstrap(global_temperature, n=100) creates 100 bootstrap samples of the global_temperature dataset. The list boot has two entries: (1) .id which is a reference (1 to 100) of a particular bootstrap sample and (2) strap which contains the information about the bootstrap sample. The next portion of the code applies the map command (similar to map_df) to first compute the linear regression for each bootstrap sample. The linear regression fits are stored in the data frame models. Finally we extract out the information about parameters using the command tidy from the broom package. The data frame tidied is organized by the bootstrap sample .id and has several columns, but we are going to focus on two of them: estimate, which tells you the numerical value of the coefficient in the column term. Other information about error estimates and statistical significance are included. In our histogram’s horizontal axis we want to plot the value of the quantitative variable estimate. You should see an interesting plot here. We are facetting the histograms by the coefficients of the histogram (hence the term facet_wrap(~term,scales=\"free\"), which says “plot a histogram for each variable in the column term.” We use scales=\"free\" because each coefficient has a different range of values. (You can see the difference if you remove scales=\"free\" from the plotting command.) Figure ?? is an example of a “small multiples” plot - the title of the plot is the value of the coefficient multiplying each term: Small multiple title Coefficient of \\(T=a + bY + cY^{2}\\) Intercept \\(a\\) yearSince1880 \\(b\\) I(yearSince1880^2) \\(c\\) Notice that with the bootstrap we can get information about the distribution of the estimated parameters, which includes the median and the 95% confidence interval. This is super useful in reporting results from parameter estimates. The idea of sampling with replacement, generating a parameter estimate, and then repeating over several iterations is at the heart of many computational parameter estimation methods. Such an approach helps make non-linear problems more tractable. While we did a bootstrap parameter estimate on a quadratic function, this also works for a log transformation of a dataset, as with the phosphorous dataset of algae and daphnia we have studied previously. You will investigate this in one of the homework exercises. This section extended your abilities in R by showing you how to generate histograms, sample a dataset, and compute statistics. The goal here is to give you examples that you can re-use in this section’s exercises. Enjoy! "],["exercises-10.html", "11.5 Exercises", " 11.5 Exercises Histograms are an important visualization tool in descriptive statistics. Read the following essays on histograms, and then summarize 2-3 important points of what you learned reading these articles. - (http://tinlizzie.org/histograms/)[Visualizing histograms] - (https://flowingdata.com/2017/06/07/how-histograms-work/)[How histograms work] - (https://flowingdata.com/2014/02/27/how-to-read-histograms-and-use-them-in-r/)[How to read histograms and use them in R] Exercise 11.1 Average snow cover from 1970 - 1979 in October over Eurasia (in million km\\(^{2}\\)) were reported as the following: \\[\\begin{equation*} \\{6.5, 12.0, 14.9, 10.0, 10.7, 7.9, 21.9, 12.5, 14.5, 9.2\\} \\end{equation*}\\] Exercise 11.2 Consider the equation \\(\\displaystyle S(\\theta)=(3-1.5^{1/\\theta})^{2}\\). This function is an idealized example for the cost function in Figure ??. Exercise 11.3 Repeat the bootstrap sample for the precipitation dataset where the number of bootstrap samples is 1000 and 10000. Report the median and confidence intervals for the mean and the standard deviation of \\(R\\). What do you notice as the number of bootstrap samples increases? Exercise 11.4 Using the data in Exercise 11.1, do a bootstrap sample with \\(N=1000\\) to compute the a bootstrap estimate for the mean and the 95% confidence interval for October snowfall cover in Eurasia. Exercise 11.5 We computed the 95% confidence interval using the quantile command. An alternative approach to summarize a distribution is with the summary command. Here is the output for the summary command for a dataframe: knitr::include_graphics(“figures/11-bootstrap/summary-output-11.png”) We call this command using summary(data_frame), where data_frame is the particular dataframe you want to summarize. The output reports the minimum and maximum values of a dataset. The output 1st Qu. and 3rd Qu. are the 25th and 75th percentiles. Do 1000 bootstrap samples using the data in Exercise 11.1 and report the output from the summary command. Exercise 11.6 The dataset snowfall lists the snowfall data from a snowstorm that came through the Twin Cities on April 14, 2018. Exercise 11.7 This question tackles the dataset to determine plausible models for a relationship between time and average global temperature. For this exercise we are going to look the variability in bootstrap estimates for models up to fourth degree. Using the function , generate a bootstrap sample of \\(n=1000\\) for each of the following functions. Include in your write up the graph of the data with the bootstrap predictions and the prediction from the linear regression model. How does the variability in the parameters change (\\(a,b,c,d,e\\)) as more terms in the model are added? How does the variability in the bootstrap predictions change as more terms in the model are added? Exercise 11.8 Similar to the problems we have worked with before, the equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\(\\displaystyle y = c x^{1/\\theta}\\), where \\(\\theta \\geq 1\\) and \\(c\\) are both constants is a constant. We will be using the dataset phosphorous. "],["the-metropolis-hastings-algorithm.html", "Chapter 12 The Metropolis-Hastings Algorithm", " Chapter 12 The Metropolis-Hastings Algorithm In Sections 9 and 10 we discussed likelihood and cost functions, and Section 11 introduced the idea of sampling. For this section we will combined these concepts to discover a powerful algorithm that can efficient sample a distribution systematically. "],["estimating-the-growth-of-a-dog.html", "12.1 Estimating the growth of a dog", " 12.1 Estimating the growth of a dog The problem we are considering is fitting the following data to a logistic model, adapted from here. We initially plotted these data in Section 2, but here is the code again. wilson_data_plot &lt;- ggplot(data = wilson) + geom_point(aes(x = days, y = mass)) + labs(x=&#39;Days since birth&#39;, y=&#39;Weight (pounds)&#39;) wilson_data_plot Figure 12.1: Weight of the dog Wilson over time. Notice that for Figure 12.1 we stored the plot in the variable wilson_data_plot. We will be using this plot several times here, so storing it will help us save some typing. The function we wish to fit from the data is the following \\[\\begin{equation} W =f(D,p_{1})= \\frac{p_{1}}{1+e^{(p_{2}-p_{3}D)}}, \\end{equation}\\] where we have the parameters \\(p_{1}\\), \\(p_{2}\\), and \\(p_{3}\\). Notice how \\(W\\) is a function of \\(D\\) and \\(p_{1}\\). For convenience we will set \\(p_{2}= 2.461935\\) and \\(p_{3} = 0.017032\\). Now we are only estimating the parameter \\(p_{1}\\) which represents the maximum possible weight of the dog. Let’s take an initial guess for the parameter \\(p_{1}\\). You may recognize that \\(p_{1}\\) is the horizontal asymptote of this the function \\(W\\). So at first glance let’s set \\(p_{1}=78\\). Let’s plot that result along with the data: days &lt;- seq(0,1500,by=1) p1 = 78 p2 = 2.461935 p3 = 0.017032 mass &lt;- p1/(1+exp(p2-p3*days)) my_guess &lt;- tibble(days,mass) my_guess_plot &lt;- wilson_data_plot + geom_line(data=my_guess,color=&#39;red&#39;,aes(x=days,y=mass)) my_guess_plot Figure 12.2: Weight of the dog Wilson with initial guess \\(p_{1}=78\\). As we did with Figure 12.1, we are going to store the updated plot as a variable. It seems that this value of \\(p_{1}\\) does a good job capturing the initial rate of growth initially but perhaps predicts too high of a mass towards the end. For comparison let’s also examine the plot of the data when \\(p_{1}=65\\): days &lt;- seq(0,1500,by=1) p1 = 65 p2 = -2.461935 p3 = 0.017032 mass &lt;- p1/(1+exp(p2-p3*days)) my_guess_two &lt;- tibble(days,mass) my_guess_plot + geom_line(data=my_guess_two,color=&#39;blue&#39;,aes(x=days,y=mass)) Figure 12.3: Weight of the dog Wilson with initial guess \\(p_{1}=78\\) (red) and \\(p_{1}=65\\) (blue). "],["applying-the-likelihood-to-evaluate-parameters.html", "12.2 Applying the likelihood to evaluate parameters", " 12.2 Applying the likelihood to evaluate parameters So with \\(p_{1}=65\\) the long-term trend looks lower and underestimates the long term growth. However is this value more plausible? We have nineteen measurements of the dog’s weight over time. Assuming these measures are all independent and identically distributed, we have the following likelihood function: \\[\\begin{equation} L(p_{1}) = \\prod_{i=1}^{19} \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(W_{i}-f(D_{i},p_{1}))^{2}}{2 \\sigma^{2}}} \\end{equation}\\] We can easily compute the associated likelihood values with the function compute_likelihood: # Define the model we are using my_model &lt;- mass ~ p1/(1+exp(p2-p3*days)) # This allows for all the possible combinations of parameters parameters &lt;- tibble(p1 = c(78,65), p2 = 2.461935, p3 = 0.017032) # Compute the likelihood and return the likelihood from the list out_values &lt;- compute_likelihood(my_model,wilson,parameters)$likelihood # Return the likelihood from the list: out_values ## # A tibble: 2 x 5 ## p1 p2 p3 l_hood log_lik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 78 2.46 0.0170 6.86e-29 FALSE ## 2 65 2.46 0.0170 1.20e-27 FALSE Hopefully this code seems familiar to you from Section 9. We made some modifications to streamline things: We want to compare two values of p1, so when we defined parameters we included the two values of \\(p_{1}\\) when defining parameters. The same values of p2 and p3 will apply to both. Don’t believe me? Type parameters at the console line to see! Recall that when we apply compute_likelihood a list is returned (likelihood and opt_value . For this case we just want the likelihood data frame, hence the $likelihood at the end of compute_likelihood. So we computed \\(L(78)\\)=6.8560765^{-29} and \\(L(65)\\)=1.2038829^{-27}. As you can see the value of \\(L(65)\\) is a larger number compared to \\(p_{1}=78\\). One way we can get a sense for the magnitude of the scale is by examining the ratio of the likelihoods: \\[\\begin{equation} \\mbox{ Likelihood ratio: } \\frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) }, \\end{equation}\\] So let’s compute this ratio: \\(\\displaystyle \\frac{ L(65) }{ L(78) }=\\)round(out_values$l_hood[[2]]/out_values$l_hood[[1]]). So with this comparison we would say that \\(p_{1}=65\\) is more likely compared to the value of \\(p_{1}=78\\). Can we do better? What we can do is examine the likelihood ratio for another value. Since we might need to iterate through things let’s define a function that we can reuse: # Define a function that computes the likelihood ratio for Wilson&#39;s weight likelihood_ratio_wilson &lt;- function(proposed,current) { # Define the model we are using my_model &lt;- mass ~ p1/(1+exp(p2-p3*days)) # This allows for all the possible combinations of parameters parameters &lt;- tibble(p1 = c(current,proposed), p2 = 2.461935, p3 = 0.017032) # Compute the likelihood and return the likelihood from the list out_values &lt;- compute_likelihood(my_model,wilson,parameters)$likelihood # Return the likelihood from the list: ll_ratio &lt;- out_values$l_hood[[2]]/out_values$l_hood[[1]] return(ll_ratio) } # Test the function out: likelihood_ratio_wilson(65,78) ## [1] 17.55936 You should notice that the reported likelihood ratio matches up with our computation - yes! So let’s try to compute the likelihood ratio for \\(p_{1}=70\\) compared to \\(p_{1}=65\\). Try computing likelihood_ratio_wilson(70,65) - you should see that it is about 7.5 million times more likely! I think we are onto something - Figure 12.4 compares the modeled values of Wilson’s weight for the different parameters: Figure 12.4: Comparison of our three estimates for Wilson’s weight over time. So now, let’s try \\(p_{1}=74\\) and compare the likelihoods: \\(\\displaystyle \\frac{ L(74) }{ L(70) }\\)=4.5897793^{-9}. This seems to be less likely because the ratio was significantly less than one. If we are doing a hunt for the best optimum value, then we would reject \\(p_{1}=74\\) and keep moving on, perhaps selecting another value closer to 70. However, the reality is a little different. For non-linear problems we want to be extra careful that we do not accept a parameter value that leads us to a local (not global) optimum. A way to avoid this is compare the likelihood ratio to a uniform random number \\(r\\) between 0 and 1. We can do this very easily in R - remember the discussion of probability models in Section 9? At the R console type runif(1) - this creates one random number from the uniform distribution (remember the default range of the uniform distribution is \\(0 \\leq p_{1} \\leq 1\\)). The r in runif(1) stands for random. When I tried runif(1) I received a value of 0.126. Since the likelihood ratio is smaller than the random number we generated, we will reject the value of \\(p_{1}\\) and try again, keeping 70 as our value. The process keep the proposed value based on some decision metric is called a decision step. 12.2.1 An iterative method to estimate parameters The neat part of the previous discussion is that we can automate this process, such as in Table . \\begin{table} Based on the evidence from your table, what would you say is the value of \\(\\displaystyle \\lim_{t \\rightarrow 0^{+}} \\int_{-1}^{1} p(x,t) \\; dx\\)? Now make graphs of \\(p(x,t)\\) at each of the values of \\(t\\) in your table. What would you say is occuring in the graph as \\(\\displaystyle \\lim_{t \\rightarrow 0^{+}} p(x,t)\\)? Does anything surprise you? (The results you computed here lead to the foundation of what is called the Dirac delta function.) \\end{enumerate} Exercise 12.1 Consider the function \\(\\displaystyle p(x,t) = \\frac{1}{\\sqrt{4 \\pi D t}} e^{-x^{2}/(4 D t)}\\). Let \\(x=1\\). Exercise 12.2 Consider the function \\(\\displaystyle p(x,t) = \\frac{1}{\\sqrt{\\pi t}} e^{-x^{2}/t}\\): Exercise 12.3 For the one-dimensional random walk we discussed where there was an equal chance of moving to the left or the right. Here is a variation on this problem. Let’s assume there is a chance \\(v\\) that it moves to the left (position \\(x - \\Delta x\\)), and therefore a chance is \\(1-v\\) that the particle remains at position \\(x\\). The basic equation that describes the particle’s position at position \\(x\\) and time \\(t + \\Delta t\\) is: \\[\\begin{equation} p(x,t + \\Delta t) = (1-v) \\cdot p(x,t) + v \\cdot p(x- \\Delta x,t) \\end{equation}\\] Apply the techniques of local linearization in \\(x\\) and \\(t\\) to show that this random walk to derive the following partial differential equation, called the : \\[\\begin{equation} p_{t} = - \\left( v \\cdot \\frac{ \\Delta x}{\\Delta t} \\right) \\cdot p_{x} \\end{equation}\\] "],["sdes-25.html", "Chapter 13 Stochastic Differential Equations", " Chapter 13 Stochastic Differential Equations In this section we will begin to combine our knowledge of random walks to numerically simulate stochastic differential equations, or SDEs for short. Here is the good news: much of the content from the previous sections comes into focus here. We are going to focus on a model that you know (a logisitic differential equation) to develop general principles for working with other SDEs. "],["the-stochastic-logistic-model.html", "13.1 The stochastic logistic model", " 13.1 The stochastic logistic model In Section ?? we started to write down the format of a stochastic differential equation, which we will use the logistic equation for context: \\[\\begin{equation} dx = rx \\left(1 - \\frac{x}{K} \\right) \\; dt + \\mbox{ Noise } \\; dt \\tag{13.1} \\end{equation}\\] It is helpful to identify the two different parts of this equation. The first part is called the deterministic part, and it does not involve the ``Noise’’ term: \\(\\displaystyle rx \\left(1 - \\frac{x}{K} \\right) \\; dt\\). The second part is called the stochastic part and is (I bet you guessed it!) the term that contains \\(\\mbox{ Noise } \\; dt\\) is the “stochastic part.” While just writing \\(\\mbox{ Noise } \\; dt\\) doesn’t seem mathematical, its just a substitute for the type of stochastic process we have. For our purposes we are only going to consider random walks (a.k.a white noise a.k.a. Weiner processes), which we represent in shorthand with \\(dW(t)\\), so that \\(dW(t)=\\mbox{ Noise } \\; dt\\): \\[\\begin{equation} dx = rx \\left(1 - \\frac{x}{K} \\right) \\; dt + dW(t) \\tag{13.2} \\end{equation}\\] Just to be clear, here the term \\(dW(t)\\) is short hand to the differential equation \\(\\displaystyle \\frac{dW}{dt} = \\mbox{ Noise }\\), where \\(W(t)\\) is the solution to a Weiner process. This white noise has the following characteristics: \\(W(t)\\) is continuous \\(W(0)=0\\) \\(W(t)-W(s)\\) independent (they call this independent increments) \\(W(t)-W(s)\\) is normally distributed with mean 0 and standard deviation \\(\\sqrt{t-s}\\). It does seem odd to write a differential equation in this form (i.e. \\(dx = ...\\) versus \\(\\displaystyle \\frac{dx}{dt} = ...\\)). but a good way to think of this stochastic differential equation is that a small change in the variable \\(x\\) (represented by the term \\(dx\\)) is computed in two ways: \\[\\begin{equation*} \\begin{split} \\mbox{Deterministic part: } &amp; rx \\left(1 - \\frac{x}{K} \\right) \\; dt \\\\ \\mbox{Stochastic part: } &amp; dW(t) \\end{split} \\end{equation*}\\] Similar to non-stochastic (or deterministic) differential equations we are curious about equilibrium solutions. As a starting point, recall the equilibrium solutions to the logistic equation are \\(x=0\\) (unstable) and \\(x=K\\) (stable). How does the stochastic part of this differential equation change the solution trajectory? It turns out that the ``exact’’ solutions to problems like these are difficult (we will study a sample of them in 15). Rather than focus on an exact solution technique we are going to focus on how to apply a numerical method to simulate solution trajectories and then take the ensemble average each of the time points. "],["the-euler-maruyama-method.html", "13.2 The Euler-Maruyama Method", " 13.2 The Euler-Maruyama Method The Euler-Maruyama method is a variation of Euler’s method that accounts for stochasticity and implements the random walk. We are going to build this up step by step. First we write the \\(dx\\) term as a difference equation: \\(dx = x_{n+1}-x_{n}\\), where \\(n\\) is the current step of Euler’s method. Likewise \\(dW(t) = W_{n+1} - W_{n}\\). This terms represents one step of the random walk. We are going to model this difference \\(W_{n+1} - W_{n}\\) as a random walk, which we will approximate \\(W_{n+1}-W_{n} = \\sqrt{\\Delta t} Z_{1}\\), where \\(Z_{1}\\) is a random number from the standard unit normal distribution and \\(\\Delta t\\) is the timestep length. We do this in R as rnorm(1). When we put these together we have the following iterative method: Given \\(\\Delta t\\) and starting value \\(x_{0}\\). Then compute the next step: \\(\\displaystyle x_{1} = x_{0} + rx_{0} \\left(1 - \\frac{x_{0}}{K} \\right) \\; \\Delta t + \\sqrt{\\Delta t} Z_{1}\\), where \\(Z_{1}\\) is a random number from the standard unit normal distribution. (rnorm(1) in R) Repeat to step \\(n\\): \\(\\displaystyle x_{1} = x_{0} + rx_{0} \\left(1 - \\frac{x_{0}}{K} \\right) \\; \\Delta t + \\sqrt{\\Delta t} Z_{1}\\) That is it! We can run this through as many steps as we want. I have some defined code that will apply the simulation, but just like euler or systems there are some things that need to be set first. In order to apply the Euler-Maruyama method you will need to define six things: The size (\\(\\Delta t\\)) of your timesteps. The number of timesteps you wish to run the method. More timesteps means more computational time. If \\(N\\) is the number of timesteps, \\(\\Delta t \\cdot N\\) is the total time. A function that we have for our deterministic dynamics. For our example this equals \\(\\displaystyle rx \\left(1 - \\frac{x}{K} \\right)\\). A function that we have for our stochastic dynamics. For our example this equals 1. The values of the vector of parameters \\(\\vec{\\alpha}\\). For the logistic differential equation we will take \\(r=0.8\\) and \\(K=100\\). The standard deviation (\\(\\sigma\\)) of our normal distribution and random walk. Typically this is set to 1, but can be varied if needed. Sample code for this stochastic differential equation is shown below, with the resulting trajectory of the solution in Figure 13.1. # Identify the deterministic and stochastic parts of the DE: deterministic_logistic &lt;- c(dx ~ r*x*(1-x/K)) stochastic_logistic &lt;- c(dx ~ 1) # Identify the initial condition and any parameters init_logistic &lt;- c(x=3) # Be sure you have enough conditions as you do variables. logistic_parameters &lt;- c(r=0.8, K=100) # parameters: a named vector # Identify how long we run the simulation deltaT_log &lt;- .05 # timestep length timeSteps_log &lt;- 200 # must be a number greater than 1 # Identify the standard deviation of the stochastic noise sigma_log &lt;- 1 # Do one simulation of this differential equation logistic_out &lt;- euler_stochastic(deterministic_rate = deterministic_logistic, stochastic_rate = stochastic_logistic, init_cond = init_logistic, parameters = logistic_parameters, deltaT = deltaT_log, n_steps = timeSteps_log, sigma = sigma_log) # Plot out the solution ggplot(data = logistic_out) + geom_line(aes(x=t,y=x)) Figure 13.1: One realization of Equation (13.1) Let’s break this code down step by step: We identify the deterministic and stochastic parts to our differential equation with the variables deterministic_logistic and stochastic_logistic. The same structure is used for Euler’s method from Section 4 Similar with Euler’s method we need to identify the initial conditions (init_logistic), parameters (logistic_parameters), \\(\\Delta t\\) (deltaT_log), and number of timesteps (timeSteps_log). The standard deviation of the stochastic noise (\\(\\sigma\\)) is represented with sigma_log. The command euler_stochastic does one realization of the Euler-Maruyama method, returning a vector that we can then plot. Figure 13.1 shows one sample trajectory of our solution. If you re-run this code several times and replot the solution you may see different trajectories plotted. 13.2.1 Multiple simulations In Section ?? we discussed the idea of stochastic simulation and how to re-run code several times and compute the ensemble average. We can put those ideas to good use here: # Many solutions n_sims &lt;- 100 # The number of simulations # Compute solutions logistic_run &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ euler_stochastic(deterministic_rate = deterministic_logistic, stochastic_rate = stochastic_logistic, init_cond = init_logistic, parameters = logistic_parameters, deltaT = deltaT_log, n_steps = timeSteps_log, sigma = sigma_log)) %&gt;% map_dfr(~ .x, .id = &quot;simulation&quot;) # Plot these all up together ggplot(data = logistic_run) + geom_line(aes(x=t, y=x, color = simulation)) + ggtitle(&quot;Spaghetti plot for the logistic SDE&quot;) + guides(color=&quot;none&quot;) Figure 13.2: Several different realizations of the logistic SDE. As you may recall, the main engine of the code is contained in the map( ~ euler_stochastic ... ) which re-runs the codes for the number of times specified in n_sims. Figure 13.3 shows the ensemble average (median and 95% confidence interval) for the different simulations, recycling code from Section ??. ### Summarize the variables summarized_logistic &lt;- logistic_run %&gt;% group_by(t) %&gt;% # All simulations will be grouped at the same timepoint. summarise(x = quantile(x, c(0.25, 0.5, 0.75)), q = c(&quot;q0.025&quot;, &quot;q0.5&quot;, &quot;q0.975&quot;)) %&gt;% pivot_wider(names_from = q, values_from = x) ## `summarise()` has grouped output by &#39;t&#39;. You can override using the `.groups` argument. ### Make the plot ggplot(data = summarized_logistic) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x=t,ymin=q0.025,ymax=q0.975),alpha=0.2) + ggtitle(&quot;Ensemble average plot for the logistic SDE&quot;) Figure 13.3: Ensemble average plot of the stochastic logistic SDE. Breaking this code down, the variable summarized_logistic first groups the simulations by the variable t in order to compute the quantiles across each of the simulations. We then pivot resulting data frame in order to plot the ribbon. "],["adding-stochasticity-to-parameters.html", "13.3 Adding stochasticity to parameters", " 13.3 Adding stochasticity to parameters Now that we have seen an example in adding stochasticity to the logistic equation, we can also parameters of the differential equation to be stochastic. For example, let’s say that the growth rate \\(r\\) in the logistic equation is subject to stochastic effects. How we would implement this by replacing \\(r\\) with \\(r + \\mbox{ Noise }\\): \\[\\begin{equation} dx = (r + \\mbox{ Noise } )x \\left(1 - \\frac{x}{K} \\right) \\; dt, \\end{equation}\\] Now what we do is separate out the terms that are multiplied by “Noise” - they will form the stochastic part of the differential equation. The terms that aren’t multipled by “Noise” form the deterministic part of the differential equation: \\[\\begin{equation} dx = r x \\left(1 - \\frac{x}{K} \\right) \\; dt + x \\left(1 - \\frac{x}{K} \\right) \\mbox{ Noise } \\; dt, \\end{equation}\\] So we have the following, writing \\(\\mbox{ Noise } \\; dt = dW(t)\\): \\[\\begin{align*} \\mbox{Deterministic part: } &amp; rx \\left(1 - \\frac{x}{K} \\right) \\; dt \\\\ \\mbox{Stochastic part: } &amp; x \\left(1 - \\frac{x}{K} \\right) dW(t) \\end{align*}\\] There are a few things to notice here. First, the deterministic part of the differential equation is what we would expect without noise added. Second, notice how the stochastic part of the differential equation changed. Let’s take a look at the simulations with the Euler-Maruyama method, denoting the deterministic and stochastic parts as deterministic_logistic_r and stochastic_logistic_r respectively: # Identify the deterministic and stochastic parts of the DE: deterministic_logistic_r &lt;- c(dx ~ r*x*(1-x/K)) stochastic_logistic_r &lt;- c(dx ~ x*(1-x/K)) # Identify the initial condition and any parameters init_logistic &lt;- c(x=3) # Be sure you have enough conditions as you do variables. logistic_parameters &lt;- c(r=0.8, K=100) # parameters: a named vector # Identify how long we run the simulation deltaT_log &lt;- .05 # timestep length timeSteps_log &lt;- 200 # must be a number greater than 1 # Identify the standard deviation of the stochastic noise sigma_log &lt;- 1 # Do one simulation of this differential equation logistic_out &lt;- euler_stochastic(deterministic_rate = deterministic_logistic_r, stochastic_rate = stochastic_logistic_r, init_cond = init_logistic, parameters = logistic_parameters, deltaT = deltaT_log, n_steps = timeSteps_log, sigma = sigma_log) # Plot out the solution ggplot(data = logistic_out) + geom_line(aes(x=t,y=x)) Figure 13.4: One realization of the logistic differential equation with stochasticity in the parameter r. As we did before, we can run multiple iterations of this stochastic process. We will also compute and plot the ensemble average in Figure ??. # Many solutions n_sims &lt;- 100 # The number of simulations # Compute solutions logistic_run_r &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ euler_stochastic(deterministic_rate = deterministic_logistic_r, stochastic_rate = stochastic_logistic_r, init_cond = init_logistic, parameters = logistic_parameters, deltaT = deltaT_log, n_steps = timeSteps_log, sigma = sigma_log) ) %&gt;% map_dfr(~ .x, .id = &quot;simulation&quot;) # Plot these all up together ggplot(data = logistic_run_r) + geom_line(aes(x=t, y=x, color = simulation)) + ggtitle(&quot;Spaghetti plot for the logistic SDE&quot;) + guides(color=&quot;none&quot;) Figure 13.5: Several different realizations of the logistic SDE with stochasticity in the parameter r, along with the ensemble average plot. ### Summarize the variables summarized_logistic_r &lt;- logistic_run_r %&gt;% group_by(t) %&gt;% # All simulations will be grouped at the same timepoint. summarise(x = quantile(x, c(0.25, 0.5, 0.75)), q = c(&quot;q0.025&quot;, &quot;q0.5&quot;, &quot;q0.975&quot;)) %&gt;% pivot_wider(names_from = q, values_from = x) ## `summarise()` has grouped output by &#39;t&#39;. You can override using the `.groups` argument. ### Make the plot ggplot(data = summarized_logistic_r) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x=t,ymin=q0.025,ymax=q0.975),alpha=0.2) + ggtitle(&quot;Ensemble average plot for the logistic SDE&quot;) Figure 13.6: Several different realizations of the logistic SDE with stochasticity in the parameter r, along with the ensemble average plot. Wow! Adding in stochasticity to the parameters changes things. Notice how the dynamics are different - there is a lot more variability in how quickly the solution rises to the steady state of \\(K=100\\). "],["concluding-thoughts-1.html", "13.4 Concluding thoughts", " 13.4 Concluding thoughts If you start with a known differential equation and want to add stochasticity to a parameter, here is a process: Replace whatever parameter with a “parameter + Noise” term (i.e \\(a \\rightarrow a + \\mbox{ Noise }\\)). Collect terms multiplied by Noise - they will form the stochastic part of the differential equation. The deterministic part of the differential equation should be your original differential equation. The most general form of the stochastic differential equation is: \\(\\displaystyle d\\vec{y} = f(\\vec{y},\\vec{\\alpha},t) \\; dt + g(\\vec{y},\\vec{\\alpha},t) \\; dW(t)\\), where \\(\\vec{y}\\) is the vector of state variables you want to solve for, and \\(\\vec{\\alpha}\\) is your vector of parameters, and \\(dW(t)\\) is the stochastic noise from the random walk. At a given initial condition, the Euler-Maruyama method applies locally linear approximations to forecast the solution forward \\(\\Delta t\\) time units: \\(\\displaystyle \\vec{y}_{n+1} = y_{n} + f(\\vec{y}_{n},\\vec{\\alpha},t_{n}) \\cdot \\Delta t + g(\\vec{y}_{n},\\vec{\\alpha},t_{n}) \\cdot \\sigma \\cdot \\mbox{rnorm(N)} \\cdot \\sqrt{\\Delta t}\\), where rnorm(N) is \\(N\\) dimensional random variable from a normal distribution with mean 0. "],["exercises-11.html", "13.5 Exercises", " 13.5 Exercises Exercise 13.1 Consider the logistic differential equation (Equation (13.2)). In this section we set \\(\\sigma = 1\\). Re-run the code to generate one simulation with \\(\\sigma = 0.01, \\; 2, \\; 10\\). In each case, how does changing \\(\\sigma\\) affect the simulation run? Exercise 13.2 Consider the logistic differential equation (Equation (13.2)). In this section we set \\(\\sigma = 1\\). Re-run the code to generate 100 simulations with \\(\\sigma = 0.01, \\; 2, \\; 10\\) and then compute the ensemble. In each case, how does changing \\(\\sigma\\) affect the ensemble average? Exercise 13.3 Return back to the example of adding stochasticity to the parameter r in the logistic differential equation (Figure ??). For these plots we set \\(\\sigma = 1\\). What happens to the resulting spaghetti and ensemble plots when \\(\\sigma = 0.01, \\; 0.1, \\; 10\\)? Exercise 13.4 Consider the logistic differential equation: \\(\\displaystyle \\frac{dx}{dt} = r x \\left( 1-\\frac{x}{K} \\right)\\). Assume there is stochasticity in the inverse carrying capacity \\(1/K\\) (so this means you will consider \\(1/K + \\mbox{ Noise }\\)). Figure 13.7: The \\(SIS\\) model Exercise 13.5 An \\(SIS\\) model is one where susceptibles \\(S\\) become infected \\(I\\), and then after recovering from an illness, become susceptible again. The schematic representing this is shown in Figure 13.7. While you can write this as a system of differential equations, assuming the population size is constant \\(N\\), this simplifies to the following differential equation: \\[\\begin{equation} \\frac{dI}{dt} = b(N-I) I - r I \\end{equation}\\] Exercise 13.6 Consider the following Lotka-Volterra (predator prey) model: \\[\\begin{equation} \\begin{split} \\frac{dV}{dt} &amp;= r V - kVP \\\\ \\frac{dP}{dt} &amp;= e k V P - dP \\end{split} \\end{equation}\\] Exercise 13.7 Consider the following model for zombie population dynamics: \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;=-\\beta S Z - \\delta S \\\\ \\frac{dZ}{dt} &amp;= \\beta S Z + \\xi R - \\alpha SZ \\\\ \\frac{dR}{dt} &amp;= \\delta S+ \\alpha SZ - \\xi R \\end{split} \\end{equation}\\] "],["simul-stoch-26.html", "Chapter 14 Simulating Stochastic Dynamics", " Chapter 14 Simulating Stochastic Dynamics In Section 13 we built up a stochastic differential equation from adding in stochasticity (noise) to the parameter values. Another approach is to assume instead the variables are stochastic. Here is the good news: for stochastic variables we will still be able to simulate stochastic equations, setting them up in a similar way (deterministic and stochastic parts). We will generate realizations and then compute the ensemble averages. "],["the-stochastic-logistic-model-redux.html", "14.1 The stochastic logistic model redux", " 14.1 The stochastic logistic model redux Let’s go back to the logistic population model but re-written in a specific way: \\[\\begin{equation} \\frac{dx}{dt} = r x \\left( 1 - \\frac{x}{K} \\right) = r x - \\frac{rx^{2}}{K} \\end{equation}\\] From our differential equation a change in the variable \\(x\\) (denoted as \\(\\Delta x\\)) over \\(\\Delta t\\) units will arise from re-writing the differential equation in differential form \\(\\displaystyle \\Delta x = r x \\Delta t - \\frac{rx^{2}}{K} \\Delta t\\). This equation is separated into two terms - one that increases the variable \\(x\\) (represented by \\(r x \\Delta t\\), same units as \\(x\\)) and one that decreases the variable (represented by \\(\\displaystyle \\frac{rx^{2}}{K} \\Delta t\\), same units as \\(x\\)). We will consider these changes as on a unit scale, organized via the following table: Outcome Probability \\(\\Delta x = 1\\) (population change by 1) \\(r x \\; \\Delta t\\) \\(\\Delta x = -1\\) (population change by -1) \\(\\displaystyle \\frac{rx^{2}}{K} \\; \\Delta t\\) \\(\\Delta x = 0\\) (no population change) \\(\\displaystyle 1 - rx \\; \\Delta t - \\frac{rx^{2}}{K} \\; \\Delta t\\) It also may be helpful to think of these changes on a random walk number line: It may seem odd to think of the different outcomes (\\(\\Delta x\\) equals 1, -1, or 1) as probabilities. Part of the reason why that formulation is useful is to apply concepts from probability theory. Here is one useful result. Let \\(Y\\) be a random variable with a finite number of finite outcomes \\(\\displaystyle y_{1},y_{2},\\ldots ,y_{k}\\) with probabilities \\(\\displaystyle p_{1},p_{2},\\ldots ,p_{k}\\), respectively, then the expected value \\(\\mu\\) of \\(Y\\) is: \\[\\begin{equation*} \\mu = E[Y] = \\sum_{i=1}^{k} y_{i}\\,p_{i}=y_{1}p_{1}+y_{2}p_{2}+\\cdots +y_{k}p_{k}. \\end{equation*}\\] If we apply this definition to the random variable \\(\\Delta x\\) we have: \\[\\begin{align*} \\mu = E[\\Delta x] &amp;= (1) \\cdot \\mbox{Pr}(\\Delta x = 1) + (- 1) \\cdot \\mbox{Pr}(\\Delta x = -1) + (0) \\cdot \\mbox{Pr}(\\Delta x = 0) \\\\ &amp;= (1) \\cdot \\left( r x \\; \\Delta t \\right) + (-1) \\frac{rx^{2}}{K} \\; \\Delta t \\\\ &amp;= r x \\; \\Delta t - \\frac{rx^{2}}{K} \\; \\Delta t \\end{align*}\\] Note how \\(\\mu\\) the formula for \\(\\mu\\) is the same as the right hand side of the original differential equation! Next let’s also calculate the variance of \\(\\Delta x\\), defined for a discrete random variable as \\(\\displaystyle \\sigma^{2} = E[(Y - \\mu)^{2}]\\), or equivalently \\(\\sigma^{2}= E[Y^{2}] - (E[Y] )^{2}\\). \\[\\begin{align*} \\sigma^{2} = E[(\\Delta x)^{2}] - (E[\\Delta x] )^{2} &amp;= (1)^{2} \\cdot \\mbox{Pr}(\\Delta x = 1) + (- 1)^{2} \\cdot \\mbox{Pr}(\\Delta x = -1) + (0)^{2} \\cdot \\mbox{Pr}(\\Delta x = 0) - (E[\\Delta x] )^{2} \\\\ &amp;= (1) \\cdot \\left( r x \\; \\Delta t \\right) + (1) \\frac{rx^{2}}{K} \\; \\Delta t - \\left( r x \\; \\Delta t - \\frac{rx^{2}}{K} \\; \\Delta t \\right)^{2} \\end{align*}\\] Along with the computation of the variance, We are going to compute the variance to first order in \\(\\Delta t\\). Because of that, we are going to assume that in \\(\\sigma^{2}\\) any terms involving \\((\\Delta t)^{2}\\) are small, or in effect negligible. While this is a huge simplifying assumption for the variance, but it is useful! So we have that \\(\\displaystyle \\sigma^{2} = \\left( r x \\; \\Delta t \\right) + \\frac{rx^{2}}{K} \\; \\Delta t\\). Computing the mean and variance will help characterize the ensemble average. In addition the following random walk properties can be applied: Since \\(\\Delta x\\) is the sum of many smaller changes we can apply the central limit theorem to characterize \\(\\Delta x\\) as a normal random variable. To first order, we assume that \\(\\Delta x\\) follows a probability distribution that is normal with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). The distribution for \\(\\Delta x\\) can be expressed as \\(\\Delta x = \\mu + \\sigma Z\\), where Z is random variable from a unit normal distribution (so in R we would use rnorm(1)). We can simulate \\(\\Delta x\\) as a Wiener process. Since \\(\\Delta x = x_{n+1}-x_{n}\\), then we have \\(x_{n+1} = x_{n} + \\mu + \\sigma Z\\). Notice how the last step provides a way to generate a solution trajectory to our differential equation. Cool! To simulate this stochastic process we will use the function birth_death_stochastic, which is set up in a similar way to euler_stochastic from Section 13. I will append _log to denote “logistic” for each of the parts # Identify the birth and death parts of the DE: birth_rate_log &lt;- c(dx ~ r*x) death_rate_log &lt;- c(dx ~ r*x^2/K) # Identify the initial condition and any parameters init_log &lt;- c(x=3) # Be sure you have enough conditions as you do variables. parameters_log &lt;- c(r=0.8, K=100) # parameters: a named vector # Identify how long we run the simulation deltaT_log &lt;- .05 # timestep length time_steps_log &lt;- 200 # must be a number greater than 1 # Identify the standard deviation of the stochastic noise sigma_log &lt;- 1 # Do one simulation of this differential equation out_log &lt;- birth_death_stochastic(birth_rate = birth_rate_log, death_rate = death_rate_log, init_cond = init_log, parameters = parameters_log, deltaT = deltaT_log, n_steps = time_steps_log, sigma = sigma_log) # Plot out the solution ggplot(data = out_log) + geom_line(aes(x=t,y=x)) Figure 14.1: One realization of the logistic differential equation with stochastic variables. Notice how the resulting spaghetti plot shows variation in the variables. Making an ensemble average plot is also similar to how we computed them in Section 13: # Many solutions n_sims &lt;- 100 # The number of simulations # Compute solutions logistic_sim_r &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ birth_death_stochastic(birth_rate = birth_rate_log, death_rate = death_rate_log, init_cond = init_log, parameters = parameters_log, deltaT = deltaT_log, n_steps = time_steps_log, sigma = sigma_log) ) %&gt;% map_dfr(~ .x, .id = &quot;simulation&quot;) # Plot these all up together ggplot(data = logistic_sim_r) + geom_line(aes(x=t, y=x, color = simulation)) + ggtitle(&quot;Spaghetti plot for the logistic SDE&quot;) + guides(color=&quot;none&quot;) Figure 14.2: Several different realizations of the logistic SDE with stochasticity in the variables, along with the ensemble average plot. ### Summarize the variables summarized_logistic_sim_r &lt;- logistic_sim_r %&gt;% group_by(t) %&gt;% # All simulations will be grouped at the same timepoint. summarise(x = quantile(x, c(0.25, 0.5, 0.75)), q = c(&quot;q0.025&quot;, &quot;q0.5&quot;, &quot;q0.975&quot;)) %&gt;% pivot_wider(names_from = q, values_from = x) ## `summarise()` has grouped output by &#39;t&#39;. You can override using the `.groups` argument. ### Make the plot ggplot(data = summarized_logistic_sim_r) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x=t,ymin=q0.025,ymax=q0.975),alpha=0.2) + ggtitle(&quot;Ensemble average plot for the logistic SDE&quot;) Figure 14.3: Several different realizations of the logistic SDE with stochasticity in the variables, along with the ensemble average plot. Notice that as before the spaghetti and ensemble average plots are generated. While there are some simulations that may always be zero, in the ensemble the median resembles the (deterministic) solution to the logistic differential equation. These types of stochastic processes are called “birth-death” processes. Here is another way to think about the stochastic differential equation: \\[\\begin{align*} r x \\; \\Delta t &amp;= \\alpha(x) \\mbox{ (birth) }\\\\ \\frac{rx^{2}}{K} \\; \\Delta t &amp;= \\delta(x) \\mbox{ (death) } \\end{align*}\\] In this way we think of \\(\\alpha(x)\\) as a “birth process” and \\(\\delta(x)\\) as a “death process.” When we computed the mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) for the logistic differential equation we had \\(\\mu = \\alpha(x)-\\delta(x)\\) and \\(\\sigma^{2}=\\alpha(x)+\\delta(x)\\). This relationship holds up for any differential equation where we have identified a birth or death process. "],["a-stochastic-system-of-equations.html", "14.2 A stochastic system of equations", " 14.2 A stochastic system of equations We can also apply this to a stochastic system of equations. Here we will return to the lynx-hare model: \\[\\begin{equation} \\begin{cases} \\frac{dH}{dt} &amp;= r H - b HL \\\\ \\frac{dL}{dt} &amp;=ebHL -dL \\end{cases} \\end{equation}\\] In this case we still split each equation into the birth (\\(\\alpha\\)) and death (\\(\\delta\\)) parts: \\[\\begin{align*} \\alpha &amp; = \\begin{cases} \\frac{dH}{dt}: &amp; r H \\\\ \\frac{dL}{dt}: &amp; ebHL\\\\ \\end{cases} \\\\ \\delta &amp; = \\begin{cases} \\frac{dH}{dt}: &amp; bHL \\\\ \\frac{dL}{dt}: &amp; dL\\\\ \\end{cases} \\end{align*}\\] To simulate this stochastic process the setup of the code is similar to previous ways we solved systems of differential equations. Since we have a system of equations to compute the expected value and variance requires additional knowledge of matrix algebra, but that is already included in the function birth_death_stochastic # Identify the birth and death parts of the DE: birth_rate_pred &lt;- c(dH ~ r*H, dL ~ e*b*H*L) death_rate_pred &lt;- c(dH ~ b*H*L, dL ~ d*L) # Identify the initial condition and any parameters init_pred &lt;- c(H=1, L=3) # Be sure you have enough conditions as you do variables # Identify the parameters parameters_pred &lt;- c(r = 2, b = 0.5, e = 0.1, d = 1) # Identify how long we run the simulation deltaT_pred &lt;- .05 # timestep length time_steps_pred &lt;- 200 # must be a number greater than 1 # Identify the standard deviation of the stochastic noise sigma_pred &lt;- .1 # Do one simulation of this differential equation out_pred &lt;- birth_death_stochastic(birth_rate = birth_rate_pred, death_rate = death_rate_pred, init_cond = init_pred, parameters = parameters_pred, deltaT = deltaT_pred, n_steps = time_steps_pred, sigma = sigma_pred) # Visualize the solution ggplot(data = out_pred) + geom_line(aes(x=t,y=H),color=&#39;red&#39;) + geom_line(aes(x=t,y=L),color=&#39;blue&#39;) + labs(x=&#39;Time&#39;, y=&#39;Lynx (red) or Hares (blue)&#39;) Figure 14.4: One realization of the stochastic predator-prey model. Excellent! Notice how Figure 14.4 has stochasticity in the variables for both H and L. The next step would be to do many simulations and then compute the ensemble average. First let’s compute the individual simulations: # Many solutions n_sims &lt;- 100 # The number of simulations # Compute solutions pred_sim &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ birth_death_stochastic(birth_rate = birth_rate_pred, death_rate = death_rate_pred, init_cond = init_pred, parameters = parameters_pred, deltaT = deltaT_pred, n_steps = time_steps_pred, sigma = sigma_pred) ) %&gt;% map_dfr(~ .x, .id = &quot;simulation&quot;) The above code may take a while to complete - but this is certainly shorter than computing these all by hand! In order to make the spaghetti plot, in Figure ?? we will plot the variables separately: # Plot the hares first: ggplot(data = pred_sim) + geom_line(aes(x=t,y=H,color=simulation)) + ggtitle(&quot;Spaghetti plot for hares in the predator-prey SDE&quot;) + guides(color=&quot;none&quot;) Figure 14.5: Several different realizations of the stochastic predator-prey system. # Then plot the lynx: ggplot(data = pred_sim) + geom_line(aes(x=t,y=L,color=simulation)) + ggtitle(&quot;Spaghetti plot for lynx in the predator-prey SDE&quot;) + guides(color=&quot;none&quot;) Figure 14.6: Several different realizations of the stochastic predator-prey system. For the ensemble average plots, we will first utilize the pivot_longer command, group and then summarize: ### Summarize the variables summarized_pred_sim &lt;- pred_sim %&gt;% pivot_longer(cols=c(&quot;H&quot;,&quot;L&quot;)) %&gt;% group_by(name,t) %&gt;% # All simulations will be grouped at the same timepoint. summarise(value = quantile(value, c(0.25, 0.5, 0.75)), q = c(&quot;q0.025&quot;, &quot;q0.5&quot;, &quot;q0.975&quot;)) %&gt;% pivot_wider(names_from = q, values_from = value) ## `summarise()` has grouped output by &#39;name&#39;, &#39;t&#39;. You can override using the `.groups` argument. # Let&#39;s take a look at the resulting data frame glimpse(summarized_pred_sim) ## Rows: 400 ## Columns: 5 ## Groups: name, t [400] ## $ name &lt;chr&gt; &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;… ## $ t &lt;dbl&gt; 0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.5… ## $ q0.025 &lt;dbl&gt; 1.0000000, 0.9877551, 0.9803345, 0.9909105, 1.0192345, 1.034657… ## $ q0.5 &lt;dbl&gt; 1.000000, 1.042378, 1.067787, 1.108199, 1.117504, 1.164413, 1.2… ## $ q0.975 &lt;dbl&gt; 1.000000, 1.110797, 1.153093, 1.207242, 1.254222, 1.303825, 1.3… Let’s break this down: - The data frame pred_sim has columns simulation, t, H, and L. We want to group each variable by the time t. In order to do that efficiently we need to pivot the data frame longer. By applying the command pivot_longer(cols=c(\"H\",\"L\")) we will still have 4 columns, but this time they are called simulation, t, name, and value. The column name is a categorical variable that is either H or L (the columns we made longer), and the column value has the numerical values for both at each time. - We then group by the column name first, and then by time. - The remainder of the code is similar to how we computed the ensemble average above. The tricky part is that resulting columns of summarized_pred_sim are name, t, and the quantile values. This may seem a challenge to plot, but we can easily make a small multiples plot with the command facet_grid: ggplot(data = summarized_pred_sim) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x=t,ymin=q0.025,ymax=q0.975),alpha=0.2) + facet_grid(. ~ name) Figure 14.7: Ensemble average plot of the predator-prey system. The last command facet_grid(. ~ name) splits the plot up into different panels (or facets) by the column name. So easy to make! If you are feeling a little overwhelmed by all these new plotting commands - don’t worry. You can easily modify these examples for a different systme of differential equations. You’ve got this! "],["generalizing-the-approach-.html", "14.3 Generalizing the approach.", " 14.3 Generalizing the approach. Another way to think about the logistic differential equation as what as known as a “birth-death process” Let’s call the part of the differential equation that contributes to a positive rate as a “birth process” and parts that contribute to a negative rate as a “death process.” If we have the differential equation \\[\\begin{equation*} \\frac{dx}{dt} = \\alpha(x)-\\delta(x) \\end{equation*}\\] Then we would simulate the birth death process with \\(\\mu = \\alpha(x)-\\delta(x)\\) and \\(\\sigma^{2} = \\alpha(x)+\\delta(x)\\). For a multivariable system of equations the process is the same, however because we have a system of equations the calculations for the expected value and variance require more knowledge of matrix algebra which is beyond the scope here. The provided code does take this into account. "],["exercises-12.html", "14.4 Exercises", " 14.4 Exercises Exercise 14.1 Return back to one simulation of the logistic differential equation. (Figure 14.1). For this plot we set \\(\\sigma = 1\\). What happens to the resulting spaghetti and ensemble plots when \\(\\sigma =0, \\; 0.01, \\; .1, \\; 10\\)? Exercise 14.2 Return back to the example of one simulation of the stochastic predator prey (Figure 14.4). For these plots we set \\(\\sigma = .1\\). What happens to the resulting spaghetti and ensemble plots when \\(\\sigma =0, \\; 0.01, \\; 1, \\; 10\\)? Exercise 14.3 For the predatory-prey simulation of stochastic variables we used group_by(name,t) to begin summarizing our variables. Modify the code so you apply group_by(t,name) and generate the ensemble plot. Do you receive a similar result? Exercise 14.4 For the logistic differential equation consider the following splitting of \\(\\alpha(x)\\) and \\(\\delta(x)\\): \\[\\begin{align*} \\alpha(x) &amp;= rx + \\frac{rx^{2}}{2K} \\\\ \\delta(x) &amp;= \\frac{rx^{2}}{2K} \\end{align*}\\] Simulate this SDE using the same values of parameters for the logistic example and compare your results. Exercise 14.5 Let \\(R(t)\\) denote the rainfall at a location at time \\(t\\), which is a random process. Assume that probability of the change in rainfall from day \\(t\\) to day \\(t+\\Delta t\\) is the following: Exercise 14.6 Consider the model for zombie population dynamics: \\[\\begin{align*} \\frac{dS}{dt} &amp;=-\\beta S Z - \\delta S \\\\ \\frac{dZ}{dt} &amp;= \\beta S Z + \\xi R - \\alpha SZ \\\\ \\frac{dR}{dt} &amp;= \\delta S+ \\alpha SZ - \\xi R \\end{align*}\\] Figure 14.8: The \\(SIS\\) model Exercise 14.7 Consider the stochastic differential equation \\(\\displaystyle dS = \\left( 1 - S \\right) + \\sigma dW(t)\\), where \\(\\sigma\\) controls the amount of stochastic noise. For this stochastic differential equation what is \\(E[S]\\) and Var\\((S)\\)? Exercise 14.8 An \\(SIS\\) model is one where susceptibles \\(S\\) become infected \\(I\\), and then after recovering from an illness, become susceptible again. The schematic representing this is shown in Figure 14.8. While you can write this as a system of differential equations, assuming the population size is constant \\(N\\) we have the following differential equation: \\[\\begin{equation} \\frac{dI}{dt} = b(N-I) I - r I \\end{equation}\\] Exercise 14.9 Consider the equation \\[\\begin{equation*} \\Delta x = \\alpha(x) \\; \\Delta t - \\delta(x) \\; \\Delta t \\end{equation*}\\] If we consider \\(\\Delta x\\) to be a random variable, show that the expected value \\(\\mu\\) equals \\(\\alpha(x) \\; \\Delta t - \\delta(x) \\; \\Delta t\\) and the variance \\(\\sigma^{2}\\), to first order, equals \\(\\alpha(x) \\; \\Delta t + \\delta(x) \\; \\Delta t\\). "],["solvingSDEs-27.html", "Chapter 15 Solving Stochastic Differential Equations", " Chapter 15 Solving Stochastic Differential Equations Stochastic differential equations arise when we consider variation (think randomness) in a biological model. Through simulation and examining the ensemble average we found that the solution to a stochastic differential equation is an distribution of solutions. For this final section we will examine how we can characterize solutions to stochastic differential equations. This section will introduce methods to develop exact solutions to a stochastic differential equation. You will learn about some powerful mathematics that hopefully you will want to study at some point in the future! "],["meet-the-fokker-planck-equation.html", "15.1 Meet the Fokker-Planck Equation", " 15.1 Meet the Fokker-Planck Equation Let’s start with a general way to express a stochastic differential equation: \\[\\begin{equation} dx = a(x,t) \\; dt + b(x,t) \\; dW(t) \\end{equation}\\] The “solution” to this SDE will be a probability density function \\(f(x,t)\\). Our goal is to have a function that describes the evolution of \\(f(x,t)\\) in both time and space. Based on our work with birth-death processes, the probability density function \\(f(x,t)\\) should have the following properties: \\(E[f(x,t)]\\) is the deterministic solution to \\(\\displaystyle \\frac{dx}{dt} = a(x,t)\\). Var\\([f(x,t)]\\) is proportional to \\(b(x,t)\\). We determine the probability density function through solution of the following differential equation, which is called the Fokker-Planck Equation: \\[\\begin{equation} \\frac{\\partial f}{\\partial t} = - \\frac{\\partial}{\\partial x} \\left(f(x,t) \\cdot a(x,t) \\right) + \\frac{1}{2}\\frac{\\partial^{2} }{\\partial x^{2}} \\left(\\; f(x,t) \\cdot (b(x,t))^{2} \\;\\right) \\end{equation}\\] We can write this equation in shorthand, dropping the dependence of \\(x\\) and \\(t\\) for \\(f(x,t)\\), \\(a(x,t)\\) and \\(b(x,t)\\): \\(\\displaystyle f_{t} = - (f \\cdot a)_{x} + \\frac{1}{2} (f \\cdot b^{2})_{xx}\\). Example 15.1 Consider the SDE \\(dx = dW(t)\\) and apply the Fokker-Planck equation to characterize the solution \\(f(x,t)\\). Remark. In this case \\(a(x,t)=0\\) and \\(b(x,t)=1\\), so the Fokker-Planck Equation is: \\[\\begin{equation*} f_{t} = \\frac{\\sigma^{2}}{2} f_{xx}. \\end{equation*}\\] This equation should look familiar - it is the partial differential equation for diffusion!4 The solution to the Fokker-Planck equation is \\(\\displaystyle f(x,t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2} t}} e^{-x^{2}/(2 \\sigma^{2} t)}\\). I didn’t mention the initial condition to the SDE - in this case it is \\(f(x,0)=\\delta(x)\\). The function \\(\\delta(x)\\) is called the Dirac delta Function. This function is a special type of probability density function, which you may study in a courses such as Functional Analysis (or Analysis). We can plot the evolution of this solution in Figure 15.1: Figure 15.1: The solution to SDE \\(dx = dW(t)\\). Now that we have a handle on the SDE \\(dx = dW(t)\\), let’s extend this next example a little more. 15.1.1 Another Fokker-Planck Equation Consider the SDE \\(dx = r \\; dt + \\sigma \\; dW(t)\\), where r and \\(\\sigma\\) are constants. As a first step, let’s take a look at the deterministic equation: \\(dx = r \\; dt\\). This is the differential equation \\(\\displaystyle \\frac{dx}{dt} = r\\), which has a linear function \\(x(t) = rt + x_{0}\\) as its solution. We will apply the Fokker-Planck equation to characterize the solution \\(f(x,t)\\). In this case, the Fokker-Planck equation is \\(\\displaystyle f_{t} = -r f_{x}+ \\frac{\\sigma^{2}}{2} f_{xx}\\). The partial differential equation is an example of a diffusion-advection equation. Amazingly this equation can be reduced to a diffusion equation through a change of variables and application of the multivariable change of variables. Let’s get to work to figure this out. First, let \\(z=x-rt\\). This change of variables may seem odd, but our goal here is to write \\(f(x,t)=f(z)\\) and to develop expressions for \\(f_{t}\\) \\(f_{x}\\), and \\(f_{xx}\\) with this change of variables. But in order to do that, we will need to apply the multivariate chain rule (see Figure 15.2). Figure 15.2: Multivariable chain rule By the multivariable chain rule we can develop expressions for \\(f_{t}\\) and \\(f_{x}\\): \\[\\begin{align*} \\frac{\\partial f}{\\partial t} &amp; = \\frac{\\partial f}{\\partial \\tau} \\cdot \\frac{ \\partial \\tau}{\\partial t} + \\frac{\\partial f}{\\partial z} \\cdot \\frac{ \\partial z}{\\partial t} \\\\ \\frac{\\partial f}{\\partial x} &amp; = \\frac{\\partial f}{\\partial z} \\cdot \\frac{ \\partial z}{\\partial x} \\end{align*}\\] Now let’s consider the partial derivatives \\(\\displaystyle \\frac{ \\partial \\tau}{\\partial t}\\), \\(\\displaystyle \\frac{ \\partial z}{\\partial t}\\), and \\(\\displaystyle \\frac{ \\partial z}{\\partial x}\\) We will define \\(z=x-r\\tau\\). By direct differentiation \\(z_{\\tau} = -r\\), \\(z_{x} = 1\\). Also since \\(\\tau=t\\), \\(\\tau_{t}=1\\). These expressions help: since \\(f_{z}=f_{x}\\), then \\(f_{zz} = f_{xx}\\). With these substitutions, we can now re-write \\(f_{t}\\) and \\(f_{xx}\\): \\[\\begin{align*} \\frac{\\partial f}{\\partial t} &amp; = \\frac{\\partial f}{\\partial \\tau} \\cdot \\frac{ \\partial \\tau}{\\partial t} + \\frac{\\partial f}{\\partial z} \\cdot \\frac{ \\partial z}{\\partial t} = \\frac{\\partial f}{\\partial \\tau} -r \\frac{\\partial f}{\\partial z} \\\\ \\frac{\\partial f}{\\partial xx} &amp; = \\frac{\\partial^{2} f}{\\partial z^{2}} \\end{align*}\\] So if we re-write our original Fokker-Planck equation with the variables \\(z\\) and \\(\\tau\\) we have: \\[\\begin{align*} f_{t} &amp;=-r f_{x}+ \\frac{\\sigma^{2}}{2} f_{xx} \\\\ f_{\\tau} - r f_{z} &amp;= -r f_{z} + \\frac{\\sigma^{2}}{2} f_{zz} \\\\ f_{\\tau} &amp;= \\frac{\\sigma^{2}}{2} f_{zz} \\end{align*}\\] Ok: I’ll admit that doing change of variables may seem like it doesn’t help the situation. But guess what: with this change of variables our Fokker-Planck equation becomes a diffusion equation in the variables \\(z\\) and \\(\\tau\\)! So if we can write down the solution with the variables \\(z\\) and \\(\\tau\\), we can write the solution \\(f(x,t)\\)! Here how we do this: \\(\\displaystyle f(z, \\tau) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2} \\tau}} e^{-z^{2}/(2 \\sigma^{2} \\tau)}\\), and then transform back into the original variables \\(x\\) and \\(t\\): \\[\\begin{equation} f(x, t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2} t}} e^{-(x-rt)^{2}/(2 \\sigma^{2} t)} \\end{equation}\\] Now that we have an equation, next let’s visualize the solution. Let’s take a look at some representative plots: Figure 15.3: Representative plots for the solution to the SDE \\(dx = r \\; dt + \\sigma \\; dW(t)\\). Based on what we know of this distribution is that it should look like a normal distribution with mean \\(\\mu = rt\\) and variance \\(\\sigma^{2} t\\). What the mean and variance tells us that the mean is shifting and growing more diffuse as time increases. Remember that our solution to the deterministic equation was linear, and the mean of our distribution grows linearly as well! Also notice in Figure 15.3 as \\(t\\) increases the solution shifts (‘’advects’’) to the right. The differential equation is an example of a diffusion-advection equation, or a solution that drifts as time increases. The examples we study here are just a few examples of how to build a deeper understanding of stochastic processes and differential equations. There is a lot of power in understanding the theoretical distribution for some test cases. Stochastic differential equations is a fascinating field of study with a lot of interesting mathematics - I hope what you learned here will make you want to study it further! To remind you, the solution to \\(p_{t} = D p_{xx}\\) is \\(\\displaystyle p(x,t) = \\frac{1}{\\sqrt{4 \\pi Dt} } e^{-x^{2}/(4 D t)}\\). So in this case \\(D = \\sigma^{2}/2\\).↩︎ "],["exercises-13.html", "15.2 Exercises", " 15.2 Exercises Exercise 15.1 Let \\(R(t)\\) denote the rainfall at a location at time \\(t\\), which is a random process. Assume that probability of the change in rainfall from day \\(t\\) to day \\(t+\\Delta t\\) is the following: The stochastic differential equation generated by this process is \\(dR = \\lambda \\rho \\; dt + \\sqrt{\\lambda \\rho^{2}} \\; dW(t)\\). Exercise 15.2 A particle is moving in a gravitational field but still allowed to diffuse randomly. In this case the stochastic differential equation is \\(dx = -g \\; dt + \\sqrt{D} \\; dW(t)\\). &amp;nbsp Exercise 15.3 Consider the stochastic differential equation \\(\\displaystyle dS = \\left( 1 - S \\right) + \\sigma dW(t)\\), where \\(\\sigma\\) controls the amount of stochastic noise. Exercise 15.4 Consider the differential equation \\(\\displaystyle x&#39; = \\lambda x - c \\mu x^{2}\\), which is similar to a logistic differential equation. The per capita rate equation for this differential equation is \\(\\displaystyle \\frac{x&#39;}{x} = \\lambda - c \\mu x\\). Exercise 15.5 A type of chemical reaction is \\(X + A \\leftrightarrow 2X\\), where \\(A\\) acts like an enzyme. The stochastic differential equation that describes this scenario is: \\[\\begin{equation} dX = \\left( A X - X^{2} \\right) \\; dt + \\left( AX + X^{2} \\right) dW(t) \\end{equation}\\] Exercise 15.6 Models of cell membranes take account for the energy needed for ions and other materials to cross the cell membrane, usually expressed as a membrane potential \\(U(x)\\), where \\(x\\) is the current position of a particle distance. The probability \\(p\\) of the particle being at position \\(x\\) at time \\(t\\) is given by the Fokker-Planck equation: \\[\\begin{equation} v \\frac{\\partial p}{\\partial t} = \\frac{\\partial}{\\partial x} \\left( U&#39;(x) p \\right) + k T \\frac{\\partial^{2} p}{\\partial x^{2}}, \\end{equation}\\] where \\(k\\) is Boltzmann’s constant and \\(T\\) is the temperature. "]]
