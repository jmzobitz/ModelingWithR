[["index.html", "Exploring Modeling with Data and Differential Equations Using R Welcome Computational code Questions? Comments? Issues? About the cover Acknowledgments", " Exploring Modeling with Data and Differential Equations Using R John M. Zobitz Version 3.0.0 Welcome This book is written for you, the student learning about modeling and differential equations. Perhaps you first encountered models, differential equations, and better yet, building plausible models from data in your calculus course. This book sits “at the intersection” of several different mathematics courses: differential equations, linear algebra, statistics, calculus, data science - as well as the partner disciplines of biology, chemistry, physics, business, and economics. An important idea is one of transference where a differential equation model applied in one context can also be applied (perhaps with different variable names) in a separate context. I intentionally emphasize models from biology and the environmental sciences, but throughout the text you can find examples from the other disciplines. In some cases I’ve created homework exercises based on sources that I have found useful for teaching (denoted with “Inspired by …”). I hope you see the connections of this content to your own intended major. This book is divided into 4 parts: Models with differential equations Parameterizing models with data Stability analysis for differential equations Stochastic differential equations You may notice the interwoven structure for this book: models are introduced first, followed by data analysis and parameter estimation, returning back to analyzing models, and ending with simulating random (stochastic) models. Unsure what about all these topics mean? Do not worry! The topics are presented with a “modeling first” paradigm that first introduces models, and equally important, explains how data are used to inform a model. This “conversation” between models and data are important to help build plausibility and confidence in a model. Stability analysis helps to solidify the connection between models and parameters (which may change the underlying dynamical stability). Finally the notion of randomness is extended with the introduction of stochastic differential equations. Unifying all of these approaches is the idea of developing workflows for analysis, visualization results, and interpreting any results in the context of the problem. Computational code This book makes heavy use of the R programming language, and unabashedly develops programming principles using the tidyverse syntax and programming approach. This is intentional to facilitate direct connections to courses in introductory data science or data visualization. Throughout my years learning (and teaching) different programming languages, I have found R to be the most versatile and adaptable. The tidyverse syntax, in my opinion, has transformed my own thinking about sustainable computation and modeling processes - and I hope it does for you as well. There is a companion R package available called demodelr to run programs and functions in the text (Zobitz 2022). Instructions to install this package are given in Chapter 2. The minimum version of R used was Version 4.0.2 (2020-06-22) (R Core Team 2021) and RStudio is Version 1.4.1717 (RStudio Team 2020). The demodelr package uses the following R packages: tidyverse (and the associated packages) (Version 1.3.1) (Wickham et al. 2019) GGally (Version 2.1.2) (Schloerke et al. 2021) formula.tools (Version 1.7.1) (Brown 2018) expm (Version 0.999-6) (Goulet et al. 2021) Questions? Comments? Issues? Any errors or omissions are of my own accord, so please contact me at zobitz@augsburg.edu. Feel free to file an issue with the demodelr package to my github. About the cover The photo on the back cover was taken by Shannon Zobitz during a hike at Orinoro Gorge in Finland. The photo is indicative of several things: (1) the journey ahead as you commence learning about modeling, differential equations, and R, (2) the occasional roots in the path that may cause you to stumble (such as coding errors). Everyone makes them, so you are in good company. (3) the yellow markings on the trees indicate the way forward. The vector field image on the cover is an example of a spiral node, indicating my hope that the knowledge contained here spirals out and informs your future endeavors. May this textbook be the guide for you as you progress over the hill and onward. Let’s get started! Acknowledgments This book has been developed over the course of several years in a variety of places: two continents, between meetings, in the early mornings, at coffee shops, or while waiting for practices to end. Special thanks are to the following: Augsburg University: You have been my professional home for over a decade and given me the space and support to be intellectually creative in my teaching and scholarship. Special thanks to my Mathematics, Statistics, and Computer Science Department colleagues - it is a joy to work with all of you. Augsburg University students: Thank you for your interest and engagement in this topic, allowing me to test ideas in an upper division course titled (wait for it …) Modeling and Differential Equations in the Biological and Natural Sciences. While the course title is a mouthful, you provided concise, honest, and insightful feedback, shaping this text. I am forever indebted to you. Kiitos to students in the Fall 2019 and 2021 courses. My family: Shannon, Colin, Grant, and Phoebe for humoring me (and my occasional grumpiness) while this project has been completed. Taylor &amp; Francis: Thank you for your confidence in me with this project, and to my editor Lara Spieker for shepherding the project and Robin Lloyd Starkes and her team for their careful copyediting. References "],["intro-01.html", "Chapter 1 Models of Rates with Data 1.1 Rates of change in the world: a model is born 1.2 Modeling in context: the spread of a disease 1.3 Model solutions 1.4 Which model is best? 1.5 Start here 1.6 Exercises", " Chapter 1 Models of Rates with Data 1.1 Rates of change in the world: a model is born This book focuses on understanding rates of change and their application to modeling real-world phenomena with contexts from the natural sciences. Additionally, this book emphasizes using equations with data, building both competence and confidence to construct and evaluate a mathematical model with data. Perhaps these emphases are different from when you analyzed rates of change in a calculus course; consider the following types of questions: If \\(y = xe^{-x}\\), what is the derivative function \\(f&#39;(x)\\)? What is the equation of the tangent line to \\(y=x^{3}-x\\) at \\(a=1\\)? Where is the graph of \\(\\sin(x)\\) increasing at an increasing rate? If you release a ball from the top of a skyscraper 500 meters above the ground, what is its speed when it impacts the ground? What is the largest area that can be enclosed in a chicken coop with 100 feet of fencing, with one side being along a wall? The first three questions do not appear to be connected in a real-world context in their framing - but the last two questions do have some context from real-world situations. The given context may reveal underlying assumptions or physical principles, which are the starting point to build a mathematical model. For the chicken coop problem, perhaps the next step is to use the assumed geometry (rectangle) with the 100 feet of fencing to develop a function for the area. Maybe the context includes observational data and several different (perhaps conflicting) assumptions about the context at hand. For example, how does air resistance affect the ball’s velocity? Would a circular chicken coop maximize the area more than a rectangular coop? For both of these cases, which model is the best one to approximate any observational data? The short answer: it depends. To understand why, let’s take a look at another problem in context. 1.2 Modeling in context: the spread of a disease Consider the data in Figure 1.1, which come from an Ebola outbreak in Sierra Leone in 2014. (Data provided from Matthes (2021).) The vertical axis in Figure 1.1 represents Ebola infections over 2 years from initial monitoring in March 2014. FIGURE 1.1: Infections from a 2014 Ebola outbreak in Sierra Leone, with the initial monitoring in March 2014. Constructing a model from disease dynamics is part of the field of mathematical epidemiology. Here we focus on person to person or population spread of Ebola. Other types of models could focus on the immune response within a single person - perhaps with a goal to design effective types of treatments to reduce the severity of infection. How we construct a mathematical model for this outbreak largely depends on the assumptions underlying the biological dynamics of disease transmission (which we will call the infection rate). Three plausible assumptions for the infection rate are the following: The infection rate is proportional to the number of people infected. The infection rate is proportional to the number of people not infected. The infection rate is proportional to the number of infected people coming into contact with those not infected. Now let’s explore how to translate these assumptions into a mathematical model. Since we are discussing rates of infection, this means we will need a rate of change or derivative. Let’s use the letter \\(I\\) to represent the number of people that are infected. 1.2.1 Model 1: Infection rate proportional to number infected The first assumption states that the infection rate is proportional to the number of people infected. Translated into an equation this would be the following: \\[\\begin{equation} \\frac{dI}{dt} = kI \\tag{1.1} \\end{equation}\\] Equation (1.1) is an example of a differential equation, which is just a mathematical equation with rates of change. In Equation (1.1) \\(k\\) is a proportionality constant or parameter, with units of time\\(^{-1}\\) for consistency. The solution to a differential equation is a function \\(I(t)\\). When we “solve” a differential equation we determine the family of functions consistent with our rate equation.1 There are a lot of techniques to solve a differential equation; we will explore some in Chapter 7. The proportionality constant or parameter \\(k\\) is important to understand the solution to Equation (1.1). Even though no numerical value for \\(k\\) is specified, you can always solve an equation without specifying the parameter. In some situations we may not be as concerned with the particular value of the parameter but rather its influence on the long-term behavior of the system (this is a key aspect of bifurcation theory described in Chapter 20). Otherwise we can use the collected data shown above with the given model to determine the value for \\(k\\). This combination of a mathematical model with data is called data assimilation or model-data fusion (see Chapters 8-14). How plausible is this first model? The first model assumes the rate of change (Equation (1.1)) gets larger as the number of infected people \\(I\\) increases. This reasoning certainly seems plausible: when there are so many people infected it can be hard to stay healthy! At some point the number of people who are not sick will reach zero, making the rate of infection zero (or no increase). In the case of Ebola or any other infectious disease, stringent public health measures would be enacted if the number of people infected became too large.2 Following public health measures we would expect that the rate of infection would decrease and the number of infections to slow. So perhaps another model this can capture this “slowing down” of the infection rate is more plausible. 1.2.2 Model 2: Infection rate proportional to number NOT infected The second model considers the interaction between people who are sick (which we have denoted as \\(I\\)) and people who are not sick, which we will call \\(S\\), or susceptible. Equation (1.2) is an example of a differential equation that models this interation: \\[\\begin{equation} \\frac{dI}{dt} = kS \\tag{1.2} \\end{equation}\\] As with Equation (1.1) the parameter \\(k\\) represents an infection rate. We would expect that both \\(I\\) and \\(S\\) change in time as the infection occurs; for a finite population as more people get sick (\\(I\\)), that would mean that \\(S\\) would decrease. In effect, Model 2 should have two rates of change: one for \\(I\\) and one for \\(S\\). Figure 1.2 shows a schematic of this process of infection. FIGURE 1.2: Schematic diagram for Model 2, showing that the rate of infection is proportional to the number of susceptible people \\(S\\). Assuming a constant population size \\(N\\), the differential equation for Model 2 is given by Equation . There are three reasons why I like to use diagrams like Figure 1.2: Diagrams build a bridge between biological processes and mathematical models. Diagrams signal which rates (if any) can be conserved (more on this below). Diagrams help to identify assumed parameters (i.e. \\(k\\) in Figure 1.2). Diagrams suggest how to construct differential equations for this mathematical model. Figure 1.2 suggests a flow between the suspectible state \\(S\\) to the infected state \\(I\\). So then the rate of change equation for \\(S\\) is \\(\\displaystyle \\frac{dS}{dt} = -kS\\) (the parameter listed above the arrow in Figure 1.2). Equation (1.3) combines all this thinking and Equation (1.2) into the following coupled system of differential equations in Equation (1.3): \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;= -kS \\\\ \\frac{dI}{dt} &amp;= kS \\end{split} \\tag{1.3} \\end{equation}\\] The solution to Equation (1.3) is functions \\(S(t)\\) and \\(I(t)\\) that evolve over time. We don’t have the tools to determine the exact solutions for Equation (1.3) yet (we will study systems like these in Chapters 15-20). However something interesting occurs with Equation (1.3) when we add the rates \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) together (Equation (1.4)): \\[\\begin{equation} \\frac{dS}{dt} + \\frac{dI}{dt} = \\frac{d}{dt}(S+I) = 0 \\tag{1.4} \\end{equation}\\] If a rate of change equals zero then the corresponding function is constant. In effect, Equation (1.4) means that the combined variable \\(S+I\\) is constant, so we could say that \\(S+I=N\\), where \\(N\\) is the total population size. The expression \\(S+I=N\\) is an example of a conservation law for our system.3 Figure 1.2 also suggests a conservation law because there are no additional arrows going into or from the variables \\(S\\) or \\(I\\). Since \\(S=N-I\\), Equation (1.3) can be re-written with a single equation (Equation (1.5)): \\[\\begin{equation} \\frac{dI}{dt} = k(N-I) \\tag{1.5} \\end{equation}\\] Equation (1.5) also indicates limiting behavior for Model 2. As the number of infected people reaches \\(N\\) (the total population size), the values of \\(\\displaystyle \\frac{dI}{dt}\\) approaches zero, meaning \\(I\\) doesn’t change. Biologically this would suggest that eventually everyone in the population would get sick with the disease (assuming no one has any natural immunity). Equation (1.5) also has one caveat: if there are no infected people around (\\(I=0\\)) the disease can still be transmitted, which might not make good biological sense. The next model (Model 3) tries to amend that shortcoming. 1.2.3 Model 3: Infection rate proportional to infected meeting not infected Now consider a third model that rectifies some of the shortcomings of the second model (the second model rectified the shortcomings of the first model). The third model states that the rate of infection is due to those who are sick infecting those who are not sick. This scenario would also make some sense, as it focuses on the transmission of the disease between susceptibles and infected people. So if nobody is sick (\\(I=0\\)) then the disease is not spread. Likewise if there are no susceptibles (\\(S=0\\)), the disease is not spread as well. In this case the diagram outlining the third model looks something like this: FIGURE 1.3: Schematic diagram for Model 3, showing that the rate of infection is proportional to the number of susceptible people \\(S\\) encountering an infected person \\(I\\). Assuming a constant population size \\(N\\), the differential equation for Model 3 is given by Equation . Notice how in Figure 1.3 there is an additional variable \\(S\\) associated with \\(k\\) to show how the rate of infection depends on \\(S\\). Equation (1.6) contains the differential equations that describe the scenario outlined in Figure 1.3: \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;= -kSI \\\\ \\frac{dI}{dt} &amp;= kSI \\end{split} \\tag{1.6} \\end{equation}\\] Similar to Model 2 we can combine the two equations to yield a single differential equation (Equation (1.7)): \\[\\begin{equation} \\frac{dI}{dt} = k\\cdot I \\cdot (N-I) \\tag{1.7} \\end{equation}\\] Equation (1.7) appears similar to Equation (1.5), doesn’t it? However in Equation (1.7) notice the variable \\(I\\) outside the expression \\((N-I)\\). If \\(I=0\\), then there is no increase in infection (the rate is zero). If \\(I=N\\) (the total population size) then there is no increase in the infection (the rate is zero as well). Model 3 seems to be more consistent with the biological reasoning for the spread of infection. Let’s compare all the rates for all three models together in Figure 1.4. Figure 1.4 has a lot to unpack, but we can use some of our understanding of rates of change in calculus to compare the three models. Notice how the sign of \\(\\displaystyle \\frac{dI}{dt}\\) is always positive for Model 1, indicating that the solution (\\(I\\)) is always increasing. For Models 2 and 3, \\(\\displaystyle \\frac{dI}{dt}\\) equals zero when \\(I=10\\), which also is the value for \\(N\\) After that case, \\(\\displaystyle \\frac{dI}{dt}\\) turns negative, meaning that \\(I\\) is decreasing. FIGURE 1.4: Comparing the rates of change for three models (Equation (1.1), Equation (1.5), and Equation (1.7), with \\(k=1\\) and \\(N=10\\)). In summary, examining the graphs of the rates can tell a lot about the qualitative behavior of a solution to a differential equation even without the solution. 1.3 Model solutions Let’s return back to possible solutions (in this case formulas for \\(I(t)\\)) for our models. Usually a differential equation also has a starting or an initial value (typically at \\(t=0\\)) that actualizes the solution. When we state a differential equation with a starting value we have an initial value problem. We will represent that initial value as \\(I(0)=I_{0}\\). With that assumption, we can (and will solve later!) the following solutions for these models: \\[\\begin{equation} \\begin{split} \\mbox{ Model 1 (Exponential): } &amp; I(t) = I_{0}e^{kt} \\\\ \\mbox{Model 2 (Saturating): } &amp; I(t) = N-(N-I_{0})e^{-kt} \\\\ \\mbox{Model 3 (Logistic): } &amp; I(t) = \\frac{N \\cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}} \\end{split} \\tag{1.8} \\end{equation}\\] Notice how I assigned the names to each model (Exponential, Saturating, and Logistic). That may not mean much at the moment, but Figure 1.5 plots the three functions \\(I(t)\\) together when \\(I_{0}=250\\), \\(k=0.023\\), and \\(N=13600\\). FIGURE 1.5: Three models (Exponential, Saturating, and Logistic; Equation (1.8)) compared. Notice how in Figure 1.5 Model 1 increases quickly - it actually grows without bound off the chart! Model 2 and Model 3 have saturating behavior, but it looks like Model 3 might be the one that actually captures the trend of the data. 1.4 Which model is best? All three of these scenarios describe different modeling scenarios. With the saturating and logistic models (Models 2 and 3) we have some limiting behavior, the possibility that the rate of infection slows. Of the two models, which one is the best one? Here could be some possible criteria we could evaluate: Do the model outputs match the data? For timeseries data are the trends accurately represented? Can the model be coded easily into the computer? How will model outputs compare with newly collected measurements? Regarding model complexity - how many equations do we have? Are the number of model parameters too few or too many? We will address several of these critera later on in this textbook when we discuss model selection (Chapter 14). Model selection is one key part of the modeling hypothesis - where we investigate the implications of a particular model analyzed. If we don’t do this, we don’t have an opportunity to test out what is plausible for our models. 1.5 Start here In summary, it turns out that even with some initial assumptions we can very quickly build up a mathematical model to explain data. Even with these first steps we have a lot more to uncover: How would you determine the parameters \\(k\\) and \\(N\\) with the collected data? Are there other more complicated models? What exact techniques are used to determine the solution \\(I(t)\\)? Are there other numerical techniques to approximate the solution \\(I(t)\\)? What happens to our solutions when the parameters \\(k\\) and \\(N\\) change? What happens to our solutions when the number of infected people changes randomly for some reason? This text will study answers to these questions and more. Let’s get started! 1.6 Exercises Exercise 1.1 Solutions to an outbreak model of the flu are the following: \\[\\begin{equation} \\begin{split} \\mbox{Saturating model: } &amp; I(t) = 3000-2990e^{-.1t} \\\\ \\mbox{Logistic model: } &amp; I(t) = \\frac{30000 }{10+2990e^{-.15t}}, \\end{split} \\end{equation}\\] where \\(t\\) is in days. Use these two functions to answer the following questions: Plot the saturating and logistic models when \\(0 \\leq t \\leq 100\\). For both models, how would you describe the growth of the outbreak as \\(t\\) increases? How many people will be infected overall? Finally, for both models evaluate \\(\\lim_{t \\rightarrow \\infty} I(t)\\). How do these results compare to values found on your graph? FIGURE 1.6: Infections from a 2014 Ebola outbreak in Liberia, with the initial monitoring in March 2014. Exercise 1.2 Figure 1.6 shows the Ebola outbreak for the country of Liberia in 2014. If we were to apply the logistic model (Model 3) based on this graphic what would be your estimate for \\(N\\)? Exercise 1.3 The general solutions for the saturating and the logistic models are: \\[\\begin{equation} \\begin{split} \\mbox{Saturating model: } &amp; I(t) = N-(N-I_{0})e^{-kt} \\\\ \\mbox{Logistic model: } &amp; I(t) = \\frac{N \\cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}}, \\end{split} \\end{equation}\\] where \\(I_{0}\\) is the initial number of people infected and \\(N\\) is the overall population size. Using the functions from Exercise 1.1 for both models, what are the values for \\(N\\) and \\(I_{0}\\)? Exercise 1.4 The general solutions for the saturating and the logistic models are: \\[\\begin{equation} \\begin{split} \\mbox{Saturating model: } &amp; I(t) = N-(N-I_{0})e^{-kt} \\\\ \\mbox{Logistic model: } &amp; I(t) = \\frac{N \\cdot I_{0} }{I_{0}+(N-I_{0})e^{-kt}}, \\end{split} \\end{equation}\\] where \\(I_{0}\\) is the initial number of people infected and \\(N\\) is the overall population size. For both models carefully evaluate the limits to show \\(\\lim_{t \\rightarrow \\infty} I(t)=N\\). How do these limiting values compare to the steady-state values you found for Models 2 and 3 in Figure 1.5, where \\(N=13600\\)? FIGURE 1.7: Schematic diagram for Exercise . Exercise 1.5 Figure 1.7 shows a schematic diagram which is a variation on Figure 1.2. In this case people are entering the the susceptible population \\(S\\) at a rate \\(\\beta\\), so the population is not conserved. What is the coupled system of differential equations for this model? Exercise 1.6 A model that describes the growth of sales of a product in response to advertising is the following: \\[\\begin{equation} \\frac{dS}{dt} = .55\\sqrt{1-S}-S, \\end{equation}\\] where \\(S\\) is the product’s share of the market (scaled between 0 and 1) (Sethi 1983). Use this information to answer the following questions: Make a plot of the function \\(f(S)=.55\\sqrt{1-S}-S\\). for \\(0 \\leq S \\leq 1\\). Interpret your plot to predict when the market share will be increasing and decreasing. At what value is \\(\\displaystyle \\frac{dS}{dt}=0\\)? (This is called the steady state value.) A second campaign has the following differential equation: \\[\\begin{equation} \\frac{dS}{dt} = .2\\sqrt{1-S}-S \\end{equation}\\] What is the steady state value and how does it compare to the previous one? Exercise 1.7 A more general form of the advertising model is \\[\\begin{equation} \\frac{dS}{dt} = r\\sqrt{1-S}-S, \\end{equation}\\] where \\(S\\) is the product’s share of the market (scaled between 0 and 1). The parameter \\(r\\) is related to the effectiveness of the advertising (between 0 and 1). Solve \\(\\displaystyle \\frac{dS}{dt} = r\\sqrt{1-S}-S\\) for the steady state value (where \\(\\displaystyle \\frac{dS}{dt}=0\\)). Your final answer should be expressed as a function \\(S(r)\\) - for which you will need to use the quadratic formula. Make a plot of the steady state value as a function of \\(r\\), where \\(0 \\leq r \\leq 1\\). Based on your plot, what can you conclude about the steady state value as the effectiveness of the advertising increases? Exercise 1.8 A common saying is “you are what you eat.” An equation that relates an organism’s nutrient content (denoted as \\(y\\)) to the nutrient content of food or resource (denoted as \\(x\\)) is given by: \\[\\begin{equation} y = c x^{1/\\theta}, \\end{equation}\\] where \\(\\theta\\) and \\(c\\) are both constants. Units on \\(x\\) and \\(y\\) are expressed as a proportion of a given nutrient (such as nitrogen or carbon). For example, when \\(c=1\\) and \\(\\theta = 1\\) the function is \\(y=x\\). In this case the point \\((0.05,0.05)\\) would say that nutrient composition for the organism and resource would be the same. Now assume that \\(c=1\\). How does the nutrient content of the organism compare to the resource when \\(\\theta=2\\)? Draw a sample curve and interpret it, contrasting it to when \\(\\theta = 1\\). Now assume that \\(c=1\\). How does the nutrient content of the organism compare to the resource when \\(\\theta=5\\)? Draw a sample curve and interpret it, contrasting this curve to the previous two. What do you think will happen when \\(\\theta \\rightarrow \\infty\\)? Draw some sample curves to help illustrate your findings. Exercise 1.9 A model for the outbreak of a cold virus assumes that the rate people get infected is proportional to infected people contacting susceptible people, as with Model 3 (the Logistic model). However people who are infected can also recover and become susceptible again with rate \\(\\alpha\\). Construct a diagram similar to Figure 1.3 for this scenario and also write down what you think the system of differential equations would be. Exercise 1.10 A model for the outbreak of the flu assumes that the rate people get infected is proportional to infected people contacting susceptible people, as in Model 3. However people also recover from the flu, denoted with the variable \\(R\\). Assume that the rate of recovery is proportional to the number of infected people with parameter \\(\\beta\\). Construct a diagram similar to Figure 1.3 for this scenario and also write down what you think the system of differential equations would be. Exercise 1.11 (Inspired by Hugo van den Berg (2011)) Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of \\(S\\) in the blood is the following: \\[\\begin{equation} \\frac{dS}{dt} = I + p \\cdot (W - S), \\end{equation}\\] where the parameter \\(I\\) represents the active uptake of salt, \\(p\\) is the permeability of the skin, and \\(W\\) is the salinity in the water. Use this information to answer the following questions: What is that value of \\(S\\) at steady state, or when \\(\\displaystyle \\frac{dS}{dt} = 0\\)? Your final answer should be a function \\(S(I,p,W)\\). With the steady state solution, what parameters (\\(I\\), \\(p\\), or \\(W\\)) cause the steady state value \\(S\\) to increase? Exercise 1.12 (Inspired by Logan and Wolesensky (2009)) The immigration rate of bird species (species per time) from a mainland to an offshore island is \\(I_{m} \\cdot (1-S/P)\\), where \\(I_{m}\\) is the maximum immigration rate, \\(P\\) is the size of the source pool of species on the mainland, and \\(S\\) is the number of species already occupying the island. Additionally the extinction rate is \\(E \\cdot S / P\\), where \\(E\\) is the maximum extinction rate. The growth rate of the number of species on the island is the immigration rate minus the extinction rate. Make representative plots of the immigration and the extinction rates as a function of \\(S\\). You may set \\(I_{m}\\), \\(P\\), and \\(E\\) all equal to 1. Determine the number of species for which the net growth rate is zero, or the number of species is in equilibrium. Express your answer as \\(S\\) as a function of \\(I_{m}\\), \\(P\\), and \\(E\\). Suppose that two islands of the same size are at different distances from the mainland. Birds arrive from the source pool and they have the same extinction rate on each island. However the maximum immigration rate is larger for the island farther away. Which of the two islands will have the larger number of species at equilibrium? Exercise 1.13 (Inspired by Logan and Wolesensky (2009)) Assume that an animal assimilates nutrients at a rate \\(R\\) proportional to its surface area. Also assume that it uses nutrients at a rate proportional to its volume. You may assume that the size of the animal is implicitly a function of the nutrient intake and usage, so \\(R = k_{A} A - k_{V} V\\), where \\(k_{A}\\) and \\(k_{V}\\) are constants, \\(A\\) is the surface area, and \\(V\\) the volume. Determine expressions for the size of the animal if its intake and use rates were in balance (meaning \\(R\\) is set to zero), assuming the animal is the following shapes: A sphere (assume size is measured with radius \\(r\\)) Note: first determine the geometric formulas for surface area and volume. A cube (assume size is measured with length \\(l\\)) References "],["r-intro-02.html", "Chapter 2 Introduction to R 2.1 R and RStudio 2.2 First steps: getting acquainted with R 2.3 Increasing functionality with packages 2.4 Working with R: variables, data frames, and datasets 2.5 Visualization with R 2.6 Defining functions 2.7 Concluding thoughts 2.8 Exercises", " Chapter 2 Introduction to R The primary tools we will use to analyze models for this book are R (R Core Team 2021) and RStudio (RStudio Team 2020).4 These programs are powerful ones to learn! Admittedly learning a new software may be challenging; however I think it is worth it. With R you will have enormous flexibility to efficiently utilize data, design effective visualizations, and process statistical models. Let’s get started! 2.1 R and RStudio First let’s talk terminology. The program RStudio is called an Integrated Development Environment for the statistical software language R. To get both R and RStudio requires two separate downloads and files, which can be found here: R: https://cran.r-project.org/mirrors.html (You need to select a location to download from; choose any one that is geographically close to you.) RStudio: https://www.rstudio.com/products/rstudio/download/ 2.1.1 Why do we have two programs? Think of R as your basic program - this is the engine that does the computation. RStudio is a program where you can see everything you are working on in one place. Figure 2.1 shows an example of a typical RStudio workspace: FIGURE 2.1: A sample workspace from one of my projects. There are 4 key panels that I work with, clockwise from the top: The source window is in the upper left - notice how those have different tabs associated with them. You can have multiple source files that you can toggle between. For the moment think of these as commands that you will want to send to R. The environment and history pane - these tables allow you to see what variables are stored locally in your environment, or the history of commands. The files or plots pane (a simple plot I was working on is shown currently), but you can toggle between the tabs. The files tab shows the files in the current Rstudio project directory. Finally, the console pane is the place where R works and runs commands. You can type in there directly; otherwise we will also just “send” commands from the source down to the console. Now we are ready to work with R and RStudio! 2.2 First steps: getting acquainted with R Open up RStudio. The first task is to create a project file. A project is a central place to organize your materials. If you have previous experience with R you may be familiar with how the program is picky about its working directory - or the location on the computer where computations, files, and data are currently saved. Creating a project file is an easy way to avoid some of that fussiness. Here are the steps to accomplish this: In RStudio select “File” then “New Project.” Next select the first option “New Directory” in the window - this will create a new folder on your computer. At the next window choose “New Directory” or “Existing Directory.” Depending on the option you choose, you will have some choice as to where you want to place this project. Name the project as you like. Click the “Create Project” button. It might be helpful to think of a project file as a physical folder where you store papers that have something in common (such as class notes). When you want to work on the project, you open up your folder (and similarly close your project when you are done). At the point of return, you can re-open your folder and pick up where you left off. An RStudio project is similar in that regard as well. 2.2.1 Working with R Our next step: how does R compute something? For example if we wanted to compute 4+9 we could type this command in the R console (lower left) window.5 Try this out now: In the console type 4+9 Then hit enter (or return) Is the result 13? Success! While this workflow is helpful for a single expression, making use of script files (.R file) can help run multiple steps of code at once. Script files are located in the upper left hand corner of your RStudio window - or the source window. (You may not have anything there when you start working on a project, so let’s create one.) In RStudio select “File” then “New File” Next select the first option “New Script” A new window called “UntitledX” should appear, where X is a number. You are set to go!6 Source files allow you to type in R code and then evaluate, which is sometimes called “sending a command to the console” - or moving an R statement from the source window to the console. Working with a script file allows you to fix any coding errors more quickly and then re-run your code rather than re-type everything. Let’s practice this.Type 4+9 in the script file. To evaluate this statement you have several options: Copying and pasting the command to the window. Shortcuts are Ctrl+C / Command+C for copying and Ctrl+V / Command+C for Windows / Mac. Run one line at a time. This means that your cursor is at the line in your source file; then click the “Run” button in the upper right hand side of the source window. Shortcuts are Ctrl+Enter / Command+Enter. You can also source the entire script file, which means running all the lines from top to bottom. You do this by clicking the source button, or with shortcuts Ctrl+Shift+Enter / Cmd+Shift+Enter (Windows / Mac). Understandably for several lines of code this makes things easier. Sometimes source files contain comments, which are helpful notes to you, or future you, or anyone else you want to share your work with. Comments in R are used with the hashtag (#), which appear as green text in RStudio. 2.2.2 Saving your work The neat part about a source file is that it allows you to save the file (Ctrl+S / Cmd+S). The first time you do this you may need to give this a name. The location where this file will be saved is in the same directory as your .Rproj project file. Now you have a file that you can come back to! In general I try to use descriptive names of files so I can refer back to them later. 2.3 Increasing functionality with packages Packages are one way that R gets some awesome versatility. Packages are contributed, specialized code produced by users (just like you!), and shared with the world. Packages are similar to apps on your phone, which rather than obtaining them from the app store can be found in two different places: CRAN, which stands for Comprehensive R Archive Network. This is the clearing house for many contributed packages - and allows for easy cross-platform functionality. Github. This is another place where people can share code and packages (including myself!). The code here has not been vetted through CRAN for compatibility, but if you trust the person sharing the code, it should work. Let’s now start to download some useful packages. The first package is tidyverse, which is actually a collection of packages. If you take an introductory data science course you will most likely be learning more about this package, but to install this at the command line you type the following: install.packages(&quot;tidyverse&quot;) Typing this line will connect to the CRAN download mirrors and install this set of packages locally to your computer. It make take some time, but be patient. Sometimes when you are installing packages you may be prompted to install additional packages. In this case just say yes. For this textbook I have written a collection of functions and data that we will use. This package name is called demodelr (Differential Equations and Models in R; Zobitz (2022)).7 To install this package you will run the following line: install.packages(&quot;demodelr&quot;) Here is the good news: you only need to install a package once before using it! To load the package up into your workspace you use the command library: # Purpose: compute the growth in weight of a dog over time. # Author: JMZ # Last modified: 02-17-2022 library(tidyverse) library(demodelr) You need to load these libraries each time you restart your R session. This is part of the benefit of a script file - at the start I always declare the libraries that I will need. In addition, the first few lines of the script file contains comments (prefaced with #) to denote the basic purpose of the file, who wrote it, and the date it was last revised. This type of information is good programming practice. If you are a newbie to programming with R, building these habits will become second nature as you progress in your abilities. 2.4 Working with R: variables, data frames, and datasets 2.4.1 Creating variables The next thing we will want to do is to define variables that are stored locally, which is easy to do: my_result &lt;- 4 + 9 The symbol &lt;- is assignment (you can use equals (=), but it is good coding practice to use the arrow for assignment). Notice how I named the variable called my_result. Generally I prefer using descriptive names for variables for the context at hand. (In other words, the variable x would be an odd choice - too ambiguous.) I also used snake case to string together multiple words. In practice you can use snake case, or alphabetic cases (myResult) or even my.result (although that may not be preferred practice in the long run). However, if you name variables as my-result it looks like subtraction between variables my and result. I try to follow the tidyverse style guide whenever possible. Once we have defined a variable, we can compute with it. For example 10*my_result should yield 130. Cool, no? Sequences defined as vectors are another useful construction. In R this is done with the seq function along with additional information such as the starting value, ending value, and step size. As an example, let’s define a sequence, spaced from 0 to 5 with spacing of 0.05 and then store this sequence as variable called my_sequence: my_sequence &lt;- seq(from = 0, to = 5, by = 0.05) The format for the function seq is seq(from=start,to=end,by=step_size). The seq command is a pretty flexible - there are alternative ways you can generate a sequence by specifying the starting and the end values along with the number of points. If you want to know more about seq you can always use ? followed by the command - that will bring up the help values: ?seq Once you get more comfortable with syntax in R, you will see that seq(0,5,0.5) gives the same result as seq(from=0,to=5,by=0.05), but it is helpful to write your code so that you can understand what it does.8 2.4.2 Data frames A key structure in R is that of a data frame, which allows different types of data to be collected together. A data frame is like a spreadsheet where each column is a value and each row a value (much like you would find in a spreadsheet). As an example, a data frame may list values for solutions to a differential equation, like we did with our three infection models in Chapter 1 (Table 2.1). TABLE 2.1: Sample model solutions for an exponential, saturating, or logistic differential equation time model_1 model_2 model_3 0 250 250 250 1 258 645 257 2 265 1027 265 3 274 1399 273 4 282 1760 281 Data frames are an example of tidy data, where each row is an observation, each column a variable (which can be quantitative or categorical). There are several different ways to define a data frame in R. I am going to rely on the approach utilized by the tidyverse, which defines data frames as tibbles. As an example, the following code defines a data frame that computes the quadratic function \\(y=3x^2-2x\\) for \\(-5 \\leq x \\leq 2\\). x &lt;- seq(from = -5, to = 2, by = 0.05) y &lt;- 3 * x^2 - 2 * x my_data &lt;- tibble( x = x, y = y ) # Notice how x and y are specifically defined Notice that the data frame my_data uses the column (variable) names of x and y. You could have also used tibble(x,y), but it is helpful to name the columns in the way that you would like them to be named. In addition to defining a data frame, R also contains several datasets in memory. In fact to see all the datasets, type data() at the console. Packages may also have datasets bundled with them. If you want to see the datasets for the demodelr package, you would type data(package = \"demodelr\") at the console. 2.4.3 Reading in datasets Another R skill is importing data into R. Data come in several different types of formats, but one of the more versatile ones is a csv (comma separated values) file. A csv file is a simplified version of an Excel or Google spreadsheet.9 To read in the file you will use the command read_csv (part of the readr package in the tidyverse). The read_csv command which has the following structure, where FILENAME refers to the location of the file on your computer: in_data &lt;- read_csv(FILENAME) For example the following code would read in a csv file of Ebola data located in the project directory: ebola &lt;- read_csv(&quot;ebola.csv&quot;) Notice the quotes around the FILENAME.10 The command read_csv is part of the tidyverse, but the function read.csv uses base R. They operate a little differently, but this book will use the read_csv command. 2.5 Visualization with R Now we are ready to begin visualizing data frames. Two types of plots that we will need to make will be a scatter plot and a line plot. We are going to consider both of these separately, with examples that you should be able to customize. 2.5.1 Making a scatterplot One dataset we have is the weight of a dog over time, adapted from this referenced website. The data frame we will use is called wilson and is part of the demodelr library. You can also explore the documentation for this dataset by typing ?wilson at the console. The wilson dataset has two variables here: \\(D=\\) the age of the dog in days and \\(W=\\) the weight of the dog in pounds. I have the data loaded into the demodelr package, which you can investigate by typing the following at the command line: glimpse(wilson) Notice that this data frame has two variables: days and weight. To make a scatter plot of these data we are going to use the command ggplot in Figure 2.2: ggplot(data = wilson) + geom_point(aes(x = days, y = weight)) + labs( x = &quot;Days since birth&quot;, y = &quot;Weight (pounds)&quot; ) FIGURE 2.2: Measured weight of the dog Wilson over time. Wow! The code to produce Figure 2.2 looks complicated. Let’s break this down step by step: ggplot(data = wilson) + sets up the graphics structure and identifies the name of the data frame we are including. geom_point(aes(x = days, y = weight)) defines the type of plot we are going to be making. geom_point() defines the type of plot geometry (or geom) we are using here - in this case, a point plot. aes(x = days, y = weight) maps the aesthetics of the plot. On the \\(x\\) axis is the days variable; on the \\(y\\) axis is the weight variable. You may also write this as mapping = aes(x = days, y = weight). The statement beginning with labs(x=...) defines the labels on the \\(x\\) and \\(y\\) axes. I know this seems like a lot of code to make a visualization, but this structure is actually used for some more advanced data visualization. Think of the + structure at the end of each line as the connector between ggplot and the plot geom. Trust me - learning how to make informative plots can be a useful skill! 2.5.2 Making a line plot Using the same wilson data, later on we will discover that the function \\(\\displaystyle W =f(D)= \\frac{70}{1+e^{2.46-0.017D}}\\). represents these data. In order to make a graph of this function we need to first build a data frame (Figure 2.3): # Choose spacing that is &quot;smooth enough&quot; days &lt;- seq(from = 0, to = 1500, by = 1) weight &lt;- 70 / (1 + exp(2.46 - 0.017 * days)) wilson_model &lt;- tibble( days = days, weight = weight ) ggplot(data = wilson_model) + geom_line(aes(x = days, y = weight)) + labs( x = &quot;Days since birth&quot;, y = &quot;Weight (pounds)&quot; ) FIGURE 2.3: Logistic model equation to describe the weight of the dog Wilson over time. Notice that once we have the data frame set up, the structure is very similar to the scatter plot - but this time we are using geom_line() rather than geom_point. 2.5.3 Changing options Curious about using a different color in your plot or a thicker line? That is fairly easy to do. For example if we wanted to make either our points or line a different color, we adjust the ggplot to the following code (not evaluated here, but try it out on your own): ggplot(data = wilson) + geom_point(aes(x = days, y = weight), color = &quot;red&quot;, size = 2) labs( x = &quot;Days since birth&quot;, y = &quot;Weight (pounds)&quot; ) Notice how the command color='red' was applied outside of the aes - which means it gets mapped to each of the points in the data frame. size=2 refers to the size (in millimeters) of the points. I’ve linked more options about the colors and sizes you can use here: Named colors in R: gallery of R colors. Scroll down to “Picking one color in R” - you can see the list of options! More colors: colors in ggplot.. More information about working with colors. Using hexadecimal colors: hexadecimal colors. (You specify these by the code so \"#FF3300\" is a red color.) Changing sizes of lines and points: modifying a ggplot. 2.5.4 Combining scatter and line plots. Combining the data (Figure 2.2) with the model (Figure 2.3) in the same plot can be done by combining the geom_point with the geom_line, as shown in the following code (try it out on your own): ggplot(data = wilson) + geom_point(aes(x = days, y = weight), color = &quot;red&quot;) + geom_line(data = wilson_model, aes(x = days, y = weight)) + labs( x = &quot;Days since birth&quot;, y = &quot;Weight (pounds)&quot; ) Notice in the above code a subtle difference when I added in the dataset wilson_model with geom_line: you need to name the data bringing in a new data frame to a plot geom. While it may be useful to have a plot legend, for this textbook the context will be apparent without having a legend. 2.6 Defining functions We will study lots of other built-in functions for this course, but you may also be wondering how you define your own function (let’s say \\(y=x^{3}\\)). We need the following construct for our code: function_name &lt;- function(inputs) { # Code return(outputs) } Here function_name serves as what you call the function, inputs are what you need in order to run the function, and outputs are what gets returned. So if we are doing \\(y=x^{3}\\) then we will call that function cubic: cubic &lt;- function(x) { y &lt;- x^3 return(y) } So now if we want to evaluate \\(y(2)=2^{3}\\) at the console we type cubic(2). Neat! The following code will make a plot of the function \\(y=x^{3}\\) using cubic (try this out on your own): x &lt;- seq(from = 0, to = 2, by = 0.05) y &lt;- cubic(x) my_data &lt;- tibble(x = x, y = y) ggplot(data = my_data) + geom_line(aes(x = x, y = y)) + labs( x = &quot;x&quot;, y = &quot;y&quot; ) 2.6.1 Functions with multiple inputs Sometimes you may want to define a function with different input parameters, so for example the function \\(y=x^{3}+c\\). To define that, we can modify the function to have input variables: cubic_revised &lt;- function(x, c) { y &lt;- x^3 + c return(y) } To create and plot several examples of the function cubic for different values of \\(c\\) is shown in the following code and Figure 2.4. x &lt;- seq(from = 0, to = 2, by = 0.05) my_data_revised &lt;- tibble( x = x, c_zero = cubic_revised(x, 0), c_pos1 = cubic_revised(x, 1), c_pos2 = cubic_revised(x, 2), c_neg1 = cubic_revised(x, -1) ) ggplot(data = my_data_revised) + geom_line(aes(x = x, y = c_zero)) + geom_line(aes(x = x, y = c_pos1)) + geom_line(aes(x = x, y = c_pos2)) + geom_line(aes(x = x, y = c_neg1)) + labs( x = &quot;x&quot;, y = &quot;y&quot; ) FIGURE 2.4: Plot of several cubic functions \\(y=x^{3}+c\\) when \\(c=-1,0,1,2\\). Notice how I defined multiple columns of the data frame my_data_revised in the tibble command, and then used mutiple geom_line commands to plot the data. Since we had combined the different values of c in a single data frame we didn’t need to define the data with each instance of geom_line. 2.7 Concluding thoughts This is not meant to be a self-contained chapter in R but rather one so that you can quickly compute to code. Curious to learn more? Thankfully there are several good resources. Here are few of my favorites that I turn to: R Graphics. This is a go-to resource for making graphics. (I also use Google a lot too.) The Pirates Guide to R. This book promises to build your R knowledge from the ground up. R for Reproducible Scientific Analysis. This set of guided tutorials can help you build your programming skills in R. R for Data Science. This is a useful book to take your R knowledge to the next level. The best piece of advice: DON’T PANIC! Patience and persistence are your friend. Reach out for help, and recognize that like with any new endeavor, practice makes progress. 2.8 Exercises Exercise 2.1 Create a folder on your computer and a project file where you will store all your R work. Exercise 2.2 Install the packages devtools, tidyverse to your R installation. Once that is done, then install the package demodelr. Exercise 2.3 What are the variables listed in the dataset phosphorous in the demodelr library? (Hint: try the command ?phosphorous.) Exercise 2.4 Make a scatterplot (geom_point()) of the dataset phosphorous in the demodelr library. Be sure to label the axes with descriptive titles. Exercise 2.5 Change Figure 2.3 so the line is blue and the size is 4 mm. Exercise 2.6 Change the color of the points in Figure 2.2 to either a hexadecimal color or a named color of your choice. Exercise 2.7 For this exercise you will do some plotting: Define a sequence (call this sequence \\(x\\)) that ranges between \\(-12\\) to \\(12\\) with spacing of \\(.05\\). Also define the variable \\(y\\) such that \\(y=\\sin(x)\\). Make a scatter plot to graph \\(y=\\sin(x)\\). Set the points to be red. Make a line plot to graph \\(y=\\sin(x)\\). Label the x-axis with your favorite book title. Label the y-axis with your favorite food to eat. Exercise 2.8 An equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\(\\displaystyle y = c x^{1/\\theta}\\), where \\(\\theta \\geq 1\\) and \\(c&gt;0\\) are both constants. Let’s just assume that \\(c=1\\) and the \\(0 \\leq x \\leq 1\\). Construct a function called nutrient that will make a sequence of y values for an input x and theta (\\(\\theta\\)). Use your nutrient function to create a line plot (geom_line()) for five different values of \\(\\theta&gt;1\\), appropriately labeling all axes. Exercise 2.9 The dataset phosphorous in the demodelr library contains measurements of the phosphorous content of Daphnia and its primary food source algae. Researchers believe that Daphnia has strict homeostatic regulation of the phosphorous contained in algae, and want to determine the value of \\(\\theta\\) in the equation \\(y= \\displaystyle y = c x^{1/\\theta}\\). They have already determined that the value of \\(c=1.737\\). Complete Exercise 2.4. Be sure to label the axes correctly. Use your function nutrient from Exercise 2.8 to make an initial guess for theta (\\(\\theta\\)) that is consistent with the data. You can evaluate your guess by plotting (with geom_line()) against the data. Use guess and check to refine the value of \\(\\theta\\) that seems to work best. Report your value of \\(\\theta\\). Exercise 2.10 For this exercise you will investigate some built-in functions. Remember you can learn more about a function by typing ?FUNCTION, where FUNCTION is the name. Explain (using your own words) what the function runif(1,100,1000) does. Explain (using your own words) what the function ceiling() does, showing an example of its use. Exercise 2.11 The Ebola outbreak in Africa in 2014 severely affected the country of Sierra Leone. A model for the number of Ebola infections \\(I\\) is given by the following equation: \\[ I(t) = \\frac{K \\cdot I_{0} }{I_{0} + (K-I_{0}) \\exp(-rt)}, \\] where \\(K = 13580\\), \\(I_{0}=251\\) and \\(r = 0.0227\\). The variable \\(t\\) is in days. Use geom_line() to visualize this curve from \\(0 \\leq t \\leq 700\\). Exercise 2.12 Consider the following piecewise function: \\[\\begin{equation} y = \\begin{cases} x^2 &amp; \\text{ for } 0 \\leq x &lt; 1,\\\\ 2-x &amp;\\text{ for } 1 \\leq x \\leq 2 \\\\ \\end{cases} \\end{equation}\\] Define a function in R that computes \\(y\\) for \\(0 \\leq x \\leq 2\\). Use geom_line() to generate a graph of \\(y(x)\\) over the interval \\(0 \\leq x \\leq 2\\). Exercise 2.13 An insect’s development rate \\(r\\) depends on temperature \\(T\\) (degrees Celsius) according to the following equation: \\[\\begin{equation} r = \\begin{cases} 0.1 &amp; \\text{ for } 17 \\leq T &lt; 27,\\\\ 0 &amp;\\text{ otherwise.} \\end{cases} \\end{equation}\\] Define a function in R that computes \\(r\\) for \\(0 \\leq T \\leq 30\\). Use geom_line() to generate a graph of \\(r(T)\\) over the interval \\(0 \\leq T \\leq 30\\). References "],["modeling-rates-03.html", "Chapter 3 Modeling with Rates of Change 3.1 Competing plant species and equilibrium solutions 3.2 The Law of Mass Action 3.3 Coupled differential equations: lynx and hares 3.4 Functional responses 3.5 Exercises", " Chapter 3 Modeling with Rates of Change Chapter 1 provided examples for modeling with rates of change, and Chapter 2 introduced the computational and visualization software R and RStudio, and how we can translate equations with rates of change to understand phenomena. The focus for this chapter will be on taking a contextual description and starting to develop differential equation models for them. Oftentimes when we construct differential equations from a contextual description we bring our own understanding and knowledge to this situation. How you may write down the differential equation may be different from someone else - do not worry! This is the fun part of modeling: models can be considered testable hypotheses that can be refined when confronted with data. Let’s get started 3.1 Competing plant species and equilibrium solutions Consider the following context to develop a mathematical model: A newly introduced plant species is introduced to a region. It competes with another established species for nutrients (and is a better competitor). However, the growth rate of the new species is proportional to the difference between the current number of established species and the number of new species. You may assume that the number of established species is a constant E. For this problem we will start by naming our variables. Let \\(N\\) represent number of new species and \\(E\\) the number of established species. We will break this down accordingly: “the growth rate of the new species” describes the rate of change, or derivative, expressed as \\(\\displaystyle \\frac{dN}{dt}\\). “is proportional to the difference between the current number of established species and the number of new species” means \\(\\displaystyle \\alpha \\cdot (E-N)\\), where \\(\\alpha\\) is the proportionality constant. Including this parameter helps to avoid assuming we have a 1:1 correspondence between the growth rate of the new species and the population difference. “and is a better competitor” helps to explain why the term is \\(\\displaystyle \\alpha \\cdot (E-N)\\) instead of \\(\\displaystyle \\alpha \\cdot (N-E)\\). We know that the newly established species will start out in much smaller numbers than \\(N\\). But since it is a better competitor, we would expect its rate to increase initially. So \\(\\displaystyle \\frac{dN}{dt}\\) should be positive rather than negative. Taking all these assumptions together, Equation (3.1) shows the differential equation to model this context: \\[\\begin{equation} \\frac{dN}{dt} = \\alpha \\cdot (E-N) \\tag{3.1} \\end{equation}\\] You may recognize that Equation (3.1) is similar to Equation (1.4) in Chapter 1 for the spread of Ebola. It is not surprising to have similar differential equations appear in different contexts. We will see throughout this book that it is more advantageous to learn techniques to analyze models qualitatively rather than memorize several different types of models and not see the connections between them. An interesting solution to a differential equation is the steady state or equilibrium solution. Equilibrium solutions occur where the rates of change are zero. For Equation (3.1), this means that we are solving \\(\\displaystyle \\frac{dE}{dt} = \\alpha \\cdot (E-N) = 0\\). Granted, the expression \\(\\alpha \\cdot (E-N)\\) may look like alphabet soup, but it is helpful to remember that \\(\\alpha\\) and \\(E\\) are both parameters; the steady state occurs when the expression \\(E-N\\) equals zero, or when \\(N = E\\). We may consider the new species \\(N\\) to be established when it reaches the same population level as \\(E\\). Identifying steady states in a model aids in understanding the behavior of any solutions for a differential equation. Chapters 5 and 6 dig deeper into steady states and their calculation. 3.2 The Law of Mass Action Our next example focuses on how to generate a model that borrows concepts from modeling chemical reactions. For example let’s say you have a substrate A that reacts with enzyme B to form a product S. One common way to represent this process is with a reaction equation (Equation (3.2)): \\[\\begin{equation} A+B \\rightarrow S \\tag{3.2} \\end{equation}\\] Figure 3.1 is a schematic diagram of Equation (3.2): FIGURE 3.1: Schematic diagram of a substrate-enzyme reaction. One key quantity is the rate of formation for the product \\(P\\), which we express by Equation (3.3): \\[\\begin{equation} \\frac{dP}{dt}= kAB, \\tag{3.3} \\end{equation}\\] where \\(k\\) is the proportionality constant or the rate constant associated with the reaction. Notice how we express the interaction between \\(A\\) and \\(B\\) as a product - if either the substrate \\(A\\) or enzyme \\(B\\) is not present (i.e. \\(A\\) or \\(B\\) equals zero), then product \\(P\\) is not formed. Equation (3.3) is an example of the law of mass action. Modeling interactions (whether between susceptible and infected individuals, enzymes and substrates, or predators and prey) with the law of mass action is always a good first assumption to understand the system, which can be subsequently refined. For example, if we consider that the substrate might decay, we can revise Figure 3.1 to Figure 3.2: FIGURE 3.2: Revised schematic diagram of substrate-enzyme reaction with decay of the product \\(P\\). In this instance the rate of change of \\(P\\) would then include a term \\(dP\\) (Equation (3.4): \\[\\begin{equation} \\frac{dP}{dt}= kAB - dP \\tag{3.4} \\end{equation}\\] 3.3 Coupled differential equations: lynx and hares Another example is a system of differential equations. The context is between the snowshoe hare and the Canadian lynx, shown in Figure 3.3. Figure 3.4 also displays a timeseries of the two populations overlaid. Notice how in Figure 3.4 both populations show regular periodic fluctuations. One plausible reason is that the lynx prey on the snowshoe hares, which causes the population to initially decline. Once the snowshoe hare population declines, then there is less food for the lynx to survive, so their population declines. The decline in the lynx population causes the hare population to increase, and the cycle repeats.11 FIGURE 3.3: Examples of lynx and hare - aren’t they beautiful? FIGURE 3.4: Timeseries of the combined lynx and hare populations. Notice how the populations are coupled with each other. In summary it is safe to say that the two populations are coupled to one another, yielding a coupled system of equations. But in order to understand how they are coupled together, first let’s consider the two populations separately. To develop the mathematical model we will make some simplifying assumptions. The hares grow much more quickly than then lynx - in fact some hares have been known to reproduce several times a year. A reasonable assumption for large hare populations is that rate of change of the hares is proportional to the hare population. Based on this assumption Equation (3.5) describes the rate of change of the hare population, with \\(H\\) as the population of the hares: \\[\\begin{equation} \\frac{dH}{dt} = r H \\tag{3.5} \\end{equation}\\] Since the growth rate \\(r\\) is positive, so then the rate of change (\\(H&#39;\\)) will be positive as well, and \\(H\\) will be increasing. A representative value for \\(r\\) is 0.5 year\\(^{-1}\\) (Mahaffy 2010; Brady and Butler 2021). You may be thinking that the units on \\(r\\) seem odd - (year\\(^{-1}\\)), but that unit on \\(r\\) makes the term \\(rH\\) dimensionally consistent to be a rate of change. Let’s consider the lynx now. An approach is to assume their population declines exponentially, or changes at the rate proportional to the current population. Let’s consider \\(L\\) to be the lynx population, with the following differential equation (Equation (3.6)): \\[\\begin{equation} \\frac{dL}{dt} = -dL \\tag{3.6} \\end{equation}\\] We assume the death rate \\(d\\) in Equation (3.6) is positive, leading to a negative rate of change for the Lynx population (and a decreasing value for \\(L\\)). A typical value of \\(d\\) is 0.9 yr\\(^{-1}\\) (Mahaffy 2010; Brady and Butler 2021). The next part to consider is how the lynx and hare interact. Since the hares are prey for the lynx, when the lynx hunt, the hare population decreases. We can represent the process of hunting with the following adjustment to our hare equation: \\[\\begin{equation} \\frac{dH}{dt} = r H - b HL \\end{equation}\\] So the parameter \\(b\\) represents the hunting rate. Notice how we have the term \\(HL\\) for this interaction. This term injects a sense of realism: if the lynx are not present (\\(L=0\\)), then the hare population can’t decrease due to hunting. We model the interaction between the hares and the lynx with multiplication between the \\(H\\) and \\(L\\). A typical value for \\(b\\) is .024 lynx\\(^{-1}\\) year\\(^{-1}\\). It is okay if that unit seems a little odd to you - it should be! As before, if we multiply out the units on \\(bHL\\) we would get units of hares per year. How does hunting affect the lynx population? One possibility is that it increases the lynx population: \\[\\begin{equation} \\frac{dL}{dt} =bHL -dL \\end{equation}\\] Notice the symmetry between the rate of change for the hares and the lynx equations. In many cases this makes sense - if you subtract a rate from one population, then that rate should be added to the receiving population. You could also argue that there is some efficiency loss in converting the hares to lynx - not all of the hare is converted into lynx biomass. In this situation we sometimes like to adjust the hunting term for the lynx equation with another parameter \\(e\\), representing the efficiency that hares are converted into lynx: \\[\\begin{equation} \\frac{dL}{dt} =e \\, bHL -dL \\end{equation}\\] (sometimes people just make a new parameter \\(c=e \\, b\\), but for now we will just leave it as is and set \\(e=0.2\\)). Equation (3.7) shows the coupled system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= r H - b HL \\\\ \\frac{dL}{dt} &amp;=e\\,bHL -dL \\end{split} \\tag{3.7} \\end{equation}\\] The schematic diagram representing these interactions is shown in Figure 3.5: FIGURE 3.5: Schematic diagram Lynx-Hare system. Equation (3.7) is a classical model in mathematical biology and differential equations - it is called the predator-prey model, also known as the Lotka-Volterra model (Lotka 1920, 1926; Volterra 1926). 3.4 Functional responses In several examples we have seen a rate of change proportional to the current population, as, for example, the rate of growth of the hare population is \\(rH\\). This is one example of what we would call a functional response. Another type of functional response assumes that the rate reaches a limiting value proportional to the population size, so \\(\\displaystyle \\frac{dH}{dt} = \\frac{rH}{1+arH}\\). This is an example of a type II functional response. Finally, the type II response has also been generalized (a type III functional response) \\(\\displaystyle \\frac{dH}{dt} = \\frac{rH^{2}}{1+arH^{2}}\\). Figure 3.6 shows all three functional responses together: FIGURE 3.6: Comparison between examples of Type I - Type III functional responses. For a Type I functional response the rate grows proportional to population size H, whereas for Types II and III the rate reaches a saturating value. Notice the limiting behavior in the Type II and Type III functional responses. These responses are commonly used in ecology and predator-prey dynamics and in problems of how animals search for food. 3.5 Exercises Exercise 3.1 Consider the following types of functional responses: \\[\\begin{equation} \\begin{split} \\mbox{ Type I: } \\frac{dP}{dt} &amp;= 0.1 P \\\\ \\mbox{ Type II: } \\frac{dP}{dt} &amp;= \\frac{0.1P}{1+.03P} \\\\ \\mbox{ Type III: } \\frac{dP}{dt} &amp;= \\frac{0.1P^{2}}{1+.05P^{2}} \\end{split} \\end{equation}\\] For each of the functional responses evaluate \\(\\displaystyle \\lim_{P \\rightarrow \\infty} \\frac{dP}{dt}\\). Since these functional responses represent a rate of change of a population, what are some examples (hypothetical or actual) in which each of these responses would be appropriate? Exercise 3.2 A population grows according to the equation: \\[\\begin{equation} \\frac{dP}{dt} = \\frac{P}{1+.05P} -.1P = f(P) - g(P) \\end{equation}\\] On the same axis, plot the equations \\(f(P)\\) and \\(g(P)\\). What are the two positive values of \\(P\\) where \\(f(P)\\) and \\(g(P)\\) intersect? Next algebraically determine the two steady state values of \\(P\\), that is solve \\(\\displaystyle \\frac{dP}{dt}=0\\) for \\(P\\). (Hint: factor a \\(P\\) out of the expression \\(\\displaystyle f(P)-g(P)\\).) Does your algebraic solution match your graphical solutions? Exercise 3.3 A population grows according to the equation: \\[\\begin{equation} \\frac{dP}{dt} = 2P - \\frac{4P^{2}}{1+P^{2}} = r(P)-d(P) \\end{equation}\\] On the same axis, plot the equations \\(r(P)\\) and \\(d(P)\\). What are the two positive values of \\(P\\) where \\(r(P)\\) and \\(d(P)\\) intersect? Next algebraically determine the two steady state values of \\(P\\), that is solve \\(\\displaystyle \\frac{dP}{dt}=0\\) for \\(P\\). (Hint: factor a \\(P\\) out of the expression \\(r(P)-d(P)\\).) Does your algebraic solution match your graphical solutions? Exercise 3.4 A population grows according to the equation: \\[\\begin{equation} \\frac{dP}{dt} = \\frac{aP}{1+abP} - dP, \\end{equation}\\] where \\(a\\), \\(b\\), and \\(d\\) are all positive parameters. Determine the two steady state values of \\(P\\), that is solve \\(\\displaystyle \\frac{dP}{dt}=0\\) for \\(P\\). Exercise 3.5 A chemical reaction takes two chemicals \\(X\\) and \\(Y\\) to form a substrate \\(Z\\) through the law of mass action. However the substrate can also disassociate. The reaction schematic is the following: \\[\\begin{equation} X + Y \\rightleftharpoons Z, \\end{equation}\\] where you may define the proportionality constant \\(k_+\\) as associated with the formation of the substrate \\(Z\\) and \\(k_-\\) the disassociation (\\(Z\\) decays back to \\(X\\) and \\(Y\\)).   Write down a differential equation that represents the rate of reaction \\(\\displaystyle \\frac{dZ}{dt}\\). Exercise 3.6 (Inspired from Thornley and Johnson (1990) and Logan and Wolesensky (2009)) For each of the following exercises consider the following contextual situations modeling rates of change. For each problem you will need to: Name and describe all variables and parameters; Determine a differential equation representing the context; Write a brief one-two sentence explanation of why your differential equation models the situation at hand; Hand sketch a rough graph of what you think the solution is as a function of time, consistent with the context given. The rate of change of an animal’s body temperature is proportional to the difference in temperature between the environment and the current body temperature of the animal. A plant grows proportional to its current length \\(L\\). Assume this proportionality constant is \\(\\mu\\), whose rate also decreases proportional to its current value. You will need to write down a system of two equations with variables \\(L\\) and \\(\\mu\\). A patient undergoing chemotherapy receives an injection at rate \\(I\\). This injection decreases the rate that a tumor accumulates mass. Independent of the injection, the tumor accumulates mass at a rate proportional to the mass of the tumor. A cell with radius \\(r\\) assimilates nutrients at a rate proportional to its surface area, but uses nutrients proportional to its volume. Determine an equation that represents the rate of change of the radius. The rate that a cancer cell divides (increases in amount) is proportional to the number of healthy cells in its surrounding environment. You may assume that a healthy cell has mortality \\(\\delta_{H}\\) and a cancer cell has mortality \\(\\delta_{C}\\). Be sure to write down a system of differential equations for the population of cancer cells \\(C\\) and healthy cells \\(H\\). The rate that a virus is spread to the population is proportional to the probability that a person is sick (out of \\(N\\) total sick and healthy individuals). FIGURE 3.7: Modeled reaction schemes representing the potential effect of a pesticide on water quality. Exercise 3.7 (Inspired by Burnham and Anderson (2002)) You are tasked with the job of investigating the effect of a pesticide on water quality, in terms of its effects on the health of the plants and fish in the ecosystem. Different models can be created that investigate the effect of the pesticide. Different types of reaction schemes for this system are shown in Figure 3.7, where \\(F\\) represents the amount of pesticide in the fish, \\(W\\) the amount of pesticide in the water, and \\(S\\) the amount of pesticide in the soil. The prime (e.g. \\(F&#39;\\), \\(W&#39;\\), and \\(S&#39;\\) represent other bound forms of the respective state). In all seven different models can be derived. For each of the model schematics, apply the Law of Mass Action to write down a system of differential equations. References "],["euler-04.html", "Chapter 4 Euler’s Method 4.1 The flu and locally linear approximation 4.2 A workflow for approximation 4.3 Building an iterative method 4.4 Euler’s method and beyond 4.5 Exercises", " Chapter 4 Euler’s Method Chapter 3 examined modeling with rates of change. Once a differential equation model is defined one possible next step is to determine the solution to the differential equation. While in some cases an exact solution can be found (Chapter 7), in many instances we will rely on numerical methods. The focus of this chapter is on approximation of solutions to a differential equation via a numerical method. Typically a first numerical method you might learn is Euler’s method, popularized in the movie Hidden Figures. This chapter will develop Euler’s method from tangent line equations or locally linear approximations from calculus. Let’s get started! 4.1 The flu and locally linear approximation Consider Equation (4.1), which is one way to model the rate of change of the flu through a population: \\[\\begin{equation} \\frac{dI}{dt} = 3e^{-.025t} \\tag{4.1} \\end{equation}\\] In Equation (4.1) the variable \\(I\\) represents the number of people infected at day \\(t\\). One question we could address using Equation (4.1) is to predict the value of \\(I\\) after 1 day, assuming that \\(I(0)=10\\). To do that we will build a locally linear approximation at \\(t=0\\) and use the approximation to forecast and estimate \\(I(1)\\). In order to solve this problem, the formula for the locally linear approximation to \\(I(t)\\) at \\(t=0\\) is \\(L(t) = I(0) + I&#39;(0) \\cdot (t-0)\\). Here, \\(I(0)=10\\) and \\(I&#39;(0)=3\\) (found by evaluating Equation (4.1) at \\(t=0\\)). Using \\(L(t) \\approx I(t)\\), the formula for the locally linear approximation is given by Equation (4.2). To define Equation (4.2) we used two pieces of information: the (given) value of the function at \\(t=0\\) and the estimate of the derivative from Equation (4.1). \\[\\begin{equation} L(t)=10 + 3t \\tag{4.2} \\end{equation}\\] At \\(t=1\\) we can make a prediction with Equation (4.2) to estimate that there will be 13 people sick. To evaluate this approximation it is helpful to compare our prediction from \\(L(1)\\) (Equation (4.2)) to the actual value from the solution to the differential equation given in Equation (4.3): \\[\\begin{equation} I(t) = 130-120e^{-.025t} \\tag{4.3} \\end{equation}\\] Table 4.1 compares the values of the linear approximation (Equation (4.2)) to Equation (4.3): TABLE 4.1: Comparison of the exact solution \\(I(t)\\) (Equation (4.3)) to the linear approximation \\(L(t)\\) (Equation (4.2)) at \\(t=0\\) and \\(t=1\\). \\(t\\) Linear approximation \\(L(t)\\) Exact solution \\(I(t)\\) 0 10 10 1 13 12.96 Table 4.1 shows that \\(L(1)\\) is an overestimate compared to \\(I(1)\\). Let’s expand Equation (4.2) even more by constructing another linear approximation using the differential equation. We will denote this linear approximation as \\(L_{1}(t)\\) to distinguish it from \\(L(t)\\) from Equation (4.2). First we evaluate Equation (4.1), which yields \\(I&#39;(1)=2.92\\). The formula for the linear approximation at \\(t=1\\) is \\(L_{1}(t) = I(1) + I&#39;(1) \\cdot (t-1)\\). Here we will use \\(I(1) = 13\\), recognizing that this value is a pretty close estimate for the number infected (\\(I\\)) at \\(t=1\\). This assumption yields \\(L_{1}(t) = 13 +2.92(t-1)\\). We can continue to build out the solution in a similar manner to develop a locally linear approximation at \\(t=2\\), shown graphically in Figure 4.1. The approximation and the exact solution in Figure 4.1 appear very close to each other, suggesting that approximation using local linearization could work for other types of differential equations. FIGURE 4.1: Approximation of a solution to Equation (4.1) using local linearity. By eye, the approximate and exact solutions in Figure 4.1 appear indistinguishable from each other. Encouraged by these results, let’s develop the approach with linear approximations even more. 4.2 A workflow for approximation The previous chapter alludes to a possible workflow to numerically approximate a solution to a differential equation: Determine the locally linear approximation at a given point. Forecast out to another time value. Repeat the locally linear approximation. The results of continuing this workflow (approximate \\(\\rightarrow\\) forecast \\(\\rightarrow\\) repeat) several times is shown in Table 4.2. Comparison of the exact solution \\(I(t)\\) (Equation (4.3)) to the linear approximation \\(L(t)\\) (Equation (4.2)) at \\(t=0\\) and \\(t=1\\). TABLE 4.2: Comparison of the exact solution \\(I(t)\\) (Equation (4.3)) to forecasting with linear approximations at \\(t=90\\) and \\(t=95\\). \\(t\\) Approximate solution Exact solution \\(I(t)\\) 90 118.4 117 95 119.9 118.6 Table 4.2 suggests that the accuracy of our solution decreases as time increases. A potential fix would be to approximate the solution not at every day, but every half day. The length of time that we forecast out our solution is called the step size, denoted as \\(\\Delta t\\). While approximating our solution every half day (\\(\\Delta t = 0.5\\)) would require more computation (or more iterations) of the locally linear approximation, perhaps a smaller \\(\\Delta t\\) would lead to more accurate solutions. Let’s start out smaller with the first few timesteps (Table 4.3): TABLE 4.3: Calculation of the solution \\(I(t)\\) for Equation (4.1) using the linear approximations at each timestep with \\(\\Delta t = 0.5\\). \\(t\\) \\(I\\) \\(\\displaystyle \\frac{dI}{dt}\\) \\(\\displaystyle \\frac{dI}{dt} \\cdot \\Delta t\\) 0 10 3 1.5 0.5 = 10 + 1.5 = 11.5 2.96 1.48 1 = 11.5 + 1.48 = 12.98 2.92 1.46 1.5 = 12.92 + 1.46 = 14.38 2.88 1.44 2 = 14.38 + 1.44 = 15.82 Notice how Table 4.3 organizes a way to compute the solution \\(I\\) with linear approximations. Each row is a “step” of the method, computing the solution based on our step size \\(\\Delta t\\). The third column computes the value of the derivative for a particular time (Equation (4.1)), and then the fourth column represents the forecasted change in the solution by the next timestep.12 This idea of approximate, forecast, repeat is at the heart of many numerical methods that approximate solutions to differential equations. The particular method that we have developed here is called Euler’s method. We display the results from additional steps in Figure 4.2. Based on the trend of the solution in Figure 4.2, it appears that the number of infections might start to level off at \\(I=130\\), which is the steady state value in Equation (4.3) when evaluating \\(\\displaystyle \\lim_{t \\rightarrow \\infty} I(t)\\). FIGURE 4.2: Longer-term approximation of a solution to Equation (4.1). Notice how the solution seems to level off to a steady state at \\(I=130\\) (dashed line). 4.3 Building an iterative method Now that we have worked on an example, let’s carefully formulate Euler’s method with another example. In Chapter 1 we discussed the spread of Ebola through a population. Equation (4.5) is the differential equation for the logistic model (see Equation (1.8) and Figure 1.5), modeled with Equation (4.4): \\[\\begin{equation} \\frac{dI}{dt} = 0.023 I \\cdot (13600-I) \\tag{4.4}, \\end{equation}\\] where the variable \\(I\\) represents the proportion of people that are infected. The carrying capacity, or the place where the solution levels off in Equation (4.4) is at \\(I=13600\\) (notice that when \\(I=13600\\), \\(\\displaystyle \\frac{dI}{dt}=0\\)). Numerical methods such as Euler’s method can become unstable for large values of the independent variable, because the rates are so large. To account for this, we will re-define Equation (4.4) with the variable \\(p = \\frac{I}{13600}\\), leading to the revised model: \\[\\begin{equation} \\frac{dp}{dt} = 0.023 p \\cdot (1-p) \\tag{4.5}, \\end{equation}\\] where the variable \\(p\\) represents the proportion of infected, So \\(p=1\\) means that 13600 people are infected. Once we have our solution \\(p(t)\\), we can just multiply that by \\(N=13600\\) to return back to the total infected. In Equation (4.5) we define the function \\(f(p) = 0.023 p\\cdot (1-p)\\). In order to numerically approximate the solution, we will need to recall some concepts from calculus. This first step is that we will approximate the rate of change \\(\\displaystyle \\frac{dp}{dt}\\) with a difference quotient (Equation (4.6)): \\[\\begin{equation} \\frac{dp}{dt} = \\lim_{\\Delta t \\rightarrow 0} \\frac{p(t+\\Delta t) - p(t)}{\\Delta t} \\tag{4.6} \\end{equation}\\] When the quantity \\(\\Delta t\\) in Equation (4.6) is small (for example \\(\\Delta t = 1\\) day), this difference quotient provides a reasonable way to organize the problem: \\[\\begin{equation} \\begin{split} \\frac{p(t+\\Delta t) - p(t)}{\\Delta t} &amp;= 0.023 p \\cdot (1-p) \\\\ p(t+\\Delta t) - p(t) &amp;= 0.023 p \\cdot (1-p) \\cdot \\Delta t \\\\ p(t+\\Delta t) &amp;= p(t) + 0.023 p \\cdot (1-p) \\cdot \\Delta t \\end{split} \\end{equation}\\] The last expression (\\(p(t+\\Delta t) = p(t) + 0.023 p \\cdot (1-p) \\cdot \\Delta t\\)) defines an iterative system, easily computed with a spreadsheet program, or with a for loop in R: # Define your timestep and time vector deltaT &lt;- 1 t &lt;- seq(0, 600, by = deltaT) # Define the number of steps we take. This is equal to 10 / dt (why?) N &lt;- length(t) # Define current solution state: p_approx &lt;- 250/13600 # Define a vector for your solution:the derivative equation for (i in 2:N) { # We start this at 2 because the first value is 10 dpdt &lt;- .023 * p_approx[i - 1] * (1 - p_approx[i - 1]) p_approx[i] &lt;- p_approx[i - 1] + dpdt * deltaT } # Define your data for the solution into a tibble: solution_data &lt;- tibble( time = t, prop_infected = p_approx ) # Plot your solution: ggplot(data = solution_data) + geom_line(aes(x = time, y = prop_infected)) + labs( x = &quot;Time (days)&quot;, y = &quot;Proportion infected&quot; ) FIGURE 4.3: Results from applying an iterative method to solve Equation (4.5). Let’s break the code down that produced Figure 4.3 step by step:13 deltaT &lt;- 0.1 and t &lt;- seq(0,2,by=deltaT) define the timesteps (\\(\\Delta t\\)) and the output time vector t. The statement N &lt;- length(t) defines how many steps we take. p_approx&lt;- 250/13600 defines the proportion of the population initially infected (assuming that \\(I(0)=250\\)). We will use this as the starting point to the solution vector. The for loop goes through this system - first computing the value of \\(\\displaystyle \\frac{dp}{dt}\\) and then forecasing out the next timestep \\(p(t+\\Delta t) = f(p) \\cdot \\Delta t\\). We iteratively build the vector p_approx, adding another element at each timestep. The remaining code plots the data frame, like we learned in Chapter 2. 4.3.1 Euler’s method in demodelr To generate Figure 4.3 we created the solution directly in R - but you don’t want to copy and paste the code. The demodeler package has a function called euler that does the same process to generate the output solution:14 Try running the following code and plotting your solution: # Define the rate equation: infection_eq &lt;- c(dpdt ~ .023 * p * (1 - p)) # Define the initial condition (as a named vector): prop_init &lt;- c(p = 250/13600) # Define deltaT and the time steps: deltaT &lt;- 1 n_steps &lt;- 600 # Compute the solution via Euler&#39;s method: out_solution &lt;- euler(system_eq = infection_eq, initial_condition = prop_init, deltaT = deltaT, n_steps = n_steps ) Once the vector out_solution is created, it has variables t and p, which can then be plotted with a ggplot statement. Let’s talk through the steps of this code as well: The line infection_eq &lt;- c(dpdt ~ .023 * p * (13600-i)) represents the differential equation, written in formula notation. So \\(\\displaystyle \\frac{dp}{dt} \\rightarrow\\) dpdt and \\(f(p) \\rightarrow\\) .023 * p * (1-p)), with the variable p. The initial condition \\(p(0)=250/13600 = .018\\) is written as a named vector: prop_init &lt;- c(p=250/13600). Make sure the name of the variable is consistent with your differential equation. As before we need to identify \\(\\Delta t\\) (deltaT) and the number of steps \\(N\\) (n_steps). When we generated the solution in Figure 4.3, in the for loop we defined the ending point at \\(t=2\\) so the number of steps (N) was 20.15 The command euler then computes the solution applying Euler’s method, returning a data frame so we can plot the results. Note the columns of the data frame are the variables \\(t\\) and \\(i\\) that have been named in our equations. 4.3.2 Euler’s method applied to systems Now that we have some experience with Euler’s method, let’s see how we can apply the function euler to a system of differential equations. Here is a sample code that shows the dynamics for the lynx-hare equations, as studied in Chapter 3: \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= r H - bHL \\\\ \\frac{dL}{dt} &amp;= e b H L - dL \\end{split} \\tag{4.7} \\end{equation}\\] The variables \\(H\\) and \\(L\\) are already in thousands of animals, so we don’t need to rescale anything like we did with Equation (4.5). We are going to use Euler’s method to solve this differential equation, using the code below: # Define the rate equation: lynx_hare_eq &lt;- c( dHdt ~ r * H - b * H * L, dLdt ~ e * b * H * L - d * L ) # Define the parameters (as a named vector): lynx_hare_params &lt;- c(r = 2, b = 0.5, e = 0.1, d = 1) # Define the initial condition (as a named vector): lynx_hare_init &lt;- c(H = 1, L = 3) # Define deltaT and the number of time steps: deltaT &lt;- 0.05 n_steps &lt;- 200 # Compute the solution via Euler&#39;s method: out_solution &lt;- euler(system_eq = lynx_hare_eq, parameters = lynx_hare_params, initial_condition = lynx_hare_init, deltaT = deltaT, n_steps = n_steps ) # Make a plot of the solution, # using different colors for lynx or hares: ggplot(data = out_solution) + geom_line(aes(x = t, y = H), color = &quot;red&quot;) + geom_line(aes(x = t, y = L), color = &quot;blue&quot;,linetype=&#39;dashed&#39;) + labs( x = &quot;Time&quot;, y = &quot;Lynx (red) or Hares (blue/dashed)&quot; ) FIGURE 4.4: Euler’s method solution for Lynx-Hare system (Equation ). This example is structured similarly when we used Euler’s method to solve a single variable differential equation, with some key changes (that are easy to adapt): The variable lynx_hare_eq is now a vector, with each entry one of the rate equations. We need to identify both variables in their initial condition. Most importantly, Equation (4.7) has parameters, which we define as a named vector lynx_hare_params &lt;- c(r = 2, b = 0.5, e = 0.1, d = 1) that we pass through to the command euler with the option parameters. If your equation does not have any parameters you do not need to worry about specifying this input. We plot both solutions together at the end, or you can make two separate plots. Remember that you can choose the color in your plot. I included the additional option linetype=dashed for the hares population for ease of viewing. 4.4 Euler’s method and beyond Sometimes when working with Euler’s method you encounter a differential equation that produces some nonsensible results. For example, consider a model that represents infection with quarantine (see Exercise 1.10 in Chapter 1): \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;= -kSI \\\\ \\frac{dI}{dt} &amp;= -kSI - \\beta I \\end{split} \\tag{4.8} \\end{equation}\\] In Equation (4.8), susceptibles become sick by encountering an infected person, but infected people are removed from the population at a rate \\(\\beta\\). The model in Figure 4.5 illustrates the results when this model is implemented using euler: FIGURE 4.5: Surprising results when using Euler’s method to solve Equation (4.8). Notice how some values for \\(I\\) are negative. You may notice in Figure 4.5 the solution for \\(S\\) falls below \\(S=0\\) around \\(t=0.75\\).16 Negative values for \\(S\\) are concerning because we know there can’t be negative people! At a given timestep, Euler’s method constructs a locally linear approximation and forecasts the solution forward to the next timestep. Using Figure 4.5, at \\(t=0.75\\) the value for \\(S \\approx 1\\) and the value for \\(I \\approx 280\\). If we let \\(k=0.05\\) and \\(\\beta=0.2\\), this means that \\(\\displaystyle \\frac{dS}{dt}=-14\\) and \\(\\displaystyle \\frac{dI}{dt}=-42\\). At this point, the values of \\(S\\) and \\(I\\) are both decreasing. In turn, the forecast value for \\(S\\) at \\(t=0.75\\) is \\(S = 1 -14\\cdot 0.1 = -0.4\\). Mathematically, Euler’s method is working correctly, but we know realistically that neither \\(S\\) nor \\(I\\) can be negative. While Euler’s method is useful, it does quite poorly in cases where the solution is changing rapidly, such as described above. A way to circumvent this is to adjust the value of \\(\\Delta t\\) to be smaller, which comes at the expense of more computational time. A second way is to use a higher order solver than euler, and one such method is called the Runge-Kutta method. (You study these methods when you take a course in numerical analysis. How we implement the Runge-Kutta method is to replace the command euler with rk4: # Define the rate equation: quarantine_eq &lt;- c( dSdt ~ -k * S * I, dIdt ~ k * S * I - beta * I ) # Define the parameters (as a named vector): quarantine_parameters &lt;- c(k = .05, beta = .2) # Define the initial condition (as a named vector): quarantine_init &lt;- c(S = 300, I = 1) # Define deltaT and the number of time steps: deltaT &lt;- .1 # timestep length n_steps &lt;- 10 # must be a number greater than 1 # Compute the solution via Runge-Kutta method: out_solution &lt;- rk4(system_eq = quarantine_eq, parameters = quarantine_parameters, initial_condition = quarantine_init, deltaT = deltaT, n_steps = n_steps ) # Make a plot of the solution: ggplot(data = out_solution) + geom_line(aes(x = t, y = S), color = &quot;red&quot;) + geom_line(aes(x = t, y = I), color = &quot;blue&quot;,linetype=&quot;dashed&quot;) + geom_hline(yintercept=0,size=0.25) + labs( x = &quot;Time&quot;, y = &quot;Susceptible (red) or Infected (blue/dashed)&quot; ) FIGURE 4.6: Runge-Kutta solution for Equation . Notice how the solution curve for the variable \\(S\\) does not fall below zero as it does in Figure 4.5. Another benefit to the rk4 method is the numerical error when computing the solution. The numerical error is quantified as the difference between the actual solution and the numerical solution. Euler’s method has an error on the order of the stepsize \\(\\Delta t\\), whereas the Runge-Kutta method has an error of \\((\\Delta t)^4\\). For this example, \\(\\Delta t = 0.1\\), whereas for the Runge-Kutta method the numerical error is on the order of 0.0001 (\\((\\Delta t)^{4} =.0001\\)) - noticeably different! We can improve Euler’s method by taking a smaller timestep - BUT that means we need a larger number of steps \\(N\\) - which may take more computational time (see Exercise 4.15). Does this discussion of numerical error sounds familiar? In calculus you may have examined the numerical error when using Riemann sums (left, right, trapezoid, midpoint sums) to approximate the area underneath a curve. While the context is different, Riemann sums and numerical differential equation solvers are closely related. In summary, the most general form of a differential equation is: \\[\\begin{equation} \\displaystyle \\frac{d\\vec{y}}{dt} = f(\\vec{y},\\vec{\\alpha},t), \\tag{4.9} \\end{equation}\\] where in Equation (4.9) \\(\\vec{y}\\) is the vector of state variables you want to solve for, and \\(\\vec{\\alpha}\\) is your vector of parameters. At a given initial condition, Euler’s method applies locally linear approximations to forecast the solution forward \\(\\Delta t\\) time units (Equation (4.10)): \\[\\begin{equation} \\vec{y}_{n+1} = \\vec{y}_{n} + f(\\vec{y}_{n},\\vec{\\alpha},t_{n}) \\cdot \\Delta t \\tag{4.10} \\end{equation}\\] Both the Euler or Runge-Kutta methods define a workflow (approximate \\(\\rightarrow\\) forecast \\(\\rightarrow\\) repeat) to generate a numerical solution to a system of differential equations. The process of defining a workflow is a powerful technique that we will revisit several times throughout this textbook, so stay tuned! 4.5 Exercises Exercise 4.1 Verify that \\(I(t) = 130-120e^{-0.025t}\\) is a solution to the differential equation \\[\\displaystyle \\frac{dI}{dt} = 130-0.025I \\] with \\(I(0)=10\\). Exercise 4.2 Apply the rk4 solver with \\(\\Delta t = 1\\) with \\(N=600\\) to the initial value problem \\(\\displaystyle \\frac{dp}{dt} = 0.023 p \\cdot (13600-p)\\), \\(p(0)=250/13600\\). Compare your solution to Figure 4.3. What differences do you observe? Which solution method (euler or rk4) is better (and why)? Exercise 4.3 In the model presented by Equation (4.8), is \\(S+I\\) constant? Hint: add \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\). Exercise 4.4 The following exercise will help you explore the relationships between stepsize, ending points, and number of steps needed. You may assume that we will start at \\(t=0\\) in all parts. If we wish to do a Euler’s method solution with step size 1 second and ending at \\(T=5\\) seconds, how many steps will we take? If we wish to do a Euler’s method solution with step size 0.5 seconds and ending at \\(T=5\\) seconds, how many steps will we take? If we wish to do a Euler’s method solution with step size 0.1 seconds and ending at \\(T=5\\) seconds, how many steps will we take? If we wish to do a Euler’s method solution with step size \\(\\Delta t\\) and go to ending value of \\(T\\), what is an expression that relates the number steps \\(N\\) as a function of \\(\\Delta t\\) and \\(T\\)? Exercise 4.5 To get a rough approximation between error and step size, let’s say for a particular differential equation that we are starting at \\(t=0\\) and going to \\(t=2\\), with \\(\\Delta t = 0.2\\) with 10 steps. We know that the Runge-Kutta error will be on the order of \\((\\Delta t)^{4} =0.0016\\). If we want to use Euler’s method with the same order of error, we could say \\(\\Delta t = .0016\\). For that case, how many steps will we need to take? Exercise 4.6 For each of the following differential equations, apply Euler’s method to generate a numerical solution to the differential equation and plot your solution. The stepsize (\\(\\Delta t\\)) and number of iterations (\\(N\\)) are listed. Differential equation: \\(\\displaystyle \\frac{dS}{dt} =3-S\\). Set \\(\\Delta t = 0.1\\), \\(N = 50\\). Initial conditions: \\(S(0) = 0.5\\), \\(S(0) = 5\\). Differential equation: \\(\\displaystyle \\frac{dS}{dt} =\\frac{1}{1-S}\\). Set \\(\\Delta t = 0.01\\), \\(N = 30\\). Initial conditions: \\(S(0) = 0.5\\), \\(S(0) = 2\\). Differential equation: \\(\\displaystyle \\frac{dS}{dt} = 0.8 \\cdot S \\cdot (10-S)\\). Set \\(\\Delta t = 0.1\\), \\(N = 50\\). Initial conditions: \\(S(0) = 3\\), \\(S(0) = 10\\). Exercise 4.7 For each of the following differential equations, apply the Runge-Kutta method to generate a numerical solution to the differential equation and plot your solution. The stepsize (\\(\\Delta t\\)) and number of iterations (\\(N\\)) are listed. Contrast your answers with Exercise 4.6. Differential equation: \\(\\displaystyle \\frac{dS}{dt} =3-S\\). Set \\(\\Delta t = 0.1\\), \\(N = 50\\). Initial conditions: \\(S(0) = 0.5\\), \\(S(0) = 5\\). Differential equation: \\(\\displaystyle \\frac{dS}{dt} =\\frac{1}{1-S}\\). Set \\(\\Delta t = 0.01\\), \\(N = 30\\). Initial conditions: \\(S(0) = 0.5\\), \\(S(0) = 2\\). Differential equation: \\(\\displaystyle \\frac{dS}{dt} = 0.8 \\cdot S \\cdot (10-S)\\). Set \\(\\Delta t = 0.1\\), \\(N = 50\\). Initial conditions: \\(S(0) = 3\\), \\(S(0) = 10\\). Exercise 4.8 Complete the following steps: Apply the code euler to generate a numerical solution to the differential equation: Differential equation: \\(\\displaystyle \\frac{dS}{dt} = r \\cdot S \\cdot (K-S)\\). Set \\(r=1.2\\) and \\(K=3\\). Set \\(\\Delta t = 0.1\\), \\(N = 50\\). Initial conditions (three different ones): \\(S(0) = 1\\), \\(S(0) = 3\\), \\(S(0) = 5\\). Plot your Euler’s method solutions with the three initial conditions on the same plot. What do you notice when you do plot them together? Make a hypothesis regarding the long term behavior of this system. Then plot a few more solution curves to verify your guess. Exercise 4.9 Complete the following steps: Apply the code euler to generate a numerical solution to the differential equation: Differential equation: \\(\\displaystyle \\frac{dS}{dt} =K-S\\). Set \\(K=2\\). Set \\(\\Delta t = 0.1\\), \\(N = 50\\). Initial conditions (three different ones): \\(S(0) = 0\\), \\(S(0) = 2\\), \\(S(0) = 5\\). Plot your Euler’s method solutions with the three initial conditions on the same plot. What do you notice when you do plot them together? Make a hypothesis regarding the long term behavior of this system. Then plot a few more solution curves to verify your guess. Exercise 4.10 This exercise uses the following differential equation: \\[\\begin{equation} \\frac{dS}{dt} = 0.8 \\cdot S \\cdot (10-S) \\end{equation}\\] Apply Euler’s method with \\(S(0)=15\\), \\(\\Delta t = 0.1\\), \\(N = 10\\). When you examine your solution, what is incorrect about the Euler’s method solution based on your qualitative knowledge of the underlying dynamics? Now calculate Euler’s method for the same differential equation for the following conditions: \\(S(0)=15\\), \\(\\Delta t = 0.01\\), \\(N = 100\\). What has changed in your solution? Exercise 4.11 Apply Euler’s method to the differential equation \\(\\displaystyle \\frac{dS}{dt} =\\frac{1}{1-S}\\) with the following conditions: \\(S(0)=1.5\\), \\(\\Delta t = 0.1\\), \\(N = 10\\) \\(S(0)=1.5\\), \\(\\Delta t = 0.01\\), \\(N = 100\\). Between these two solutions, what has changed? Do you think it is numerically possible to calculate a reasonable solution for Euler’s method near \\(S=1\\)? (note: this differential equation is an example of finite time blow up) Exercise 4.12 One way to model the growth rate of hares is with \\(\\displaystyle f(H) = \\frac{r H}{1+kH}\\), where \\(r\\) and \\(k\\) are parameters. This is in constrast to exponential growth, which assumes \\(f(H) = rH\\). First evaluate \\(\\displaystyle \\lim_{H \\rightarrow \\infty} rH\\). Then \\(\\displaystyle \\lim_{H \\rightarrow \\infty} \\frac{r H}{1+kH}\\). Compare your two answers. Discuss how the growth rate \\(\\displaystyle f(H) = \\frac{r H}{1+kH}\\) seems to be a more realistic model. Exercise 4.13 In the lynx-hare example we can also consider an alternative system where the growth of the hare is not exponential: \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= \\frac{2 H}{1+kH} - 0.5HL \\\\ \\frac{dL}{dt} &amp;= 0.05 H L - L \\end{split} \\end{equation}\\] Set the number of timesteps to be 2000, \\(\\delta t = 0.1\\), with initial condition \\(H=1\\) and \\(L=3\\). Apply Euler’s method to numerically solve this system of equations when \\(k=0.1\\) and \\(k=1\\) and plot your simulation results. Exercise 4.14 Consider the differential equation \\(\\displaystyle \\frac{dS}{dt} = \\frac{1}{1-S}\\). Notice that at \\(S=1\\) the rate \\(\\displaystyle \\frac{dS}{dt}\\) is not defined. If you applied Euler’s method solution with initial condition \\(S(0)=0.9\\), what would the values of \\(S\\) approach as time increases? If you applied Euler’s method solution with initial condition \\(S(0)=1.1\\), what would the values of \\(S\\) approach as time increases? Explain how you could come to the same conclusion as the previous two problems if you graphed \\(\\displaystyle f(S) = \\frac{1}{1-S}\\). Exercise 4.15 Building on Exercise 4.5, let’s say for a particular differential equation we have \\(N\\) steps from \\(0 \\leq t \\leq b\\). An error of \\(\\epsilon\\) is desired. What is the ratio \\(\\displaystyle \\frac{N_{E}}{N_{RK4}}\\), where \\(N_{RK4}\\) represents the number of steps needed for the Runge-Kutta method, and \\(N_{E}\\) the number of steps for Euler’s method? Make a plot of the ratio \\(\\displaystyle \\frac{N_{E}}{N_{RK4}}\\) for \\(0 \\leq \\epsilon \\leq 1\\). How many more steps does Euler’s method need to do to achieve the same level of error, compared to the Runge-Kutta method? When you have a rate of change multiplied by a time increment this will give you an approximation of the net change in a function.↩︎ You may notice that when you run the code to produce Figure 4.3 on your own it may not look like the output shown here. I’ve customized how the plots are displayed in this book using the pacakge ggthemes.↩︎ Don’t forget to load up the demodelr library in your code at the top of your R code.↩︎ In general if we know \\(\\Delta t\\) and the time we wish to end computing (\\(t_{end}\\), then \\(N = t_{end}/\\Delta t\\).↩︎ Notice in the code I’ve added a line for the horizontal axis with geom_hline.↩︎ "],["phase-05.html", "Chapter 5 Phase Lines and Equilibrium Solutions 5.1 Equilibrium solutions 5.2 Phase lines for differential equations 5.3 A stability test for equilibrium solutions 5.4 Exercises", " Chapter 5 Phase Lines and Equilibrium Solutions Chapter 4 explored numerical techniques to solve initial value problems. This chapter takes a step back to examine the general family of solutions to a differential equation. Will the family of solutions converge in the long run (as \\(t \\rightarrow \\infty\\)) to a constant value? Are these specific solutions that always remain constant (or independent of time)? Answering questions such as these address the qualitative behavior for a single differential equation.17 Let’s get started! 5.1 Equilibrium solutions Chapter 3 introduced the concept of an equilibrium solution, or where the rate of change for a differential equation is zero. We can determine equilibrium solutions for a single-variable differential equation by setting the left hand side of \\(\\displaystyle \\frac{dy}{dt}=f(y)\\) equal to zero and solving for \\(y\\) (or whatever dependent variable describes the problem). Example 5.1 What are the equilibrium solutions to \\(\\displaystyle \\frac{dy}{dt}=- y\\)? Solution. For this example we know that when the rate of change is zero, this means that \\(\\displaystyle \\frac{dy}{dt} = 0\\), or when \\(0 = -y\\). So \\(y=0\\) is the equilibrium solution. The general solution to the differential equation \\(\\displaystyle \\frac{dy}{dt}=- y\\) is \\(y(t)=Ce^{-t}\\), where \\(C\\) is an arbitrary constant. (We will explore techniques to determine this in Chapter 7.) Figure 5.1 plots different initial conditions, with the equilibrium solution shown as a horizontal line: FIGURE 5.1: Solution curves to \\(y&#39;=-y\\) for different initial conditions (values of \\(C\\)). Notice that in Figure 5.1 as \\(t\\) increases, all solutions approach the equilibrium solution \\(y=0\\), regardless if the initial condition is positive or negative. This observation is also confirmed by evaluating the limit \\(\\displaystyle \\lim_{t\\rightarrow \\infty} Ce^{-t}\\), which is 0. Example 5.2 Determine equilibrium solutions to \\[\\begin{equation} \\displaystyle \\frac{dN}{dt} = N \\cdot(1-N) \\tag{5.1} \\end{equation}\\] Solution. In this case the equilibrium solutions for Equation (5.1) occur when \\(N \\cdot(1-N) = 0\\), or when \\(N=0\\) or \\(N=1\\). The general solution to Equation (5.1) is \\[\\begin{equation} N(t)= \\frac{C}{C +(1-C) e^{-t}}. \\tag{5.2} \\end{equation}\\] Figure 5.2 displays several different solution curves for Equation (5.2). FIGURE 5.2: Solution curves for Equation (5.1) with different initial conditions (values of \\(C\\)). In Figure 5.2 notice how all the solutions tend towards \\(N=1\\), but even solutions that start close to \\(N=0\\) seem to move away from the \\(N=0\\) equilibrium solution. Solutions in Figure 5.2 exhibit the idea of the stability of an equilibrium solution, which we discuss next. 5.2 Phase lines for differential equations The stability of an equilibrium solution describes the long-term behavior of the family of solutions. Solutions can converge to the equilibrium solution in the long run, or they may not. More formally stated: An equilibrium solution \\(y_{*}\\) to a differential equation \\(\\displaystyle \\frac{dy}{dt} = f(y)\\) is considered stable when for a given solution \\(\\displaystyle \\lim_{t \\rightarrow \\infty} y(t) = y_{*}\\). You may note that the definition of stability relies on determining the solution \\(y(t)\\). However we can circumvent determining this solution by using ideas from calculus and the rate of change: If \\(\\displaystyle \\frac{dy}{dt}&lt;0\\), the solution \\(y(t)\\) is decreasing. If \\(\\displaystyle \\frac{dy}{dt}&gt;0\\), the solution \\(y(t)\\) is increasing. So to classify stability of an equilibrium solution we can investigate the behavior of the differential equation around the equilibrium solutions. Let’s apply this logic to our differential equation \\(\\displaystyle \\frac{dy}{dt}=- y\\) from Example 5.1. When \\(y=3\\), \\(\\displaystyle \\frac{dy}{dt}=- 3 &lt;0\\), so we say the function is decreasing to \\(y=0\\). When \\(y=-2\\), \\(\\displaystyle \\frac{dy}{dt}=- (-2) = 2 &gt;0\\), so we say the function is increasing to \\(y=0\\). This can be represented neatly in the phase line diagram for Figure 5.3.18 FIGURE 5.3: Phase line for the differential equation \\(y&#39;=-y\\). Because the solution is increasing to \\(y=0\\) when \\(y &lt;0\\), and decreasing to \\(y=0\\) when \\(y &gt;0\\), we say that the equilibrium solution for the differential equation \\(y&#39;=-y\\) is stable, which is also confirmed by the solutions plotted in Figure 5.1. Now let’s generalize the example \\(y&#39;=-y\\) to classify the stability of the equilibrium solutions to \\(\\displaystyle \\frac{dy}{dt} = r y\\), where \\(r\\) is a parameter. Fortunately the equilibrium solution is still \\(y=0\\). We will need to consider three different cases for the stability depending on the value of \\(r\\) (\\(r&gt;0\\), \\(r&lt;0\\), and \\(r=0\\)): When \\(r&lt;0\\), the phase line will be similar to Figure 5.3. When \\(r&gt;0\\) the phase line will be as shown in Figure 5.4. We say in this case that the equilibrium solution is unstable, as all solutions flow away from the equilibrium. Several different solutions are shown in Figure 5.5 . When \\(r=0\\) we have the differential equation \\(\\displaystyle \\frac{dy}{dt}=0\\), which has \\(y=C\\) as a general solution. For this special case the equilibrium solution is neither stable or unstable19. FIGURE 5.4: Phase line for the differential equation \\(y&#39;=ry\\), with \\(r&gt;0\\). FIGURE 5.5: Solution curves for the differential equation \\(y&#39;=ry\\), with \\(r&gt;0\\) for different initial conditions (values of \\(C\\)). Based on the above discussion, let’s return back to the differential equation \\(\\displaystyle \\frac{dN}{dt} = N \\cdot(1-N)\\) from Example 5.2. We evaluate the stability of the equilibrium solutions \\(N=0\\) and \\(N=1\\) in Table 5.1. TABLE 5.1: Evaluation of the stability of the equilibrium solutions for Equation (5.1). Test point Sign of \\(N&#39;\\) Tendency of solution \\(N=-1\\) Negative Decreasing \\(N=0\\) Zero Equilibrium solution \\(N=0.5\\) Positive Increasing \\(N=1\\) Zero Equilibrium solution \\(N=2\\) Negative Decreasing Notice how the selected test points in the first column of Table 5.1 were selected to the left or the right of the equilibrium solutions. The phase line diagram of Figure 5.6 also presents the same information as in Table 5.1, but in contrast to Figures 5.3 and 5.4 we need to include two equilibrium solutions. Phase line diagrams should include all the computed equilibrium solutions. FIGURE 5.6: Phase line diagram for Equation (5.1). Table 5.1 and Figure 5.6 confirm that solutions move away from the equilibrium solution \\(N=0\\) and move towards the equilibrium solution \\(N=1\\). These results suggest that the equilibrium solution at \\(N=1\\) is stable and the equilibrium solution at \\(N=0\\) is unstable. Therefore, one way to define an equilibrium solution \\(y_{*}\\) as unstable is when \\(\\displaystyle \\lim_{t \\rightarrow -\\infty} y(t) = y_{*}\\). 5.3 A stability test for equilibrium solutions Notice how when constructing the phase line diagram we relied on the behavior of solutions around the equilibrium solution to classify the stability. As an alternative we can also use the point at the equilibrium solution itself. Consider the general differential equation \\(\\displaystyle \\frac{dy}{dt}=f(y)\\) with an equilibrium solution at \\(y_{*}\\). Next we apply local linearization to construct a locally linear approximation to \\(L(y)\\) to \\(f(y)\\) at \\(y=y_{*}\\) (Equation (5.3)): \\[\\begin{equation} L(y) = f(y_{*}) + f&#39;(y_{*}) \\cdot (y-y_{*}) \\tag{5.3} \\end{equation}\\] There are two follow-on steps to simplify Equation (5.3). First, because we have an equilibrium solution, \\(f(y_{*}) =0\\). Second, Equation (5.3) can be written with a new variable \\(P\\), defined by variable \\(P = y-y_{*}\\). With these two steps Equation (5.3) translates to Equation (5.4): \\[\\begin{equation} \\frac{dP}{dt} = f&#39;(y_{*}) \\cdot P \\tag{5.4} \\end{equation}\\] Does Equation (5.4) look familiar? It should! This equation is similar to the example where we classified the stability of \\(\\displaystyle \\frac{dy}{dt} = ry\\) (notice that \\(f&#39;(y_{*})\\) is a number). Using this information, a test to classify the stability of an equilibrium solution is the following: Local linearization stability test for equilibrium solutions: For a differential equation \\(\\displaystyle \\frac{dy}{dt} = f(y)\\) with equilibrium solution \\(y_{*}\\), we can classify the stability of the equilibrium solution through the following: If \\(f&#39;(y_{*})&gt;0\\) at an equilibrium solution, the equilibrium solution \\(y=y_{*}\\) will be unstable. If \\(f&#39;(y_{*}) &lt;0\\) at an equilibrium solution, the equilibrium solution \\(y=y_{*}\\) will be stable. If \\(f&#39;(y_{*}) = 0\\), we cannot conclude anything about the stability of \\(y=y_{*}\\). Let’s return back to the differential equation \\(\\displaystyle \\frac{dN}{dt} = N \\cdot(1-N)\\) from Example 5.2 and apply the local linearization stability test, \\(f&#39;(N)=1-2N\\). Since \\(f&#39;(0)=1\\), which is greater than 0, the equilibrium solution \\(N=0\\) is unstable. Likewise, if \\(f&#39;(1)=-1\\), the equilibrium solution \\(N=1\\) is stable. Applying the local linearization test may be easier to quickly determine stability of an equilibrium solution. Guess what? This test also is a simplified form of determining stability of equilibrium solutions for systems of differential equations. We will explore this more in Chapter 19. 5.4 Exercises Exercise 5.1 For the following differential equations, (1) determine any equilibrium solutions, and (2) classify the stability of the equilibrium solutions by applying the local linearization test. \\(\\displaystyle \\frac{dS}{dt} = 0.3 \\cdot(10-S)\\) \\(\\displaystyle \\frac{dP}{dt} = P \\cdot(P-1)(P-2)\\) Exercise 5.2 Using your results from Exercise 5.1, construct a phase line for each of the differential equations and classify the stability of the equilibrium solutions. Exercise 5.3 A population grows according to the equation \\(\\displaystyle \\frac{dP}{dt} = \\frac{P}{1+2P} - 0.2P\\). Determine the equilibrium solutions for this differential equation. Classify the stability of the equilibrium solutions using the local linearization stability test. Exercise 5.4 (Inspired by Logan and Wolesensky (2009)) A cell with radius \\(r\\) assimilates nutrients at a rate proportional to its surface area, but uses nutrients proportional to its volume, according to the following differential equation: \\[\\begin{equation} \\frac{dr}{dt} = k_{A} 4 \\pi r^{2} - k_{V} \\frac{4}{3} \\pi r^{3}, \\end{equation}\\] where \\(k_{A}\\) and \\(k_{V}\\) are positive constants. Determine the equilibrium solutions for this differential equation. Construct a phase line for this differential equation to classify the stability of the equilibrium solutions. Classify the stability of the equilibrium solutions using the local linearization stability test. Are your conclusions the same from the previous part? Exercise 5.5 (Inspired by Thornley and Johnson (1990)) The Chanter equation of growth is the following, where \\(W\\) is the weight of an object: \\[\\begin{equation} \\frac{dW}{dt} = W(3-W)e^{-Dt} \\end{equation}\\] Use this differential equation to answer the following questions. What happens to the rate of growth (\\(\\displaystyle \\frac{dW}{dt}\\)) as \\(t\\) grows large? What are the equilibrium solutions to this model? Are they stable or unstable? Notice how the equilbrium solutions are the same as those for the logistic model. Based on your previous work, why would this model be a more realistic model of growth than the logistic model \\(\\displaystyle \\frac{dW}{dt} = W(3-W)\\)? Exercise 5.6 Red blood cells are formed from stem cells in the bone marrow. The red blood cell density \\(r\\) satisfies an equation of the form \\[\\begin{equation} \\frac{dr}{dt} = \\frac{br}{1+r^{n}} - c r, \\end{equation}\\] where \\(n&gt;1\\) and \\(b&gt;1\\) and \\(c&gt;0\\). Find all the equilibrium solutions \\(r_{*}\\) to this differential equation. Hint: can you factor an \\(r\\) from your equation first? Exercise 5.7 (Inspired by Hugo van den Berg (2011)) Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of \\(S\\) in the blood is the following: \\[\\begin{equation} \\frac{dS}{dt} = I + p \\cdot (W - S), \\end{equation}\\] where the parameter \\(I\\) represents the active uptake of salt, \\(p\\) is the permeability of the skin, and \\(W\\) is the salinity in the water. First set \\(I=0\\). Determine the equilibrium solutions for this differential equation. Express your answer \\(S_{*}\\) in terms of the parameters \\(p\\) and \\(W\\). Next consider \\(I&gt;0\\). Determine the equilibrium solutions for this differential equation. Express your answer \\(S_{*}\\) in terms of the parameters \\(p\\), \\(W\\), and \\(I\\). Why should your new equilbrium solution be greater than the equilibrium solution from the previous problem? Classify the stability of both equilibrium solutions in both cases using the local linearization stability test. Exercise 5.8 (Inspired by Logan and Wolesensky (2009)) The immigration rate of bird species (species per time) from a mainland to an offshore island is \\(I_{m} \\cdot (1-S/P)\\), where \\(I_{m}\\) is the maximum immigration rate, \\(P\\) is the size of the source pool of species on the mainland, and \\(S\\) is the number of species already occupying the island. Further, the extinction rate is \\(E \\cdot S / P\\), where \\(E\\) is the maximum extinction rate. The growth rate of the number of species on the island is the immigration rate minus the extinction rate, given by the following differential equation: \\[\\begin{equation} \\frac{dS}{dt} = I_{m} \\left(1-\\frac{S}{P} \\right) - \\frac{ES}{P} \\end{equation}\\] Determine the equilibrium solutions \\(S_{*}\\) for this differential equation. Expression your answer in terms of \\(I_{M}\\), \\(P\\), and \\(E\\). Classify the stability of the equilibrium solutions using the local linearization stability test. Exercise 5.9 A colony of bacteria growing in a nutrient-rich medium depletes the nutrient as they grow. As a result, the nutrient concentration \\(x(t)\\) is steadily decreasing. The equation describing this decrease is the following: \\[\\begin{equation} \\frac{dx}{dt} = - \\mu \\frac{x \\cdot (\\xi- x)}{\\kappa + x}, \\end{equation}\\] where \\(\\mu\\), \\(\\kappa\\), and \\(\\xi\\) are all parameters greater than zero. Determine the equilibrium solutions \\(x_{*}\\) for this differential equation. Construct a phase line for this differential equation and classify the stability of the equilibrium solutions. Exercise 5.10 Can a solution curve cross an equilibrium solution of a differential equation? References "],["coupled-06.html", "Chapter 6 Coupled Systems of Equations 6.1 Flu with quarantine and equilibrium solutions 6.2 Nullclines 6.3 Phase planes 6.4 Generating a phase plane in R 6.5 Slope fields 6.6 Exercises", " Chapter 6 Coupled Systems of Equations Chapter 5 focused on qualitative analysis of a single differential equation using phase lines and slope fields. This chapter extends this idea further to systems of differential equations, where the natural extension of a phase line is a phase plane. Here is the good news: many of the techniques are similar to the ones introduced in Chapter 5, so let’s get started! 6.1 Flu with quarantine and equilibrium solutions In Exercise 1.10 in Chapter 1 we developed the following model for the flu as a coupled system of equations shown in Equation (6.1): \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;= -kSI \\\\ \\frac{dI}{dt} &amp;= kSI-\\beta I \\\\ \\frac{dR}{dt} &amp;= \\beta I, \\end{split} \\tag{6.1} \\end{equation}\\] where \\(S\\) represents susceptible people, \\(I\\) infected people, and \\(R\\) recovered people. Another way to represent this context is with the schematic shown in Figure 6.1: FIGURE 6.1: Schematic of the flu model with quarantine. While Equation (6.1) is a system of three differential equations, notice that the variable \\(R\\) is not present on the right hand sides of each equation. As a result, the variable \\(R\\) is decoupled from this system of equations, so we can just focus on the rates of change for \\(S\\) and \\(I\\) (Equation (6.2)): \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;= -kSI \\\\ \\frac{dI}{dt} &amp;= kSI-\\beta I \\\\ \\end{split} \\tag{6.2} \\end{equation}\\] With Equation (6.2) we will solve for equilibrium solutions (similar to what we did in Chapters 3 and 5), which we focus on next. 6.2 Nullclines The process to determine equilibrium solutions for a system of differential equations starts with computing the nullclines for each rate in the system of equations. The nullclines are solutions in the plane where one of the rates is zero, so for example either \\(\\displaystyle \\frac{dS}{dt}\\) or \\(\\displaystyle \\frac{dI}{dt}\\) is zero. For coupled systems of equations, the equilibrium solutions are where the rates \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) in Equation (6.2) are both zero, found through algebraically solving the system of equations in Equation (6.3): \\[\\begin{equation} \\begin{split} 0 &amp;= -kSI \\\\ 0 &amp;= kSI - \\beta I \\end{split} \\tag{6.3} \\end{equation}\\] Let’s examine the first equation (\\(0 = -kSI\\)). Since all the terms are expressed as a product, then nullclines for \\(S\\) occur when either \\(S=0\\) or \\(I=0\\). In a similar manner, the nullclines for \\(I\\) occur when \\(0 = kSI - \\beta I\\). For this expression we can factor out an \\(I\\), yielding \\(0 = I \\cdot (kS - \\beta)\\). Because the last equation is factored as a product, nullclines for \\(I\\) are either \\(I=0\\) or by solving \\(kS-\\beta\\) for \\(S\\) to yield \\(\\displaystyle S = \\frac{\\beta}{k}\\). Nullclines are not equilibrium solutions by themselves - it is the intersection of two different nullclines that determines equilibrium solutions. Figure 6.2 shows the nullclines in the \\(S-I\\) plane (since we have two equations), with \\(S\\) on the horizontal axis and \\(I\\) on the vertical axis. In Figure 6.2 we have also assumed that \\(\\beta=1\\) and \\(k=1\\). The \\(S-I\\) plane shown in Figure 6.2 is the beginning of the construction of the phase plane for Equation (6.2) and also to determine the equilibrium solutions. FIGURE 6.2: Nullclines for Equation (6.2). To generate the plot we assumed \\(\\beta=1\\) and \\(k=1\\). A key thing to note is that where two different nullclines cross is an equilibrium solution to the system of equations (both \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) are zero at this point). Examining Figure 6.2 three possibilities appear: There is an equilibrium solution at \\(S=0\\) and \\(I=0\\) (otherwise known as the origin). This equilibrium solution makes biological sense: if there is nobody susceptible or infected there are no flu cases (everyone is perfectly healthy - yay!) . The entire horizontal axis is an equilibrium solution because \\(I=0\\), which makes both \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) zero. There is a practical interpretation of this nullcline - whenever \\(I=0\\), meaning there are no infected people around, infection cannot occur. There is also a third possibility where the vertical line at \\(S=1\\) crosses the horizontal axis (\\(S=1\\), \\(I=0\\)), but that also falls under the second equilibrium solution.20 Now that we have identified our nullclines and equilibrium solutions, we will add additional context with the flow of the solution. 6.3 Phase planes Next we can add more context to the Figure 6.2 by evaluating different values of \\(S\\) and \\(I\\) into our system of equations and plotting the phase plane. How we plot the phase plane is similar to the method in Chapter 5. We will test points around an equilibrium solution to determine if the solution is increasing or decreasing in \\(S\\) or \\(I\\) independently. Table 6.1 evaluates the derivatives \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) in (6.2) for different values of \\(S\\) and \\(I\\). TABLE 6.1: Values of \\(\\displaystyle \\frac{dS}{dt}\\) (as dSdt) and \\(\\displaystyle \\frac{dI}{dt}\\) (as dIdt) for Equation . S 0 1 2 0 1 2 0 1 2 I 0 0 0 1 1 1 2 2 2 dSdt 0 0 0 0 -1 -2 0 -2 -4 dIdt 0 0 0 -1 0 1 -2 0 2 Notice in Table 6.1 the different values of \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) in Equation (6.2) at each of the given \\(S\\) and \\(I\\) values. We can plot each of the coordinate pairs of \\(\\displaystyle \\left( \\frac{dS}{dt}, \\frac{dI}{dt} \\right)\\) as a vector (arrows) in the \\((S,I)\\) plane. To do so, associate \\(\\displaystyle \\frac{dS}{dt}\\) with left-right motion, so positive values of \\(\\displaystyle \\frac{dS}{dt}\\) mean the vector points to the right. Likewise, we associate \\(\\displaystyle \\frac{dI}{dt}\\) with up-down motion, so positive values \\(\\displaystyle \\frac{dI}{dt}\\) mean the vector points up. Defining the directions of the vectors in this way is also consistent when Equation (6.2) is evaluated at the nullcline solutions. At the point \\((S,I)=(1,1)\\), we have an arrow that points directly to the west because \\(\\displaystyle \\frac{dS}{dt} &lt; 0\\) and \\(\\displaystyle \\frac{dI}{dt} =0\\). Continuing on in this manner, by sequentially sampling points in the \\((S,I)\\) plane we get a vector field plot (Figure 6.3), superimposed with the nullclines. FIGURE 6.3: Phase plane for Equation (6.2), with \\(\\beta=1\\) and \\(k=1\\). 6.3.1 Motion around the nullclines We can also extend the motion around the nullclines to investigate the stability of an equilbrium solution. With a one-dimensional differential equation we used a number line to quantify values where the solution is increasing / decreasing. The problem with several differential equations is that the notion of “increasing” or “decreasing” becomes difficult to understand - there is an additional degree of freedom! Simply put, in a plane you can move left/right or up/down. The benefit for having nullclines is that they isolate the motion in one direction. When \\(\\displaystyle \\frac{dS}{dt}=0\\) the only allowed motion is up and down; when \\(\\displaystyle \\frac{dI}{dt}=0\\) the only allowed motion is left and right. In general for a two-dimensional system: When a horizontal axis variable has a nullcline, the only allowed motion is up/down. When a vertical axis variable has a nullcline, the only motion is up/down. Applying this knowledge to Equation (6.2), if we choose points where \\(I&#39;=0\\) then we know that the only motion is to the left and the right because \\(S\\) can still change along that curve. If we choose points where \\(S&#39;=0\\) then we know that the only motion is to the up/down because \\(I\\) can still change along that curve. 6.3.2 Stability of an equilbrium solution Figure 6.3 qualitatively tells us about the stability of an equilibrium point. One of the equilibrium solutions is at the origin \\((S,I)=(0,0)\\). As before we want to investigate if the equilibrium solution is stable or unstable. As you can see the arrows appear to be pointing into and towards the equilibrium solution. So we would classify this equilbrium solution as stable. 6.4 Generating a phase plane in R Let’s take what we learned from the case study of the flu model with quarantine to qualitatively analyze a system of differential equations: We determine nullclines by setting the derivatives equal to zero. Equilibrium solutions occur where nullclines for the two different equations intersect. The arrows in the phase plane help us characterize the stability of the equilibrium solution. The demodelr package has some basic functionality to generate a phase plane. Consider the following system of differential equations (Equation (6.4)): \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= x-y \\\\ \\frac{dy}{dt} &amp;= x+y \\end{split} \\tag{6.4} \\end{equation}\\] In order to generate a phase plane diagram for Equation (6.4) we need to define functions for \\(x&#39;\\) and \\(y&#39;\\), which I will annotate as \\(dx\\) and \\(dy\\) respectively. We are going to collect these equations in one vector called system_eq, using the tilde (~) as a replacement for the equals sign: system_eq &lt;- c( dx ~ x - y, dy ~ x + y ) Then what we do is apply the command phaseplane, which will generate a vector field over a domain: phaseplane(system_eq, x_var = &quot;x&quot;, y_var = &quot;y&quot; ) FIGURE 6.4: Phase plane for Equation (6.4). Let’s discuss how the phaseplane function works, first with the required inputs: You will need a system of differential equations (which we defined as system_eq). Next you need to define which variable belongs on the horizontal axis (x_var = 'x') or the vertical axis (y_var = 'y'). In Exercise 6.3 you will explore what happens if these get mixed up. There are some additional options for phaseplane: The option eq_soln = TRUE will determine if there are any equilibrium solutions to be found and report them to the console. This option does not provide a definitive answer, but at least it tells you where to look. You can always confirm if a point is an equilibrium solution by evaluating the differential equation. You can adjust the windows that are plotted with the options x_window and y_window. Both of these need to be defined as a vector (e.g. x_window = c(-0.1,0.1). The default window size is \\([-4,4]\\) for both axes. There is an option parameters that allows you to pass any parameters to the phase plane. Later chapters will introduce systems where you can modify the parameters - we won’t worry about that now. 6.5 Slope fields For a one-dimensional differential equation, we call the phase plane a slope field. For a given differential equation \\(y&#39;=f(t,y)\\), at each point in the \\(t-y\\) plane the differential equation is evaluated, showing the direction of the tangent line at that particular point. The phaseplane function can also plot slope fields. Let’s take a look at an example first and then discuss how that it works. Example 6.1 A colony of bacteria growing in a nutrient-rich medium depletes the nutrient as they grow. As a result, the nutrient concentration \\(x(t)\\) is steadily decreasing. Determine the slope field for the following differential equation: \\[\\begin{equation} \\frac{dx}{dt} = - 0.7 \\cdot \\frac{x \\cdot (3- x)}{1 + x} \\tag{6.5} \\end{equation}\\] The R code shown below will generate the slope field for Equation (6.5) (shown in Figure 6.5): # Define the windows where we make the plots t_window &lt;- c(0, 3) x_window &lt;- c(0, 5) # Define the differential equation system_eq &lt;- c( dt ~ 1, dx ~ -0.7 * x * (3 - x) / (1 + x) ) phaseplane(system_eq, x_var = &quot;t&quot;, y_var = &quot;x&quot;, x_window = t_window, y_window = x_window ) + theme_bw() + theme( legend.position = &quot;bottom&quot;, legend.text = element_text(size = 14), axis.title.x = element_text(size = 14), axis.text.x = element_text(size = 10), axis.text.y = element_text(size = 10), axis.title.y = element_text(size = 14) ) + scale_color_colorblind() FIGURE 6.5: Slope field for Equation (6.5). A few notes about the code that generated Figure 6.5: The variable on the horizontal axis (x_var) is \\(t\\), and on the vertical axis (y_var) is \\(x\\). Confusing, I know. The viewing window for the axis is also defined accordingly. Notice how the variable system_eq also contains the additional equation dt = 1. What we are doing is re-writing Equation (6.5) by introducing a new variable \\(s\\) (Equation (6.6)): \\[\\begin{equation} \\begin{split} \\frac{dt}{ds} &amp;= 1 \\\\ \\frac{dx}{ds} &amp;= - 0.7 \\cdot \\frac{x \\cdot (3- x)}{1 + x} \\end{split} \\tag{6.6} \\end{equation}\\] The differential equation \\(\\displaystyle \\frac{dt}{ds} = 1\\) has a solution \\(s=t\\), so really Equation (6.6) is a (slightly more) complicated way to express Equation (6.5). Hacky? Perhaps. However re-writing Equation (6.5) was a quick and handy workaround to re-use code. This chapter introduced a lot of useful R code to aid in visualization. The good news is that we will explore additional analyses of systems of differential equations starting with Chapter 15 - there is so much more to learn. Onward! 6.6 Exercises Exercise 6.1 Determine equilibrium solutions for Equation (6.4). Exercise 6.2 Generate a phase plane for Equation (6.4), but set the option eq_soln = TRUE. Did phaseplane detect the equilibrium solution you found in Exercise 6.1? If not, repeat the phase plane, but set x_window = c(-0.1,0.1) and y_window = c(-0.1,0.1) and repeat. Exercise 6.3 Generate a phase plane for Equation (6.4), but this time set x_var = y and y_var = x (swap which variable is which). Notice that the incorrect phase plane is produced. What is the corresponding differential equation that is visualized by this phase plane? Exercise 6.4 This problem considers the following system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y \\\\ \\frac{dy}{dt} &amp;= -x \\end{split} \\end{equation}\\] Determine the equations of the nullclines and equilibrium solution of this system of differential equations. Modify the function phaseplane to generate a phase plane of this system. For each point along a nullcline, determine the resulting motion (up-down or left-right). Based on the work you generated, determine if the equilibrium solution is stable, unstable, or inconclusive. Verify that the functions \\(x(t) = \\sin(t)\\) and \\(y=\\cos(t)\\) is one solution to this system of differential equations. Exercise 6.5 Consider the following system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y \\\\ \\frac{dy}{dt} &amp;= 3x^{2}-1 \\end{split} \\end{equation}\\] Determine the equations of the nullclines and equilibrium solutions for this system of differential equations. For each point along a nullcline, determine the resulting motion (up-down or left-right). Modify the function phaseplane to generate a phase plane of this system. Adjust the windows for \\(x\\) and \\(y\\) to be between -1 and 1. Make a hypothesis to classify if the equilibrium point is stable or unstable. Exercise 6.6 (Inspired by Thornley and Johnson (1990)) A plant grows proportional to its current length \\(L\\). Assume this proportionality constant is \\(\\mu\\), whose rate also decreases proportional to its current value. The system of equations that models this plant growth is the following: \\[\\begin{equation} \\begin{split} \\frac{dL}{dt} &amp;= \\mu L \\\\ \\frac{d\\mu}{dt} &amp;= -0.1 \\mu \\\\ \\end{split} \\end{equation}\\] Explain why \\(L=0\\) and \\(\\mu=0\\) is an equilibrium solution to this differential equation. Modify the function phaseplane to generate a phase plane of this system. Use the window \\(-0.1 \\leq L \\leq 0.1\\) and \\(-0.1 \\leq \\mu \\leq 0.1\\). (For this problem negative values of \\(L\\) and \\(\\mu\\) are not sensible, but it aids in visualizing the equilibrium solution.) Is the origin a stable equilibrium solution? Exercise 6.7 (Inspired by Logan and Wolesensky (2009)) Red blood cells are formed from stem cells in the bone marrow. The red blood cell density \\(r\\) satisfies an equation of the form \\[\\begin{equation} \\frac{dr}{dt} = \\frac{0.2r}{1+r^{2}} - 0.1 r \\end{equation}\\] What are the equilibrium solutions for this differential equation? Modify the function phaseplane to generate a phase line for this differential equation for \\(0 \\leq t \\leq 5\\) and \\(0 \\leq r \\leq 5\\). Based on the phase line, are the equilibrium solutions stable or unstable? Exercise 6.8 (Inspired by Hugo van den Berg (2011)) Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of \\(S\\) in the blood is the following: \\[ \\frac{dS}{dt} = 1 + 0.3 \\cdot (3 - S) \\] What are the equilibrium solutions for this differential equation? Modify the function phaseplane to generate a phase line for this differential equation for \\(0 \\leq t \\leq 10\\) and \\(0 \\leq S \\leq 10\\). Based on the phase line, are the equilibrium solutions stable or unstable? Exercise 6.9 Consider the differential equation \\(\\displaystyle \\frac{dx}{dt} = -3x\\). Here you will examine creating a two-dimensional system of equations by re-parameterizing \\(s=t\\). Define the variable \\(t = s\\). For this case, what is \\(\\displaystyle \\frac{dt}{ds}\\)? When \\(x = f (t (s))\\) (\\(x\\) is a composition between \\(t\\) and \\(s\\)), one way to express the chain rule is \\(\\displaystyle \\frac{dx}{ds} = \\frac{dx}{dt} \\cdot \\frac{dt}{ds}\\). Use this fact to explain why \\(\\displaystyle \\frac{dx}{ds} = -3x\\). Finally use your previous work to determine the system of equations for \\(\\displaystyle \\frac{dx}{ds}\\) and \\(\\displaystyle \\frac{dt}{ds}\\). Exercise 6.10 (Inspired by Hugo van den Berg (2011)) The core body temperature (\\(T\\)) of a mammal is coupled to the heat production (scaled by heat capacity \\(Q\\)) with the following system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dT}{dt} &amp;= Q + 0.5 \\cdot (20-T) \\\\ \\frac{dQ}{dt} &amp;= 0.1 \\cdot (38-T) \\end{split} \\end{equation}\\] Determine the equations of the nullclines and equilibrium solution of this system of differential equations. For each point along a nullcline, determine the resulting motion (up-down or left-right). You may assume that both \\(T&gt;0\\) and \\(Q&gt;0\\). Make a hypothesis to classify if the equilibrium solution is stable or unstable. Exercise 6.11 Consider the following system of differential equations for the lynx-hare model (Equation (3.7) from Chapter 3): \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= r H - b HL \\\\ \\frac{dL}{dt} &amp;=ebHL -dL \\end{split} \\end{equation}\\] Determine the equilibrium solutions for this system of differential equations. Determine equations for the nullclines, expressed as \\(L\\) as a function of \\(H\\). There should be two nullclines for each rate. Exercise 6.12 (Inspired by Hugo van den Berg (2011)) A chemostat is a tank used to study microbes and ecology, where microbes grow under controlled conditions. Think of this like a large tank with nutrient-rich water that is continuously cycled. For example, differential equations that describe the microbial biomass \\(W\\) and the nutrient concentration \\(C\\) (in the culture) are the following: \\[\\begin{equation} \\begin{split} \\frac{dW}{dt} &amp;= \\mu W - F \\frac{W}{V} \\\\ \\frac{dC}{dt} &amp;= D \\cdot (C_{R}-C) - S \\mu \\frac{W}{V}, \\end{split} \\end{equation}\\] where we have the following parameters: \\(\\mu\\) is the per capita reproduction rate, \\(F\\) is the flow rate, \\(V\\) is the volume of the culture solution, \\(D\\) is the dilution rate, \\(C_{R}\\) is the concentration of nutrients entering the culture, and \\(S\\) is a stoichiometric conversion of nutrients to biomass. Write the equations of the nullclines for this differential equation. Determine the equilibrium solutions for this system of differential equations. Generate a phase plane for this differential equation with the values \\(\\mu=1\\), \\(D=1\\), \\(C_{R}=2\\), \\(S=1\\), and \\(V=1\\). Classify the stability of the equilbrium solutions. Exercise 6.13 A classical paper Experimental Studies on the Struggle for Existence: I. Mixed Population of Two Species of Yeast by Gause (1932) examined two different species of yeast growing in competition with each other. The differential equations given for two species in competition are: \\[\\begin{equation} \\begin{split} \\frac{dy_{1}}{dt} &amp;= -b_{1} y_{1} \\frac{(K_{1}-(y_{1}+\\alpha y_{2}) )}{K_{1}} \\\\ \\frac{dy_{2}}{dt} &amp;= -b_{2} y_{2} \\frac{(K_{2}-(y_{2}+\\beta y_{1}) )}{K_{2}}, \\\\ \\end{split} \\end{equation}\\] where \\(y_{1}\\) and \\(y_{2}\\) are the two species of yeast with the parameters \\(b_{1}, \\; b_{2}, \\; K_{1}, \\; K_{2}, \\; \\alpha, \\; \\beta\\) describing the characteristics of the yeast species. Determine the equilibrium solutions for this differential equation. Express your answer in terms of the parameters \\(b_{1}, \\; b_{2}, \\; K_{1}, \\; K_{2}, \\; \\alpha, \\; \\beta\\). Gause (1932) computed the following values of the parameters: \\(b_{1}=0.21827, \\; b_{2}=0.06069, \\; K_{1}=13.0, \\; K_{2}=5.8, \\; \\alpha=3.15, \\; \\beta=0.439\\). Using these values and your results from part a, what would be the predicted values for the equilibrium solutions? Is there anything odd about the values for these equilibrium solutions? Use the function rk4 to solve this system of differential equations numerically and plot your solutions. Use initial conditions of \\(y_{1}(0)=.375\\) and \\(y_{2}(0)=.291\\), with \\(\\Delta t = 1\\) and \\(N=600\\). References "],["exact-solns-07.html", "Chapter 7 Exact Solutions to Differential Equations 7.1 Verify a solution 7.2 Separable differential equations 7.3 Integrating factors 7.4 Applying the verification method to coupled equations 7.5 Exercises", " Chapter 7 Exact Solutions to Differential Equations Chapters 4, 5, and 6 studied numerical and qualitative tools to analyze differential equations. Phase planes and slope fields helped to determine the long-term stability of an equilibrium solution. Beyond these approaches, it is also helpful to know the exact solution to a differential equation. In this chapter we will study three techniques to determine exact solutions to differential equations, making connections to some tools that you know from calculus. We briefly illustrate the methods but also have lots of exercises for you to practice these techniques, with problems in and out of a given context. Let’s get started! 7.1 Verify a solution The first approach is direct verification also known as the guess and check method. Consider the differential equation shown in Equation (7.1): \\[\\begin{equation} \\frac{dS}{dt} = 0.7 S \\tag{7.1} \\end{equation}\\] Direct verification starts with a candidate solution and then checks to see if the candidate solution is consistent with the differential equation. If we have an initial value problem (e.g. \\(S(0)=4\\)), then the candidate solution is also consistent with the initial condition. For example, let’s verify if the function \\(\\tilde{S}(t) = 5 e^{0.7t}\\) is a solution to Equation (7.1). We do this by differentiating \\(\\tilde{S}(t)\\), which, using our knowledge of calculus, is \\(0.7 \\cdot 5 e^{0.7t}\\). Finally, by rearrangement, \\(\\displaystyle \\frac{d\\tilde{S}}{dt} = 0.7 \\cdot 5 e^{0.7t} = 0.7 e^{0.7t}\\). Therefore the function \\(\\tilde{S}\\) is a solution to the differential equation. Let’s build off this to try other candidate functions: Example 7.1 Verify if the following functions are solutions to Equation (7.1): \\(\\tilde{R}(t) = 10e^{0.7t}\\) \\(\\tilde{P}(t) = e^{0.7t}\\) \\(\\tilde{Q}(t) = 5e^{0.7t}\\) \\(\\tilde{F}(t)=3\\) \\(\\tilde{G}(t)=0\\) Solution. First apply direct differentiation to each of these functions (this represents the left hand side of each differential equation): \\(\\tilde{R}(t) = 10e^{0.7t} \\rightarrow \\tilde{R}&#39;(t) = 7e^{0.7t}\\) \\(\\tilde{P}(t) = e^{0.7t} \\rightarrow \\tilde{P}&#39;(t) = 0.7e^{0.7t}\\) \\(\\tilde{Q}(t) = 5e^{0.7t} \\rightarrow \\tilde{Q}&#39;(t) = 3.5e^{0.7t}\\) \\(\\tilde{F}(t)=3 \\rightarrow \\tilde{F}&#39;(t) = 0\\) \\(\\tilde{G}(t)=0 \\rightarrow \\tilde{G}&#39;(t) = 0\\) Next compare each of these solutions to the right hand side of Equation (7.1): \\(0.7\\tilde{R}(t) = 0.7 \\cdot 10e^{0.7t} \\rightarrow 7e^{0.7t}\\) \\(0.7\\tilde{P}(t) = 0.7 e^{0.7t}\\) \\(0.7\\tilde{Q}(t) = 0.7 \\cdot 5e^{0.7t} \\rightarrow = 3.5e^{0.7t}\\) \\(0.7 \\tilde{F}(t)=0.7 \\cdot 3 \\rightarrow 2.1\\) \\(0.7 \\tilde{G}(t)=0.7 \\cdot 0 \\rightarrow 0\\) Notice how in the candidate functions (with the exception of \\(\\tilde{F}(t)\\)) the right hand side of each equation equals the left hand side. When that is the case, our candidate functions are indeed solutions to Equation (7.1)! Verifying a solution to a differential equation relies on your knowledge of differentiation versus other more involved methods, which may be an under-appreciated approach. In some instances you may not be given a candidate function as in Example 7.1. Deciding “what function to try” is the hardest step, but a safe bet would be an exponential equation (especially if the right hand side involves the dependent variable). As we will see in Chapter 18 the guess and check approach will help us to determine general solutions to systems of linear differential equations. 7.1.1 Superposition of solutions Related to the verification method is a concept called superposition of solutions. Here is how this works: if you have two known solutions to a differential equation, then the sum (or difference) is a solution as well. Let’s look at an example: Example 7.2 Show that \\(\\tilde{R}(t) + \\tilde{Q}(t) = 5e^{0.7t} + e^{0.7t}\\) from Example 7.1 is a solution to Equation (7.1). Solution. By direct differentiation, \\(\\tilde{R}&#39;(t) + \\tilde{Q}&#39;(t) = 3.5e^{0.7t} + 0.7e^{0.7t}\\). Furthermore, \\(0.7 \\cdot (\\tilde{R}(t) + \\tilde{Q}(t)) = 0.7 \\cdot (5e^{0.7t} + e^{0.7t}) = 3.5 e^{0.7t} + 0.7 e^{0.7t}\\), which equals \\(\\tilde{R}&#39;(t) + \\tilde{Q}&#39;(t)\\). Example 7.2 illustrates the principle that different solutions to a differential equation can be added together and produce a new solution. More generally, adding two solutions together is an example of a linear combination of solutions, and we can state this more formally: If \\(x(t)\\) and \\(y(t)\\) are solutions to the differential equation \\(z&#39; = f(t,z)\\), then \\(c(t) = a \\cdot x(t) + b \\cdot y(t)\\) are also solutions, where \\(a\\) and \\(b\\) are constants. 7.2 Separable differential equations The next techninque is called separation of variables. This method has a defined workflow (Separate \\(\\rightarrow\\) Integrate \\(\\rightarrow\\) Solve), which we illustrate by considering the following differential equation: \\[\\begin{equation} \\frac{dy}{dt} = yt^{2} \\tag{7.2} \\end{equation}\\] Separate: This step uses algebra to collect variables involving \\(x\\) on one side of the equation, and the variables involving \\(y\\) on the other (Equation (7.3)): \\[\\begin{equation} \\frac{1}{y} dy = t^{2} \\; dt \\tag{7.3} \\end{equation}\\] Integrate: This step computes the antiderivative of both sides of Equation (7.3): \\[\\begin{equation} \\begin{split} \\int \\frac{1}{y} dy = \\ln(y) + C_{1}. \\\\ \\int t^{2} \\; dt = \\frac{1}{3} t^{3} + C_{2} \\end{split} \\tag{7.4} \\end{equation}\\] You may remember from calculus that whenever you compute antiderivatives to always include a \\(+C\\) (hence the \\(C_{1}\\) and \\(C_{2}\\) in Equation (7.4)). For separable differential equations it is okay just to keep only one of the \\(+C\\) terms in Equation (7.4), which usually is best on the side of the independent variable (in this case \\(t\\)). Since both sides of the separated equation are equal, we can rewrite Equation (7.4) on a single line (Equation (7.5)): \\[\\begin{equation} \\ln(y) = \\frac{1}{3} t^{3} + C \\tag{7.5} \\end{equation}\\] Solve: This last step solves Equation (7.5) for the dependent variable \\(y\\): \\[\\begin{equation} \\ln(y) =\\frac{1}{3} t^{3} + C \\rightarrow e^{\\ln(y)} = e^{\\frac{1}{3} t^{3} + C} = e^{C} \\cdot e^{\\frac{1}{3} t^{3}} \\rightarrow y = Ce^{\\frac{1}{3} t^{3}} \\tag{7.6} \\end{equation}\\] We are in business! Notice how in Equation (7.6) at each step we just kept the constant to be \\(C\\), since exponentiating a constant will still be constant. To summarize, the workflow for the separating of variables technique is the following: Separate the variables on one side of the equation. Integrate both sides individually. Solve for the dependent variable. 7.3 Integrating factors Chapters 1 and 4 examined a model for the spread of Ebola where that was proportional to the number infected: \\[\\begin{equation} \\frac{dI}{dt} = .023(13600-I) = 312.8 - .023I \\tag{7.7} \\end{equation}\\] While Equation (7.7) can be solved via separation of variables, let’s try a different approach to illustrate another useful technique. First let’s write the terms in Equation (7.7) that involve \\(I\\) on one side of the equation: \\[\\begin{equation} \\frac{dI}{dt} + .03I = 30. \\tag{7.8} \\end{equation}\\] What we are going to do is multiply both sides of this Equation (7.8) by \\(e^{.023t}\\) (I’ll explain more about that later): \\[\\begin{equation} \\frac{dI}{dt} \\cdot e^{.023t} + .023I \\cdot e^{.023t} = 312.8 \\cdot e^{.023t} \\tag{7.9} \\end{equation}\\] Hmmm - this seems like we are making the differential equation harder to solve, doesn’t it? However the left hand side of Equation (7.9) is actually the derivative of the expression \\(I \\cdot e^{.023t}\\) (courtesy of the product rule from calculus). Let’s take a look: \\[\\begin{equation} \\frac{d}{dt} \\left( I \\cdot e^{.023t} \\right) = \\frac{dI}{dt} \\cdot e^{.023t} + I \\cdot .023 e^{.023t} \\tag{7.10} \\end{equation}\\] Equation (7.10) allows us to express the left hand side of Equation (7.9) as a derivative and then integrate both sides: \\[\\begin{equation} \\begin{split} \\frac{d}{dt} \\left( I \\cdot e^{.023t} \\right) &amp;= 312.8 \\cdot e^{.023t} \\rightarrow \\\\ \\int \\frac{d}{dt} \\left( I \\cdot e^{.023t} \\right) \\; dt &amp;= \\int 312.8 \\cdot e^{.023t} \\; dt \\rightarrow \\\\ I \\cdot e^{.023t} &amp;= 13600 \\cdot e^{.023t} + C \\end{split} \\tag{7.11} \\end{equation}\\] All that is left to do is to solve Equation (7.11) in terms of \\(I(t)\\) by dividing by \\(e^{.023t}\\), labeled as \\(I_{1}(t)\\) (Equation (7.12)): \\[\\begin{equation} I_{1}(t) = 13600 + Ce^{-.023t} \\tag{7.12} \\end{equation}\\] Cool! The function \\(f(t)=e^{.023t}\\) is called an integrating factor. Let’s explore this technique with a second example: Example 7.3 Apply the integrating factor technique to determine a general solution to the differential equation: \\[\\begin{equation} \\frac{dI}{dt} = .023t (13600-I) = 312.8 t - .023 t \\cdot I \\tag{7.13} \\end{equation}\\] (Equation (7.13) is a modification of Equation (7.7), where the rate of infection is time dependent.) Solution. Re-writing Equation (7.13) we have: \\[\\begin{equation} \\frac{dI}{dt} + .023 t \\cdot I = 312.8 t \\tag{7.14} \\end{equation}\\] To write the left hand side of Equation (7.14) as the derivative of a product of functions, multiply the entire differential equation by \\(\\displaystyle e^{\\int .023t \\; dt} = e^{.0115 t^{2}}\\). This term is called the integrating factor (Equation (7.15)): \\[\\begin{equation} \\frac{dI}{dt} \\cdot e^{ .0115 t^{2}} + .023 t \\cdot I \\cdot e^{0.0115 t^{2}} = 312.8 t \\cdot e^{0.0115 t^{2}} \\tag{7.15} \\end{equation}\\] Rewrite the left hand side of Equation (7.15) with the product rule: \\[\\begin{equation} \\frac{dI}{dt} \\cdot e^{ 0.0115 t^{2}} + .023 t \\cdot I \\cdot e^{0.0115 t^{2}} = \\frac{d}{dt} \\left( I \\cdot e^{0.0115 t^{2}} \\right) \\tag{7.16} \\end{equation}\\] Next integrate Equation (7.16): \\[\\begin{equation} \\begin{split} \\frac{d}{dt} \\left( I \\cdot e^{.0115 t^{2}} \\right) &amp;= 312.8 t \\cdot e^{.0115 t^{2}} \\rightarrow \\\\ \\int \\frac{d}{dt} \\left( I \\cdot e^{.0115 t^{2}} \\right) \\; dt &amp;= \\int 312.8t \\cdot e^{.0115 t^{2}} \\; dt \\rightarrow \\\\ I \\cdot e^{0.0115 t^{2}} &amp;= 27200 \\cdot e^{.0115 t^{2}} + C \\end{split} \\tag{7.17} \\end{equation}\\] The final step is to write the Equation (7.17) in terms of \\(I(t)\\); we will label this solution as \\(I_{2}(t)\\): \\[\\begin{equation} I_{2}(t) = 27200 + C e^{-.0115 t^{2}} \\tag{7.18} \\end{equation}\\] Figure 7.1 compares the solutions \\(I_{1}(t)\\) and \\(I_{2}(t)\\) when the initial condition (in both cases) is 10 (so \\(I_{1}(0)=I_{2}(0)=10\\)). Notice how the extra time-dependent factor in Equation (7.13) makes the cases grow quickly before leveling off. FIGURE 7.1: Comparison of two integrating factor solutions, Equation (7.12) in red and Equation (7.18) in blue. The integrating factor approach can be applied for differential equations that can be written in the form \\(\\displaystyle \\frac{dy}{dt} + f(t) \\cdot y = g(t)\\), using the following workflow: Determine the integrating factor \\(\\displaystyle e^{\\int f(t) \\; dt}\\). Hopefully the integral \\(\\displaystyle \\int f(t) \\; dt\\) is easy to compute! Multiply the integrating factor across your equation to rewrite the differential equation as \\(\\displaystyle \\frac{d}{dt} \\left( y \\cdot e^{\\int f(t) \\; dt} \\right) = g(t) \\cdot e^{\\int f(t) \\; dt}\\). Evaluate the integral \\(\\displaystyle H(t) = \\int g(t) \\cdot e^{\\int f(t) \\; dt} \\; dt\\). This looks intimidating - but hopefully is manageable to compute! Don’t forget the \\(+C\\)! Solve for \\(y(t)\\): \\(\\displaystyle y(t) = H(t) \\cdot e^{-\\int f(t) \\; dt} + C e^{-\\int f(t) \\; dt}\\). 7.4 Applying the verification method to coupled equations Finally, the method of verifying a solution helps to introduce a useful solution technique for systems of differential equations. We are going to study a simplified version of the lynx-hare model from Chapter 6. Equation (7.19) assumes both lynx and hares decline at a rate proportional to their respective population sizes, with the decline in the lynx population representing predation: \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= -b H \\\\ \\frac{dL}{dt} &amp; = b H - d L \\end{split} \\tag{7.19} \\end{equation}\\] Based on these simplified assumptions a good approach is to assume a solution that is exponential for both \\(H\\) and \\(L\\) (Equation (7.20)): \\[\\begin{equation} \\begin{split} \\tilde{H}(t) &amp;= C_{1} e^{\\lambda t} \\\\ \\tilde{L}(t) &amp;= C_{2} e^{\\lambda t} \\end{split} \\tag{7.20} \\end{equation}\\] Notice that Equation (7.20) has \\(C_{1}\\), \\(C_{2}\\), and \\(\\lambda\\) as parameters.21 Let’s apply the verification method to determine expressions for \\(C_{1}\\), \\(C_{2}\\), and \\(\\lambda\\) that are consistent with Equation (7.19). By differentiation of Equation (7.20), we have the following (Equation (7.21)): \\[\\begin{equation} \\begin{split} \\frac{d\\tilde{H}}{dt} &amp;= \\lambda C_{1} e^{\\lambda t} \\\\ \\frac{d\\tilde{L}}{dt} &amp;= \\lambda C_{2} e^{\\lambda t} \\end{split} \\tag{7.21} \\end{equation}\\] Comparing Equation (7.21) to Equation (7.19) we can show that \\[\\begin{equation} \\begin{split} \\lambda C_{1} e^{\\lambda t} &amp;= - b C_{1} e^{\\lambda t} \\\\ \\lambda C_{2} e^{\\lambda t} &amp;= b C_{1} e^{\\lambda t} - d C_{2} e^{\\lambda t} \\end{split} \\tag{7.22} \\end{equation}\\] Let’s rearrange Equation (7.21) some more: \\[\\begin{equation} \\begin{split} (\\lambda + b) C_{1} e^{\\lambda t} &amp;= 0\\\\ (\\lambda + d) C_{2} e^{\\lambda t} &amp;=b C_{1} e^{\\lambda t} \\end{split} \\tag{7.23} \\end{equation}\\] Notice that for the second expression in Equation (7.23) we can express \\(\\displaystyle C_{1} e^{\\lambda t}\\) as \\(\\displaystyle\\frac{(\\lambda + d)}{b} C_{2} e^{\\lambda t}\\). Next, we can substitute this expression for \\(C_{1} e^{\\lambda t}\\) into the first expression of Equation (7.23): \\[\\begin{equation} (\\lambda + b) \\frac{(\\lambda + d)}{b} C_{2} e^{\\lambda t} = 0 \\end{equation}\\] If we assume that \\(b \\neq 0\\), then we have the following simplified expression (Equation (7.24)): \\[\\begin{equation} (\\lambda +b) (\\lambda +d) C_{2} e^{\\lambda t} = 0 \\tag{7.24} \\end{equation}\\] Because the exponential function in Equation (7.24) never equals zero, the only possibility is that \\((\\lambda + b)(\\lambda + d)=0\\), or that \\(\\lambda = -b\\) or \\(\\lambda = -d\\).22 Next we need to determine values of \\(C_{1}\\) and \\(C_{2}\\). We can do this by going back to the the second expression in Equation (7.22), which we rearrange to Equation (7.25): \\[\\begin{equation} (\\lambda + d) C_{2} e^{\\lambda t} -b C_{1} e^{\\lambda t}=0 \\tag{7.25} \\end{equation}\\] Let’s analyze Equation (7.25) for each of the values of \\(\\lambda\\): Case \\(\\lambda = -d\\) For this case we have Equation (7.26): \\[\\begin{equation} (-d +d) C_{2} e^{-d t} - b C_{1} e^{-d t} =0 \\rightarrow -b C_{1} e^{-d t} =0 \\tag{7.26} \\end{equation}\\] The only way for Equation (7.26) to be consistent and remain zero is if \\(C_{1}=0\\). We don’t have any restrictions on \\(C_{2}\\), so the general solution is given with Equation (7.27): \\[\\begin{equation} \\begin{split} \\tilde{H}(t) &amp;=0 \\\\ \\tilde{L}(t) &amp;= C_{2} e^{-d t} \\end{split} \\tag{7.27} \\end{equation}\\] Case \\(\\lambda = -d\\) For this situation, Equation (7.25) becomes Equation (7.28): \\[\\begin{equation} \\left( (-d +b) C_{2} - d C_{1} \\right) e^{-d t} =0 \\tag{7.28} \\end{equation}\\] The only way for Equation (7.28) to be consistent is if \\(\\left( (-d +b) C_{2} - d C_{1} \\right)=0\\), or if \\(\\displaystyle C_{2} = \\left( \\frac{d}{-d + b} \\right) C_{1}\\). In this case, Equation (7.29) then represents the general solution: \\[\\begin{equation} \\begin{split} \\tilde{H}(t) &amp;= C_{1} e^{-d t} \\\\ \\tilde{L}(t) &amp;= \\left( \\frac{d}{-d + b} \\right) C_{1} e^{-d t} \\tag{7.29} \\end{split} \\end{equation}\\] The parameter \\(C_{2}\\) in Equation (7.29) can be determined by the initial condition. Notice that we need to have \\(d \\neq b\\) or our solution will be undefined. Finally we can write down a general solution to Equation (7.19) by combining our Equations (7.27) and (7.29) by superposition (Equation (7.30)): \\[\\begin{equation} \\begin{split} H(t) &amp;= C_{1} e^{-d t} \\\\ L(t) &amp;= \\left( \\frac{d}{-d + b} \\right) C_{1} e^{-d t} + C_{2} e^{-b t} \\end{split} \\tag{7.30} \\end{equation}\\] The method outlined here only works on linear differential equations (i.e. it wouldn’t work if there was a term such as \\(kHL\\) in Equation (7.19)). In Chapter 18 explores this method more systematically to determine general solutions to linear systems of equations. As you can see, there are a variety of techniques that can be applied in the solution of differential equations. Many more solution techniques exist - but by and large the techniques presented here probably will be your “go-tos” when working to find an exact solution to a differential equation. 7.5 Exercises Exercise 7.1 Determine the value of \\(C\\) when \\(I(0)=10\\) for the two equations: \\[\\begin{equation} \\begin{split} I_{1}(t) = 1000 + Ce^{-.03t} \\\\ I_{2}(t) = 1000 + C e^{-0.015 t^{2}} \\end{split} \\end{equation}\\] Exercise 7.2 Solve Equation (7.7) using the separation of variables technique. Exercise 7.3 Verify that \\(I_{2}(t) = N + C e^{-0.5 k t^{2}}\\) is the solution to the differential equation \\(\\displaystyle \\frac{dI}{dt} = kt (N-I)\\). Set \\(N=3\\) and \\(C=1\\). Plot \\(I_{2}(t)\\) with various values of \\(k\\) ranging from .001 to .1. What effect does \\(k\\) have on the solution? Exercise 7.4 Consider the following model of an infection: \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;= -k SI \\\\ \\frac{dI}{dt} &amp;= k SI - \\beta I \\end{split} \\end{equation}\\] Use this equation to solve the following questions: Show that \\(\\displaystyle \\frac{I&#39;}{S&#39;} = -1 + \\frac{\\beta}{k} \\frac{1}{S}\\), where \\(\\displaystyle S&#39;=\\frac{dS}{dt}\\) and \\(\\displaystyle I&#39;=\\frac{dI}{dt}\\). We will call \\(\\displaystyle \\frac{I&#39;}{S&#39;} = \\frac{dI}{dS}\\). Using separation of variables, show that the general solution to \\(\\displaystyle \\frac{I&#39;}{S&#39;} = -1 + \\frac{\\beta}{k} \\frac{1}{S}\\) is \\(\\displaystyle I(S) = -S + \\frac{\\beta}{k} \\ln (S) + C\\). At the beginning of the epidemic, \\(S_{0}+I_{0} = N\\), where \\(N\\) is the total population size. Use this fact to determine \\(C\\) in the equation \\(\\displaystyle I_{0} = - S_{0} + \\frac{\\beta}{k} \\ln (S_{0}) + C\\). Using your previous answer, show that \\(\\displaystyle I(S) = N- S + \\frac{\\beta}{k} \\ln \\left(\\frac{S}{S_{0}} \\right)\\). Plot a solution curve for \\(I(S)\\) with \\(\\beta = 1\\), \\(k=0.1\\), \\(N=100\\), and \\(S_{0}=5\\). Exercise 7.5 (Inspired by Scholz and Scholz (2014)) A chemical reaction \\(2A \\rightarrow C + D\\) can be modeled with the following differential equation: \\[\\begin{equation} \\frac{dA}{dt} = -2 k A^{2} \\end{equation}\\] Apply the method of separation of variables to determine a general solution for this differential equation. Exercise 7.6 (Inspired by Hugo van den Berg (2011)) Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of \\(S\\) in the blood is the following: \\[\\begin{equation} \\frac{dS}{dt} = I + p \\cdot (W - S) \\end{equation}\\] where the parameter \\(I\\) represents the active uptake of salt, \\(p\\) is the permeability of the skin, and \\(W\\) is the salinity in the water. For this problem, set \\(I = 0.1\\) (10% / hour), \\(p = 0.05\\) hr\\(^{-1}\\), \\(W = 0.4\\) (40% salt concentration), and \\(S(0)=0.6\\) (60% salt concentration). Generate a slope field of the differential equation \\(\\displaystyle \\frac{dS}{dt} = 0.1 + 0.05 \\cdot (.6 - S)\\). Apply integrating factors to solve the differential equation \\(\\displaystyle \\frac{dS}{dt} = 0.1 + 0.05 \\cdot (.6 - S)\\). Does your solution conform to the slope field diagram? Exercise 7.7 Which of the following differential equations can be solved via separation of variables? \\(\\displaystyle \\frac{dy}{dx} = x^{2} + xy\\) \\(\\displaystyle \\frac{dy}{dx} = e^{x+y}\\) \\(\\displaystyle \\frac{dy}{dx} = y \\cdot \\cos(2+x)\\) \\(\\displaystyle \\frac{dy}{dx} = \\ln x + \\ln y\\) \\(\\displaystyle \\frac{dy}{dx} = x \\cdot (y^{2}+2)\\) Once you have identified which ones can be solved via separation of variables, apply that technique to solve each differential equation. Exercise 7.8 Consider the following differential equation \\(\\displaystyle \\frac{dP}{dt} = - \\delta P\\), \\(P(0)=P_{0}\\), where \\(\\delta\\) is a constant parameter. Solve this equation using the method of separation of variables. Solve this equation using an integrating factor. Your two solutions from the two methods should be the same - are they? Exercise 7.9 A differential equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\[\\begin{equation} \\frac{dy}{dx} = \\frac{1}{\\theta} \\frac{y}{x}, \\end{equation}\\] where \\(\\theta \\geq 1\\) is a constant. Apply separation of variables to determine the general solution to this differential equation. Exercise 7.10 Apply separation of variables to determine general solutions to the following systems of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= x \\\\ \\frac{dy}{dt} &amp;= y \\end{split} \\tag{7.31} \\end{equation}\\] (Equation (7.31) is an example of an uncoupled system of equations.) Exercise 7.11 (Inspired by Thornley and Johnson (1990)) A plant grows proportional to its current length \\(L\\). Assume this proportionality constant is \\(\\mu\\), whose rate also decreases proportional to its current value. The system of equations that models this plant growth is the following: \\[\\begin{equation} \\begin{split} \\frac{dL}{dt} = \\mu L \\\\ \\frac{d\\mu}{dt} = -k \\mu \\\\ \\mbox{($k$ is a constant parameter)} \\end{split} \\end{equation}\\] Apply separation of variables to determine the general solutions to this system of equations. Exercise 7.12 Apply the verification method developed in Chapter 7.4 to determine the general solution to the following system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= x-y \\\\ \\frac{dy}{dt} &amp; = 2y \\end{split} \\end{equation}\\] Exercise 7.13 Apply the integrating factors technique to determine the solution to the differential equation \\(\\displaystyle \\frac{dI}{dt} = (N-I) = kN - kI\\), where \\(k\\) and \\(N\\) are parameters. Exercise 7.14 For each of the following differential equations: Determine equilibrium solutions for the differential equation. Apply separation of variables to determine general solutions to the following differential equations. Choose reasonable values of any parameters and plot the solution curve for an initial condition that you select. \\(\\displaystyle \\frac{dy}{dx} = -\\frac{x}{y}\\) \\(\\displaystyle \\frac{dy}{dx} = 8-y\\) \\(\\displaystyle \\frac{dW}{dt} = k (N-W)\\) (\\(k\\) and \\(N\\) are constant parameters) \\(\\displaystyle \\frac{dR}{dt} =-aR \\ln \\frac{R}{K}\\) (\\(a\\) and \\(K\\) are constant parameters) Exercise 7.15 Consider the following differential equation, where \\(M\\) represents a population of mayflies and \\(t\\) is time (given in months), and \\(\\delta\\) is a mortality rate (units % mayflies / month): \\[\\begin{equation} \\frac{dM}{dt} = - \\delta \\cdot M \\end{equation}\\] Determine the general solution to this differential equation and plot a few different solution curves with different values of \\(\\delta&gt;0\\). Assume that \\(M(0) = 10,000\\). Describe the effect of changing \\(\\delta\\) on your solution. Exercise 7.16 An alternative model of mayfly mortality is the following: \\[\\begin{equation} \\displaystyle \\frac{dM}{dt} = - \\delta(t) \\cdot M, \\end{equation}\\] where \\(\\delta(t)\\) is a time dependent mortality function. Determine a solution and plot a solution curve (assuming \\(M(0)=10,000\\) and over the interval from \\(0 \\leq t \\leq 5\\), assuming time is scaled appropriately) for this differential equation when \\(\\delta(t)\\) has the following forms: \\(\\delta(t) = 1\\) \\(\\delta(t) = 2t\\) \\(\\delta(t) = 1-e^{-t}\\) \\(\\delta(t) = 1+e^{-t}\\) Provide a reasonable biological explanation justifying the use of these alternative mayfly models. References "],["linear-regression-08.html", "Chapter 8 Linear Regression and Curve Fitting 8.1 What is parameter estimation? 8.2 Parameter estimation for global temperature data 8.3 Moving beyond linear models for parameter estimation 8.4 Parameter estimation with nonlinear models 8.5 Towards model-data fusion 8.6 Exercises", " Chapter 8 Linear Regression and Curve Fitting 8.1 What is parameter estimation? Chapters 1 - 7 introduced the idea of modeling with rates of change. There is much more to be stated regarding qualitative analyses of a differential equation (Chapters 5 and 6), which we will return to starting in Chapter 15. But for the moment, let’s pause and recognize that a key motivation for modeling with rates of change is to quantify observed phenomena. Oftentimes, we wish to compare model outputs to measured data. While that may seem straightforward, sometimes models have parameters (such as \\(k\\) and \\(\\beta\\) for Equation (6.1) in Chapter 6). Parameter estimation is the process of determining model parameters from data. Stated differently: Parameter estimation is the process that determines the set of parameters \\(\\vec{\\alpha}\\) that minimize the difference between data \\(\\vec{y}\\) and the output of the function \\(f(\\vec{x}, \\vec{\\alpha})\\) and measured error \\(\\vec{\\sigma}\\). Over the next several chapters we will examine aspects of parameter estimation. Sometimes parameter estimation is synonymous with “fitting a model to data” and can also be called data assimilation or model-data fusion. We can address the parameter estimation problem from several different mathematical areas: calculus (optimization), statistics (likelihood functions), and linear algebra (systems of linear equations). We will explore how we define “best” over several chapters, but let’s first explore techniques of how this is done in R using simple linear regression.23 Let’s get started! 8.2 Parameter estimation for global temperature data Let’s take a look at a specific example. Table 8.1 shows anomalies in average global temperature since 1880, relative to 1951-1980 global temperatures.24 This dataset can be found in the demodelr package with the name global_temperature. To name our variables let \\(Y=\\mbox{ Year since 1880 }\\) and \\(T= \\mbox{ Temperature anomaly}\\). TABLE 8.1: First few years of average global temperature anomalies. The anomaly represents the global surface temperature relative to 1951-1980 average temperatures. year_since_1880 0.00 1.00 2.00 3.00 4.00 5.00 6.00 temperature_anomaly -0.17 -0.08 -0.11 -0.18 -0.29 -0.33 -0.31 We will be working with these data to fit a function \\(f(Y,\\vec{\\alpha})=T\\). In order to fit a function in R we need three essential elements, distilled into a workflow of: Identify \\(\\rightarrow\\) Construct \\(\\rightarrow\\) Compute Identify data for the formula to estimate parameters. For this example we will use the tibble (or data frame) global_temperature. Construct the regression formula we will use for the fit. We want to do a linear regression so that \\(T = a+bY\\). How we represent the regression formula in R is with temperature_anomaly ~ 1 + year_since_1880. Notice that this regression formula must include named columns from your data. Said differently, this regression formula “defines a linear regression where the factors are a constant term and one is proportional to the predictor variable.” It is helpful to assign this regression formula as a variable: regression_formula &lt;- temperature_anomaly ~ 1 + year_since_1880. In Chapter 8.3 we will discuss other types of regression formulas. Compute the regression with the command lm (which stands for linear model). That’s it! So if we need to do a linear regression of global temperature against year since 1880, it can be done with the following code: regression_formula &lt;- temperature_anomaly ~ 1 + year_since_1880 linear_fit &lt;- lm(regression_formula, data = global_temperature) summary(linear_fit) ## ## Call: ## lm(formula = regression_formula, data = global_temperature) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35606 -0.13185 -0.03044 0.12449 0.45518 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.4880971 0.0296270 -16.48 &lt;2e-16 *** ## year_since_1880 0.0076685 0.0003633 21.11 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1775 on 140 degrees of freedom ## Multiple R-squared: 0.7609, Adjusted R-squared: 0.7592 ## F-statistic: 445.6 on 1 and 140 DF, p-value: &lt; 2.2e-16 What is printed on the console (and shown above) is the summary of the fit results. This summary contains several interesting things that you would study in advanced courses in statistics, but here is what we will focus on: The estimated coefficients (starting with Coefficients: above) of the linear regression. The column Estimate lists the constants in front of our regression formula \\(y=a+bx\\). What follows is the statistical error for that estimate. The other additional columns concern statistical tests that show significance of the estimated parameters. One helpful thing to look at is the Residual standard error (starting with Residual standard error above), which represents the overall, total effect of the differences between the model predicted values of \\(\\vec{y}\\) and the measured values of \\(\\vec{y}\\). The goal of linear regression is to minimize this model-data difference. The summary of the statistical fit is a verbose readout, which may prohibit quickly identifying the regression coefficients or plotting the fitted results. Thankfully the R package called broom can help us! The broom package produces model output in what is called “tidy” data format. You can read more about broom from its documentation. Since we are only going to use one or two functions from this package, I am going to refer to the functions I need with the syntax PACKAGE_NAME::FUNCTION. First we will make a data frame with the predicted coefficients from our linear model, as shown with the following code that you can run on your own: global_temperature_model &lt;- broom::augment(linear_fit, data = global_temperature) glimpse(global_temperature_model) Notice how the augment command takes the results from linear_fit with the data global_temperature to produce model estimated results (under the variable named .fitted.25 There is a lot to unpack with this new data frame, but the important ones are the columns year_since_1880 (the independent variable) and .fitted, which represents the fitted coefficients. Finally, Figure 8.1 compares the data to the fitted regression line (also known as the “best fit line”). ggplot(data = global_temperature) + geom_point(aes(x = year_since_1880, y = temperature_anomaly), color = &quot;red&quot;, size = 2 ) + geom_line( data = global_temperature_model, aes(x = year_since_1880, y = .fitted) ) + labs( x = &quot;Year Since 1880&quot;, y = &quot;Temperature anomaly&quot; ) FIGURE 8.1: Global temperature anomaly data along with the fitted linear regression line. A word of caution: this example is one of an exploratory data analysis illustrating parameter estimation with R. Global temperature is a measurement of many different types of complex phenomena integrated from the local, regional, continental, and global levels (and interacting in both directions). Global temperature anomalies cannot be distilled down to a simple linear relationship between time and temperature. What Figure 8.1 does suggest though is that over time the average global temperature has increased. I encourage you to study the pages at NOAA to learn more about the scientific consensus in modeling climate (and the associated complexities - it is a fascinating scientific problem that will need YOUR help to solve it!) 8.3 Moving beyond linear models for parameter estimation We can also estimate parameters or fit additional polynomial models such as the equation \\(y = a + bx + cx^{2} + dx^{3} ...\\) (here the estimated parameters \\(a\\), \\(b\\), \\(c\\), \\(d\\), …). There is a key distinction here: the regression formula is nonlinear in the predictor variable \\(x\\), but linear with respect to the parameters. Incorporating these regression formulas in R modifies the structure of the regression formula. A few templates are show in Table 8.2: TABLE 8.2: Comparison of model equations to regression formulas used for R. The variable \\(y\\) is the response variable and \\(x\\) the predictor variable. Notice the structure I(..) is needed for R to signify a factor of the form \\(x^{n}\\). Equation Regression Formula \\(y=a+bx\\) y ~ 1 + x \\(y=a\\) y ~ 1 \\(y=bx\\) y ~ -1+x \\(y=a+bx+cx^{2}\\) y ~ 1 + x + I(x^2) \\(y=a+bx+cx^{2}+dx^{3}\\) y~ 1 + x + I(x^2) + I(x^3) 8.3.1 Can you linearize your model? We can estimate parameters for nonlinear models in cases where the function can be transformed mathematically to a linear equation. Here is one example: while the equation \\(y=ae^{bx}\\) is nonlinear with respect to the parameters, it can be made linear by a logarithmic transformation of the data:\\index{logarithmic transformation} \\[\\begin{equation} \\ln(y) = \\ln(ae^{bx}) = \\ln(a) + \\ln (e^{bx}) = \\ln(a) + bx \\end{equation}\\] The advantage to this approach is that the growth rate parameter \\(b\\) is easily identifiable from the data, and the value of \\(a\\) is found by exponentiation of the fitted intercept value. The disadvantage is that you need to do a log transform of the \\(y\\) variable first before doing any fits. Example 8.1 A common nonlinear equation in enzyme kinetics is the Michaelis-Menten law, which states that the rate of the uptake of a substrate \\(V\\) is given by the equation: \\[\\begin{equation} V = \\frac{V_{max} s}{s+K_{m}}, \\end{equation}\\] where \\(s\\) is the amount of substrate, \\(K_{m}\\) is half-saturation constant, and \\(V_{max}\\) the maximum reaction rate. (Typically \\(V\\) is used to signify the “velocity” of the reaction.) Consider you have the following data (from J. Keener and Sneyd (2009)): s (mM) 0.1 0.2 0.5 1.0 2.0 3.5 5.0 V (mM / s) 0.04 0.08 0.17 0.24 0.32 0.39 0.42 Apply parameter estimation techniques to estimate \\(K_{m}\\) and \\(V_{max}\\) and plot the resulting fitting curve with the data. Solution. The first thing that we will need to do is to define a data frame (tibble) for these data: enzyme_data &lt;- tibble( s = c(0.1, 0.2, 0.5, 1.0, 2.0, 3.5, 5.0), V = c(0.04, 0.08, 0.17, 0.24, 0.32, 0.39, 0.42) ) Figure 8.2 shows a plot of \\(s\\) and \\(V\\): FIGURE 8.2: Scatterplot of enzyme substrate data from Example 8.1. Figure 8.2 definitely suggests a nonlinear relationship between \\(s\\) and \\(V\\). To dig a little deeper, try running the following code that plots the reciprocal of \\(s\\) and the reciprocal of \\(V\\) (do this on your own): ggplot(data = enzyme_data) + geom_point(aes(x = 1 / s, y = 1 / V), color = &quot;red&quot;, size = 2 ) + labs( x = &quot;1/s (1/mM)&quot;, y = &quot;1/V (s / mM)&quot; ) Notice how easy it was to plot the reciprocals of \\(s\\) and \\(V\\) inside the ggplot command. Here’s how to see this with the provided equation (and a little bit of algebra): \\[\\begin{equation} V = \\frac{V_{max} s}{s+K_{m}} \\rightarrow \\frac{1}{V} = \\frac{s+K_{m}}{V_{max} s} = \\frac{1}{V_{max}} + \\frac{1}{s} \\frac{K_{m}}{V_{max}} \\end{equation}\\] In order to do a linear fit to the transformed data we will use the regression formulas defined above and the handy structure I(VARIABLE) and plot the transformed data with the fitted equation (do this on your own as well):26 # Define the regression formula enzyme_formula &lt;- I(1 / V) ~ 1 + I(1 / s) # Apply the linear fit enzyme_fit &lt;- lm(enzyme_formula,data = enzyme_data) # Show best fit parameters summary(enzyme_fit) # Added fitted data to the model enzyme_data_model &lt;- broom::augment(enzyme_fit, data = enzyme_data) # Compare fitted model to the data ggplot(data = enzyme_data) + geom_point(aes(x = 1 / s, y = 1 / V), color = &quot;red&quot;, size = 2 ) + geom_line( data = enzyme_data_model, aes(x = 1 / s, y = .fitted) ) + labs( x = &quot;1/s (1/mM)&quot;, y = &quot;1/V (s / mM)&quot; ) In Exercise 8.6 you will use the coefficients from your linear fit to determine \\(V_{max}\\) and \\(K_{m}\\). When plotting the fitted model values with the original data (Figure 8.3), we need to take the reciprocal of the column .fitted when we apply augment because the response variable in the linear model is \\(1 / V\\) (confusing, I know!). For convenience, the code that does the all the fitting is shown below:27 # Define the regression formula enzyme_formula &lt;- I(1 / V) ~ 1 + I(1 / s) # Apply the linear fit enzyme_fit &lt;- lm(enzyme_formula,data = enzyme_data) # Added fitted data to the model enzyme_data_model &lt;- broom::augment(enzyme_fit, data = enzyme_data) ggplot(data = enzyme_data) + geom_point(aes(x = s, y = V), color = &quot;red&quot;, size = 2 ) + geom_line( data = enzyme_data_model, aes(x = s, y = 1 / .fitted) ) + labs( x = &quot;s (mM)&quot;, y = &quot;V (mM / s)&quot; ) FIGURE 8.3: Scatterplot of enzyme substrate data from Example 8.1 along with the fitted curve. 8.4 Parameter estimation with nonlinear models In many cases you will not be able to write your model in a linear form by applying functional transformations. Here’s the good news: you can still do a non-linear curve fit using the function nls, which is similar to the command lm with some additional information. Let’s return to an example from Chapter 2 where we examined the weight of a dog named Wilson (Figure 2.2). One model for the dog’s weight \\(W\\) from the days since birth \\(D\\) is a saturating exponential equation (Equation (8.1)): \\[\\begin{equation} W =f(D,a,b,c)= a - be^{-ct}, \\tag{8.1} \\end{equation}\\] where we have the parameters \\(a\\), \\(b\\), and \\(c\\). We can apply the command nls (nonlinear least squares) to estimate these parameters. Because nls is an iterative numerical method it needs a starting value for these parameters (here we set \\(a = 75\\), \\(b=30\\), and \\(c=0.01\\)). Determining a starting point can be tricky - it does take some trial and error. # Define the regression formula wilson_formula &lt;- weight ~ a - b * exp(-c * days) # Apply the nonlinear fit nonlinear_fit &lt;- nls(formula = wilson_formula, data = wilson, start = list(a = 75, b = 30, c = 0.01) ) # Summarize the fit parameters summary(nonlinear_fit) ## ## Formula: weight ~ a - b * exp(-c * days) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## a 7.523e+01 1.573e+00 47.84 &lt; 2e-16 *** ## b 9.077e+01 3.400e+00 26.70 1.07e-14 *** ## c 6.031e-03 4.324e-04 13.95 2.26e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.961 on 16 degrees of freedom ## ## Number of iterations to convergence: 11 ## Achieved convergence tolerance: 6.899e-06 Similar as before, we can augment the data to display the fitted curve. However once you have your fitted model, you can still plot the fitted values with the coefficients (try this out on your own). # Augment the model wilson_model &lt;- broom::augment(nonlinear_fit, data = wilson) # Plot the data with the model ggplot(data = wilson) + geom_point(aes(x = days, y = weight), color = &quot;red&quot;, size = 2 ) + geom_line( data = wilson_model, aes(x = days, y = .fitted) ) + labs( x = &quot;Days since birth&quot;, y = &quot;Weight (pounds)&quot; ) 8.5 Towards model-data fusion The R language (and associated packages) has many excellent tools for parameter estimation and comparing fitted models to data. These tools are handy for first steps in parameter estimation. More broadly the technique of estimating models from data can also be called data assimilation or model-data fusion. Whatever terminology you happen to use, you are combining the best of both worlds: combining observed measurements with what you expect should happen, given the understanding of the system at hand. We are going to dig into data assimilation even more - and one key tool is understanding likelihood functions, which we will study in the next chapter. 8.6 Exercises Exercise 8.1 Determine if the following equations are linear with respect to the parameters. Assume that \\(y\\) is the response variable and \\(x\\) the predictor variable. \\(y=a + bx+cx^{2}+dx^{3}\\) \\(y=a \\sin (x) + b \\cos (x)\\) \\(y = a \\sin(bx) + c \\cos(dx)\\) \\(y = a + bx + a\\cdot b x^{2}\\) \\(y = a e^{-x} + b e^{x}\\) \\(y = a e^{-bx} + c e^{-dx}\\) Exercise 8.2 Each of the following equations can be written as linear with respect to the parameters, through applying some elementary transformations to the data. Write each equation as a linear function with respect to the parameters. Assume that \\(y\\) is the response variable and \\(x\\) the predictor variable. \\(y=ae^{-bx}\\) \\(y=(a+bx)^{2}\\) \\(\\displaystyle y = \\frac{1}{a+bx}\\) \\(y = c x^{n}\\) Exercise 8.3 Use the dataset global_temperature and the function lm to answer the following questions: Complete the following table, which represents various regression fits to global temperature anomaly \\(T\\) (in degrees Celsius) and years since 1880 (denoted by \\(Y\\)). In the table Coefficients represent the values of the parameters \\(a\\), \\(b\\), \\(c\\), etc. from your fitted equation; P = number of parameters; RSE = Residual standard error. Equation Coefficients P RSE \\(T=a+bY\\) \\(T=a+bY+cY^{2}\\) \\(T=a+bY+cY^{2}+dY^{3}\\) \\(T=a+bY+cY^{2}+dY^{3}+eY^{4}\\) \\(T=a+bY+cY^{2}+dY^{3}+eY^{4}+fY^{5}\\) \\(T=a+bY+cY^{2}+dY^{3}+eY^{4}+fY^{5}+gY^{6}\\) After making this table, choose the polynomial of the function that you believe fits the data best. Provide reasoning and explanation why you chose the polynomial that you did. Finally show the plot of your selected polynomial with the data. Exercise 8.4 An equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\(\\displaystyle y = c x^{1/\\theta},\\) where \\(\\theta \\geq 1\\) and \\(c&gt;0\\) are both constants. Use the dataset phosphorous to make a scatterplot with algae as the predictor (independent) variable and daphnia the response (dependent) variable. Show that you can linearize the equation \\(\\displaystyle y = c x^{1/\\theta}\\) with logarithms. Determine a linear regression fit for your new linear equation. Determine the value of \\(c\\) and \\(\\theta\\) in the original equation with the parameters from the linear fit. Exercise 8.5 Similar to Exercise 8.4, do a non-linear least squares fit for the dataset phosphorous to the equation \\(\\displaystyle y = c x^{1/\\theta}\\). For a starting point, you may use the values of \\(c\\) and \\(\\theta\\) from Exercise 8.4. Then make a plot of the original phosphorous data with the fitted model results. Exercise 8.6 Example 8.1 guided you through the process to linearize the following equation: \\[\\begin{equation} V = \\frac{V_{max} s}{s+K_{m}}, \\end{equation}\\] where \\(s\\) is the amount of substrate, \\(K_{m}\\) is half-saturation constant, and \\(V_{max}\\) the maximum reaction rate. (Typically \\(V\\) is used to signify the “velocity” of the reaction.) When doing a fit of the reciprocal of \\(s\\) with the reciprocal of \\(V\\), what are the resulting values of \\(V_{max}\\) and \\(K_{m}\\)? Exercise 8.7 Following from Example 8.1 and Exercise 8.6, apply the command nls to conduct a nonlinear least squares fit of the enzyme data to the equation: \\[\\begin{equation} V = \\frac{V_{max} s}{s+K_{m}}, \\end{equation}\\] where \\(s\\) is the amount of substrate, \\(K_{m}\\) is the half-saturation constant, and \\(V_{max}\\) the maximum reaction rate. As starting points for the nonlinear least squares fit, you may use the values of \\(K_{m}\\) and \\(V_{max}\\) that were determined from Example 8.1. Then make a plot of the actual data with the fitted model curve. Exercise 8.8 Consider the following data which represent the temperature over the course of a day: Hour Temperature 0 54 1 53 2 55 3 54 4 58 5 58 6 61 7 63 8 67 9 66 10 67 11 69 12 68 13 68 14 66 15 67 16 63 17 60 18 59 19 57 20 56 21 53 22 52 23 54 24 53 Make a scatterplot of these data, with the variable on the horizontal axis. A function that describes these data is \\(\\displaystyle T = A + B \\sin \\left( \\frac{\\pi}{12} \\cdot H \\right) + C \\cos \\left( \\frac{\\pi}{12} \\cdot H \\right)\\), where \\(H\\) is the hour and \\(T\\) is the temperature. Explain why this equation is linear for the parameters \\(A\\), \\(B\\), and \\(C\\). Define a tibble that includes the variables \\(T\\), \\(\\displaystyle \\sin \\left( \\frac{\\pi}{12} \\cdot H \\right)\\), \\(\\displaystyle \\cos \\left( \\frac{\\pi}{12} \\cdot H \\right)\\). Do a linear fit on your new data frame to report the values of \\(A\\), \\(B\\), and \\(C\\). Define a new tibble that has a sequence in \\(H\\) starting at 0 from 24 with at least 100 data points, and a value of \\(T\\) (T_fitted) using your coefficients of \\(A\\), \\(B\\), and \\(C\\). Add your fitted curve to the scatterplot. How do your fitted values compare to the data? Exercise 8.9 Use the data from Exercise 8.8 to conduct a nonlinear fit (use the function nls) to the equation \\(\\displaystyle T = A + B \\sin \\left( \\frac{\\pi}{12} \\cdot H \\right) + C \\cos \\left( \\frac{\\pi}{12} \\cdot H \\right)\\). Good starting points are \\(A=50\\), \\(B=1\\), and \\(C=-10\\). References "],["likelihood-09.html", "Chapter 9 Probability and Likelihood Functions 9.1 Linear regression on a small dataset 9.2 Continuous probability density functions 9.3 Connecting probabilities to linear regression 9.4 Visualizing likelihood surfaces 9.5 Looking back and forward 9.6 Exercises", " Chapter 9 Probability and Likelihood Functions In Chapter 8 we began to the process of parameter estimation. We revisit parameter estimation here by applying likelihood functions, which is a topic from probability and statistics. Probability is the association of a set of observable events to a quantitative scale between 0 and 1. Informally, a value of zero means that event is not possible; 1 means that it definitely can happen.28 We will only consider continuous events with the range of parameter estimation problems examined here. This chapter will introduce likelihood functions but also discuss some interesting visualization techniques of multivariable functions and contour plots. As with Chapter 8 we are starting to build out some R skills and techniques that you can apply in other contexts. Let’s get started! 9.1 Linear regression on a small dataset Table 9.1 displays a dataset with a limited number of points where we wish to fit the function \\(y=bx\\): TABLE 9.1: A small, limited dataset. x 1 2 4 4 y 3 5 4 10 For this example we are forcing the intercept term to equal zero - for most cases you will just fit the linear equation (see Exercise 9.6 where you will consider the intercept \\(a\\)). Figure 9.1 displays a quick scatterplot of these data: FIGURE 9.1: A scatterplot of a small, limited dataset (Table 9.1). The goal here is to work to determine the value of \\(b\\) that is most likely - or consistent - with the data. However, before we tackle this further we need to understand how to quantify most likely in a mathematical sense. In order to do this, we need to take a quick excursion into continuous probability distributions. 9.2 Continuous probability density functions Consider Figure 9.2, which may be familiar to you as the normal distribution or the bell curve: FIGURE 9.2: The standard normal distribution, with a shaded area between \\(x=\\pm 1\\) We tend to think of the plot and the associated function \\(f(x)\\) as something with input and output (such as \\(f(0)=\\) 0.3989). However because it is a probability density function, the area between two points yields the probability of an event to fall within two values as shown in Figure 9.2. In this case, the numerical value of the shaded area would represent the probability that our measurement is in the interval \\(-1 \\leq x \\leq 1\\). The value of the area, or the probability, is 0.68269. Perhaps when you studied calculus the area was expressed as a definite integral: \\(\\displaystyle \\int_{-1}^{1} f(x) \\; dx=\\) 0.68269, where \\(f(x)\\) is the formula for the probability density function for the normal distribution. Here, the formula for the normal distribution \\(f(x)\\) is given by Equation (9.1), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation.29 \\[\\begin{equation} f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma } e^{-(x-\\mu)^{2}/(2 \\sigma^{2})} \\tag{9.1} \\end{equation}\\] With this intuition we can summarize key facts about probability density functions: \\(f(x) \\geq 0\\) (this means that probability density functions are positive values) Area integrates to one (in probability, this means we have accounted for all of our outcomes) 9.3 Connecting probabilities to linear regression Now that we have made that small excursion into probability, let’s return to the parameter estimation problem. Another way to phrase this problem is to examine the probability distribution of the model-data residual for each measurement \\(\\epsilon_{i}\\) (Equation (9.2)): \\[\\begin{equation} \\epsilon_{i} = y_{i} - f(x_{i},\\vec{\\alpha} ). \\tag{9.2} \\end{equation}\\] The approach with likelihood functions assumes a particular probability distribution on each residual. One common assumption is that the model-data residual is normally distributed. In most applications the mean of this distribution is zero (\\(\\mu=0\\)) and the standard deviation \\(\\sigma\\) (which could be specified as measurement error, etc.). We formalize this assumption with a likelihood function \\(L\\) in Equation (9.3). \\[\\begin{equation} L(\\epsilon_{i}) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\epsilon_{i}^{2} / 2 \\sigma^{2} } \\tag{9.3} \\end{equation}\\] To extend this further across all measurements, we use the idea of independent, identically distributed measurements so the joint likelihood of all the residuals (each \\(\\epsilon_{i}\\)) is the product of the individual likelihoods (Equation (9.4). The assumption of independent, identically distributed is a common one. As a note of caution you should always evaluate if this is a valid assumption for more advanced applications. \\[\\begin{equation} L(\\vec{\\epsilon}) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\epsilon_{i}^{2} / 2 \\sigma^{2} } \\tag{9.4} \\end{equation}\\] We are making progress here; however to fully characterize the solution we need to specify the parameters \\(\\vec{\\alpha}\\). A simple redefining of the likelihood function where we specify the measurements (\\(x\\) and \\(y\\)) and parameters (\\(\\vec{\\alpha}\\)) is all we need (Equation (9.5)). \\[\\begin{equation} L(\\vec{\\alpha} | \\vec{x},\\vec{y} )= \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp(-(y_{i} - f(x_{i},\\vec{\\alpha} ))^{2} / 2 \\sigma^{2} ) \\tag{9.5} \\end{equation}\\] Now with Equation (9.5) we have a function where the best parameter estimate is the one that optimizes the likelihood. Returning to our original linear regression problem (Table 9.1 and Figure 9.1), we want to determine the \\(b\\) for the function \\(y=bx\\). Equation (9.6) then characterizes the likelihood of \\(b\\), given the data \\(\\vec{x}\\) and \\(\\vec{y}\\): \\[\\begin{equation} L(b | \\vec{x},\\vec{y} ) = \\left( \\frac{1}{\\sqrt{2 \\pi} \\sigma}\\right)^{4} e^{-\\frac{(3-b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(5-2b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(4-4b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(10-4b)^{2}}{2\\sigma^{2}}} \\tag{9.6} \\end{equation}\\] For the purposes of our argument here, we will assume \\(\\sigma=1\\). Figure 9.3 shows a plot of the likelihood function \\(L(b | \\vec{x},\\vec{y} )\\). FIGURE 9.3: The likelihood function (Equation (9.6)) for the small dataset (Table 9.1), with the value of the maximum likelihood at \\(b=1.865\\) marked with a vertical line. Note that in Figure 9.3 the values of \\(L(b | \\vec{x},\\vec{y} )\\) on the vertical axis are really small. (This typically may be the case; see Exercise 9.2.) An alternative to the small numbers in \\(L(b)\\) is to use the log-likelihood (Equation (9.7)): \\[\\begin{equation} \\begin{split} \\ln(L(\\vec{\\alpha} | \\vec{x},\\vec{y} )) &amp;= N \\ln \\left( \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\right) - \\sum_{i=1}^{N} \\frac{ (y_{i} - f(x_{i},\\vec{\\alpha} )^{2}}{ 2 \\sigma^{2}} \\\\ &amp; = - \\frac{N}{2} \\ln (2) - \\frac{N}{2} \\ln(\\pi) - N \\ln( \\sigma) - \\sum_{i=1}^{N} \\frac{ (y_{i} - f(x_{i},\\vec{\\alpha} )^{2}}{ 2 \\sigma^{2}} \\end{split} \\tag{9.7} \\end{equation}\\] In Exercise 9.5 you will be working on how to transform the likelihood function \\(L(b)\\) to the log-likelihood \\(\\ln(L(b))\\) and showing that Equation (9.6) is maximized at \\(b=1.865\\). The data with the fitted line is shown in Figure 9.4. FIGURE 9.4: A scatterplot of a small dataset (Table 9.1) with fitted line \\(y=1.865x\\) from optimizing Equation (9.6). 9.4 Visualizing likelihood surfaces Next we are going to examine a second example from Gause (1932) which modeled the growing of yeast in solution. This classic paper examines the biological principal of competitive exclusion, how one species can out-compete another one for resources. Some of the data from Gause (1932) is encoded in the data frame yeast in the demodelr package. For this example we are going to examine a model for one species growing without competition. Figure 9.5 shows a scatterplot of the yeast data. ### Make a quick ggplot of the data ggplot() + geom_point( data = yeast, aes(x = time, y = volume), color = &quot;red&quot;, size = 2 ) + labs(x = &quot;Time&quot;, y = &quot;Volume&quot;) FIGURE 9.5: Scatterplot of Sacchromyces volume growing by itself in a container. We are going to assume the population of yeast (represented with the measurement of volume) over time changes according to the differential equation \\(\\displaystyle \\frac{dy}{dt} = -by \\frac{(K-y)}{K}\\), where \\(y\\) is the population of the yeast, and \\(b\\) represents the growth rate, and \\(K\\) is the carrying capacity of the population. Equation (9.8) shows the solution to this differential equation, where the additional parameter \\(a\\) can be found through application of the initial condition \\(y_{0}\\). \\[\\begin{equation} y = \\frac{K}{1+e^{a-bt}} \\tag{9.8} \\end{equation}\\] In Gause (1932) the value of \\(a\\) was determined by solving the initial value problem \\(y(0)=0.45\\). In Exercise 9.1 you will show that \\(\\displaystyle a = \\ln \\left( \\frac{K}{0.45} - 1 \\right)\\). Equation (9.8) then has two parameters: \\(K\\) and \\(b\\). Here we are going to explore the likelihood function to try to determine the best set of values for the two parameters \\(K\\) and \\(b\\) using a function in the demodelr package called compute_likelihood. Inputs to the compute_likelihood function are the following: A function \\(y=f(x,\\vec{\\alpha})\\) A dataset \\((\\vec{x},\\vec{y})\\) Ranges of your parameters \\(\\vec{\\alpha}\\). The compute_likelihood function also has an optional input logLikely that allows you to specify if you want to compute the likelihood or the log-likelihood. The default is that logLikely is FALSE, meaning that the normal likelihoods are plotted. First we will define the equation used to compute our model in the likelihood. As with the functions euler or systems in Chapter 4 we need to define this function: library(demodelr) # Define the solution to the differential equation with # parameters K and bGause model equation gause_model &lt;- volume ~ K / (1 + exp(log(K / 0.45 - 1) - b * time)) # Identify the ranges of the parameters that we wish to investigate kParam &lt;- seq(5, 20, length.out = 100) bParam &lt;- seq(0, 1, length.out = 100) # Allow for all the possible combinations of parameters gause_parameters &lt;- expand.grid(K = kParam, b = bParam) # Now compute the likelihood gause_likelihood &lt;- compute_likelihood( model = gause_model, data = yeast, parameters = gause_parameters, logLikely = FALSE ) Ok, let’s break this code down step by step: The line gause_model &lt;- volume ~ K/(1+exp(log(k/0.45-1)-b*time)) identifies the formula that relates the variables time to volume in the dataset yeast. We define the ranges (minimum and maximum values) for our parameters by defining a sequence. Because we want to look at all possible combinations of these parameters we use the command expand.grid. The input logLikely = FALSE to compute_likelihood reports back likelihood values. Some care is needed in defining the number of points (length.out = 100) in the sequence that we want to evaluate - we will have \\(100^{2}\\) different combinations of \\(K\\) and \\(b\\) on our grid, which does take time to evaluate. The output to compute_likelihood is a list, which is a flexible data structure in R. You can think of this as a collection of items - which could be data frames of different sizes. In this case, what gets returned are two data frames: likelihood, which is a data frame of likelihood values for each of the parameters and opt_value, which reports back the values of the parameters that optimize the likelihood function. Note that the optimum value is an approximation, as it is just the optimum from the input values of \\(K\\) and \\(b\\) provided on our grid. Let’s take a look at the reported optimum values, which we can do with the syntax LIST_NAME$VARIABLE_NAME, where the dollar sign ($) helps identify which variable from the list you are investigating. gause_likelihood$opt_value ## # A tibble: 1 × 4 ## K b l_hood log_lik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 12.7 0.242 0.000348 FALSE It is also important to visualize this likelihood function. For this dataset we have the two parameters \\(K\\) and \\(b\\), so the likelihood function will be a likelihood surface, rather than a two-dimensional plot. To visualize this in R we can use a contour diagram. Figure 9.6 displays this countour plot. # Define the likelihood values my_likelihood &lt;- gause_likelihood$likelihood # Make a contour plot ggplot(data = my_likelihood) + geom_tile(aes(x = K, y = b, fill = l_hood)) + stat_contour(aes(x = K, y = b, z = l_hood)) FIGURE 9.6: Likelihood surface and contour lines for the yeast dataset. Similar to before, let’s take this step by step: The command my_likelihood just puts the likelihood values in a data frame. The ggplot command is similar to that used before. We use geom_tile to visualize the likelihood surface. There are three required inputs from the my_likelihood data frame: the x and y axis data values and the fill value, which represents the height of the likelihood function. The command stat_contour draws the contour lines, or places where the likelihood function has the same value. Notice how we used z = l_hood rather than fill here. This function helps “smooth” out any jaggedness in the contours. In Figure 9.6 there appears to be a large region where the likelihood has the same value. (Admittedly I chose some broad parameter ranges for \\(K\\) and \\(b\\)). We can refine that by producing a second contour plot that focuses in on parameters closer to the calculated optimum value at \\(K=13\\) and \\(b=0.07\\) (Figure 9.7): ## # A tibble: 1 × 4 ## K b l_hood log_lik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 12.8 0.241 0.000349 FALSE FIGURE 9.7: Zoomed in likelihood surface. for the yeast dataset. The computed location of the optimum value is shown as a red point. The reported values for \\(K\\) (12.8) and \\(b\\) (0.241) may be close to what was reported from Figure 9.6. Notice that in Figure 9.7 I also added in the location of the optimum point with geom_point(). As a final step, once you have settled on the value that optimizes the likelihood function, is to compare the optimized parameters against the data (Figure 9.8): # Define the parameters and the times to evaluate: my_params &lt;- gause_likelihood_rev$opt_value time &lt;- seq(0, 60, length.out = 100) # Get the right hand side of your equations new_eq &lt;- gause_model %&gt;% formula.tools::rhs() # This collects the parameters and data into a list in_list &lt;- c(my_params, time) %&gt;% as.list() # The eval command evaluates your model out_model &lt;- eval(new_eq, envir = in_list) # Now collect everything into a data frame: my_prediction &lt;- tibble(time = time, volume = out_model) ggplot() + geom_point( data = yeast, aes(x = time, y = volume), color = &quot;red&quot;, size = 2 ) + geom_line( data = my_prediction, aes(x = time, y = volume) ) + labs(x = &quot;Time&quot;, y = &quot;Volume&quot;) FIGURE 9.8: Model and data comparison of the yeast dataset from maximum likelihood estimation. All right, this code block has some new commands and techniques that need explaining. Once we have the parameter estimates we need to compute the modeled values. First we define the params and the time we wish to evaluate with our model. We need to evaluate the right hand side of \\(\\displaystyle y = \\frac{K}{1+e^{a+bt}}\\), so the definition of new_eq helps to do that, using the package formula.tools. The %&gt;% is the tidyverse pipe. This is a very useful command to help make code more readable! in_list &lt;- c(params,my_time) %&gt;% as.list() collects the parameters and input times in one list to evaluate the model with out_model &lt;- eval(new_eq,envir=in_list) We make a data frame called my_prediction so we can then plot. And the rest of the plotting commands you should be used to. This activity focused on the likelihood function - Exercise 9.3 has you repeat this analysis with the log-likelihood function. 9.5 Looking back and forward This chapter covered a lot of ground - from probability and likelihood functions to computing and visualizing these. A good strategy for a likelihood function is to visualize the function and then explore to find values that optimize the likelihood or log-likelihood function. This approach is one example of successive approximations or using an iterative method to determine a solution. While this chapter focused on optimizing a likelihood function with one or two parameters, the successive approximation method does generalize to more parameters. However searching for parameters becomes tricky (read: tedious and slow) in high-dimensional spaces. In later chapters we will explore numerical methods to accelerate convergence to an optimum value. 9.6 Exercises Exercise 9.1 Algebraically solve the equation \\(\\displaystyle 0.45 = \\frac{K}{1+e^{a}}\\) for \\(a\\). Exercise 9.2 Compute the values of \\(L(b|\\vec{x},\\vec{y})\\) and \\(\\ln(L(b|\\vec{x},\\vec{y}))\\) for each of the data points in Equation (9.6) when \\(b=1.865\\) and \\(\\sigma=1\\). (This means that these 4 values would be multiplied or added when you compute the full likelihood function.) Explain if the likelihood or log-likelihood would be easier to calculate in instances when the number of observations is large. Exercise 9.3 Visualize the likelihood function for the yeast dataset, but in this case report out and visualize the log-likelihood. (This means that you are setting the option logLikely = TRUE in the compute_likelihood function.) Compare the log-likelihood surface to Figure 9.7. Exercise 9.4 When we generated our plot of the likelihood function in Figure 9.3 we assumed that \\(\\sigma=1\\) in Equation (9.6). For this exercise you will explore what happens in Equation (9.6) as \\(\\sigma\\) increases or decreases. Use desmos () or R to generate a plot of Equation (9.6). What happens to the shape of the likelihood function as \\(\\sigma\\) increases? How does the estimate of \\(b\\) change as \\(\\sigma\\) changes? The spread of the distribution (in terms of it being more peaked or less peaked) is a measure of uncertainty of a parameter estimate. How does the resulting parameter uncertainty change as \\(\\sigma\\) changes? Exercise 9.5 Using Equation (9.6) with \\(\\sigma = 1\\): Apply the natural logarithm to both sides of this expression. Using properties of logarithms, show that the log-likelihood function is \\(\\displaystyle \\ln(L(b)) =-2 \\ln(2) - 2 \\ln (\\pi) -\\frac{(3-b)^{2}}{2}-\\frac{(5-2b)^{2}}{2}-\\frac{(4-4b)^{2}}{2}-\\frac{(10-4b)^{2}}{2}\\). Make a plot of the log-likelihood function (in desmos or R). At what values of \\(b\\) Where is this function optimized? Does your graph indicate that it is a maximum or a minimum value? Compare this likelihood estimate for \\(b\\) to what was found in Figure 9.3. Exercise 9.6 Consider the linear model \\(y=a+bx\\) for the following dataset: x y 1 3 2 5 4 4 4 10 With the function compute_likelihood, generate a contour plot of both the likelihood and log-likelihood functions. You may assume \\(0 \\leq a \\leq 5\\) and \\(0 \\leq b \\leq 5\\). Make a scatterplot of these data with the equation \\(y=a+bx\\) with your maximum likelihood parameter estimates. Earlier when we fit \\(y=bx\\) we found \\(b=1.865\\). How does adding \\(a\\) as a model parameter affect your estimate of \\(b\\)? Exercise 9.7 For the function \\(\\displaystyle P(t)=\\frac{K}{1+e^{a+bt}}\\), with \\(P(0)=P_{0}\\), determine an expression for the parameter \\(a\\) in terms of \\(K\\), \\(b\\), and \\(P_{0}\\). Exercise 9.8 The values returned by the maximum likelihood estimate for Equation (9.8) were a little different from those reported in Gause (1932): Parameter Maximum Likelihood Estimate Gause (1932) \\(K\\) 12.7 13.0 \\(b\\) 0.24242 0.21827 Using the yeast dataset, plot the function \\(\\displaystyle y = \\frac{K}{1+e^{a-bt}}\\) (setting \\(\\displaystyle a = \\ln \\left( \\frac{K}{0.45} - 1 \\right)\\)) using both sets of parameters. Which approach (the Maximum Likelihood estimate or Gause (1932)) does a better job representing the data? Exercise 9.9 An equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\(\\displaystyle y = c x^{1/\\theta}\\), where \\(\\theta \\geq 1\\) and \\(c&gt;0\\) are both constants. Use the dataset phosphorous to make a scatterplot with the variable algae on the horizontal axis, daphnia on the vertical axis. Generate a contour plot for the likelihood function for these data. You may assume \\(0 \\leq c \\leq 5\\) and \\(1 \\leq \\theta \\leq 20\\). What are the values of \\(c\\) and \\(\\theta\\) that optimize the likelihood? Hint: for the dataset phosphorous be sure to use the variables \\(x=\\)algae and \\(y=\\)daphnia. Add the fitted curve to your scatterplot and evaluate your fitted results. Exercise 9.10 A dog’s weight \\(W\\) (pounds) changes over \\(D\\) days according to the following function: \\[\\begin{equation} W =f(D,p_{1},p_{2})= \\frac{p_{1}}{1+e^{2.462-p_{2}D}}, \\end{equation}\\] where \\(p_{1}\\) and \\(p_{2}\\) are parameters. This function can be used to describe the data wilson. Make a scatterplot with the wilson data. What is the long term weight of the dog? Generate a contour plot for the likelihood function for these data. What are the values of \\(p_{1}\\) and \\(p_{2}\\) that optimize the likelihood? You may assume that \\(p_{1}\\) and \\(p_{2}\\) are both positive. With your values of \\(p_{1}\\) and \\(p_{2}\\) add the function \\(W\\) to your scatterplot and compare the fitted curve to the data. References "],["cost-fns-10.html", "Chapter 10 Cost Functions qnd Bayes’ Rule 10.1 Cost functions and model-data residuals 10.2 Further extensions to the cost function 10.3 Conditional probabilities and Bayes’ rule 10.4 Bayes’ rule in action 10.5 Next steps 10.6 Exercises", " Chapter 10 Cost Functions qnd Bayes’ Rule Chapter 9 introduced likelihood functions as an approach to tackle parameter estimation. However this is not the only approach to understand model-data fusion. This chapter introduces cost functions, which estimates parameters from data using a least squares approach. Want to know a secret? Cost functions are very closely related to log-likelihood functions. This chapter will explore this idea some more, first by exploring model-data residuals, defining a cost function, and then connecting them back to likelihood functions. To complete the circle, this chapter ends by discussing Bayes’ Rule, which will further strengthen the connection between cost and likelihood functions. Let’s get started! 10.1 Cost functions and model-data residuals Let’s revisit the linear regression problem from Chapter 9. Recall Table 9.1 from Chapter 9. With these data we wanted to fit a function of the form \\(y=bx\\) (forcing the intercept term to be zero). We will extend Table 9.1 to include the model-data residual computed as \\(y-bx\\) in Table 10.1: TABLE 10.1: A small, limited dataset (Table 9.1) with the computed model-data residual with parameter \\(b\\), along with model-data residuals for different values of \\(b\\). \\(x\\) \\(y\\) \\(bx\\) \\(y-bx\\) \\(b=1\\) \\(b=3\\) \\(b=-1\\) 1 3 \\(b\\) \\(3-b\\) 2 0 4 2 5 \\(2b\\) \\(5-2b\\) 3 -1 7 4 4 \\(4b\\) \\(4-4b\\) 0 -8 8 4 10 \\(4b\\) \\(10-4b\\) 6 -2 14 Also included in Table 10.1 are the model-data residual values for different values of \\(b\\). Notably values of the residuals can be negative and some can be positive - which makes it tricky to assess the “best” value of \\(b\\) from the residuals alone. (If we found a value of \\(b\\) where the residuals were all zero, then we would have the “best” value of \\(b\\)!30). To assess the overall residuals as a function of the value of \\(b\\), we need to take into consideration not just the value of the residual (positive or negative), but rather some way to measure the overall distance of all the residuals from a given value of \\(b\\). One way to define that is with a function that squares each residual (so that negative and positive values don’t cancel each other) and adds each of those results together. We call this the sum squared residuals. So for example, the sum squared residual when \\(b=1\\) is shown in Equation (10.1): \\[\\begin{equation} \\mbox{ Sum square residual: } 2^{2}+3^{2}+0^{2}+6^{2} = 49 \\tag{10.1} \\end{equation}\\] The other square residuals are \\(68\\) when \\(b=3\\) and \\(325\\) when \\(b=-1\\). So of these choices for \\(b\\), the one that minimizes the square residual is \\(b=1\\). Let’s generalize this to determine a function to compute the sum square residual for any value of \\(b\\). This function, denoted as \\(S(b)\\), is called the cost function (Equation (10.2)): \\[\\begin{equation} S(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 \\tag{10.2} \\end{equation}\\] Equation (10.2) is a function of one variable (\\(b\\)). Figure 10.1 shows a graph of \\(S(b)\\). Notice how the plot of \\(S(b)\\) is a nice quadratic function, with a minimum at \\(b=1.865\\). Did you notice that this value for \\(b\\) is the same value for the minimum that we found from Equation (9.6) in Chapter 9? In Exercise 10.1 you will use calculus to determine the optimum value of \\(S(b)\\). FIGURE 10.1: Plot of Equation (10.2). The vertical line denotes the minimum value at \\(b=1.865\\). 10.1.1 Accounting for uncertainty The cost function can also incorporate uncertainty in the value of the response variable \\(y\\). We will define this uncertainty as \\(\\sigma\\) and have it be the same for each value \\(y_{i}\\). In some cases the uncertainty may vary from measurement to measurement - but the concepts presented here can generalize. To account for this uncertainty we divide each of the square residuals in Equation (10.2) by \\(\\sigma^{2}\\), as shown in Equation (10.3) using \\(\\sum\\) notation. \\[\\begin{equation} S(\\vec{\\alpha}) = \\sum_{i=1}^{N} \\frac{(y_{i}-f(x,\\vec{\\alpha}))^{2}}{\\sigma^{2}} \\tag{10.3} \\end{equation}\\] As an example, comparing Equation (10.2) to Equation (10.3) we have \\(N=4\\), \\(f(x_{i},\\vec{\\alpha} ) =bx\\), and \\(\\sigma = 1\\). 10.1.2 Comparing cost and log-likelihood functions Chapter 9 defined the log-likelihood function (Equation (9.7)), which for the small dataset we are studying is represented with Equation (10.4) where \\(\\sigma = 1\\) and \\(N=4\\): \\[\\begin{equation} \\begin{split} \\ln(L(b | \\vec{x},\\vec{y} )) &amp;= -2 \\ln(2) - 2 \\ln (\\pi) - 2 \\ln(1) -\\frac{(3-b)^{2}}{2}-\\frac{(5-2b)^{2}}{2} \\\\ &amp;-\\frac{(4-4b)^{2}}{2}-\\frac{(10-4b)^{2}}{2} \\\\ &amp;= -2 \\ln(2) - 2 \\ln (\\pi) -\\frac{(3-b)^{2}}{2}-\\frac{(5-2b)^{2}}{2} \\\\ &amp; -\\frac{(4-4b)^{2}}{2}-\\frac{(10-4b)^{2}}{2} \\end{split} \\tag{10.4} \\end{equation}\\] (Note that in Equation (10.4) the expression \\(-2 \\ln(1)\\) is 0.) If we compare Equation (10.4) with Equation (10.2), then we have \\(\\ln(L(b | \\vec{x},\\vec{y} )) = -2 \\ln(2) - 2 \\ln (\\pi) - \\frac{1}{2} \\cdot S(b)\\). This is no coincidence: log-likelihood functions are similar to cost functions! While Equation (10.4) contains some extra factors, they only shift vertically or expand the graph of \\(\\ln(L(b | \\vec{x},\\vec{y} ))\\) compared to \\(S(b)\\). This “coincidence” is only true when \\(\\sigma\\) is the same for all \\(y_{i}\\). Can you explain why? For both log-likelihood or cost functions the goal is optimization. Vertically shifting or expanding a function does not change the location of an optimum value (Why? Think back to derivatives from Calculus I). In fact, a quadratic cost function yields the same results as the log-likelihood function assuming the residuals are normally distributed. 10.2 Further extensions to the cost function The cost function \\(S(b)\\) can be extended additionally to incorporate other types of data. For example, if we knew there was a given range of values that would make sense (say \\(b\\) is near 1.3 with a standard deviation of 0.1), we should be able to incorporate this information into the cost function. We do this by adding an additional term to Equation (10.2) (\\(\\tilde{S}(b)\\), Equation (10.5), also is graphed in Figure 10.2). \\[\\begin{equation} \\tilde{S}(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 + \\frac{(b-1.3)^2}{0.1^2} \\tag{10.5} \\end{equation}\\] FIGURE 10.2: Comparing two cost functions \\(S(b)\\) (black) and \\(\\tilde{S}(b)\\) (black dashed line) Aha! Figure 10.2 shows how the revised cost function \\(\\tilde{S}(b)\\) changes the optimum value. Numerically this works out to be \\(\\tilde{b}=\\) 1.45. In Exercise 10.1 you will verify this new minimum value and compare the results to the fitted value of \\(b=1.86\\). Adding this prior information seems like an effective approach. Many times a study wants to build upon the existing body of literature and to take that into account. This approach of including prior information into the cost function can also be considered a Bayesian approach. 10.3 Conditional probabilities and Bayes’ rule We first need to understand Bayes’ rule and conditional probability, with an example. Example 10.1 The following table shows results from a survey of people’s views on the economy (optimistic or pessimistic) and whether or not they voted for the incumbent President in the last election. Percentages are reported as decimals. Probability tables are a clever way to organize this information. Probability Optimistic Pessimistic Total Voted for the incumbent President 0.20 0.20 0.40 Did not vote for incumbent President 0.15 0.45 0.60 Total 0.35 0.65 1.00 Compute the probability of having an optimistic view on the economy. Solution. Based on the probability table, we define the following probabilities: The probability you voted for the incumbent President and have an optimistic view on the economy is 0.20 The probability you did not vote for the incumbent President and have an optimistic view on the economy is 0.15 The probability you voted for the incumbent President and have an pessimistic view on the economy is 0.20 The probability you did not vote for the incumbent President and have an pessimistic view on the economy is 0.45 We calculate the probability of having an optimistic view on the economy by adding the probabilities with an optimistic view, whether or not they voted for the incumbent President. For this example, this probability sums to 0.20 + 0.15 = 0.35, or 35%. On the other hand, the probability you have a pessimistic view on the economy is 0.20 + 0.45 = 0.65, or 65%. Notice how the two of these together (probability of optimistic and pessimistic views of the economy is 1, or 100% of the outcomes.) 10.3.1 Conditional probabilities Next, let’s discuss conditional probabilities. A conditional probability is the probability of an outcome given some previous outcome, or \\(\\mbox{Pr} (A | B)\\), where Pr means “probability of an outcome” and \\(A\\) and \\(B\\) are two different outcomes or events. In probability theory you might study the following law of conditional probability: \\[\\begin{equation} \\begin{split} \\mbox{Pr}(A \\mbox { and } B) &amp;= \\mbox{Pr} (A \\mbox{ given } B) \\cdot \\mbox{Pr}(B) \\\\ &amp;= \\mbox{Pr} (A | B) \\cdot \\mbox{Pr}(B) \\\\ &amp;= \\mbox{Pr} (B | A) \\cdot \\mbox{Pr}(A) \\end{split} \\tag{10.6} \\end{equation}\\] Typically when expressing conditional probabilities we remove “and” and write \\(P(A \\mbox{ and } B)\\) as \\(P(AB)\\) and “given” as \\(P(A \\mbox{ given } B)\\) as \\(P(A|B)\\). Example 10.2 Continuing with Example 10.1, sometimes people believe that your views of the economy influence whether you are going to vote for the incumbent President in an election. Use the information from the table in Example 10.1 to compute the probability you voted for the incumbent President given you have an optimistic view of the economy. Solution. To compute the probability you voted for the incumbent President given you have an optimistic view of the economy is a rearrangement of Equation (10.6): \\[\\begin{equation} \\begin{split} \\mbox{Pr(Voted for incumbent President | Optimistic View on Economy)} = \\\\ \\frac{\\mbox{Pr(Voted for incumbent President and Optimistic View on Economy)}}{\\mbox{Pr(Optimistic View on Economy)}} = \\\\ \\frac{0.20}{0.35} = 0.57 \\end{split} \\tag{10.7} \\end{equation}\\] So if you have an optimistic view on the economy, there is a 57% chance you will vote for the incumbent President. Contrast this result to the probability that you voted for the incumbent President (Example 10.1), which is only 40%. Perhaps your view of the economy does indeed influence whether or not you would vote to re-elect the incumbent President. 10.3.2 Bayes’ rule Using the incumbent President and economy example as a framework, we will introduce Bayes’ Rule, which is a re-arrangment of the rule for conditional probability: \\[\\begin{equation} \\mbox{Pr} (A | B) = \\frac{ \\mbox{Pr} (B | A) \\cdot \\mbox{Pr}(A)}{\\mbox{Pr}(B) } \\tag{10.8} \\end{equation}\\] It turns out Bayes’ Rule is a really helpful way to understand how we can systematically incorporate this prior information into the likelihood function (and by association the cost function). For parameter estimation our goal is to estimate parameters, given the data. Another way to state Bayes’ Rule in Equation (10.8) is using terms of parameters and data: \\[\\begin{equation} \\mbox{Pr}( \\mbox{ parameters } | \\mbox{ data }) = \\frac{\\mbox{Pr}( \\mbox{ data } | \\mbox{ parameters }) \\cdot \\mbox{ Pr}( \\mbox{ parameters }) }{\\mbox{Pr}(\\mbox{ data }) } \\tag{10.9} \\end{equation}\\] While Equation (10.9) seems pretty conceptual, here are some key highlights: In practice, the term \\(\\mbox{Pr}( \\mbox{ data } | \\mbox{ parameters })\\) in Equation (10.9) is the likelihood function (Equation (9.5)). The term \\(\\mbox{Pr}( \\mbox{ parameters })\\) is the probability distribution of the prior information on the parameters, specifying the probability distribution functions for the given context. When this distribution is the same as \\(\\mbox{Pr}( \\mbox{ data } | \\mbox{ parameters })\\) (typically normally distributed), prior information has a multiplicative effect on the likelihood function (\\(\\mbox{Pr}( \\mbox{ parameters } | \\mbox{ data })\\)). (Or an additive effect on the log-likelihood function.) This is good news! When we added that additional term for prior information into \\(\\tilde{S}(b)\\) in Equation (10.5), we accounted for the prior information correctly. In Exercise 10.6 you will explore how the log-likelihood is related to the cost function. The expression \\(\\mbox{Pr}( \\mbox{ parameters } | \\mbox{ data })\\) is the start of a framework for a probability density function, which should integrate to unity. (You will explore this more if you study probability theory.) This denominator term is called a normalizing constant. Since our overall goal is to select parameters that optimize \\(\\mbox{Pr}( \\mbox{ parameters } | \\mbox{ data })\\), the expression in the denominator (\\(\\mbox{Pr}(\\mbox{ data })\\) ) does not change the location of the optimum values. 10.4 Bayes’ rule in action Wow - we made some significant progress in our conceptual understanding of how to incorporate models and data! Let’s see how this applies to our linear regression problem (\\(y=bx\\)). We have the following assumptions: Assumption 1: The data are independent, identically distributed. We can then write the likelihood function as the following: \\[\\begin{equation} \\mbox{Pr}(\\vec{y} | b) = \\left( \\frac{1}{\\sqrt{2 \\pi} \\sigma}\\right)^{4} e^{-\\frac{(3-b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(5-2b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(4-4b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(10-4b)^{2}}{2\\sigma^{2}}} \\end{equation}\\] Assumption 2: Prior knowledge expects us to say that \\(b\\) is normally distributed with mean 1.3 and standard deviation 0.1. Incorporating this information allows us to write the following: \\[\\begin{equation} \\mbox{Pr}(b) =\\frac{1}{\\sqrt{2 \\pi} \\cdot 0.1} e^{-\\frac{(b-1.3)^{2}}{2 \\cdot 0.1^{2}}} \\end{equation}\\] When we combine the two pieces of information, the probability of \\(b\\), given the data \\(\\vec{y}\\), is the following: \\[\\begin{equation} \\mbox{Pr}(b | \\vec{y}) \\approx e^{-\\frac{(3-b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(5-2b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(4-4b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(10-4b)^{2}}{2\\sigma^{2}}} \\cdot e^{-\\frac{(b-1.3)^{2}}{2 \\cdot 0.1^{2}}} \\tag{10.10} \\end{equation}\\] Notice we are ignoring the terms \\(\\displaystyle \\left( \\frac{1}{\\sqrt{2 \\pi} \\cdot \\sigma }\\right)^{4}\\) and \\(\\displaystyle \\frac{1}{\\sqrt{2 \\pi} \\cdot 0.1}\\), because per our discussion above not including them does not change the location of the optimum value, only the value of the likelihood function. The plot of \\(\\mbox{Pr}(b | \\vec{y})\\), assuming \\(\\sigma = 1\\) is shown in Figure 10.3: FIGURE 10.3: Equation (10.10) with optimum value at \\(b=1.45\\) denoted in a blue dashed line. It looks like the value that optimizes our posterior probability is \\(b=\\) 1.45. This is similar the value of \\(\\tilde{b}\\) from Equation (10.5). Again, this is no coincidence. Adding in prior information to the cost function or using Bayes’ Rule are equivalent approaches. 10.5 Next steps Now that we have seen the usefulness of cost functions and Bayes’ Rule we can begin to apply this to larger problems involving more equations and data. In order to do that we need to explore some computational methods to scale this problem up - which we will do in subsequent chapters. 10.6 Exercises Exercise 10.1 The following problem works with Table 9.1 to determine the value of \\(b\\) with the function \\(y=bx\\) as in this chapter. Using calculus, show that the cost function \\(S(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2\\) has a minimum value at \\(b=1.86\\). What is the value of \\(S(1.865)\\)? Use a similar approach to determine the minimum of the revised cost function \\(\\tilde{S}(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 + (b-1.3)^2\\). Call this value \\(\\tilde{b}\\). How do the values of \\(S(1.865)\\) and \\(\\tilde{S}(\\tilde{b})\\) compare? Make a plot of the cost functions \\(S(b)\\) and \\(\\tilde{S}(b)\\) to verify the optimum values. Make a scatter plot with the data and the function \\(y=bx\\) and \\(y=\\tilde{b}x\\). How do the two estimates compare with the data? Exercise 10.2 Use calculus to determine the optimum value of \\(b\\) for Equation (10.4). Do you obtain the same value of \\(b\\) for Equation (10.2)? Exercise 10.3 (Inspired from Hugo van den Berg (2011)) Consider the nutrient equation \\(\\displaystyle y = c x^{1/\\theta}\\) using the dataset phosphorous. Write down a formula for the objective function \\(S(c,\\theta)\\) that characterizes this equation (that includes the dataset phosphorous). Fix \\(c=1.737\\). Make a ggplot of \\(S(1.737,\\theta)\\) for \\(1 \\leq \\theta \\leq 10\\). How many critical points does this function have over this interval? Which value of \\(\\theta\\) is the global minimum? Exercise 10.4 Use the cost function \\(S(1.737,\\theta)\\) from Exercise 10.3 to answer the following questions: Researchers believe that \\(\\theta \\approx 7\\). Re-write \\(S(1.737,\\theta)\\) to account for this additional (prior) information. How does the inclusion of this additional information change the shape of the cost function and the location of the global minimum? Finally, reconsider the fact that \\(\\theta \\approx 7 \\pm .5\\) (as prior information). How does that modify \\(S(1.737,\\theta)\\) further and the location of the global minimum? Exercise 10.5 One way to generalize the notion of prior information using cost functions is to include a term that represents the degree of uncertainty in the prior information, such as \\(\\sigma\\). For the problem \\(y=bx\\) this leads to the following cost function: \\(\\displaystyle \\tilde{S}_{revised}(b)=(3-b)^2+(5-2b)^2+(4-4b)^2+(10-4b)^2 + \\frac{(b-1.3)^2}{\\sigma^{2}}\\). Use calculus to determine the optimum value for \\(\\tilde{S}_{revised}(b)\\), expressed in terms of \\(\\tilde{b}_{revised} = f(\\sigma)\\) (your optimum value will be a function of \\(\\sigma\\)). What happens to \\(\\tilde{b}_{revised}\\) as \\(\\sigma \\rightarrow \\infty\\)? Exercise 10.6 For this problem you will minimize some generic functions. Using calculus, verify that the optimum value of \\(y=ax^{2}+bx+c\\) occurs at \\(\\displaystyle x=-\\frac{b}{2a}\\). (You can assume \\(a&gt;0\\).) Using calculus, verify that a critical point of \\(z=e^{-(ax^{2}+bx+c)^{2}}\\) also occurs at \\(\\displaystyle x=-\\frac{b}{2a}\\). Note: this is a good exercise to practice your differentiation skills! Algebraically show that \\(\\ln(z) = -y\\). Explain why \\(y\\) is similar to a cost function \\(S(b)\\) and \\(z\\) is similar to a likelihood function. Exercise 10.7 This problem continues the re-election of the incumbent President and viewpoint on the economy in Example 10.1. Determine the following conditional probabilities: Determine the probability that you voted for the incumbent President given that you have a pessimistic view on the economy. Determine the probability that you did not vote for the incumbent President given that you have an pessimistic view on the economy. Determine the probability that you did not vote for the incumbent President given that you have an optimistic view on the economy. Determine the probability that you have an pessimistic view on the economy given that you voted for the incumbent President. Determine the probability that you have an optimistic view on the economy given that you did not vote for the incumbent President. Exercise 10.8 Incumbents have an advantage in re-election due to wider name recognition, which may boost their re-election chances, as shown in the following table: Probability Being elected Not being elected Total Having name recognition 0.55 0.25 0.80 Not having name recognition 0.05 0.15 0.20 Total 0.60 0.40 1.00 Use Bayes’ Rule to determine the probability of being elected, given that you have name recognition. Exercise 10.9 Demonstrate how Bayes’ Rule differs from the law of conditional probability. References "],["bootstrap-11.html", "Chapter 11 Sampling Distributions and the Bootstrap Method 11.1 Histograms and their visualization 11.2 Statistical theory: sampling distributions 11.3 Summary and next steps 11.4 Exercises", " Chapter 11 Sampling Distributions and the Bootstrap Method In Chapters and 9 and 10 we saw how the parameter estimation problem is related to optimizing the likelihood or cost function. In most cases - and especially when the model is linear - optimization is straightforward. However for nonlinear models the cost function is trickier. Consider the cost function shown in Figure 11.1, which tries to optimize \\(\\theta\\) for the nutrient equation \\(\\displaystyle y = 1.737 x^{1/\\theta}\\) using the dataset phosphorous: FIGURE 11.1: Nonlinear cost function plot for phosphorous data set with the model \\(\\displaystyle y = 1.737 x^{1/\\theta}\\). While Figure 11.1 shows a clearly defined minimum around \\(\\theta \\approx 6\\), the shape of the cost function is not quadratic like Figure 10.2 in Chapter 10. For nonlinear models direct optimization of the cost function using techniques learned in calculus may not be computationally feasible. An alternative approach to optimization relies on the idea of sampling, which randomly selects parameter values and then computes the cost function. After a certain amount of time or iterations, all the values of the cost function are compared to each other to determine the optimum value. We will refine the concept of sampling to determine the optimum value in subsequent chapters (Chapters 12 and 13), but this chapter develops some foundations in sampling - we will study how to plot histograms in R and then apply these to the bootstrap method, which relies on random sampling. Let’s get started! 11.1 Histograms and their visualization To introduce the idea of a histogram, consider Table 11.1, which is a sample of the dataset snowfall in the demodelr package. The snowfall dataset is measurements of precipitations collected at weather stations in the Twin Cities (Minneapolis, Saint Paul, and surrounding suburbs) from a spring snowstorm. These data come from a cooperative network of local observers. Yes, it can snow in Minnesota in April. Sigh. TABLE 11.1: Weather station data from a Minnesota snowstorm. date time station_id station_name snowfall 4/16/18 5:00 AM MN-HN-78 Richfield 1.9 WNW 22.0 4/16/18 7:00 AM MN-HN-9 Minneapolis 3.0 NNW 19.0 4/16/18 7:00 AM MN-HN-14 Minnetrista 1.5 SSE 12.5 4/16/18 7:00 AM MN-HN-30 Plymouth 2.4 ENE 18.5 4/16/18 7:00 AM MN-HN-58 Champlin 1.5 ESE (118) 20.0 The header row in Table 11.1 also lists the names of the associated variables in this data frame. We are going to focus on the variable snowfall. A histogram is an easy way to view the distribution of measurements, using geom_histogram (Figure 11.2). You may recall that a histogram represents the frequency count of a random variable, typically binned over certain intervals. ggplot(data = snowfall) + geom_histogram(aes(x = snowfall)) + labs( x = &quot;Snowfall amount&quot;, y = &quot;Number of observations&quot; ) FIGURE 11.2: Initial histogram of snowfall data in Table 11.1. This code introduces geom_histogram. To create the historgram we use the aesthetic (aes(x = snowfall)) to display the histogram for the snowfall column in the dataset snowfall. At the R console you may have received a warning about such as stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The function geom_histogram doesn’t guess a bin width, but one rule of thumb is to select the number of bins to be equal to the square root of the number of observations (16 in this instance). So let’s adjust the number of bins to 4 in Figure 11.3. ggplot() + geom_histogram(data = snowfall, aes(x = snowfall), bins = 4) + labs( x = &quot;Snowfall amount&quot;, y = &quot;Number of observations&quot; ) FIGURE 11.3: Adjusted histogram of snowfall data in Table 11.1 with the number of bins set to 4. 11.2 Statistical theory: sampling distributions 11.2.1 Sampling the empirical distribution The histogram of snowfall measurements (Figure 11.3) from the snowfall data (Table 11.1) represents an empirical probability distribution of measurements. While it is great to have all these observations, a key question is: What is an estimate for the representative amount of snowfall for this storm? The snowfall measurements illustrate the difference between what statisticians call a population (the true distribution of measurements) and a sample (what people observe). To go a little deeper, let’s define a random variable \\(S\\) for this population, which has an (unknown) probability density function associated with it. The measurements shown Table 11.1 are samples of this distribution. While the average of the precipitation data (16.1 inches) might be a defensible value for the average amount of snow that fell, what we don’t know is how well this empirical mean approximates the expected value of the distribution for \\(S\\). Each of the entries in Table 11.1 represents a measurement made by a particular observer. To get the true distribution we would need to add more observers, but that isn’t realistic for this case as the snowfall event has already passed - we can’t go “back in time.” One way is to generate a bootstrap sample, which is a sample of the original dataset with replacement. The workflow that we will apply is the following: Do once \\(\\rightarrow\\) Do several times \\(\\rightarrow\\) Visualize \\(\\rightarrow\\) Summarize “Do once” is the first step, using sampling with replacement. This process is easily done with the R command slice_sample (you should try this out yourself), which in the following code assigns a sample to the variable p_new: p_new &lt;- slice_sample(snowfall, prop = 1, replace = TRUE) Let’s break this code down: We are sampling the snowfall data frame with replacement (replace = TRUE). If you are uncertain about how sampling with replacement works, here is one visual: say you have each of the snowfall measurements written on a piece of paper in a hat. You draw one slip of paper, record the measurement, and then place that slip of paper back in the hat to draw again, until you have as many measurements as the original data frame. The command prop=1 means that we are sampling 100% of the snowfall data frame. Once we have taken a sample, we can then compute the mean (average) and the standard deviation of the sample: slice_sample(snowfall, prop = 1, replace = TRUE) %&gt;% summarize( mean = mean(snowfall, na.rm = TRUE) ) ## mean ## 1 17.84375 How the above code works is to first do the sampling, and then the summary. The command summarize collapses the snowfall data frame and computes the mean and the standard deviation sd of the column snowfall. We have the command na.rm=TRUE to remove any NA values that may affect the computation. “Do several times” is the second step. Here we are going to rely on some powerful functionality from the purrr package. This package has the wonderful command map_df, which allows you to efficiently repeat a process several times and return a data frame as output. Evaluate the following code on your own: purrr::map_df( 1:10, ~ ( slice_sample(snowfall, prop = 1, replace = TRUE) %&gt;% summarize( mean = mean(snowfall, na.rm = TRUE) ) ) # Close off the tilde ~ () ) # Close off the map_df Let’s review this code step by step: The basic structure is map_df(1:N,~(COMMANDS)), where N is the number of times you want to run your code (in this case N=10). The second part ~(COMMANDS) lists the different commands we want to re-run (here the resampling of our dataset and then subsequent summarizing). What should be returned is a data frame that lists the mean of each bootstrap sample. The process of randomly sampling a dataset is called bootstrapping. I can appreciate that this programming might be a little tricky to understand and follow - don’t worry - the goal is to give you a tool that you can easily adapt to a situation. “Visualize” is the step where we will use a histogram to examine the distribution of the bootstrap samples. The following code does this all, changing the number of bootstrap samples to 1000: bootstrap_samples &lt;- purrr::map_df( 1:1000, ~ ( slice_sample(snowfall, prop = 1, replace = TRUE) %&gt;% summarize( mean_snow = mean(snowfall, na.rm = TRUE) ) ) # Close off the tilde ~ () ) # Close off the map_df # Now make the histogram plots for the mean and standard deviation: ggplot(bootstrap_samples) + geom_histogram(aes(x = mean_snow)) + ggtitle(&quot;Distribution of sample mean&quot;) FIGURE 11.4: Histogram of 1000 bootstrap samples for the mean of the snowfall dataset. Excellent! This is shaping up nicely. Once we have sampled as much we want, then investigate the distribution of the computed sample statistics (we call this the sampling distribution). It turns out that the statistics of the sampling distribution (such as the mean or the standard deviation) will approximate the population distribution statistics when the number of bootstrap samples is large (and 1000 is sufficiently large in this instance). “Summarize” is the final step, where we compute summary statistics of the distribution of bootstrap means. We will do this by computing a confidence interval, which comes from the percentiles of the distribution. Here is how we would compute these different statistics using the quantile command shown below: # Compute the average of the distribution: mean(bootstrap_samples$mean_snow) ## [1] 16.07825 # Compute the 2.5% and 97.5% of the distribution: quantile(bootstrap_samples$mean_snow, probs = c(0.025, .975)) ## 2.5% 97.5% ## 14.26250 17.81938 Notice how we used the probs=c(0.025,.975) command to compute the different 2.5% and 97.5% quantiles of the distribution of the sample means. Let’s discuss the distribution of bootstrap means. The 2.5 percentile is approximately 14.3 inches. This means 2.5% of the distribution is at 14.3 inches or less. The 97.5 percentile is approximately 17.8 inches, so 97.5% of the distribution is 17.8 inches or less. If we take the difference between 2.5% and 97.5% that is 95%, so 95% of the distribution is contained between 14.3 and 17.8 inches. If we are using the bootstrap mean, we would report that the average precipitation is 16.1 inches with a 95% confidence interval of 14.3 to 17.8 inches. The confidence interval is to give some indication of the uncertainty in the measurements. 11.3 Summary and next steps The idea of sampling with replacement, generating a parameter estimate, and then repeating over several iterations is at the heart of many computational parameter estimation methods, such as Markov Chain Monte Carlo methods that we will explore in Chapter 13. Bootstrapping (and other sampling with replacement techniques) makes nonlinear problems more tractable. This chapter extended your abilities in R by showing you how to generate histograms, sample a dataset, and compute statistics. The goal here is to give you examples that you can re-use in this chapter’s exercises. Enjoy! 11.4 Exercises Exercise 11.1 Histograms are an important visualization tool in descriptive statistics. Read the following essays on histograms, and then summarize 2-3 important points of what you learned reading these articles. Visualizing histograms How histograms work How to read histograms and use them in R Exercise 11.2 Display the bootstrap histogram of 1000 bootstrap samples for the standard deviation of the snowfall dataset. From this bootstrap distribution (for the standard deviation) what is the mean and 95% confidence interval? Exercise 11.3 (Inspired by Devore, Berk, and Carlton (2021)) Average snow cover from 1970 - 1979 in October over Eurasia (in million km\\(^{2}\\)) was reported as the following: \\[\\begin{equation*} \\{6.5, 12.0, 14.9, 10.0, 10.7, 7.9, 21.9, 12.5, 14.5, 9.2\\} \\end{equation*}\\] Create a histogram for these data. Compute the sample mean and median of this dataset. What would you report as a representative or typical value of snow cover for October? Why? The 21.9 measurement looks like an outlier. What is the sample mean excluding that measurement? Exercise 11.4 Consider the equation \\(\\displaystyle S(\\theta)=(3-1.5^{1/\\theta})^{2}\\) for \\(\\theta&gt;0\\). This function is an idealized example for the cost function in Figure 11.1. What is \\(S&#39;(\\theta)\\)? Make a plot of \\(S&#39;(\\theta)\\). What are the locations of the critical points? Algebraically solve \\(S&#39;(\\theta)=0\\). Does your computed critical point match up with the graph? Exercise 11.5 Repeat the bootstrap sample for the mean of the snowfall dataset (snowfall) where the number of bootstrap samples is 10,000. Report the median and confidence intervals for the bootstrap distribution. What do you notice as the number of bootstrap samples increases? Exercise 11.6 Using the data in Exercise 11.3, do a bootstrap sample with \\(N=1000\\) to compute the a bootstrap estimate for the mean October snowfall cover in Eurasia. Compute the mean and 95% confidence interval for the bootstrap distribution. FIGURE 11.5: Example computing a confidence interval with the summary command. Exercise 11.7 We computed the 95% confidence interval using the quantile command. An alternative approach to summarize a distribution is with the summary command, as shown in Figure 11.5. We call this command using summary(data_frame), where data_frame is the particular data frame you want to summarize. The output reports the minimum and maximum values of a dataset. The output 1st Qu. and 3rd Qu. are the 25th and 75th percentiles. Do 1000 bootstrap samples using the data in Exercise 11.3 and report the output from the summary command. Exercise 11.8 The dataset precipitation lists rainfall data from a fall storm that came through the Twin Cities. Make an appropriately sized histogram for the precipitation observations. What is the mean precipitation across these observations? Do a bootstrap estimate with \\(N=100\\) and \\(N=1000\\) and plot their respective histograms. For each of your bootstrap samples (\\(N=100\\) and \\(N=1000\\)) compute the mean and 95% confidence interval for the bootstrap distribution. What would you report for the mean and its 95% confidence interval for this storm? References "],["metropolis-12.html", "Chapter 12 The Metropolis-Hastings Algorithm 12.1 Estimating the growth of a dog 12.2 Likelihood ratios for parameter estimation 12.3 The Metropolis-Hastings algorithm for parameter estimation 12.4 Exercises", " Chapter 12 The Metropolis-Hastings Algorithm Cost or likelihood functions (Chapters 9 and 10) are a powerful approach to estimate model parameters for a dataset. Bootstrap sampling (Chapter 11) is an efficient computational method to extend the reach of a dataset to estimate population level parameters. With all these elements in place we will discuss a powerful algorithm that will efficiently sample a likelihood function to estimate parameters for a model. Let’s get started! 12.1 Estimating the growth of a dog In Chapter 2 we introduced the dataset wilson, which reported data on the weight of the puppy Wilson as he grew, shown again in Figure 12.1. Because we will re-use the scatter plot in Figure 12.1 several times, we define the variable wilson_data_plot so we don’t have to re-copy the code over and over.31 wilson_data_plot &lt;- ggplot(data = wilson) + geom_point(aes(x = days, y = weight), size = 1, color = &quot;red&quot;) + labs( x = &quot;D (Days since birth)&quot;, y = &quot;W (Weight in pounds)&quot; ) wilson_data_plot FIGURE 12.1: Weight of the dog Wilson over time. Let \\(D\\) be the days since birth and \\(W\\) Wilson’s weight in pounds, Equation (12.1) is a model that describes Wilson’s weight over time: \\[\\begin{equation} W =f(D,p_{1})= \\frac{p_{1}}{1+e^{(p_{2}-p_{3}D)}} \\tag{12.1} \\end{equation}\\] Equation (12.1) has three different parameters \\(p_{1}\\), \\(p_{2}\\), and \\(p_{3}\\) to be estimated. For convenience we will set \\(p_{2}= 2.461935\\) and \\(p_{3} = 0.017032\\) and estimate \\(p_{1}\\), which for this model represents the long-term weight for Wilson, or a horizontal asymptote for Equation (12.1) (Exercise 12.2). Let’s take an initial guess for the parameter \\(p_{1}\\). From Figure 12.1 a reasonable guess for \\(p_{1}\\) would be 78. As we did with Figure 12.1, we are going to store the updated plot as a variable. Try this code out on your own. # Define a data frame for W = f(D,78) days &lt;- seq(0, 1500, by = 1) p1 &lt;- 78 p2 &lt;- 2.461935 p3 &lt;- 0.017032 weight &lt;- p1 / (1 + exp(p2 - p3 * days)) my_guess_78 &lt;- tibble(days, weight) ### Now add our guess of p1 = 78 to the plot. my_guess_plot &lt;- wilson_data_plot + geom_line( data = my_guess_78, color = &quot;red&quot;, aes(x = days, y = weight) ) my_guess_plot Perhaps let’s try another guess for \\(p_{1}\\) a little lower than \\(p_{1}=78\\). Re-run the prior code, but this time set \\(p_{1}=65\\). Does that model better represent the wilson data? 12.2 Likelihood ratios for parameter estimation We have two potential values for \\(p_{1}\\) (78 or 65). While you may be able to decide which parameter is better “by eye,” let’s discuss a way to quantify this some more. How we do that is with the likelihood function for these data. We will apply the standard assumptions that the nineteen measurements of Wilson’s weight over time are all independent and identically distributed, creating the following likelihood function (Equation (12.2)): \\[\\begin{equation} L(p_{1}) = \\prod_{i=1}^{19} \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(W_{i}-f(D_{i},p_{1}))^{2}}{2 \\sigma^{2}}} \\tag{12.2} \\end{equation}\\] We can easily compute the associated likelihood values for Equation (12.2) with the R function compute_likelihood from Chapter 9: # Define the model we are using my_model &lt;- weight ~ p1 / (1 + exp(p2 - p3 * days)) # Define a tibble for the two different estimates of p1 parameters &lt;- tibble( p1 = c(78, 65), p2 = 2.461935, p3 = 0.017032 ) # Compute the likelihood and return the likelihood from the list out_values &lt;- compute_likelihood(my_model, wilson, parameters)$likelihood # Return the likelihood from the list: out_values ## # A tibble: 2 × 5 ## p1 p2 p3 l_hood log_lik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 78 2.46 0.0170 6.86e-29 FALSE ## 2 65 2.46 0.0170 1.20e-27 FALSE Hopefully this code seems familiar to you from Chapter 9, but of note are the following: We want to compare two values of p1, so when we defined parameters we included the two values of \\(p_{1}\\) when defining parameters. The same values of p2 and p3 will apply to both. Don’t believe me? Type parameters at the console line to see! Recall that when we apply compute_likelihood a list is returned (likelihood and opt_value). For this case we just want the likelihood data frame, hence the code $likelihood at the end of compute_likelihood. So we computed \\(L(78)\\)=6.8560765^{-29} and \\(L(65)\\)=1.2038829^{-27}. Since \\(L(65)&gt;L(78)\\) we would say \\(p_{65}\\) is the more likely parameter value. Notice how we computed \\(L(65)\\) and \\(L(78)\\) separately and then compared the two values. Another approach is to examine the ratio of the likelihoods (Equation (12.3)): \\[\\begin{equation} \\mbox{ Likelihood ratio: } \\frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) } \\tag{12.3} \\end{equation}\\] The utility of the likelihood ratio is that we can say that if the likelihood ratio is greater than 1, \\(p_{1}^{proposed}\\) is preferred. If this ratio is less than 1, \\(p_{1}^{current}\\) is preferred. Applying Equation (12.3) with \\(p_{1}^{proposed}=65\\) and \\(p_{1}^{current}=78\\), we have \\(\\displaystyle \\frac{ L(65) }{ L(78) }=\\) 18, further confirming \\(p_{1}=65\\) is more likely compared to the value of \\(p_{1}=78\\). 12.2.1 Iterative improvement with likelihood ratios We can improve on estimating \\(p_{1}\\) for Equation (12.1) by continuing to compute likelihood ratios. However, since \\(p_{1}=65\\) is the more likely value (currently), then we will set \\(p_{1}^{current}=65\\) for Equation (12.3). To simplify things, let’s define a function that will quickly compute Equation (12.1) for this dataset: # A function that computes the likelihood ratio for Wilson&#39;s weight likelihood_ratio_wilson &lt;- function(proposed, current) { # Define the model we are using my_model &lt;- weight ~ p1 / (1 + exp(p2 - p3 * days)) # This allows for all the possible combinations of parameters parameters &lt;- tibble( p1 = c(current, proposed), p2 = 2.461935, p3 = 0.017032 ) # Compute the likelihood and return the likelihood from the list out_values &lt;- compute_likelihood(my_model, wilson, parameters)$likelihood # Return the likelihood from the list: ll_ratio &lt;- out_values$l_hood[[2]] / out_values$l_hood[[1]] return(ll_ratio) } # Test the function out: likelihood_ratio_wilson(65, 78) ## [1] 17.55936 You should notice that the reported likelihood ratio matches up with our earlier computations! Perhaps a better guess for \\(p_{1}\\) would be somewhere between 65 and 78. Let’s try to compute the likelihood ratio for \\(p_{1}=70\\) compared to \\(p_{1}=65\\). Try computing likelihood_ratio_wilson(70,65) - you should see that it is about 7.5 million times more likely! I think we are onto something - Figure 12.2 compares the modeled values of Wilson’s weight for the different parameters: FIGURE 12.2: Comparison of our three estimates for Wilson’s weight over time. So now, let’s try \\(p_{1}=74\\) and compare the likelihoods: \\(\\displaystyle \\frac{ L(74) }{ L(70) }\\)=4.5897793^{-9}. This seems to be less likely because the ratio was significantly less than one. If we are doing a hunt for the best optimum value, then perhaps we would reject \\(p_{1}=74\\) and keep moving on, perhaps selecting another value closer to 70. While rejecting \\(p_{1}=74\\) as less likely, a word of caution is warranted. For non-linear problems we want to be extra careful that we do not accept a parameter value that leads us to a local (not global) optimum. A way to avoid this is to compare the calculated likelihood ratio to a uniform random number \\(r\\) between 0 and 1. At the R console type runif(1) - this creates one random number from the uniform distribution (remember the default range of the uniform distribution is \\(0 \\leq p_{1} \\leq 1\\)). The r in runif(1) stands for random. When I tried runif(1) I received a value of 0.126. Since the likelihood ratio is smaller than the random number I generated, we will reject the value of \\(p_{1}=74\\) and try again, keeping 70 as our value. The process to keep the proposed value based on some decision metric is called a decision step. 12.3 The Metropolis-Hastings algorithm for parameter estimation Section 12.2 outlined an iterative approach of applying likelihood ratios to estimate \\(p_{1}\\). Let’s organize all the work in a table (Table 12.1). TABLE 12.1: Organizational table of Metropolis-Hastings algorithm to estimate \\(p_{1}\\) from the wilson dataset. Iteration Current value of \\(p_{1}\\) Proposed value of \\(p_{1}\\) \\(\\displaystyle\\frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) }\\) Value of runif(1) Accept proposed \\(p_{1}\\)? 0 78 NA NA NA NA 1 78 65 17.55936 NA yes 2 65 70 7465075 NA yes 3 70 74 0.09985308 0.126 no 4 70 … … … … Table 12.1 is the essence of what is called the Metropolis-Hastings algorithm. The goal of this algorithm method is to determine the parameter set that optimizes the likelihood function, or makes the likelihood ratio greater than unity. Here are the key components for this algorithm: A defined likelihood function. A starting value for your parameter. A proposed value for your parameter. Comparison of the likelihood ratios for the proposed to the current value (\\(\\displaystyle \\mbox{ Likelihood ratio: } \\frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) }\\)). Parameter values that increase the likelihood will be preferred. A decision to accept the proposed parameter value. If the likelihood ratio is greater than 1, then we accept this value. However if the likelihood ratio is less than 1, we generate a random number \\(r\\) (using runif(1)) and use this following process: If \\(r\\) is less than the likelihood ratio we accept (keep) the proposed parameter value. If \\(r\\) is greater than the likelihood ratio we reject the proposed parameter value. 12.3.1 Improving the Metropolis-Hastings algorithm We have done the Metropolis-Hastings algorithm “by hand,” which may seem tedious to do, but it helps build your own iterative understanding of the underlying process. Here’s the good news: we can easily automate the Metropolis-Hastings algorithm, which we will explore in Chapter 13. To note, there are several modifications we can do to make the Metropolis-Hastings algorithm a more efficient and robust method: While we have focused on implementation of the Metropolis-Hastings algorithm with one parameter, this is easily extended to sets of parameter values (e.g. estimating \\(p_{1}\\), \\(p_{2}\\), and \\(p_{3}\\) in Equation (12.1)). However it may take longer to determine the global optimum because we change one parameter value at a time. As you would expect, the more times we iterate through this process, the better. Your initial guesses probably weren’t that great (or close to the global optimum), so a common procedure is to throw out the first percentage of iterations and call that the “burn-in” period. Different (usually in parallel) “chains” of parameter estimates are used. Each chain starts from a different starting point. Once the number of iterations is reached, the final parameter set is chosen from the chain with the optimized likelihood. The practice of running multiple chains is a safeguard to prevent the algorithm converging on a local optimum. Since the likelihood ratio may generate large or small numbers, the ratio of log-likelihoods is usually implemented. A log-likelihood ratio then computes the difference between the proposed and current parameter values. Computing the difference between two numbers is computationally easier than dividing two numbers. Here is some sample code that implements the log-likelihood ratio: # A function to computes the LOG likelihood ratio for Wilson&#39;s weight log_likelihood_ratio_wilson &lt;- function(proposed, current) { # Define the model we are using my_model &lt;- weight ~ p1 / (1 + exp(p2 - p3 * days)) # This allows for all the possible combinations of parameters parameters &lt;- tibble( p1 = c(current, proposed), p2 = 2.461935, p3 = 0.017032 ) # Compute the likelihood and return the likelihood from the list # Notice we&#39;ve set logLikely = TRUE to compute the log likelihood out_values &lt;- compute_likelihood(my_model, wilson, parameters, logLikely = TRUE )$likelihood # Return the likelihood from the list # here we compute the DIFFERENCE of likelihoods: ll_ratio &lt;- out_values$l_hood[[2]] - out_values$l_hood[[1]] return(ll_ratio) } We can systematically explore the parameter space, where the jump distance changes depending on if we are always accepting new parameters or not. This process has several different implementations, but one is called simulated annealing. 12.4 Exercises Exercise 12.1 Using likelihood_ratio_wilson, explain why likelihood_ratio_wilson(65,78)\\(\\neq\\)likelihood_ratio_wilson(78,65). Exercise 12.2 Show that \\(\\displaystyle \\lim_{D \\rightarrow \\infty} \\frac{p_{1}}{1+e^{(p_{2}-p_{3}D)}} = p_{1}\\). Hint: use the fact that \\(e^{A-B}= e^{A}e^{-B}\\). Note that \\(p_{1}\\), \\(p_{2}\\), and \\(p_{3}\\) are all positive parameters. Exercise 12.3 Simplify the expression \\(\\displaystyle \\ln \\left( \\frac{ L(p_{1}^{proposed}) }{ L(p_{1}^{current}) } \\right)\\). Exercise 12.4 Using the dataset wilson from this chapter, complete 10 iterations of the Metropolis-Hastings algorithm by continuing Table 12.1. See if you can get the value of \\(p_{1}\\) to 2 decimal places of accuracy. Be sure to include a plot of the data and the model with the final estimated value of \\(p_{1}\\). Exercise 12.5 Apply 10 iterations of the Metropolis-Hastings Algorithm to estimate \\(\\theta\\) for the nutrient equation \\(\\displaystyle y=1.737 x^{1/\\theta}\\) using the dataset phosphorous, where \\(y=\\)daphnia and \\(x=\\)algae. You will first need to construct a likelihood ratio similar to the function likelihood_ratio_wilson or log_likelihood_ratio_wilson in this chapter. Compare your final estimated value of \\(\\theta\\) with the data in one plot. Exercise 12.6 An alternative model for the dog’s mass is the following differential equation: \\[\\begin{equation} \\frac{dW}{dt} = -k (W-p_{1}) \\end{equation}\\] Apply separation of variables and \\(W(0)=5\\) and the value of \\(p_{1}\\) from Exercise 12.4 to determine the solution for this differential equation. Apply 10 iterations of the Metropolis-Hastings algorithm to estimate the value of \\(k\\) to three decimal places accuracy. The true value of \\(k\\) is between 0 and 1. Compare your final estimated value of \\(k\\) with the data in one plot. Exercise 12.7 Consider the linear model \\(y=6.94+bx\\) for the following dataset: x -0.365 -0.14 -0.53 -0.035 0.272 y 6.57 6.78 6.39 6.96 7.20 Apply 10 iterations of the Metropolis-Hastings algorithm to determine \\(b\\). Exercise 12.8 For the wilson dataset, repeat three steps of the parameter estimation to determine \\(p_{1}\\) as in this chapter, but this time use log_likelihood_ratio_wilson to estimate \\(p_{1}\\). Which function (likelihood_ratio_wilson or log_likelihood_ratio_wilson) do you think is easier in practice? Making a base plot and repeatedly adding to it is a really good coding practice for R.↩︎ "],["mcmc-13.html", "Chapter 13 Markov Chain Monte Carlo Parameter Estimation 13.1 The recipe for MCMC 13.2 MCMC parameter estimation with an empirical model 13.3 MCMC parameter estimation with a differential equation model 13.4 Timing your code 13.5 Further extensions to MCMC 13.6 Exercises", " Chapter 13 Markov Chain Monte Carlo Parameter Estimation We have explored likelihood functions, iterative methods, and the Metropolis-Hastings algorithm. In this chapter all these together introduce a sophisticated parameter estimation algorithm called Markov Chain Monte Carlo (MCMC) parameter estimation, which has a rich history (Richey 2010). MCMC methods can be highly computational; more importantly you already have the skills in place to understand how the MCMC method works. To do the heavy lifting we will rely on functions from the demodelr package. Let’s get started! 13.1 The recipe for MCMC The MCMC approach is a systematic exploration to determine the set of parameters that optimizes the value of the log-likehood function, given the data. It may be helpful to think of the MCMC method as a recipe, and in order to “run” the MCMC method, you will need four key ingredients: Model: a function that we have for our dynamics (this is \\(\\displaystyle \\frac{d\\vec{y}}{dt} = f(\\vec{y},\\vec{\\alpha},t)\\)), or an empirical equation \\(\\vec{y}=f(\\vec{x},\\vec{\\alpha})\\). Data: a data frame (tibble) or a spreadsheet file (to read into R) of the data you wish to use for parameter estimation. Parameter bounds: upper and lower bounds on your parameter values. We typically assume an initial uniform distribution on the parameters. Initial conditions: (optional) needed if your model is a differential equation. Run diagnostics: specifications for the MCMC code, which may include how long you will run the code and other aspects of the MCMC algorithm. We will work step by step through two examples of an application of the MCMC algorithm using both a differential equation and an empirical model. Example code is provided so you can also run your own estimates. The workflow that we will use is: Define the model, parameters, and data \\(\\rightarrow\\) Determine MCMC settings \\(\\rightarrow\\) Compute MCMC estimate \\(\\rightarrow\\) Analyze results. Having an established workflow helps to breakdown the process step by step, making it easier to check for any coding errors. 13.2 MCMC parameter estimation with an empirical model The first step of our workflow is to “Define the model and parameters.” Here we return to the problem exploring of the phosphorous content in algae (denoted by \\(x\\)) compared to the phosphorous content in daphnia (denoted by \\(y\\)), and estimating \\(c\\) and \\(\\theta\\) from Equation (13.1). \\[\\begin{equation} y = c \\cdot x^{1/\\theta} \\tag{13.1} \\end{equation}\\] The parameters \\(c\\) and \\(\\theta\\) from Equation (13.1) range from \\(0 \\leq c \\leq 2\\) and \\(1 \\leq \\theta \\leq 20\\). To define the model we use similar code to how we defined models in Chapter 8. Then to define the parameters we will use a tibble, specifying the upper and lower bounds: ## Step 1: Define the model and parameters phos_model &lt;- daphnia ~ c * algae^(1 / theta) # Define the parameters that you will use with their bounds phos_param &lt;- tibble( name = c(&quot;c&quot;, &quot;theta&quot;), lower_bound = c(0, 1), upper_bound = c(2, 20) ) Notice how we defined that tibble called phos_param, which has three columns: name which contains the name of the variables in our model (c and theta), the lower (lower_bound) and upper (upper_bound) for the parameters (listed in the same order as the parameters listed in name). The data that we use is the dataset phosphorous, which is already located in the demodelr package). The next two steps in our workflow (Determine MCMC settings \\(\\rightarrow\\) Compute MCMC estimate) are combined together below: ## Step 2: Determine MCMC settings # Define the number of iterations phos_iter &lt;- 1000 ## Step 3: Compute MCMC estimate phos_mcmc &lt;- mcmc_estimate( model = phos_model, data = phosphorous, parameters = phos_param, iterations = phos_iter ) The variable phos_iter specifies how many iterations we will run of the MCMC method. Notice that mcmc_estimate has several inputs, which for convenience we write on separate lines. There are four required inputs to the function mcmc_estimate and several predefined inputs; you will explore these further in Exercise 13.3. The function mcmc_estimate may take some time (which is OK). But once it finishes a tibble is produced, which we call phosphorous_mcmc (run this code on your own): glimpse(phos_mcmc) Notice phos_mcmc contains four columns: accept_flag tells you if at that particular iteration the MCMC estimate was accepted or not. This is a categorical variable of TRUE or FALSE l_hood is the value of the likelihood for that given iteration. The values of the parameters follow on the next few lines. Notice that \\(\\theta\\) is written as theta in the resulting data frame. The final step of our workflow is to “Analyze results.” Fortunately the demodelr package has a function called mcmc_analyze to help you: ## Step 4: Analyze results: mcmc_analyze( model = phos_model, data = phosphorous, mcmc_out = phos_mcmc ) The function mcmc_analyze filters phos_mcmc whenever the variable accept_flag is TRUE. This function will compute parameter statistics (e.g. median and 95% confidence intervals) to be displayed at the console. In addition this function will generate two different types of graphs.32 Let’s examine each one individually. The first plot (Figure 13.1 is called a pairwise parameter plot, which is a collection of different plots together in a square matrix pattern, sized to the number of parameters that were estimated. FIGURE 13.1: Pairwise parameter histogram from the MCMC parameter estimation with Equation . Along the diagonal of Figure 13.1 is a histogram of the accepted parameter values from the Metropolis algorithm. Depending on the results that you obtain, you may have some interesting shaped histograms. Generally they are grouped in the following ways: well-constrained: the parameter takes on a definite, well-defined value. The parameter \\(c\\) seems to behave like this. edge-hitting: the parameter seems to cluster near the edges of its value. non-informative: the histogram looks like a uniform distribution. The parameter \\(\\theta\\) has some indications of being edge hitting, but we would need more iterations in order to confirm. The off-diagonal plots in Figure 13.1 are interesting as well. These plots are a scatter plot of the accepted values for the two parameters in the particular row and column, and the upper off-diagonal reports the correlation coefficient \\(r\\) from simple linear regression of the variables in that particular row and column. The asterisks (*) denote the degree of significance of the linear correlation. Examining the off-diagonal terms in Figure 13.1 helps ascertain the degree of equifinality in a particular set of variables (Beven and Freer 2001). In Figure 13.1 it looks like as c increases, \\(\\theta\\) decreases. This degree of linear coupling means that we may not be able to independently resolve each parameter separately. Each model and parameter estimation is different, so be prepared to be surprised! The presence of equifinality in a model does not mean the parameter estimation is a failure - just that we need to be aware of these relationships. Perhaps we may be able to go out in the field and measure a parameter (for example \\(c\\)) more carefully, narrowing the range of accepted values. The second figure that is produced from mcmc_analyze displays an ensemble estimate of the model results with the data (Figure 13.2). The ensemble average plot provides a high-level model-data overview. Figure 13.2 is generated when the model (phos_mcmc) is run for each accepted parameter estimate in phos_mcmc. At each of the data points in Figure 13.2 a boxplot is produced from the model runs. The median, and the 25th and 75th percentile make up the box. The whiskers extend no further than 1.5 of the difference between the 25th and 75th percentile. By eye, it seems that the model and estimated parameters fit the data, but there is still wide variation in the model predictions. Perhaps this variation is caused by relative wide confidence intervals on our parameters (Figure 13.1). FIGURE 13.2: Ensemble output results from the MCMC parameter estimation with Equation . 13.3 MCMC parameter estimation with a differential equation model Next let’s try parameter estimation with a differential equation model. Here the measured data are solutions to a differential equation, which contains unknown parameters. Once the MCMC method proposes a parameter, then the differential equation needs to be solved numerically with Euler’s or a Runge-Kutta method before evaluating the likelihood function.33 The example that we are going to use relates to land use management, in particular a coupled system between a resource (such as a national park) and the number of visitors it receives (Sinay and Sinay 2006). The tourism model relies on two nondimensional scaled variables, \\(R\\) which is the amount of the resource (as a percentage) and \\(V\\) the percentage of visitors that could visit (also as a percentage): \\[\\begin{equation} \\begin{split} \\frac{dR}{dt}&amp;=R\\cdot (1-R)-aV \\\\ \\frac{dV}{dt}&amp;=b\\cdot V \\cdot (R-V) \\end{split} \\tag{13.2} \\end{equation}\\] Equation (13.2) has two parameters \\(a\\) and \\(b\\), which relate to how the resource is used up as visitors come (\\(a\\)) and how as the visitors increase, word of mouth leads to a negative effect of it being too crowded (\\(b\\)). For this case we are going to use a pre-defined dataset of the number of resources and visitors to a national park as reported in Sinay and Sinay (2006) (this is the parks dataset in the demodelr package) which is plotted in Figure 13.3. FIGURE 13.3: Scaled data on resources and visitors to a national park over time. As the visitors \\(V\\) increase in Figure 13.3, the percentage of the resources \\(R\\) decreases. Notably though, the data on both variables may be limited. In Figure 13.3 the proportion of visitors ranges from 0.9 to 1, and the resources ranges up to 0.03. Perhaps from this limited dataset given we can estimate the parameters \\(a\\) and \\(b\\) and then forecast out as time increases. We will estimate the parameters \\(a\\) and \\(b\\) in Equation (13.2) with the data shown in Figure 13.3. We are going to assume that \\(0 \\leq a \\leq 30\\) and \\(0 \\leq b \\leq 5\\). We will still use the same workflow (Define the model, parameters, and data \\(\\rightarrow\\) Determine MCMC settings \\(\\rightarrow\\) Compute MCMC estimate \\(\\rightarrow\\) Analyze results) as we did in estimating parameters for an empirical model. Since this workflow was presented earlier we will combine the first three steps below: ## Step 1: Define the model, parameters, and data # Define the tourism model tourism_model &lt;- c( dRdt ~ resources * (1 - resources) - a * visitors, dVdt ~ b * visitors * (resources - visitors) ) # Define the parameters that you will use with their bounds tourism_param &lt;- tibble( name = c(&quot;a&quot;, &quot;b&quot;), lower_bound = c(10, 0), upper_bound = c(30, 5) ) ## Step 2: Determine MCMC settings # Define the initial conditions tourism_init &lt;- c(resources = 0.995, visitors = 0.00167) deltaT &lt;- .1 # timestep length n_steps &lt;- 15 # must be a number greater than 1 # Define the number of iterations tourism_iter &lt;- 1000 ## Step 3: Compute MCMC estimate tourism_out &lt;- mcmc_estimate( model = tourism_model, data = parks, parameters = tourism_param, mode = &quot;de&quot;, initial_condition = tourism_init, deltaT = deltaT, n_steps = n_steps, iterations = tourism_iter ) Notice how mcmc_estimate has some additional arguments. Most important is the option mode \"de\", where de stands for differential equation. (The default mode is emp, or empirical model - like the phosphorous data set.) If the de mode is specified, then you also need to define the initial conditions (tourism_init), \\(\\Delta t\\) (deltaT), and timesteps (n_steps) in order to generate the numerical solution. Visualizing the data also is done with mcmc_analyze: ## Step 4: Analyze results mcmc_analyze( model = tourism_model, data = parks, mcmc_out = tourism_out, mode = &quot;de&quot;, initial_condition = tourism_init, deltaT = deltaT, n_steps = n_steps ) Examining the parameter histograms (Figure 13.4) shows \\(b\\) to be well-constrained. The histogram for \\(a\\) seems like it could be well-constrained - but we may need to run more iterations to confirm this. FIGURE 13.4: Pairwise parameter histogram of MCMC parameter estimation results with Equation . The model results and confidence intervals show good agreement to the data (Figure 13.5). Additionally the model forecasts out in time confirming that as visitors increase, the resources in the national park will decrease due to overuse. In contrast to Figure 13.2, the black line in Figure 13.5 represents the median and the grey shading is the 95% confidence interval for all timesteps defined in solving the model. FIGURE 13.5: Ensemble output results from the MCMC parameter estimation for Equation . 13.4 Timing your code As you can imagine the more iterations we have the better our parameter estimates will be. However, this means the full estimate with that number of iterations will take some more time. Before doing that, you first should get an estimate for the length of time it takes to run this code. Fortunately R has a stopwatch function. Let’s check this out with one iteration of the phosphorous dataset: # This &quot;starts&quot; the stopwatch start_time &lt;- Sys.time() # Compute a single mcmc estimate phosphorous1_mcmc &lt;- mcmc_estimate( model = phos_model, data = phosphorous, parameters = phos_param, iterations = 1 ) # End the stopwatch end_time &lt;- Sys.time() # Determine the difference between the start and end times end_time - start_time ## Time difference of 0.224685 secs Timing the code for one iteration gives you a ballpark estimate for a full MCMC parameter estimate. If we were to run N MCMC iterations, a good benchmark would be to multiply the time difference (end_time - start_time) by N. Performance time varies by computer and the other programs / apps that are running at the same time. However, this gives you an estimate of what to expect.34 13.5 Further extensions to MCMC For the examples in this chapter we limited the number of iterations to a smaller number to make the results computationally feasible. However we can extend the MCMC approach in two notable ways: One approach is to separate the data into two different sets - one for optimization and one for validation. In this approach the “optimization data” consists of a certain percentage of the original dataset, leaving the remaining to validate the forward forecasts. This is a type of cross-validation approach, and is generally preferred because you are demonstrating the strength of your model ability against non-optimized data. We also run multiple “chains” of optimization, starting from a different value in parameter space. What we do then after running each of these chains is to select the one with the best log-likelihood value, and run another MCMC iteration starting at that value. The idea is with a different chain we have sampled the parameter space and are hopefully starting near an optimum value. As you can see, the MCMC algorithm is an extremely powerful technique for parameter estimation. While MCMC may take additional time and programming skill to analyze - it is definitely worth it! 13.6 Exercises Exercise 13.1 Re-run both of the MCMC examples in this chapter, but increase the number of iterations to \\(10,000\\). Analyze your results from both cases. How does increasing the number of iterations affect the posterior parameter estimates and their confidence intervals? Does the log-likelihood value change? Exercise 13.2 Time the MCMC parameter estimate for the phosphorous dataset for \\(1\\) iteration. Then time the MCMC parameter estimate for \\(10\\), \\(100\\), and \\(1000\\) iterations, recording the times for each one. Make a scatterplot with the number of iterations on the horizontal axis and time on the vertical axis. How would you characterize the relationship between the number of iterations and the time it takes to run the code? How long would it take to compute an MCMC estimate with \\(10,000\\) iterations? Exercise 13.3 The function mcmc_estimate has several other input variables that are set to default values. What are they and how would you explain their use? (Hint: to see the documentation associated with this function type ?mcmc_estimate at the R console.) Exercise 13.4 For the parks data (Equation (13.2)) studied in this chapter, compare the 1:1 and the posterior parameter plots (Figure 13.4). Write a summary of each panel of the plot. Apply your understanding of equifinality and other observations to determine by how much you have estimated the parameters \\(a\\) and \\(b\\) from the data. Exercise 13.5 Run an MCMC parameter estimation on the dataset yeast from Gause (1932), where the equation for the volume of yeast \\(V\\) over time is given by the following equation for a yeast growing in isolation: \\[\\begin{equation} V = \\frac{K}{1+e^{a-bt}}, \\end{equation}\\] where \\(K\\) is the carrying capacity, \\(a\\) and \\(b\\) respective rate constants. Show that when \\(V(0)=0.45\\), \\(a = \\ln(K/0.45-1)\\). Rewrite the \\(V(t)\\) equation without \\(a\\). With the yeast data, perform an MCMC estimate for this equation. (Reminder: \\(\\ln(5)\\)) is implemented as log(5) in R.) Use the following settings for your MCMC parameter estimation: \\(K:\\) 1 to 20 \\(b\\): 0 to 1 1000 iterations When setting up the MCMC method, be sure to name the variables in your model to match the yeast data frame. Report all outputs from the MCMC estimation (this includes parameter estimates, confidence intervals, log-likelihood values, and any graphs). Compare your results to the results from Exercise 13.6. Exercise 13.6 Another model for this growth of yeast is the function \\(\\displaystyle V= K + Ae^{-bt}\\). Show that when \\(V(0)=0.45\\), \\(A = K - 0.45\\). Rewrite the initial equation without \\(A\\). With the yeast data, apply an MCMC estimate for this equation. Use the following settings for your MCMC parameter estimation: \\(K:\\) 1 to 20 \\(b\\): 0 to 1 1000 iterations When setting up the MCMC method, be sure to name the variables in your model to match the yeast data frame. Report all outputs from the MCMC estimation (this includes parameter estimates, confidence intervals, log-likelihood values, and any graphs). Compare your results to the results from Exercise 13.5. Exercise 13.7 Run an MCMC parameter estimation on the dataset wilson according to the following differential equation: \\[\\begin{equation} \\frac{dP}{dt} = b(N-P), \\end{equation}\\] where \\(P\\) represents the mass of the dog. Use the following settings for your MCMC parameter estimation: \\(N\\): 60 to 90 \\(b\\): 0 to .01 \\(P(0)=5\\) \\(\\Delta t = 1\\) day Number of timesteps: 1500 Number of iterations: 1000 When setting up the MCMC method, be sure to name the variables in your model to match the wilson data frame. Be sure to report all outputs from the MCMC estimation (this includes parameter estimates, confidence intervals, log-likelihood values, and any graphs). References "],["information-criteria-14.html", "Chapter 14 Information Criteria 14.1 Model assessment guidelines 14.2 Information criteria for assessing competing models 14.3 A few cautionary notes 14.4 Exercises", " Chapter 14 Information Criteria In Exercises 13.5 and 13.6 of Chapter 13 we introduced two different empirical models for fitting the growth of yeast \\(V\\) over time \\(t\\). One model is a logistic model (\\(\\displaystyle V = \\frac{K}{1+e^{a-bt}}\\)), whereas the second model is a saturating function (\\(\\displaystyle V= K + Ae^{-bt}\\)). A plot comparing MCMC parameter estimates for the two models is shown in Figure 14.1. FIGURE 14.1: Comparison of models for the growth of yeast in culture. Dots represent measured values from Gause (1932). Figure 14.1 raises an interesting question. Sometimes we have multiple, convergent models to describe a context or situation. While having these different options is good, we also like to know which is the best model. How would you decide that? This chapter focuses on objective criteria to assess what is called the best approximating model (Burnham and Anderson 2002). We will explore what are called information criteria, which is developed from statistical theory. Let’s get started! 14.1 Model assessment guidelines The first step is to develop some guidelines and metrics for model evaluation. Here would be the start of a list of things to consider, represented as questions: The model complexity - how many equations do we have? The number of parameters - a few or many? Do the model outputs match the data? How will model prediction compare to any newly collected measurements? Are the trends accurately represented (especially for timeseries data)? Is the selected model easy to use, simulate, and forecast? I may have hinted at some of these guidelines in earlier chapters. These questions are related to one another - and answering these questions (or ranking criteria for them) is at the heart of the topic of model selection. Perhaps you may be asking, why bother? Aren’t more models better? Let’s talk about a specific example, for which we return to the dataset global_temperature in the demodelr library. Recall this dataset represents the average global temperature anomaly relative to 1951-1980. When we did linear regression with this dataset in Chapter 8 the quadratic and cubic models were approximately the same (Figure 14.2): FIGURE 14.2: Comparison of global temperature anomaly dataset with various polynomial fitted models. The variation in the different model fits for Figure 14.2 shows how different, but similar, the model results can be depending on the choice of regression function. Table 14.1 displays summary results for the log-likelihood and the root mean square error (RMSE).35 In some cases, the log-likelihood decreases (indicating a more likely model), supported by the decrease in the RMSE indicating the fitted model more closely matches the observations. However the decrease in the log-likelihood and the RMSE is changing less as the complexity of the model (i.e. a higher degree polynomial) increases. TABLE 14.1: Comparison of model fits for global temperature anomaly dataset shown in Figure 14.2. Model Log-likelihood36 RMSE Linear 45.04 0.176 Quadratic 103.305 0.117 Cubic 105.788 0.115 Quartic 112.5 0.11 Further model evaluation can be examined by the following: Compare the measured values of \\(\\vec{y}\\) to the modeled values of \\(\\vec{y}\\) in a 1:1 plot. Does \\(g\\) do a better job predicting \\(\\vec{y}\\) than \\(f\\)? Related to that, compare the likelihood function values of \\(f\\) and \\(g\\). We would favor the model that has the lower log-likelihood. Compare the number of parameters in each model \\(f\\) and \\(g\\). We would favor the model that has the fewest number of parameters. Given the above question, we can state the model selection problem as the following: When we have two \\(f(\\vec{x}, \\vec{\\alpha})\\) and \\(g(\\vec{x}, \\vec{\\beta})\\) for the data \\(\\vec{y}\\), how would we determine which one (\\(f\\) or \\(g\\) or perhaps another alternative model) is the best approximating model? 14.2 Information criteria for assessing competing models Information criteria evaluate the tradeoff between model complexity (i.e. the number of parameters used) and with the log-likelihood (a measure of how well the model fits the data). There are several types of information criteria, but we are going to focus on two: The Akaike Information Criterion (\\(AIC\\), Akaike (1974)) is the most commonly used information criteria: \\[\\begin{equation} AIC = -2 LL_{max} + 2 P \\tag{14.1} \\end{equation}\\] An alternative to the \\(AIC\\) is the Bayesian Information Criterion (\\(BIC\\), Schwartz (1978)) \\[\\begin{equation} BIC = -2 LL_{max} + P \\ln (N) \\tag{14.2} \\end{equation}\\] In Equations (14.1) and (14.2), \\(N\\) is the number of data points, \\(P\\) is the number of estimated parameters, and \\(LL_{max}\\) is the log-likelihood for the parameter set that maximized the likelihood function. Equations (14.1) and (14.2) show the dependence on the log-likelihood function and the number of parameters. For both the \\(AIC\\) and \\(BIC\\) a lower value of the information criteria indicates greater support for the model from the data. Notice how easy the \\(AIC\\) and \\(BIC\\) are to compute in Equations (14.1) and (14.2) (assuming you have the information at hand). When an empirical model fit is computed (i.e. using the command lm), R computes these easily with the functions AIC or BIC. To apply them you need to first do the model fit (with the function lm. Try this out by running the following code on your own:37: regression_formula &lt;- temperature_anomaly ~ 1 + year_since_1880 fit &lt;- lm(regression_formula, data = global_temperature) AIC(fit) BIC(fit) Table 14.2 compares \\(AIC\\) and \\(BIC\\) for the models fitted using the global temperature anomaly dataset: TABLE 14.2: Comparison of the \\(AIC\\) and \\(BIC\\) for global temperature anomaly dataset shown in Figure 14.2. Model \\(AIC\\) \\(BIC\\) Linear -84.08 -75.213 Quadratic -198.609 -186.786 Cubic -201.576 -186.797 Quartic -213 -195.265 Table 14.2 shows that the cubic model is the better approximating model for both the \\(AIC\\) and the \\(BIC\\). 14.3 A few cautionary notes Information criteria are relative measures. In a study it may be more helpful to report the change in the information criteria, or even a ratio (see Burnham and Anderson (2002) for a detailed analysis). Information criteria are not cross-comparable across studies. If you are pulling in a model from another study, it is helpful to re-calculate the information criteria. An advantage to the \\(BIC\\) is that it measures tradeoffs between favoring a model that has the fewer number of data needed to estimate parameters. Other information criteria examine the distribution of the likelihood function and parameters. The upshot: Information criteria are one piece of evidence to help you to evaluate the best approximating model. You should do additional investigation (parameter evaluation, model-data fits, forecast values) in order to help determine the best model. 14.4 Exercises Exercise 14.1 You are investigating different models for the growth of a yeast species in a population where \\(V\\) is the rate of reaction and \\(s\\) is the added substrate: \\[\\begin{equation*} \\begin{split} \\mbox{Model 1: } &amp; V = \\frac{V_{max} s}{s+K_{m}} \\\\ \\mbox{Model 2: } &amp; V = \\frac{K}{1+e^{-a-bs}} \\\\ \\mbox{Model 3: } &amp; V= K + Ae^{-bs} \\end{split} \\end{equation*}\\] With a dataset of 7 observations you found that the log-likelihood for Model 1 is 26.426, for Model 2 the log-likelihood is is 15.587, and for Model 3 the the log-likelihood is 21.537. Apply the \\(AIC\\) and the \\(BIC\\) to evaluate which model is the best approximating model. Be sure to identify the number of estimated parameters for each model. Exercise 14.2 An equation that relates a consumer’s nutrient content (denoted as \\(y\\)) to the nutrient content of food (denoted as \\(x\\)) is given by: \\(\\displaystyle y = c x^{1/\\theta}\\), where \\(\\theta \\geq 1\\) and \\(c\\) are parameters. We can apply linear regression to the dataset \\((x, \\; \\ln(y) )\\), so the intercept of the linear regression equals \\(\\ln(c)\\) and the slope equals \\(1 / \\theta\\). Show that you can write this equation as a linear equation by applying a logarithm to both sides and simplifying. With the dataset phosphorous, take the logarithm of the daphnia variable and then determine a linear regression fit for your new linear equation. What are the reported values of the slope and intercept from the linear regression, and by association, \\(c\\) and \\(\\theta\\)? Apply the function logLik to report the log-likelihood of the fit. What are the reported values of the \\(AIC\\) and the \\(BIC\\)? An alternative linear model is the equation \\(y = a + b \\sqrt{x}\\). Use the R command sqrt_fit &lt;- lm(daphnia~I(sqrt(algae)),data = phosphorous) to first obtain a fit for this model. Then compute the log-likelihood and the \\(AIC\\) and the \\(BIC\\). Of the two models (the log-transformed model and the square root model), which one is the better approximating model? Exercise 14.3 (Inspired by Burnham and Anderson (2002)) You are tasked with the job of investigating the effect of a pesticide on water quality, in terms of its effects on the health of the plants and fish in the ecosystem. Different models can be created that investigate the effect of the pesticide. Different types of reaction schemes for this system are shown in Figure 3.7, where \\(F\\) represents the amount of pesticide in the fish, \\(W\\) the amount of pesticide in the water, and \\(S\\) the amount of pesticide in the soil. The prime (e.g. \\(F&#39;\\), \\(W&#39;\\), and \\(S&#39;\\)) represent other bound forms of the respective state. In all seven different models can be derived. These models were applied to a dataset with 36 measurements of the water, fish, and plants. The table for the log-likelihood for each model is shown below: Model 1a 2a 2b 3a 3b 4a 4b Log-likelihood -90.105 -71.986 -56.869 -31.598 -31.563 -8.770 -14.238 Use Figure 3.7 to identify the number of parameters for each model. Apply the \\(AIC\\) and the \\(BIC\\) to the data in the above table to determine which is the best approximating model. Exercise 14.4 Use the information shown in Table 14.1 to compute (by hand) the \\(AIC\\) and the \\(BIC\\) for each of the models for the global_temperature dataset (there are 142 observations). Do your results conform to what is presented in Table 14.2? How far off are your results? What would be a plausible explanation for the difference? References "],["linearsystems-15.html", "Chapter 15 Systems of Linear Differential Equations 15.1 Linear systems of differential equations and matrix notation 15.2 Equilibrium solutions 15.3 The phase plane 15.4 Non-equilibrium solutions and their stability 15.5 Exercises", " Chapter 15 Systems of Linear Differential Equations Here we delve into a deeper understanding of differential equations by examining long term stability of equilibrium solutions. As a first step, Chapter 15 focuses on linear systems of differential equations, such as Equation (15.1): \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= 2x \\\\ \\frac{dy}{dt} &amp;= x+y \\end{split} \\tag{15.1} \\end{equation}\\] Equation (15.1) is a linear system of differential equations because it does contain terms such as \\(y^{2}\\) or \\(\\sin(x)\\) on the right hand side of the equation. This chapter focuses on visualizing the phase plane for linear systems and determining the equilibrium solutions. Let’s get started! 15.1 Linear systems of differential equations and matrix notation Another way to express Equation (15.1) is with matrix notation: \\[\\begin{equation} \\begin{split} \\begin{pmatrix} \\frac{dx}{dt} \\\\ \\frac{dy}{dt} \\end{pmatrix} &amp;= \\begin{pmatrix} 2x \\\\ x+y \\end{pmatrix} \\\\ &amp;=\\begin{pmatrix} 2 &amp; 0 \\\\ 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\end{split} \\tag{15.2} \\end{equation}\\] (Note: we can also use the prime notation (\\(x&#39;\\) or \\(y&#39;\\)) to signify \\(\\displaystyle \\frac{dx}{dt}\\) or \\(\\displaystyle \\frac{dy}{dt}\\).) We can also represent Equation (15.2) in a compact vector notation: \\(\\displaystyle \\frac{ d \\vec{x} }{dt} = A \\vec{x}\\), where for this example \\(\\displaystyle \\vec{A}=\\begin{pmatrix} 2 &amp; 0 \\\\ 1 &amp; 1 \\end{pmatrix}\\). Now let’s generalize. A system of linear equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= ax+by \\\\ \\frac{dy}{dt} &amp;= cx+dy \\end{split} \\end{equation}\\] can be expressed in the following way: \\[\\begin{equation} \\begin{pmatrix} \\frac{dx}{dt} \\\\ \\frac{dy}{dt} \\end{pmatrix} = \\begin{pmatrix} ax+by \\\\ cx+dy \\end{pmatrix} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\tag{15.3} \\end{equation}\\] Equation (15.1) is an example of a coupled system of equations, mainly due to the expression \\(\\displaystyle \\frac{dy}{dt}=x+y\\). An example of an uncoupled system of equations would be \\(\\displaystyle \\frac{dx}{dt}=3x\\) and \\(\\displaystyle \\frac{dy}{dt}=-2y\\), which could be solved with separation of variables. 15.2 Equilibrium solutions Now let’s discuss equilibrium solutions for Equation (15.1). Recall equilibrium solutions are places where both \\(\\displaystyle \\frac{dx}{dt}=0 \\mbox{ and } \\frac{dy}{dt}=0\\). Since \\(\\displaystyle \\frac{dx}{dt}=2x\\), an equilibrium solution would be \\(x=0\\). Substituting \\(x=0\\) into \\(\\displaystyle \\frac{dy}{dt}=x+y\\) also shows \\(y=0\\) is the corresponding equilibrium solution for \\(y\\). For a general linear system of differential equations, it might be helpful to imagine what we should expect for an equilibrium solution. Think back to calculus - what types of functions have a derivative that equals zero? (Hopefully constant functions comes to mind!) The equilibrium solution is then \\(x=0\\) and \\(y=0\\). Here is an amazing fact: it turns out any linear system of differential equations has the origin as its only equilibrium solution. One way to verify this fact is to examine the theory behind solutions for linear systems of equations in linear algebra. You might be wondering why there is all the fuss with equilibrium solutions - especially the origin (\\(x=0\\) and \\(y=0\\))38. So while equilibrium solutions are not a terribly interesting question at the moment, the stability of solutions is. In order to understand what I mean by stability, let’s re-examine how to generate phase planes from Chapter 6. 15.3 The phase plane The phase plane is helpful here to understand the stability of an equilibrium solution. Remember that the phase plane shows the motion of solutions, visualized as a vector. For the system we examined earlier let’s take a look at the phase plane. Here is some R code from the demodelr package to help you visualize the phase plane for Equation (15.1), shown in Figure 15.1.39 # For a two variable system of differential equations # we need to define dx/dt and dy/dt separately: linear_eq &lt;- c( dxdt ~ 2 * x, dydt ~ x + y ) # Now we plot the solution. phaseplane( system_eq = linear_eq, x_var = &quot;x&quot;, y_var = &quot;y&quot; ) FIGURE 15.1: phase plane for Equation (15.1). Notice how in the Figure 15.1 phaseline the arrows seem to spin out from the origin? We are going to discuss that below - but based on what we see we might expect the stability of the origin to be unstable. 15.4 Non-equilibrium solutions and their stability Even though we have already identified the equilibrium solution for Equation (15.1), there are other non-equilibrium solutions. Here are two non-equilibrium solutions that you can verify: Solution 1: \\(\\displaystyle s_{1}(t) = \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ e^{t} \\end{pmatrix}\\) Solution 2: \\(\\displaystyle s_{2}(t) = \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} e^{2t} \\\\ e^{2t} \\end{pmatrix}\\) These two non-equilibrium solutions can aid us in understanding the stability of the equilibrium solutions. All of the solutions contain terms that are exponential growth, indicating movement away from the equilibrium solution at \\(x=0\\), \\(y=0\\). This is even more evident when we plot the solutions with the phase plane by defining a new data frame and plotting \\(s_{1}(t)\\) and \\(s_{2}(t)\\) with geom_path (Figure 15.2): FIGURE 15.2: phase plane for Equation (15.1), with straight line solutions \\(s_{1}(t)\\) and \\(s_{2}(t)\\). The arrows in the phase plane of Figure 15.2 show how \\(s_{1}(t)\\) and \\(s_{2}(t)\\) move away from the solution. The phase plane suggests for Equation (15.1) that the equilibrium solution at the origin is unstable because both the arrows in both directions seem to be pointing away from the origin. Also notice that in Figure 15.2 the solutions \\(s_{1}(t)\\) and \\(s_{2}(t)\\) in the \\(xy\\) plane are straight lines! It turns out that these straight line solutions are quite useful - we will study them in a later chapter. We can also investigate stability algebraically for each solution (\\(s_{1}(t)\\) and \\(s_{2}(t)\\)). We will organize our solutions in vector format, factoring out the exponential functions in each of the expressions: Solution 1: \\(\\displaystyle \\vec{s}_{1}(t) = \\begin{pmatrix} 0 \\\\ e^{t} \\end{pmatrix}= \\begin{pmatrix} 0 \\\\ e^{t} \\end{pmatrix} =e^{t} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\) Solution 2: \\(\\displaystyle \\vec{s}_{2}(t) = \\begin{pmatrix} e^{2t} \\\\ e^{2t} \\end{pmatrix}= \\begin{pmatrix} e^{2t} \\\\ e^{2t} \\end{pmatrix} = e^{2t} \\begin{pmatrix} 1 \\\\ 1\\end{pmatrix}\\) The vectors \\(\\displaystyle \\begin{pmatrix} 0 \\\\ 1\\end{pmatrix}\\) and \\(\\displaystyle \\begin{pmatrix} 1 \\\\ 1\\end{pmatrix}\\) are the lines \\(x=0\\) and \\(y=x\\), as shown in Figure 15.2. By factoring out the exponential functions we can see the straight line solutions! To further investigate stability we investigate the long term behavior of the exponential functions that are multiplying the straight line solutions. Notice that \\(\\displaystyle \\lim_{t \\rightarrow \\infty} e^{t}\\) and \\(\\displaystyle \\lim_{t \\rightarrow \\infty} e^{2t}\\) both do not have a finite value40, so we conclusively classify the equilibrium solution as “unstable.” Conversely, if both of the exponential functions had exponential decrease we would classify the equilibrium solution as “stable.” Finally, with these straight line solutions we can create other solutions as a linear combination of \\(s_{1}(t)\\) and \\(s_{2}(t)\\). For example, we can define another solution which we will call \\(s_{3}(t)\\), where \\(\\vec{s}_{3}(t)=\\vec{s}_{1}(t) -0.1 \\vec{s}_{2}(t)\\): FIGURE 15.3: Phase plane for Equation (15.1), with straight line solutions \\(s_{1}(t)\\), \\(s_{2}(t)\\), and \\(s_{3}(t)\\). So to recap the following about straight line solutions to two-dimensional linear systems: Straight line solutions have the form \\(\\displaystyle \\vec{s}(t) = e^{\\lambda t} \\cdot \\vec{v}\\). Methods to determine \\(\\lambda\\) and \\(\\vec{v}\\) will be studied in later chapters. For a two-dimensional linear system, you generally will have two straight line solutions \\(\\vec{s}_{1}\\) and \\(\\vec{s}_{2}\\). This means you will have two different values of \\(\\lambda\\) (\\(\\lambda_{1}\\) and \\(\\lambda_{2}\\)). The most general solution to the system of differential equations is the linear sum of the \\(s_{1}(t)\\) and \\(s_{2}(t)\\): \\(\\vec{x}(t) = c_{1} \\cdot \\vec{s}_{1}(t) + c_{2} \\cdot \\vec{s}_{2}(t)\\). If both values of \\(\\lambda\\) are greater than 0, the equilibrium solution is unstable. If both values of \\(\\lambda\\) are less than 0, the equilibrium solution is stable. Geometrically these straight line solutions are lines that pass through the origin in the \\(xy\\) plane. In the exercises you will look at additional examples to understand the behavior of linear systems. 15.5 Exercises Exercise 15.1 Write the following systems of equations in matrix notation (\\(\\displaystyle \\frac{ d \\vec{x} }{dt} = A \\vec{x}\\)): \\(\\displaystyle \\frac{dx}{dt} = 2x-6y, \\; \\frac{dy}{dt} = x-2y\\) \\(\\displaystyle \\frac{dx}{dt} = 9x-22y, \\; \\frac{dy}{dt} = 3x-7y\\) \\(\\displaystyle \\frac{dx}{dt} = 4x - 2y, \\; \\frac{dy}{dt} = 2x - 2y\\) \\(\\displaystyle \\frac{dx}{dt}= 4x-15y, \\; \\frac{dy}{dt}=2x-7y\\) \\(\\displaystyle \\frac{dx}{dt} = 3x-18y, \\; \\frac{dy}{dt} = x-5y\\) \\(\\displaystyle \\frac{dx}{dt} = 5x-12y, \\; \\frac{dy}{dt} = x-2y\\) Exercise 15.2 Verify that \\(\\displaystyle s_{1}(t) = \\begin{pmatrix} 0 \\\\ e^{t} \\end{pmatrix}\\) and \\(\\displaystyle s_{2}(t) = \\begin{pmatrix} e^{2t} \\\\ e^{2t} \\end{pmatrix}\\) are both solutions for Equation (15.1). Exercise 15.3 Verify that \\(\\displaystyle s_{3}(t) = \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} -0.1e^{2t} \\\\ e^{t} - 0.1e^{2t} \\end{pmatrix}\\) is a solution for Equation (15.1). Exercise 15.4 Generate a new solution for Equation (15.1) that is a linear combination of \\(s_{1}(t)\\) and \\(s_{2}(t)\\) and plot your solution with the phase plane for Equation (15.1). Exercise 15.5 Verify that \\(x=0\\), \\(y=0\\), and \\(z=0\\) are solutions to the differential equation \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= 5x-4y + z \\\\ \\frac{dy}{dt} &amp;= y - 9z \\\\ \\frac{dz}{dt} &amp;= 7x-z \\\\ \\end{split} \\end{equation}\\] Exercise 15.6 Explain why we call \\(x=0\\) and \\(y=0\\) equilibrium solutions to the general linear differential equation \\(\\displaystyle \\frac{ d \\vec{x} }{dt} = A \\vec{x}\\). In other words, why is the word equilibrium important? (Hint: Think about what the solution curves for \\(x\\) and \\(y\\) would be in this case.) Exercise 15.7 Generate a phase plane for each of the systems in Exercise 15.1 and classify the stability of the equilibrium solution at \\(x=0\\) and \\(y=0\\) as stable, unstable, or uncertain, providing reasons for your conclusion. Exercise 15.8 This problem considers the differential equation \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= x+y \\\\ \\frac{dy}{dt} &amp;= y-x \\end{split} \\end{equation}\\] Use the command phaseplane to create a phase plane of this differential equation. Using the option plot_points, change the number of arrows shown to 5 and 20 (2 different plots). What do you notice about the updated phase plane? Change the viewing window (x_window and y_window) from the default to minus 10 to 10 in both axes. Now change the number of arrows shown to 5 and 20 (2 different plots). What do you notice about the updated phase plane? Exercise 15.9 Consider the following differential equation: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= -ax-y \\\\ \\frac{dy}{dt} &amp;= x \\end{split} \\end{equation}\\] Write this system in the form \\(\\displaystyle \\frac{d\\vec{x}}{dt}=A \\vec{x}\\). Let \\(a= -3, \\; -1, \\; 0, \\; 1, \\; 3\\). Generate a phase plane for each of these values of \\(a\\). With each of your phase plane plots, characterize the behavior of the equilibrium solution as \\(a\\) changes. Exercise 15.10 Consider the following differential equation: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= -y \\\\ \\frac{dy}{dt} &amp;= x \\end{split} \\end{equation}\\] Generate a phase plane diagram of this system. What do you notice? Verify that \\(x(t)=A \\cos(t)\\) and \\(y(t)=A \\sin(t)\\) is a solution to this differential equation. An equation of a circle of radius \\(R\\) is \\(x^{2}+y^{2}=R^{2}\\). Use implicit differentiation to differentiate this equation to get an expression for \\(\\displaystyle \\frac{dy}{dx}\\). How does your solution compare to the ratio of \\(\\displaystyle \\frac{y&#39;}{x&#39;}\\) from your differential equation? (Note: \\(\\displaystyle y&#39; = \\frac{dy}{dt}\\) and \\(\\displaystyle x&#39; = \\frac{dx}{dt}\\)) Verify that \\(x_{2}(t)=A \\cos(t) + B \\sin(t)\\) and \\(y_{2}(t)=A \\sin(t)-B \\cos(t)\\) also is a solution to the differential equation. Choose \\(A=1\\) and \\(B=1\\) to make a parametric plot of the solution. What do you notice in your parametric plot? Another name for the origin equilibrium solution is the trivial equilibrium. Can you see why it is trivial?↩︎ It is okay to refer back to Chapter 6 for a refresher on how the phaseplane command works.↩︎ In other words, \\(\\lim_{t \\rightarrow \\infty} e^{t}=\\infty\\) and \\(\\lim_{t \\rightarrow \\infty} e^{2t}=\\infty\\).↩︎ "],["nonlinear-16.html", "Chapter 16 Systems of Nonlinear Differential Equations 16.1 Introducing nonlinear systems of differential equations 16.2 Zooming in on the phase plane 16.3 Determining equilibrium solutions with nullclines 16.4 Stability of an equilibrium solution 16.5 Graphing nullclines in a phase plane 16.6 Exercises", " Chapter 16 Systems of Nonlinear Differential Equations 16.1 Introducing nonlinear systems of differential equations In Chapter 15 we discussed systems of linear equations. For this chapter we focus on non-linear systems of equations. We previously discussed coupled (nonlinear) systems of equations in Chapter 6, but we will dig in a little deeper here. Consider the following nonlinear system of equations with the associated phase plane in Figure 16.1: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y-1 \\\\ \\frac{dy}{dt} &amp;= x^{2}-1 \\end{split} \\tag{16.1} \\end{equation}\\] FIGURE 16.1: Phase plane for Equation (16.1). Wow! The phase plane in Figure 16.1 looks really interesting. Let’s dig into this deeper to understand the phase plane better. 16.2 Zooming in on the phase plane One way to investigate the phase plane is to zoom in on interesting chapters for Figure 16.1. In the upper left corner there is some swirling action, so let’s zoom in somewhat (remember you can adjust the window size in phaseplane with the option x_window and y_window): FIGURE 16.2: Zoomed in phase plane for Equation (16.1). Something interesting seems to be happening at the point \\((x,y)=(-1,1)\\) in Figure 16.2. Let’s take a look at what happens if we evaluate our differential equation at \\((x,y)=(-1,1)\\): \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= 1-1 = 0 \\\\ \\frac{dy}{dt} &amp;= (-1)^{2}-1 = 0 \\end{split} \\end{equation}\\] Aha! So the point \\((-1,1)\\) is an equilibrium solution. In later chapters we will discuss why we are observing the behavior with the swirling arrows. For now, the key point from Figure 16.2 is to recognize that nonlinear systems can have nonzero equilibrium solutions. Next, there seems to be a second interesting point in the upper right corner of Figure 16.1. Let’s zoom in near the point \\((x,y)=(1,1)\\): FIGURE 16.3: Another zoomed in phase plane for Equation (16.1). It seems like there is a second equilibrium solution at the point \\((1,1)\\)! Let’s confirm this: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= 1-1 = 0 \\\\ \\frac{dy}{dt} &amp;= (1)^{2}-1 = 0 \\end{split} \\end{equation}\\] By zooming in on the phase plane we learned something important about nonlinear systems and how they might differ compared to linear systems. In Chapter 15 we learned that the origin is the only equilibrium solution for a linear system of differential equations. On the other hand, nonlinear systems of equations may have multiple equilibrium solutions. 16.3 Determining equilibrium solutions with nullclines To determine an equilibrium solution for a system of differential equations we first need to find the intersection of different nullclines. We do this by setting each of the rate equations (\\(\\displaystyle \\frac{dx}{dt}\\) or \\(\\displaystyle \\frac{dy}{dt}\\)) equal to zero. Equation (16.1) has two nullclines: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} = 0 &amp;\\rightarrow y-1 = 0\\\\ \\frac{dy}{dt} = 0 &amp; \\rightarrow x^{2}-1 = 0 \\end{split} \\tag{16.2} \\end{equation}\\] So, solving for both nullclines in Equation (16.2) we have that \\(y=1\\) or \\(x = \\pm 1\\). You can visually see the phase plane with the nullclines in Figure 16.4, where we will add the nullclines and equilibrium solutions into the plot. FIGURE 16.4: Phase plane for Equation (16.1), with nullclines and equilibrium solutions shown. In Figure 16.4 we can see equilibrium solutions occur where a nullcline for \\(x&#39;=0\\) intersects with a nullcline where \\(y&#39;=0\\). 16.4 Stability of an equilibrium solution The idea of stability of an equilibrium solution for a nonlinear system is intuitively similar to that of a linear system: the equilibrium is stable when all the phase plane arrows point towards the equilibrium solution. For Equation (16.3), the equilibrium solution at \\((x,y)=(1,1)\\) is unstable because in Figure 16.3 some of the arrows point towards the equilibrium solution, whereas others point away from it. For Figure 16.2 it is a little harder to tell stability of the equilibrium solution at \\((x,y)=(-1,1)\\). At this point we won’t discuss more specifics of determining stable versus unstable equilibrium solutions. If the phase plane suggests that the equilibrium solution is stable or unstable, then you have established some good intuition that can be confirmed with additional analyses. 16.5 Graphing nullclines in a phase plane Let’s look at another example, but this time we will focus on generating graphs for the nullclines. \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= x-0.5yx \\\\ \\frac{dy}{dt} &amp;= yx -y^{2} \\end{split} \\tag{16.3} \\end{equation}\\] Figure 16.5 shows the phase plane for this example. Can you guess where an equilibrium solution would be? # Define the range we wish to use to evaluate this vector field system_eq &lt;- c( dx ~ x - 0.5 * y * x, dy ~ y * x - y^2 ) p1 &lt;- phaseplane(system_eq, &quot;x&quot;, &quot;y&quot;, x_window = c(0, 4), y_window = c(0, 4) ) p1 FIGURE 16.5: Phase plane for Equation (16.3). Notice how the code used to generate Figure 16.5 stores the phase plane in the variable p1 and the displays it. This will make things easier when we plot the nullclines. Speaking of nullclines, let’s find them: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} = 0 &amp; \\rightarrow x-0.5yx = 0 \\\\ \\frac{dy}{dt} = 0 &amp; \\rightarrow yx -y^{2} = 0 \\end{split} \\end{equation}\\] The algebra is becoming a little more involved. Factoring \\(x-0.5yx = 0\\) we have \\(x \\cdot (1 - 0.5 y) = 0\\), so either \\(x=0\\) or \\(y=2\\). Factoring the second equation we have \\(y \\cdot (x - y) = 0\\), so either \\(y=0\\) or \\(x=y\\). Notice how this second nullcline is a function of \\(x\\) and \\(y\\). The following code plots the phase plane (p1) along with the nullclines (try this code out on your own): # Define the nullclines for dx/dt = 0 (red): # x = 0 nullcline_x1 &lt;- tibble(x = 0, y=seq(0,4,length.out=100) ) # y = 0.5 nullcline_x2 &lt;- tibble(x = seq(0,4,length.out=100), y=2 ) # Define the nullclines for dy/dt = 0 (blue): # y = 0 nullcline_y1 &lt;- tibble(x = seq(0,4,length.out=100), y=0 ) # y = x nullcline_y2 &lt;- tibble(x = seq(0,4,length.out=100), y=x ) # Add the nullclines onto the phase plane p1 + geom_line(data = nullcline_x1,aes(x=x,y=y),color=&#39;red&#39;) + geom_line(data = nullcline_x2,aes(x=x,y=y),color=&#39;red&#39;) + geom_line(data = nullcline_y1,aes(x=x,y=y),color=&#39;blue&#39;) + geom_line(data = nullcline_y2,aes(x=x,y=y),color=&#39;blue&#39;) For each nullcline we define a data frame (tibble) that encodes the relevant information so we can plot it. In order to accomplish this we defined a sequence of values ranging from the plot window of 0 to 4 for the other variable. For nullclines where \\(y\\) was a function of \\(x\\) we defined a sequence of values for \\(x\\) and defined \\(y\\) accordingly. For Equation (16.3) the equilibrium solutions are \\((x,y)=(0,0)\\), \\((x,y)=(2,2)\\). You may be tempted to think that \\((0,2)\\) is also an equilibrium solution - however - \\(x=0\\) and \\(y=2\\) are equations for the \\(x\\) nullcline. It is easy to forget, but equilibrium solutions are determined from the intersection of distinct nullclines. Now that we have seen how nonlinear systems are different from linear systems, Chapter 17 will introduce tools for analysis for the stability of equilibrium solutions. 16.6 Exercises Exercise 16.1 Equation (16.3) equilibrium solutions are \\((x,y)=(0,0)\\), \\((x,y)=(2,2)\\). Zoom in on the phase plane at each of those points to determine the stability of the equilibrium solutions. (Set the window between \\(-0.5 \\leq x \\leq 0.5\\) and \\(-0.5 \\leq x \\leq 0.5\\) for the \\((x,y)=(0,0)\\) equilibrium solution.) Exercise 16.2 Consider the following nonlinear system of equations, which is a modification of Equation (16.1): \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y-x \\\\ \\frac{dy}{dt} &amp;= x^{2}-1 \\end{split} \\end{equation}\\] What are the equations for the nullclines for this differential equation? What are the equilibrium solutions for this differential equation? Generate a phase plane that includes all equilibrium solutions (use the window \\(-2 \\leq x \\leq 2\\) and \\(-2 \\leq y \\leq 2\\)) Based on the phase plane, evaluate the stability of the equilibrium solution. Exercise 16.3 Consider the following nonlinear system of equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= x - .5xy \\\\ \\frac{dy}{dt} &amp;= .5yx-y \\end{split} \\end{equation}\\] What are the equations for the nullclines for this differential equation? What are the equilibrium solutions for this differential equation? Generate a phase plane that includes all equilibrium solutions. Based on the phase plane, evaluate the stability of the equilibrium solution. Exercise 16.4 (Inspired by Logan and Wolesensky (2009)) A population of fish \\(F\\) has natural predators \\(P\\). A model that describes this interaction is the following: \\[\\begin{equation} \\begin{split} \\frac{dF}{dt} &amp;= F - .3FP \\\\ \\frac{dP}{dt} &amp;= .5FP - P \\end{split} \\end{equation}\\] What are the equations for the nullclines for this differential equation? What are the equilibrium solutions for this differential equation? Generate a phase plane that includes all the equilibrium solutions. Based on the phase plane, evaluate the stability of the equilibrium solution. Exercise 16.5 Consider the following system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y^{2} \\\\ \\frac{dy}{dt} &amp;= - x \\end{split} \\end{equation}\\] What are the nullclines for this system of equations? What is the equilibrium solution for this system of equations? Generate a phase plane that includes the equilibrium solution. Set the viewing window to be \\(-0.5 \\leq x \\leq 0.5\\) and \\(-0.5 \\leq y \\leq 0.5\\). Based on the phase plane, evaluate the stability of the equilibrium solution. Exercise 16.6 The Van der Pol Equation is a second-order differential equation used to study radio circuits: \\(x&#39;&#39; + \\mu \\cdot (x^{2}-1) x&#39; + x = 0\\), where \\(\\mu\\) is a parameter. Let \\(x&#39;=y\\) (note: \\(\\displaystyle x&#39; = \\frac{dx}{dt}\\)). Show that with this change of variables the Van der Pol equation can be written as a system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y \\\\ \\frac{dy}{dt} &amp;= -x-\\mu \\cdot (x^{2}-1)y \\end{split} \\end{equation}\\] By determining the nullclines, verify that the only equilibrium solution is \\((x,y)=(0,0)\\). Make a phase plane for different values of \\(\\mu\\) ranging from \\(-3\\), \\(-1\\), \\(0\\), \\(1\\), \\(3\\). Set your \\(x\\) and \\(y\\) windows to range between \\(-1\\) to \\(1\\). Based on the phase planes that you generate, evaluate the stability of the equilibrium solution as \\(\\mu\\) changes. Exercise 16.7 (Inspired by Strogatz (2015)) Consider the following nonlinear system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y-x \\\\ \\frac{dy}{dt} &amp;=-y + \\frac{5x^2}{4+x^{2}} \\end{split} \\end{equation}\\] What are the equations for the nullclines? Using desmos (or some other graphing utility), graph the two nullclines simultaneously. What are the intersection points? Generate a phase plane for this system that contains all the equilibrium solutions. Let’s say instead that \\(\\displaystyle \\frac{dx}{dt} = bx-y\\), where \\(b\\) is a parameter such that \\(0 \\leq b \\leq 2\\). Using desmos (or some other graphing utility), how many equilibrium solutions do you have as \\(b\\) changes? Exercise 16.8 (Inspired by Logan and Wolesensky (2009)) Let \\(C\\) be the amount of carbon in a forest ecosystem, with \\(P\\) as the rate of increase in carbon due to photosynthesis. Herbivores \\(H\\) consume carbon on the following predator-prey model: \\[\\begin{equation} \\begin{split} \\frac{dC}{dt}&amp;=P- bHC \\\\ \\frac{dH}{dt} &amp;= e\\cdot bHC-dC \\end{split} \\end{equation}\\] In the above equation, \\(b\\), \\(e\\), and \\(d\\) are all parameters greater than zero. What are the equations for the nullclines? Set \\(e=b=d=1\\). Plot the equations of the nullclines. How many equilibrium solutions does this system have? Determine the equilibrium solutions for this system of equations, expressed in terms of the parameters \\(b\\), \\(e\\), and \\(d\\). References "],["jacobian-17.html", "Chapter 17 Local Linearization and the Jacobian 17.1 Competing populations 17.2 Tangent plane approximations 17.3 The Jacobian matrix 17.4 Exercises", " Chapter 17 Local Linearization and the Jacobian Chapters 15 and 16 focused on systems of differential equations and using phase planes to determine a preliminary classification of any equilibrium solution. In this chapter we study local linearization and the associated Jacobian matrix. These tools are used to analyze stability of equilibrium solutions for a nonlinear system, thereby building a bridge between nonlinear and linear systems of equations. Let’s get started! 17.1 Competing populations Let’s take a look at a familiar example from Chapter 9, specifically the experiments of growing different species of yeast together (Gause 1932). Equation (17.1) (adapted from the one presented in Gause (1932)) represents the volume of two species of yeast (which we will call \\(Y\\) and \\(N\\)) growing in the same solution: \\[\\begin{equation} \\begin{split} \\frac{dY}{dt} &amp;= .2 Y \\left( \\frac{13-Y-2N}{13} \\right) \\\\ \\frac{dN}{dt} &amp;= .06 N \\left( \\frac{6-N-0.4Y}{6} \\right) \\\\ \\end{split} \\tag{17.1} \\end{equation}\\] While Equation (17.1) is a tricky model to consider, the terms \\(2N\\) and \\(0.4Y\\) represent the effect that \\(Y\\) and \\(N\\) have on each other since they are competing for the same resource. If these terms weren’t present, both \\(Y\\) and \\(N\\) would follow a logistic growth (verify this on your own). Equation (17.1) has 4 equilibrium solutions: \\((Y,N)=(0,0)\\), \\((Y,N)=(13,0)\\), \\((Y,N)=(0,6)\\), and \\((Y,N)=(5,4)\\) (Exercise 17.1). Figure 17.1 shows the phase plane for this system along with the equilibrium solutions.41 FIGURE 17.1: Phase plane for Equation (17.1), with equilibrium solutions shown as red points. Let’s take a closer look at the phase plane near the equilibrium solution \\((Y,N)=(5,4)\\) in Figure 17.2: FIGURE 17.2: A zoomed in view of Equation (17.1) near the \\((Y,N)=(5,4)\\) equilibrium solution. The phase plane in Figure 17.2 looks like this equilibrium solution is stable (the arrows seem to suggest a “swirling” into this equilibrium solution. However, another way to verify this is by applying a locally linear approximation. Better stated, because Equation (17.1) is a system of differential equations, we will construct a tangent plane approximation around \\(Y=5\\), \\(N=4\\). Let’s review how to do that next. 17.2 Tangent plane approximations For a multivariable function \\(f(x,y)\\), the tangent plane approximation at the point \\(x=a\\), \\(y=b\\) is given by Equation (17.2): \\[\\begin{equation} L(x,y) = f(a,b) + f_{x}(a,b) \\cdot (x-a) + f_{y}(a,b) \\cdot (y-b), \\tag{17.2} \\end{equation}\\] where \\(f_{x}\\) is the partial derivative of \\(f(x,y)\\) with respect to \\(x\\) and \\(f_{y}\\) is the partial derivative of \\(f(x,y)\\) with respect to \\(y\\). We will apply Equation (17.2) to Equation (17.1) at the equilibrium solution at \\((Y,N)=(5,4)\\). Since we have two equations, we need to compute two tangent plane approximations (one for each equation). The right hand sides for Equation (17.1) look complicated, but we can expand them to identify \\(f(Y,N)=.2Y - .03 Y N - .015 Y^{2}N\\) and \\(g(Y,N)= .06N-.01 N^{2} - .004YN\\). First consider \\(f(Y,N)\\). Let’s compute the partial derivatives for \\(f(Y,N)\\) at the equilibrium solution: \\[\\begin{equation} \\begin{split} f_{Y} = .2 - .03N - .03YN &amp; \\rightarrow f_{Y}(5,4)=-.52 \\\\ f_{N} = .03Y - .015Y^{2} &amp; \\rightarrow f_{N}(5,4)=-.225 \\end{split} \\end{equation}\\] We also know that \\(f(5,4)=0\\). As a result, the tangent plane approximation for \\(f(Y,N)\\) is given by Equation (17.3): \\[\\begin{equation} f(Y,N) \\approx -.52 \\cdot (Y-5) - .225 \\cdot (N-4) \\tag{17.3} \\end{equation}\\] Likewise if we consider \\(g(Y,N)= .06N-.01 N^{2} - .004YN\\), we have: \\[\\begin{equation} \\begin{split} g_{Y} &amp;= -.004N \\rightarrow g_{Y}(5,4)=-.016 \\\\ g_{N} &amp;= .06 -.02N-.004Y \\rightarrow g_{N}(5,4)=-.04 \\end{split} \\end{equation}\\] We also know that \\(g(5,4)=0\\). As a result, the tangent plane approximation for \\(g(Y,N)\\) is given by Equation (17.4): \\[\\begin{equation} g(H,L) \\approx -.016 \\cdot (Y-5) -.04 \\cdot (N-4) \\tag{17.4} \\end{equation}\\] So, at the equilibrium solution \\((Y,N)=(5,4)\\), Equation (17.1) behaves like the following system of equations (Equation (17.5)): \\[\\begin{equation} \\begin{split} \\frac{dY}{dt} &amp;= -.52 \\cdot (Y-5) - .225 \\cdot (N-4)\\\\ \\frac{dN}{dt} &amp;= -.016 \\cdot (Y-5) -.04 \\cdot (N-4) \\end{split} \\tag{17.5} \\end{equation}\\] 17.3 The Jacobian matrix Equation (17.5) looks like a linear system of equations; however, we first need to re-define our system with the change of variables \\(y = Y-5\\), \\(n = N-4\\), which essentially is a shift of the variables so the equilibrium solution is at the origin (Equation (17.6)): \\[\\begin{equation} \\begin{split} \\frac{dy}{dt} &amp;= -.52 y - .225 n\\\\ \\frac{dn}{dt} &amp;= -.016 y -.04 n \\end{split} \\tag{17.6} \\end{equation}\\] Then we can write Equation (17.6) in matrix form using the tools from Chapter 15: \\[\\begin{equation} \\begin{pmatrix} y&#39; \\\\ n&#39; \\end{pmatrix} =\\begin{pmatrix} -.52 &amp; -.225 \\\\ -.016 &amp; -.04 \\end{pmatrix} \\begin{pmatrix} y \\\\ n \\end{pmatrix} \\end{equation}\\] We define the matrix \\(\\displaystyle J= \\begin{pmatrix} -.52 &amp; -.225 \\\\ -.016 &amp; -.04 \\end{pmatrix}\\) as the Jacobian matrix. More generally, let’s say we have the following system of differential equations, with an equilibrium solution at \\((x,y)=(a,b)\\): \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= f(x,y) \\\\ \\frac{dy}{dt} &amp;= g(x,y) \\end{split} \\end{equation}\\] The Jacobian matrix at that equilibrium solution is: \\[\\begin{equation} J_{(a,b)} =\\begin{pmatrix} f_{x}(a,b) &amp; f_{y}(a,b) \\\\ g_{x}(a,b) &amp; g_{y}(a,b) \\end{pmatrix} \\tag{17.7} \\end{equation}\\] The notation \\(J_{(a,b)}\\) signifies the Jacobian matrix evaluated at the equilibrium solution \\((x,y)=(a,b)\\). The Jacobian matrix (Equation (17.7)) follows naturally from tangent plane approximations (Equation (17.2)). In later chapters we will use the Jacobian matrix to investigate stability of nonlinear systems. The Jacobian matrix is part of the following system of linear equations (Equation (17.8)): \\[\\begin{equation} \\begin{split} \\frac{dX}{dt} &amp;= f_{x}(a,b) \\cdot X + f_{y}(a,b) \\cdot Y \\\\ \\frac{dY}{dt} &amp;= g_{x}(a,b) \\cdot X + g_{y}(a,b) \\cdot Y \\end{split} \\tag{17.8} \\end{equation}\\] You may recognize that Equation (17.5) was a specific example of Equation (17.8). The variables \\(X=x-a\\) and \\(Y=y-b\\) help translate the tangent plane equation into a linear system. Also notice how Equation (17.7) does not include the terms \\(f(a,b)\\) or \\(g(a,b)\\) from Equation (17.2). This is because of the fact that we are building our tangent plane approximation at an equilibrium solution, so \\(f(a,b)=g(a,b)=0\\)! One more additional note: when we visualize the phase plane for a Jacobian matrix, we center the window at the origin \\((x,y)=(0,0)\\) (rather than at the equilibrium solution \\((a,b)\\)) because Equation (17.8) is a linear system, with an equilibrium solution at the origin. While we don’t discuss it here, the Jacobian matrix also extends to higher order systems as well. 17.3.1 An unstable equilibrium solution Let’s return to Equation (17.1) and investigate the Jacobian at the origin \\((Y,N)=(0,0)\\). The Jacobian matrix at that solution is the following: \\[\\begin{equation} J_{(0,0)} =\\begin{pmatrix} .6 &amp; 0 \\\\0 &amp; .2 \\end{pmatrix} \\tag{17.9} \\end{equation}\\] (You should verify this on your own). This Jacobian matrix leads to an uncoupled system of linear equations where \\(Y&#39; = 0.2Y\\) and \\(N&#39;=0.6N\\). In this case, both \\(Y\\) and \\(N\\) are growing exponentially. This type of behavior supports the idea that both \\(Y\\) and \\(N\\) are growing away from the origin, as indicated in Figure 17.1. The Jacobian matrix helps confirm some of our intuition from examining the phase plane of a system of differential equations. At the heart of the Jacobian matrix is that idea that we can understand the dynamics of a nonlinear system through examining a closely related linear system. Chapter 18 will continue to build on this idea, giving you a tool to quantitatively analyze the stability of an equilibrium solution. 17.4 Exercises Exercise 17.1 Using algebra, show that the 4 equilibrium solutions to Equation (17.1) are \\((Y,N)=(0,0)\\), \\((Y,N)=(13,0)\\), \\((Y,N)=(0,6)\\), and \\((Y,N)=(5,4)\\) Hint: Perhaps first determine the nullclines for each solution. Exercise 17.2 Construct the Jacobian matrices for the equilibrium solutions \\((Y,N)=(13,0)\\) and \\((Y,N)=(0,6)\\) to Equation (17.1). Exercise 17.3 By solving directly, show that \\((H,L)=(0,0)\\) and \\((4,3)\\) are equilibrium solutions to the following system of equations: \\[\\begin{equation} \\begin{split} \\frac{dH}{dt} &amp;= .3 H - .1 HL \\\\ \\frac{dL}{dt} &amp;=.05HL -.2L \\end{split} \\end{equation}\\] Exercise 17.4 A system of two differential equations has a Jacobian matrix at the equilibrium solution \\((0,0)\\) as the following: \\[\\begin{equation} J_{(0,0)}=\\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{pmatrix} \\end{equation}\\] What would be a system of differential equations that would produce that Jacobian matrix? Exercise 17.5 Consider the following nonlinear system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y-1 \\\\ \\frac{dy}{dt} &amp;= x^{2}-1 \\end{split} \\end{equation}\\] Verify that this system has equilibrium solutions at \\((-1,1)\\) and \\((1,1)\\). Determine the linear system associated with the tangent plane approximation at the equilibrium solution \\((x,y)=(-1,1)\\) and \\((1,1)\\) (two separate linear systems). Construct the Jacobian matrix at the equilibrium solutions at \\((-1,1)\\) and \\((1,1)\\). With the Jacobian matrix, visualize a phase plane at these equilbrium solutions to estimate stability of the equilibrium solution. Exercise 17.6 (Inspired by Strogatz (2015)) Consider the following nonlinear system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y-x \\\\ \\frac{dy}{dt} &amp;=-y + \\frac{5x^2}{4+x^{2}} \\end{split} \\end{equation}\\] Verify that the point \\((x,y)=(1,1)\\) is an equilibrium solution. Determine the linear system associated with the tangent plane approximation at the equilibrium solution \\((x,y)=(1,1)\\). Construct the Jacobian matrix at this equilibrium solution. With the Jacobian matrix, visualize a phase plane at that equilbrium solution to estimate stability of the equilibrium solution. Exercise 17.7 Consider the following system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y^{2} \\\\ \\frac{dy}{dt} &amp;= - x \\end{split} \\end{equation}\\] Determine the equilibrium solution. Visualize a phase plane of this system of differential equations. Construct the Jacobian at the equilibrium solution. Use the fact that \\(\\displaystyle \\frac{dy}{dt} / \\frac{dx}{dt} = \\frac{dy}{dx}\\), which should yield a separable differential equation that will allow you to solve for a function \\(y(x)\\). Plot several solutions of \\(y(x)\\). How does that solution compare to the phase plane from the Jacobian matrix? Exercise 17.8 The Van der Pol Equation is a second-order differential equation used to study radio circuits. In Chapter 16 you showed that the differential equations \\(x&#39;&#39; + \\mu \\cdot (x^{2}-1) x&#39; + x = 0\\), where \\(\\mu\\) is a parameter can be written as a system of equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y \\\\ \\frac{dy}{dt} &amp;= -x-\\mu (x^{2}-1)y \\end{split} \\end{equation}\\] Determine the general Jacobian matrix \\(J_{(x,y)}\\) for this system of equations. The point \\((0,0)\\) is an equilibrium solution. Evaluate the Jacobian matrix at the point \\((0,0)\\). Your Jacobian matrix will depend on \\(\\mu\\). Evaluate your Jacobian matrix at the \\((0,0)\\) equilibrium solution for different values of \\(\\mu\\) ranging from \\(-3\\), \\(-1\\), 0, 1, 3. Make a phase plane for the Jacobian matrices at each of the values of \\(\\mu\\). Based on the phase planes that you generate, evaluate the stability of the equilibrium solution as \\(\\mu\\) changes. Exercise 17.9 (Inspired by Logan and Wolesensky (2009)) A population of fish \\(F\\) has natural predators \\(P\\). A model that describes this interaction is the following: \\[\\begin{equation} \\begin{split} \\frac{dF}{dt} &amp;= F - .3FP \\\\ \\frac{dP}{dt} &amp;= .5FP - P \\end{split} \\end{equation}\\] What are the equilibrium solutions for this differential equation? Construct a Jacobian matrix for each of the equilibrium solutions. Based on the phase plane from the Jacobian matrices, evaluate the stability of the equilibrium solutions. Exercise 17.10 (Inspired by Pastor (2008)) The amount of nutrients (such as carbon) in soil organic matter is represented by \\(N\\), whereas the amount of inorganic nutrients in soil is represented by \\(I\\). A system of differential equations that describes the turnover of inorganic and organic nutrients is the following: \\[\\begin{equation} \\begin{split} \\frac{dN}{dt} &amp;= L + kdI - \\mu N I - \\delta N \\\\ \\frac{dI}{dt} &amp;= \\mu N I - k d I - \\delta I \\end{split} \\end{equation}\\] Verify that \\(\\displaystyle N = \\frac{L}{\\delta}, \\; I = 0\\) and \\(\\displaystyle N = \\frac{kd+\\delta}{\\mu}, \\; I = \\frac{L \\mu - \\delta k d - \\delta^{2}}{\\mu \\delta}\\) are equilibrium solutions for this system. Construct a Jacobian matrix for each of the equilibrium solutions. Exercise 17.11 (Inspired by Logan and Wolesensky (2009) &amp; Kermack, McKendrick, and Walker (1927)) A model for the spread of a disease where people recover is given by the following differential equation: \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;= -\\alpha SI \\\\ \\frac{dI}{dt} &amp;= \\alpha SI - \\gamma I \\\\ \\frac{dR}{dt} &amp;= \\gamma I \\end{split} \\end{equation}\\] Assume this population has \\(N=1000\\) people. Determine the equilibrium solutions for this system of equations. Construct the Jacobian for each of the equilibrium solutions. Let \\(\\alpha=0.001\\) and \\(\\gamma = 0.2\\). With the Jacobian matrix, generate the phase plane (using the equations for \\(\\displaystyle \\frac{dS}{dt}\\) and \\(\\displaystyle \\frac{dI}{dt}\\) only) for all of the equilibrium solutions and classify their stability. FIGURE 17.3: Glucose transporter reaction schemes. Exercise 17.12 (Inspired by J. Keener and Sneyd (2009)) The chemical glucose is transported across the cell membrane using carrier proteins. These proteins can have different states (open or closed) that can be bound to a glucose substrate. The schematic for this reaction is shown in Figure 17.3. The system of differential equations describing this reaction is: \\[\\begin{equation} \\begin{split} \\frac{dp_{i}}{dt} &amp;= k p_{e} - k p_{i} + k_{+} s_{i}c_{i}-k_{i}p_{i} \\\\ \\frac{dp_{e}}{dt} &amp;= k p_{i} - k p_{e} + k_{+} s_{e}c_{e}-k_{-}p_{e} \\\\ \\frac{dc_{i}}{dt} &amp;= k c_{e} - k c_{i} + k_{-} p_{i}-k_{+}s_{i}c_{i} \\\\ \\frac{dc_{e}}{dt} &amp;= k c_{i} - k c_{e} + k_{-} p_{e}-k_{+}s_{e}c_{e} \\end{split} \\end{equation}\\] We can reduce this to a system of three equations. First show that \\(\\displaystyle \\frac{dp_{i}}{dt} + \\frac{dp_{e}}{dt}+ \\frac{dc_{i}}{dt} + \\frac{dc_{e}}{dt} = 0\\). Given that \\(p_{i}+p_{e}+c_{i}+c_{e}=C_{0}\\), where \\(C_{0}\\) is constant, use this equation to eliminate \\(p_{i}\\) and write down a system of three equations. Determine the equilibrium solutions for this new system of three equations. Construct the Jacobian matrix for each of these equilibrium solutions. References "],["eigenvalues-18.html", "Chapter 18 What are Eigenvalues? 18.1 Introduction 18.2 Straight line solutions 18.3 Computing eigenvalues and eigenvectors 18.4 What do eigenvalues tell us? 18.5 Concluding thoughts 18.6 Exercises", " Chapter 18 What are Eigenvalues? 18.1 Introduction This chapter focuses on developing a tool to understand the stability of an equilibrium solution. This tool is determining eigenvalues and eigenvectors. We connect eigenvectors and eigenvalues back to straight-line solutions introduced in Chapter 15. You will see how eigenvalues are determined is through solving a polynomial equation. Finally we investigate how the values of the eigenvalues are reflected in the directions of the arrows in the phase plane. There is a lot here to unpack, so let’s get started! 18.2 Straight line solutions Consider this following linear system of equations: In Chapter 15 we identified two straight line solutions: Solution 1: \\(\\displaystyle \\vec{s}_{1}(t) =e^{4t} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\\) Solution 2: \\(\\displaystyle \\vec{s}_{2}(t) = e^{3t} \\begin{pmatrix} -1 \\\\ 1\\end{pmatrix}\\) Let’s verify that Solution 2 is indeed a solution to this linear system. First we will take the derivative of Solution 2: \\[\\begin{equation} \\frac{d}{dt} \\left( \\vec{s}_{2}(t) \\right) = 3 e^{3t} \\begin{pmatrix} -1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix} -3e^{3t} \\\\ 3e^{3t} \\end{pmatrix} \\end{equation}\\] Let’s compare this solution to the right hand side of the differential equation, recognizing that the \\(x\\) component of \\(\\vec{s}_{2}(t)\\) is \\(-e^{3t}\\) and the \\(y\\) component of \\(\\vec{s}_{2}(t)\\) is \\(e^{3t}\\): \\[\\begin{equation} \\begin{split} 2x-y &amp;= -2e^{3t}-e^{3t} = -3e^{3t} \\\\ 2x+5y &amp;= -2e^{3t}+5e^{3t} = 3e^{3t} \\end{split} \\end{equation}\\] So, indeed \\(\\vec{s}_{2}(t)\\) is a solution to the differential equation. However something interesting is occurring. Notice how \\(\\displaystyle \\frac{d}{dt} \\left( \\vec{s}_{2}(t) \\right)\\) equals \\(3 \\vec{s}_{2}(t)\\), which was the same as the right hand side of the differential equation. While we wrote the right hand side of Equation (18.1) component by component, we could also write it as \\(A \\vec{x}\\), where \\(\\displaystyle A = \\begin{pmatrix} 2 &amp; -1 \\\\ 2 &amp; 5 \\end{pmatrix}\\). Because we verified \\(\\vec{s}_{2}(t)\\) was a solution to the differential equation, we could also have said that \\(A \\vec{s}_{2}(t) = 3 \\vec{s}_{2}(t)\\). So we have two interesting facts here: A straight line solution to a system of linear differential equations \\((\\displaystyle \\frac{d \\vec{x}}{dt} = A \\vec{x}\\)) has the form \\(\\vec{s}(t) = c_{1} e^{\\lambda t} \\; \\vec{v}\\), where \\(c_{1}\\) is a constant and \\(\\vec{v}\\) a constant vector. Differentiating \\(\\vec{s}(t)\\) yields \\(\\displaystyle \\frac{d}{dt} \\left( \\vec{s}(t) \\right) = \\lambda \\vec{s}(t)\\). Consequently \\(\\lambda \\vec{s}(t)=A \\vec{s}(t)\\) in order for \\(\\vec{s}(t)\\) to be a solution. All of these facts (in particular \\(\\lambda \\vec{s}(t)=A \\vec{s}(t)\\)) set up an interesting equation: \\(\\displaystyle \\lambda c_{1} e^{\\lambda t}\\vec{v} = c_{1} e^{\\lambda t} A \\vec{v}\\). Applying concepts from linear algebra, in order for the solution \\(\\vec{s}(t)\\) to be consistent, \\(A \\vec{v} - \\lambda I \\vec{v} = \\vec{0}\\), where \\(\\vec{0}\\) is a vector of all zeros and \\(I\\) is called the identity matrix, or a square matrix with ones along the diagonal and zero everywhere else. The goal is to find a \\(\\lambda\\) and \\(\\vec{v}\\) consistent with this equation. Let’s apply some terminology here. For these special straight line solutions, we give a particular name to \\(\\vec{v}\\) - we call it the eigenvector. The name we give to \\(\\lambda\\) is the eigenvalue. (Eigen means own in German - get it?) So how do we determine an eigenvalue or eigenvector? We do this by first determining the eigenvalues \\(\\lambda\\). This is done by solving the equation \\(\\det (A - \\lambda I ) =0\\) for \\(\\lambda\\), where \\(\\det(M)\\) is the determinant. Once the eigenvalues are found, we then compute the eigenvectors by solving the equation \\(A \\vec{v} - \\lambda \\vec{v} = \\vec{0}\\). Let’s take a time out. I recognize that we are starting to get deeper into linear algebra which may be some unfamiliar concepts. However we will just highlight key results that we will need - so hopefully that will give you a leg up when you study linear algebra - it is a great topic! Let’s get to work. 18.3 Computing eigenvalues and eigenvectors Let’s dig into understanding the equation \\(\\det (A - \\lambda I ) =0\\) for a two-linear system of differential equations. In this case, \\(A\\) is the matrix \\(\\displaystyle \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\), for which then \\(A - \\lambda I\\) is the following matrix: \\\\tag{18.2}n} (#eq:2by2-ch18) A - \\lambda I=\\begin{pmatrix} a - \\lambda &amp; b \\\\ c &amp; d-\\lambda \\end{pmatrix} \\end{equation}\\] The determinant of a 2 \\(\\times\\) 2 matrix is formed by the product of the diagonal entries less the product of the off-diagonal entries. For Equation (18.2), \\(\\det(A-\\lambda I)=0\\) is the equation \\((a-\\lambda)(d-\\lambda)-bc=0\\). Our goal is to solve this equation for \\(\\lambda\\), which are the eigenvalues for this system. Example 18.1 Compute the eigenvalues for the matrix \\(\\displaystyle A = \\begin{pmatrix} -1 &amp; 1 \\\\ 0 &amp; 3 \\end{pmatrix}\\). Solution. The matrix \\(A-\\lambda I\\) is \\(\\displaystyle A-\\lambda I = \\begin{pmatrix} -1-\\lambda &amp; 1 \\\\ 0 &amp; 3-\\lambda \\end{pmatrix}\\). So we have: \\[\\begin{equation} \\det(A-\\lambda I) = (-1-\\lambda)(3-\\lambda) - 0 = 0 \\end{equation}\\] Solving the equation \\((-1-\\lambda)(3-\\lambda)=0\\) yields two eigenvalues: \\(\\lambda = -1\\) or \\(\\lambda = 3\\). More generally the equation \\(\\det(A - \\lambda I)\\) yields a polynomial equation in \\(\\lambda\\). We call this equation the characteristic polynomial and denote it by \\(f(\\lambda)\\). In the case of a two-dimensional system of equations, \\(f(\\lambda)\\) will be a quadratic equation (see Exercise 18.9). Once we have determined the eigenvalues, we next compute the eigenvectors associated with each eigenvalue. Remember that an eigenvector is a vector \\(\\vec{v}\\) consistent with \\(A \\vec{v} = \\lambda \\vec{v}\\) or \\(A \\vec{v} - \\lambda \\vec{v} =\\vec{0}\\). How we do this is through algebra, as is done in the following example: Example 18.2 Compute the eigenvectors for the matrix \\(\\displaystyle A = \\begin{pmatrix} -1 &amp; 1 \\\\ 0 &amp; 3 \\end{pmatrix}\\) from Example 18.1. Solution. First for general \\(\\lambda\\), consider the expression \\(A \\vec{v} - \\lambda \\vec{v} =\\vec{0}\\), where \\(\\displaystyle \\vec{v} = \\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix}\\): \\[\\begin{equation} A \\vec{v} - \\lambda \\vec{v} = \\vec{0} \\rightarrow \\begin{pmatrix} -v_{1} +v_{2} - \\lambda v_{1} = 0 \\\\ 3v_{2} - \\lambda v_{2} =0 \\end{pmatrix} \\tag{18.3} \\end{equation}\\] We use the two expressions (\\(-v_{1} +v_{2} - \\lambda v_{1} = 0\\) and \\(3v_{2} - \\lambda v_{2} =0\\)) in Equation (18.3) to determine the eigenvector \\(\\vec{v}\\). We need to consider both eigenvalues (\\(\\lambda = -1\\) and \\(\\lambda =3\\)) separately to yield two different eigenvectors: - Case 1 \\(\\lambda = -1\\): The first expression in Equation (18.3) yields \\(-v_{1} +v_{2} + v_{1} = 0\\), or \\(v_{2}=0\\) after simplifying. For the second expression we have \\(3v_{2} + v_{2} =0 \\rightarrow 4v_{2} = 0\\), so that tells us again that \\(v_{2}=0\\). Notice we’ve determined a value for the second component \\(v_{2}\\), but not \\(v_{1}\\). In this case, we say that the first component to the vector \\(\\vec{v}\\) is a free parameter., so the eigenvector can be expressed as \\(\\displaystyle \\begin{pmatrix} c_{1} \\\\ 0 \\end{pmatrix}\\), where \\(c_{1}\\) is the free parameter. Another way to express this eigenvector is \\(\\displaystyle c_{1} u_{1}\\), with \\(u_{1}=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\). The eigenvector in this case is \\(\\displaystyle \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\). (We generally write eigenvectors without the arbitrary constants.) This particular straight line solution is \\(\\displaystyle s_{1}(t)=c_{1}e^{-t}u_{1}\\), where \\(c_{1}\\) is a free variable.   - Case 2 \\(\\lambda = 3\\): For the second equation we have \\(3v_{2} - 3v_{2}=0\\), which is always true. However in the first equation we have \\(- v_{1} + v_{2} - 3v_{1} = 0\\), or \\(v_{2} = 4v_{1}\\). In this case, \\(v_{1}\\) can be a free parameter; however, \\(v_{2}\\) will have to be four times the value of \\(v_{1}\\). Hence, this particular straight line solution is \\(\\displaystyle s_{2}(t)=e^{3t} \\begin{pmatrix} c_{2} \\\\ 4 c_{2} \\end{pmatrix}\\), or also as \\(\\displaystyle s_{2}(t)=c_{2} e^{3t} \\vec{u}_{2}\\), with \\(\\displaystyle \\vec{u}_{2}= \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\\). The eigenvector in this case is \\(\\displaystyle \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\\). | Notice that in both of our cases we had a free variable (\\(c_{1}\\) or \\(c_{2}\\)), which are also constants in our final solution for the differential equation. Once we have computed the eigenvalues and eigenvectors, we are now ready to express the most general solution for a system of differential equations. For a two-dimensional system of linear differential equations (\\(\\displaystyle \\frac{d}{dt} \\vec{x} = A \\vec{x}\\)), the most general solution is given by Equation (18.4): \\[\\begin{equation} \\vec{x}(t) = c_{1} e^{\\lambda_{1}t} \\vec{v}_{1} + c_{2} e^{\\lambda_{2}t} \\vec{v}_{2} \\tag{18.4} \\end{equation}\\] Example 18.3 What is the solution to the differential equation \\(\\displaystyle \\frac{d}{dt} \\vec{x} = \\begin{pmatrix} -1 &amp; 1 \\\\ 0 &amp; 3 \\end{pmatrix} \\vec{x}\\)? Solution. Since we have already computed the eigenvalues and eigenvectors, our most general solution for this differential equation is: \\[\\begin{equation*} \\vec{x} = c_{1} e^{- t} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_{2} e^{3t} \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}, \\end{equation*}\\] with \\(c_{1}\\) and \\(c_{2}\\) defined as constants. 18.3.1 Computing eigenvalues with demodelr While computing eigenvalues and eigenvectors is a good algebraic exercise, we can also program this in R using the function eigenvalues from the demodelr package. The syntax works where \\(\\displaystyle A = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\) is entered in as eigenvalues(a,b,c,d,matrix_rows) where matrix_rows is the number of rows.42 What gets returned from the function will be the eigenvalues and eigenvectors for any square matrix. Let’s compute the eigenvalues for the matrix \\(\\displaystyle \\begin{pmatrix} -1 &amp; 1 \\\\ 0 &amp; 3 \\end{pmatrix}\\): # For a two-dimensional equation the code assumes # the default is a 2 by 2 matrix. demodelr::eigenvalues(matrix_entries = c(-1, 1, 0, 3), matrix_rows = 2) ## eigen() decomposition ## $values ## [1] 3 -1 ## ## $vectors ## X1 X2 ## 1 0.2425356 1 ## 2 0.9701425 0 Notice that the eigenvalues and the eigenvectors get returned with the eigenvalues function. How you read the output for the eigenvector is that X1 is the eigenvector associated with the first eigenvalue (\\(\\lambda = 3\\)) and X2 is the eigenvector associated with the second eigenvalue (\\(\\lambda = -1\\)). The eigenvector associated with \\(\\lambda=3\\) is a little different from what we computed - R will normalize the vector, which means that its total length will be one.43 For example \\(\\displaystyle \\vec{v}_{2} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\\), so \\(||\\vec{v}|| = \\sqrt{5}\\), and the normalized vector is \\(\\displaystyle \\begin{pmatrix} 1/\\sqrt{5} \\\\ 4/\\sqrt{5} \\end{pmatrix}\\), which you can verify is the same as the reported eigenvector from the eigenvalues function. 18.4 What do eigenvalues tell us? Here the focus of the chapter changes a little bit. Now we focus on understanding how the phase plane for a differential equation gives clues about the stability for an equilibrium solution. This is intentional: once we have found the eigenvalues, determining eigenvectors can seem rather mundane at times (and perhaps heavy on the algebra). Studying the eigenvalues helps us understand the qualitative nature of the solution to a differential equation. Let’s think about the characteristic equation \\(f(\\lambda)\\) for a two-dimensional system of differential equations: \\[\\begin{equation} \\begin{split} f(\\lambda) &amp;= \\det(A - \\lambda I) \\\\ &amp;= (a - \\lambda)(d-\\lambda)-bc \\\\ &amp;= \\lambda^{2} - (a+d) \\lambda + ad-bc \\end{split} \\tag{18.5} \\end{equation}\\] Notice how \\(f(\\lambda)\\) is a quadratic equation. You may recall that quadratic equations have zero, one, or two distinct solutions. If there are no solutions, we say the solutions are imaginary (more on that later). Also the signs of solutions may be positive or negative. There are so many different combinations! What types of phase planes do all those different types of eigenvalues produce? The following examine representative examples of all the possible eigenvalues you may obtain for a two-dimensional linear system of differential equations (which can be generalized to higher-dimensional systems). 18.4.1 Sources: all eigenvalues positive Consider the differential equation in Equation (18.6): \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} =\\begin{pmatrix} 2 &amp; 0 \\\\ 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\tag{18.6} \\end{equation}\\] Computing the eigenvalues for Equation (18.6) shows that they are both positive: eigenvalues(c(2, 0, 1, 1)) ## eigen() decomposition ## $values ## [1] 2 1 ## ## $vectors ## X1 X2 ## 1 0.7071068 0 ## 2 0.7071068 1 The phase plane for this matrix \\(A\\) is shown in Figure 18.1: FIGURE 18.1: Phase plane for Equation (18.6), which shows the equilibrium solution is a source (also known as an unstable node). Notice how the phase plane in Figure 18.1 has all the arrows pointing from the origin. In this case we call the origin equilibrium solution a source, or also an unstable node. Plotting the components of \\(\\vec{x}(t)\\) as functions of \\(t\\) would show the dependent values increase exponentially as time increases. 18.4.2 Sinks: all eigenvalues negative Consider the differential equation in Equation (18.7), which is a slight modification from Equation (18.6): \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} =\\begin{pmatrix} -2 &amp; 0 \\\\ 1 &amp; -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\tag{18.7} \\end{equation}\\] The eigenvalues for Equation (18.7) are both negative (verify this on your own). The resulting phase plane for Equation (18.7) then has all the arrows pointing towards the origin, shown in Figure 18.2. FIGURE 18.2: Phase plane for Equation (18.7), which shows the equilibrium solution is a sink (also known as a stable node). Based on the phase plane in Figure 18.2, solutions to Equation (18.7) would asymptotically approach the origin. We say the equilibrium solution is a sink, also known as a stable node. 18.4.3 Saddle nodes: one positive and one negative eigenvalue Consider the differential equation in Equation (18.8): \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} =\\begin{pmatrix} 3 &amp; -2 \\\\ 1 &amp; -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\tag{18.8} \\end{equation}\\] Equation (18.8) has \\(\\lambda_{1}=2.414\\) and \\(\\lambda_{2}=-0.414\\) (verify this on your own). Because the differential equation has one positive and one negative eigenvalue the equilibrium solution the phase plane for this differential equation looks a little different, as is shown in Figure 18.3: FIGURE 18.3: Phase plane for Equation (18.8), which shows the equilibrium solution is a saddle node. This equilibrium solution is called a saddle node. From the horizontal direction, the arrows point away from the origin, but in the vertical direction the arrows point towards the origin. This behavior is caused by the opposing signs of the eigenvalues - one part of the solution in Equation (18.4) (the one associated with the negative eigenvalue) decays asymptotically to zero. The other positive eigenvalue is associated with the asymptotically unstable, giving the solution trajectory the shape of a saddle. 18.4.4 Spirals: imaginary eigenvalues Consider the differential equation in Equation (18.9): \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} =\\begin{pmatrix} -3 &amp; -8 \\\\ 4 &amp; -6 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\tag{18.9} \\end{equation}\\] There are two eigenvalues to this system: \\(\\lambda = -4.5+5.45i\\) and \\(\\lambda = -4.5 - 5.45i\\) (you can confirm this on your own). In this case the \\(i\\) means the eigenvalues are imaginary. Notice how the eigenvalues are similar, but the signs on the second term differ. We say the eigenvalues are complex conjugates of each other, and write them in the form \\(\\lambda = \\alpha \\pm \\beta i\\). In this example \\(\\alpha = -4.5\\) and \\(\\beta = 5.45\\). Figure 18.4 shows the phase plane for this system. FIGURE 18.4: Phase plane for Equation (18.9), which shows the equilibrium solution is a spiral sink. The phase plane in Figure 18.4 has some spiraling motion to it. Why does that occur? Imaginary eigenvalues can occur when the characteristic equation \\(\\det(A-\\lambda I)=0\\) has imaginary solutions. More generally, we say \\(\\lambda = \\alpha \\pm \\beta i\\). Because the eigenvalues are complex, we would also expect the eigenvectors to be complex as well (i.e. \\(\\vec{v} \\pm i \\vec{w}\\)). Don’t let the term imaginary fool you: by using properties from complex analysis it can be shown that when eigenvalues are imaginary, the template for the solution is given in Equation (18.10): \\[\\begin{equation} \\vec{x}(t) = c_{1} e^{\\alpha t} ( \\vec{w} \\cos (\\beta t) - \\vec{v} \\sin (\\beta t)) + c_{2} e^{\\alpha t} ( \\vec{w} \\cos (\\beta t) + \\vec{v} \\sin (\\beta t)) \\tag{18.10} \\end{equation}\\] The trigonometric terms in Equation (18.10) suggest that the solution has some periodic behavior if we plot the components of \\(\\vec{x}(t)\\) as functions of \\(t\\). But when we plot the solution in the \\(xy\\) plane that periodic behavior gets translated to spiraling motion in Figure 18.4. When \\(\\alpha &lt; 0\\) we say the equilibrium solution is a spiral sink because the exponential terms in Equation (18.10) decay asymptotically to zero. As you would expect when \\(\\alpha &gt; 0\\) we classify a phase plane as a spiral source (shown in Equation (18.11) and Figure 18.5). \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} =\\begin{pmatrix} 4 &amp; -5 \\\\ 3 &amp; 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\tag{18.11} \\end{equation}\\] (). FIGURE 18.5: Phase plane for Equation (18.11), which shows the equilibrium solution is a spiral source. The final case for imaginary eigenvalues is when \\(\\alpha = 0\\), which is termed a center. As an example, let’s examine the phase plane for the system in Equation (18.12): \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} =\\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\tag{18.12} \\end{equation}\\] FIGURE 18.6: Phase plane for Equation (18.12), which shows the equilibrium solution is a center. Notice how the phase plane arrows in Figure 18.6 neither spin in nor out, but seem to point in a circle. In fact, the formula of the solution to this equation can be represented as a circle. The solution to this differential equation is a circle, which is found with \\(\\alpha = 0\\) in Equation (18.10). 18.4.5 Repeated eigenvalues For repeated eigenvalues (where \\(\\lambda_{1}=\\lambda_{2}\\)) the stability of the solution still depends on the sign of the sole eigenvalue \\(\\lambda\\). However in this case the form of the solution changes as shown in Equation (18.13): \\[\\begin{equation} \\vec{x}(t) = \\left( c_{1} \\vec{v}_{1} + c_{2} \\vec{v}_{2} \\right) e^{\\lambda t} + c_{2} \\vec{v}_{1} t e^{\\lambda t} \\tag{18.13} \\end{equation}\\] 18.5 Concluding thoughts As you can see there is a lot of interesting behavior with eigenvalues and eigenvectors! But in all cases, stability of the equilibrium solution really focuses on the eigenvalues and their relative (positive or negative) sign. How the straight line solutions approach the equilibrium solution is a function of the eigenvectors. 18.6 Exercises Exercise 18.1 What are the characteristic equations for the following systems of differential equations? \\(\\displaystyle \\frac{dx}{dt} = 4x, \\; \\frac{dy}{dt} = -y\\) \\(\\displaystyle \\frac{dx}{dt} = x+y, \\; \\frac{dy}{dt} = x-y\\) \\(\\displaystyle \\frac{dx}{dt} = 9x +15y, \\; \\frac{dy}{dt} = 7x + 2y\\) Exercise 18.2 Verify that \\(\\displaystyle s_{1}(t) = \\begin{pmatrix} e^{3t} \\\\ -e^{3t} \\end{pmatrix}\\) is a solution to the system of differential equations \\(\\displaystyle \\frac{dx}{dt} = 2x-y\\) and \\(\\displaystyle \\frac{dy}{dt} = 2x+5y\\). Exercise 18.3 The matrix \\(\\displaystyle \\begin{pmatrix} 2 &amp; -1 \\\\ 2 &amp; 5 \\end{pmatrix}\\) has \\(\\lambda = 4\\) as an eigenvalue. Use this information to calculate (by hand) the eigenvector \\(\\vec{v}\\) associated with this eigenvalue. Exercise 18.4 Compute the eigenvalues and eigenvectors for the following linear systems. Based on the eigenvalues, classify if the equilibrium solution is stable or unstable. Finally write down the most general solution for the system of equations. \\(\\displaystyle \\frac{dx}{dt} = 2x-6y, \\; \\frac{dy}{dt} = x-2y\\) \\(\\displaystyle \\frac{dx}{dt} = 9x-22y, \\; \\frac{dy}{dt} = 3x-7y\\) \\(\\displaystyle \\frac{dx}{dt} = 4x - 2y, \\; \\frac{dy}{dt} = 2x - 2y\\) \\(\\displaystyle \\frac{dx}{dt}= 4x-15y, \\; \\frac{dy}{dt}=2x-7y\\) \\(\\displaystyle \\frac{dx}{dt} = 3x-18y, \\; \\frac{dy}{dt} = x-5y\\) \\(\\displaystyle \\frac{dx}{dt} = 5x-12y, \\; \\frac{dy}{dt} = x-2y\\) Exercise 18.5 Consider the following nonlinear system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y-x \\\\ \\frac{dy}{dt} &amp;=-y + \\frac{5x^2}{4+x^{2}} \\end{split} \\end{equation}\\] Previously you verified that \\((x,y)=(1,1)\\) is an equilibrium solution for this system. What is the Jacobian matrix at that equilibrium solution? Generate a phase plane for the Jacobian matrix. What are the eigenvalues for the Jacobian matrix at the equilbrium solution? Based on the eigenvalues, how would you classify the stability of the equilibrium solution? Exercise 18.6 Consider the following nonlinear system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= x-2y-xy \\\\ \\frac{dy}{dt} &amp;= -y+xy- 2xy^{3} \\end{split} \\end{equation}\\] Verify that \\((x,y)=(0,0)\\) is an equilibrium solution for this system. What is the Jacobian matrix at that equilibrium solution? Generate a phase plane for the Jacobian matrix. What are the eigenvalues for the Jacobian matrix at the equilbrium solution? Based on the eigenvalues, how would you classify the stability of the equilibrium solution? Exercise 18.7 Consider the following system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} &amp;= y^{2} \\\\ \\frac{dy}{dt} &amp;= - x \\end{split} \\end{equation}\\] There is one equilibrium solution to this system of equations. What is it? What is the Jacobian matrix for this equilibrium solution? Generate a phase plane for the Jacobian matrix. What are the eigenvalues for the Jacobian matrix at the equilbrium solution? Based on the eigenvalues, how would you classify the stability of the equilibrium solution? Exercise 18.8 Consider the general system of differential equations \\(\\displaystyle \\frac{d}{dt} \\vec{x} = A \\vec{x}\\). Given the function \\(\\vec{s}(t)=e^{\\lambda t} \\vec{v}\\), where \\(\\vec{v}\\) is a constant vector, what is an expression for \\(\\displaystyle \\frac{d}{dt} \\vec{s}(t)\\)? Given the function \\(\\vec{s}(t)=e^{\\lambda t} \\vec{v}\\), where \\(\\vec{v}\\) is a constant vector, what is an expression for \\(A \\; \\vec{s}(t)\\)? Now use the previous results in the expression \\(\\displaystyle \\frac{d}{dt} \\vec{s}(t) = A \\vec{s}(t)\\). Explain why it must be the case that \\(\\lambda \\vec{v} = A \\vec{v}\\) (assuming \\(\\vec{v} \\neq 0\\)). Exercise 18.9 In this chapter we learned that for a two-dimensional matrix \\(\\displaystyle A = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\), eigenvalues can be found by solving the characteristic equation \\(\\det(A-\\lambda I)=0\\), or \\(\\lambda^{2} - (a+d) \\lambda + (ad-bc) = 0\\). Use the quadratic formula to get an expression for the eigenvalues \\(\\lambda\\) in terms of \\(a\\),\\(b\\), \\(c\\), and \\(d\\). If you have a 2 by 2 matrix, you can leave out matrix_rows (so just eigenvalues(a,b,c,d)) as the default is a 2 by 2 matrix.↩︎ The length of a vector \\(\\vec{v}\\) is denoted as \\(||\\vec{v}||\\) and is computed the following way: \\(||\\vec{v}||=\\sqrt{v_{1}^{2}+v_{2}^{2}+...+v_{n}^{2}}\\). We normalize a vector to a length of 1 by dividing each component by its length.↩︎ "],["stability-19.html", "Chapter 19 Qualitative Stability Analysis 19.1 The characteristic polynomial (again) 19.2 Stability with the trace and determinant 19.3 A workflow for stability analysis 19.4 Stability for higher-order systems of differential equations. 19.5 Exercises", " Chapter 19 Qualitative Stability Analysis Chapter 18 introduced eigenvalues in order to classify equilibrium solutions for a linear system of differential equations. However let’s face an ugly truth: determining eigenvalues via the characteristic polynomial isn’t easy. Even with a two-dimensional system of equations you may resort to using the quadratic formula. Here’s the good news: this chapter will develop other tools that can circumvent finding roots of a polynomial. In order to do that, we will need to understand some general relationships about the characteristic polynomial for a two-dimensional system of linear differential equations. Let’s get started! 19.1 The characteristic polynomial (again) Consider the following two-dimensional linear system, where \\(a\\), \\(b\\), \\(c\\), and \\(d\\) can be any number: \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} = \\begin{pmatrix} ax+by \\\\ cx+dy \\end{pmatrix} = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\end{equation}\\] Recall that eigenvalues are found by solving \\(\\displaystyle \\det (A - \\lambda I ) =0\\); \\(\\displaystyle \\det (A - \\lambda I )\\) is computed in Equation (19.1). \\[\\begin{equation} \\det \\begin{pmatrix} a - \\lambda &amp; b \\\\ c &amp; d-\\lambda \\end{pmatrix} = (a-\\lambda)(d-\\lambda) - bc \\tag{19.1} \\end{equation}\\] If we multiply out Equation (19.1) we obtain the characteristic polynomial: \\[\\begin{equation} f(\\lambda)=\\lambda^{2} - (a+d) \\lambda + ad - bc \\tag{19.2} \\end{equation}\\] Notice how the terms of Equation (19.2) can be expressed as functions of the entries of the matrix \\(A\\) which are (\\(a\\), \\(b\\), \\(c\\), \\(d\\)). In fact, in linear algebra the term \\(a+d\\) is the sum of the diagonal entries, also known as the trace of a matrix, denoted as \\(\\mbox{tr}(A)\\). And you may recognize that \\(ad-bc\\) is the same as \\(\\det(A)\\). So our characteristic polynomial can be rewritten as solving Equation (19.4): \\[\\begin{equation} f(\\lambda)=\\lambda^{2} - \\mbox{tr}(A)\\lambda + \\det(A) \\tag{19.3} \\end{equation}\\] Example 19.1 Determine the characteristic polynomial \\(f(\\lambda)\\) for the system \\(\\vec{x}&#39;=Ax\\) where \\(\\displaystyle A= \\begin{pmatrix} -1 &amp; 1 \\\\ 0 &amp; 3 \\end{pmatrix}\\). Solve for the eigenvalues to classify the stability of the equilibrium solution. Solution. We can see that \\(\\det(A)= -1(3) - 0(1) = -3\\) and tr\\((A)=2\\), so our characteristic equation is \\(\\lambda^{2}-2\\lambda -3\\). If we solve \\(\\lambda^{2}-2\\lambda -3=0\\) we have \\((\\lambda-3)(\\lambda+1)=0\\), so our eigenvalues are \\(\\lambda=3\\) and \\(\\lambda=-1\\). Since one eigenvalue is positive and the other one is negative, the equilibrium solution is a saddle node. As shown in Example 19.1, Equation (19.3) may be a computationally easier way to determine eigenvalues. Here is another way to think about eigenvalues. Let’s say we have two eigenvalues \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\), so \\(f(\\lambda_{1})=0\\) and \\(f(\\lambda_{2})=0\\). However we can also examine Equation (19.4) in terms of the roots of \\(f(\\lambda)\\), denoted as \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\). We make no assumptions about whether \\(\\lambda_{1}\\) or \\(\\lambda_{2}\\) are real, imaginary, or equal in value. However, since they solve the equation \\(f(\\lambda)=0\\), this also means that \\((\\lambda-\\lambda_{1})(\\lambda-\\lambda_{2})=0\\). If we multiply out this equation we have \\(\\lambda^{2}-(\\lambda_{1}+\\lambda_{2}) \\lambda + \\lambda_{1} \\lambda_{2}=0\\). If we compare this equation with Equation (19.3) we have: \\[\\begin{equation} \\begin{split} f(\\lambda) &amp;=\\lambda^{2}-(\\lambda_{1}+\\lambda_{2}) \\lambda + \\lambda_{1} \\lambda_{2} \\\\ &amp;= \\lambda^{2} - \\mbox{tr}(A)\\lambda + \\det(A) \\end{split} \\tag{19.4} \\end{equation}\\] Equation (19.4) allows us to identify that \\(\\mbox{tr}(A) = \\lambda_{1}+\\lambda_{2}\\) and \\(\\det(A) = \\lambda_{1} \\lambda_{2}\\), or the trace of \\(A\\) is the sum of the two eigenvaules and the determinant of \\(A\\) is the product of the eigenvalues. Let’s explore this a little more. 19.2 Stability with the trace and determinant The equality in Equation (19.4) uncovers some neat relationships - in particular tr\\((A)=(\\lambda_{1}+\\lambda_{2})\\) and \\(\\det(A)=\\lambda_{1}+\\lambda_{2}\\). Table 19.1 synthesizes all these relationships to provide an alternative pathway to understand stability of an equilibrium solution with the trace and determinant: TABLE 19.1: Comparison of the stability of an equilibrium solution in relation to the signs of an eigenvalue, the trace of the matrix \\(A\\), or the determinant of the matrix \\(A\\). Sign of \\(\\lambda_{1}\\) Sign of \\(\\lambda_{2}\\) Tendency of equilibrium solution Sign of tr\\((A)=\\lambda_{1}+\\lambda_{2}\\) Sign of \\(\\det(A)=\\lambda_{1} \\cdot \\lambda_{2}\\) Positive Positive Source Positive Positive Negative Negative Sink Negative Positive Positive Negative Saddle ? Negative Negative Positive Saddle ? Negative For the moment we will only consider real non-zero values of the eigenvalues - more specialized cases will occur later. But examining the above table carefully: If \\(\\det(A)\\) is negative, then the equilibrium solution is a saddle. If \\(\\det(A)\\) is positive and tr\\((A)\\) is negative, then the equilibrium solution is a sink. If \\(\\det(A)\\) and tr\\((A)\\) are both positive, then the equilibrium solution is a source. Example 19.2 Use the trace and determinant relationships to classify the stability of the equilibrium solution for the linear system \\(\\vec{x}&#39;=A\\vec{x}\\) where \\(\\displaystyle A= \\begin{pmatrix} -1 &amp; 1 \\\\ 0 &amp; 3 \\end{pmatrix}\\). Solution. We can see that \\(\\det(A)= -1(3) - 0(1) = -3\\) and tr\\((A)=2\\). Since the determinant is negative, the equilibrium solution must be a saddle node. Knowing the relationships between the trace and determinant for a two-dimensional system of equations is a pretty quick and easy way to investigate stability of equilibrium solutions! Another way to graphically represent the stability of solutions is with the trace-determinant plane (shown in Figure 19.1), with tr\\((A)\\) on the horizontal axis and det\\((A)\\) on the vertical axis: FIGURE 19.1: The trace-determinant plane, illustrating stability of an equilibrium solution based the values of the trace and determinant. See also Table 19.1. While Figure 19.1 only determines if an equilibrium solution is a sink, source, or saddle node, it can be extended further to include spiral sinks and spiral nodes. Here’s how: first we will apply the quadratic formula to Equation (19.4) to solve directly for the eigenvalues as a function of the trace and determinant: \\[\\begin{equation} \\lambda_{1,2}= \\frac{\\mbox{tr}(A)}{2} \\pm \\frac{\\sqrt{ (\\mbox{tr}(A))^2-4 \\det(A)}}{2} \\tag{19.5} \\end{equation}\\] While Equation (19.5) seems like a more complicated expression, it can be shown to be consistent with our above work. Imaginary eigenvalues can be a spiral source or sink depending on their location in the trace-determinant plane (Figure 19.2). The dividing curve is setting the discriminant of Equation (19.5) to 0, which yields the quadratic equation \\(\\displaystyle \\det(A) = \\frac{\\mbox{tr}(A))^2}{4}\\) (blue dashed curve in Figure 19.2). When \\(\\displaystyle 0 &lt; \\det(A)&lt;\\frac{\\mbox{tr}(A))^2}{4}\\), then the solution is a sink or a source depending on the sign of tr\\((A)\\). Likewise, when \\(\\displaystyle 0 &lt; \\frac{\\mbox{tr}(A))^2}{4} &lt;\\det(A)\\), then the solution is a spiral sink or a spiral source depending on the sign of tr\\((A)\\). FIGURE 19.2: Revised trace-determinant plane, illustrating all possible cases for classification of the stability of an equilibrium solution based on the values of the trace and determinant. See also Table 19.1. The one case that we haven’t considered in our stability table is a center equilibrium. For this equilibrium solution, the eigenvalues (\\(\\lambda_{1,2}\\)) equal \\(\\pm \\beta i\\). Additionally, a center equilibrium occurs when the value of tr\\((A)\\) is exactly zero and \\(\\det(A)\\) is positive (Exercise 19.10). Example 19.3 Use the trace and determinant relationships to classify the stability of the equilibrium solution for the linear system of differential equations: \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} = \\begin{pmatrix} -x+y \\\\ x-4y \\end{pmatrix} \\end{equation}\\] Solution. The matrix for this system of differential equations is \\(\\displaystyle A= \\begin{pmatrix} -1&amp; 1 \\\\ 1 &amp; -4 \\end{pmatrix}\\). This means that tr\\((A)=-5\\) and \\(\\det(A)=3\\). Since (tr\\((A)\\))\\(^{2}/4=25/4\\), which is greater than \\(\\det(A)\\), this equilibrium solution is a sink. You can verify on your own that the equilibrium solution is a sink by generating this system’s phase plane. As you can see, the trace-determinant plane (Figure 19.2) is a quick way to analyze stability of an equilibrium solution that does not require heavy algebraic analysis. 19.3 A workflow for stability analysis Chapters 15 to this one have covered a lot of ground, so perhaps it is prudent to summarize a workflow for stability analysis for a system of differential equations \\(\\displaystyle \\frac{d}{dt} \\vec{x} = f(\\vec{x},\\vec{\\alpha})\\), where \\(\\vec{\\alpha}\\) is a vector of parameters: Determine nullclines by solving \\(f(\\vec{x},\\vec{\\alpha})=0\\). Equilibrium solutions occur where all the distinct nullclines intersect. Construct the Jacobian matrix for each equilibrium solution. Analyze stability of the equilbrium solution by computing eigenvalues of the Jacobian matrix (or use the methods in this chapter). You can bypass the first few steps if the system is already linear (\\(\\displaystyle \\frac{d}{dt} \\vec{x} = A \\vec{x}\\)). You can summarize this workflow with Nullclines \\(\\rightarrow\\) Jacobian \\(\\rightarrow\\) Eigenvalues. There are a lot of separate pieces to analyze stability of a differential equation - but being systematic and careful with your approach helps. 19.4 Stability for higher-order systems of differential equations. The trace-determinant plane is a really useful approach to analyze stability of an equilibrium solution. However, one huge caveat is that the methods outlined in this chapter only apply with a two-dimensional system of differential equations. The stability of an equilibrium solution depends on the signs of the eigenvalues, which are also a function of roots of the characteristic polynomial \\(f(\\lambda)\\). For higher-order systems the Routh-Hurwitz stability criterion can help determine stability, but computational complexity increases with higher-order systems. Another apporach with several differential equations may be to apply equilibrium analyses to reduce the system to two differential equations. (See J. Keener and Sneyd (2009) for many examples related to human physiology.) In the final analysis, there are deep connections between eigenvalues and the structure of the matrix \\(A\\) (whether or not it arises from a linear system of differential equations or a Jacobian matrix). Examining stability of an equilibrium solution as it depends on an unspecified parameter is called a bifuraction analysis, which we will study in Chapter 20). There is more to this story, so let’s forge ahead! 19.5 Exercises Exercise 19.1 Compute the trace and determinant for each of these systems of differential equations. Use the trace-determinant condition to classify the stability of the equilibrium solutions. Verify your stability results are consistent when analyzing stability by calculating the eigenvalues. \\(\\displaystyle x&#39; = 2x-6y, \\; y&#39; = x-2y\\) \\(\\displaystyle x&#39; = 9x-22y, \\; y&#39; = 3x-7y\\) \\(\\displaystyle x&#39; = 4x - 2y, \\; y&#39; = 2x - 2y\\) \\(\\displaystyle x&#39;= 4x-15y, \\; y&#39;=2x-7y\\) \\(\\displaystyle x&#39; = 3x-18y, \\; y&#39; = x-5y\\) \\(\\displaystyle x&#39; = 5x-12y, \\; y&#39; = x-2y\\)   Exercise 19.2 Consider the linear system of differential equations: \\[\\begin{equation} \\begin{split} x&#39;&amp;=ax-y \\\\ y&#39; &amp;= -x+ ay \\end{split} \\end{equation}\\] Apply the relationships between the trace and determinant to classify the stability of the equilibrium solution for different values of \\(a\\). Be sure to include cases where the system will be a spiral source or sink. Exercise 19.3 Consider the following nonlinear system: \\[\\begin{equation} \\begin{split} x&#39;&amp;=y-x \\\\ y&#39; &amp;= -y + \\frac{5x^{2}}{4+x^{2}} \\end{split} \\end{equation}\\] In Chapter 16 you verified that \\((x,y)=(1,1)\\) is an equilibrium solution for this system. What is the Jacobian matrix for this equilibrium solution? What is tr\\((J)\\) and det\\((J)\\) for this equilibrium solution? Evaluate the stability of the equilibrium solution using relationships between the trace and determinant. You may use a graph to plot \\(\\det(J)\\). Exercise 19.4 (Inspired by J. Keener and Sneyd (2009)) Consider the following model of a neuron, with the two variables \\(v\\) and \\(w\\): \\[\\begin{equation} \\begin{split} x&#39; &amp;= y-x^{3} +3x^{2} \\\\ y&#39; &amp;= 1 - 5x^{2} - y \\\\ \\end{split} \\tag{19.6} \\end{equation}\\] Solve each of the nullclines as a function of \\(y\\). Using desmos or some other graphing utility, determine the equilibrium solutions. Determine the Jacobian matrix for each of the equilibrium solutions. Apply the trace-determinant conditions to determine stability of the equilibrium solutions Exercise 19.5 (Inspired by Logan and Wolesensky (2009)) Consider the following predator-prey model, where the carrying capacity of the predator (\\(y\\)) depends on the prey population (\\(x\\)): \\[\\begin{equation} \\begin{split} x&#39; &amp;= \\frac{2}{3} x \\cdot \\left(1- \\frac{x}{4} \\right) - \\frac{1}{6} xy \\\\ y&#39; &amp;= 0.5y \\cdot \\left(1 - \\frac{y}{x} \\right) \\end{split} \\end{equation}\\] There are three equilibrium solutions for this differential equation. What are they? Hint: first determine where \\(y&#39;=0\\) and then substitute your solutions into \\(x&#39;=0\\). Visualize the phase plane for this system of differential equations. Compute the Jacobian matrix for all of the equilibrium solutions. Use the trace-determinant relationships to evaluate the stability of the equilibrium solutions. Is the trace-determinant analysis consistent with your phase plane? Exercise 19.6 (Inspired by Logan and Wolesensky (2009)) Let \\(C\\) be the amount of carbon in a forest ecosystem, with \\(P\\) as the rate of increase due to photosynthesis. Herbivores \\(H\\) consume carbon on the following predator-prey model: \\[\\begin{equation} \\begin{split} \\frac{dC}{dt}&amp;=P - aC - bHC \\\\ \\frac{dH}{dt} &amp;= ebHC-dH \\end{split} \\tag{19.7} \\end{equation}\\] The parameters \\(a\\) and \\(d\\) represent the removal of carbon and herbivores from this system, and \\(b\\) the consumption of carbon by the herbivores at some efficiency \\(e\\). All parameters are greater than zero. Use this information to answer the following questions: Construct the general Jacobian matrix for this system of differential equations. What are the \\(H\\) nullclines for this system of differential equations? Your nullclines will be a function of the parameters. Use the \\(H\\) nullclines to determine the two equilibrium solutions for Equation (19.7). Under what conditions will the equilibrium solutions be positive? Evaluate tr(\\(J\\)) and det(\\(J\\)) at each of your equilibrium solutions. What do you think the stability of the equilibrium solutions would be? Exercise 19.7 (Inspired by Pastor (2008)) The amount of nutrients (such as carbon) in soil organic matter is represented by \\(N\\), whereas the amount of inorganic nutrients in soil is represented by \\(I\\). A system of differential equations that describes the turnover of inorganic and organic nutrients is the following: \\[\\begin{equation} \\begin{split} \\frac{dN}{dt} &amp;= L + kdI - \\mu N I - \\delta N \\\\ \\frac{dI}{dt} &amp;= \\mu N I - k d I - \\delta I , \\end{split} \\end{equation}\\] Construct the general Jacobian matrix for this system of differential equations. An equilibrium solution to this system of differential equations is \\(\\displaystyle N = \\frac{L}{\\delta}, \\; I = 0\\). Determine tr\\((J)\\) and det(\\(J\\)) for this equilibrium solution. Express conditions on the parameter \\(\\mu\\) (as a function of the other parameters) that determine when this equilibrium solution a saddle node (you may assume that \\(\\delta&gt;0\\) and \\(\\mu &gt; 0\\))? If \\(\\mu\\) represents the rate conversion of nutrients to inorganic matter, and \\(\\delta\\) is the removal of nutrients from the system, what does this condition mean in a biological sense? Exercise 19.8 Apply the quadratic formula to \\(\\lambda^{2} - \\mbox{tr}(A)\\lambda + \\det(A)=0\\) to obtain Equation (19.5). Exercise 19.9 Assume that you have two complex conjugate eigenvalues: \\(\\lambda_{1} = a + bi\\) and \\(\\lambda_{2} = a - bi\\). What is an expression for \\(\\lambda_{1} + \\lambda_{2}\\)? What is an expression for \\(\\lambda_{1} \\cdot \\lambda_{2}\\)? Explain why your answers from the previous two questions mean that tr\\((A)=2a\\) and \\(\\det(A)=a^{2}+b^{2}\\). Create a linear two-dimensional system of differential equations where the equilibrium solution at the origin is a spiral sink. Show your system and the corresponding phase plane. Exercise 19.10 Consider a two-dimensional system where tr\\((A)=0\\) and det\\((A)&gt;0\\). Given those conditions, explain why \\(\\lambda_{1} + \\lambda_{2}=0\\) and \\(\\lambda_{1} \\cdot \\lambda_{2}&gt;0\\). What does \\(\\lambda_{1} + \\lambda_{2}=0\\) tell you about the relationship between \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\)? What does \\(\\lambda_{1} \\cdot \\lambda_{2}&gt;0\\) tell you about the relationship between \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\)? Look back to your previous two responses. First explain why \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\) must be imaginary eigenvalues (in other words, not real values). Then explain why \\(\\lambda_{1,2}= \\pm bi\\). Given these constraints, what would the phase plane for this system be? Create a linear two-dimensional system where tr\\((A)=0\\) and det\\((A)&gt;0\\). Show your system and the phase plane. References "],["bifurcation-20.html", "Chapter 20 Bifurcation 20.1 A series of equations 20.2 Bifurcations with systems of equations 20.3 Functions as equilibrium solutions: limit cycles 20.4 Bifurcations as analysis tools 20.5 Exercises", " Chapter 20 Bifurcation In this chapter we will use bifurcation to examine how the stability of an equilibrium solution changes as the value of a parameter changes. This is a great topic of study that (by necessity) requires you to think of stability of an equilibrium solution on multiple levels. You are up for the challenge; let’s get started! 20.1 A series of equations Consider the differential equation \\(\\displaystyle x&#39; = 1-x^{2}\\). This differential equation has an equilibrium solution at \\(x=\\pm 1\\). To classify the stability of the equilibrium solutions we apply the following test for stability of an equilibrium solution that we developed in Chapter 5: If \\(f&#39;(y^{*})&gt;0\\) at an equilibrium solution, the equilibrium solution \\(y=y^{*}\\) will be unstable. If \\(f&#39;(y^{*}) &lt;0\\) at an equilibrium solution, the equilibrium solution \\(y=y^{*}\\) will be stable. If \\(f&#39;(y^{*}) = 0\\), we cannot conclude anything about the stability of \\(y=y^{*}\\). Applying this test, we know \\(f(x)=1-x^2\\) and \\(f&#39;(x)=-2x\\). Since \\(f&#39;(1)=-2\\) and \\(f&#39;(-1)=2\\), then the respective equilibrium solution \\(x=1\\) is stable and the equilibrium solution at \\(x=-1\\) is unstable. Let’s modify and extend this example further. Consider two more differential equations: \\(\\displaystyle x&#39; = -1-x^{2}\\): This differential equation does not have any equilibrium solutions, so we do not need to apply the stability test. \\(\\displaystyle x&#39; = -x^{2}\\): This differential equation has an equilibrium solution at \\(x=0\\); the stability test cannot apply because \\(f&#39;=-2x\\) and \\(f&#39;(0)=0\\). The general solution to this differential equation is \\(\\displaystyle x(t)=\\frac{1}{t+C}\\) (Exercise 20.2), which apart from the vertical asymptote at \\(t=-C\\) is always decreasing for \\(t&gt;0\\). So the equilibrium solution at \\(x=0\\) is not stable. FIGURE 20.1: Phase plane with associated solutions for \\(\\displaystyle x&#39;=c-x^{2}\\) for different values of \\(c\\). The dashed grey lines are equilibrium solutions. Figure 20.1 builds on an interesting pattern in our series of differential equations. Let’s build on these three examples in a more general context. Consider the differential equation \\(\\displaystyle x&#39; = c-x^{2}\\), which is a generalization of our examples. (For our examples \\(c=1\\) \\(-1\\), and \\(0\\) respectively.) The value of \\(c\\) influences the phase line and the resulting solution. Steady states to \\(\\displaystyle x&#39; = c-x^{2}\\) are at \\(x^{*}=\\pm \\sqrt{c}\\). We can also test out the stability of our steady states using the stability test, with \\(f(x)=c-x^{2}\\) and \\(f&#39;(x)=-2x\\). If \\(c&gt;0\\) we have two steady states, summarized in the following table: Equilibrium solution \\(f&#39;(x^{*})\\) Tendency of solution \\(x^{*}=\\sqrt{c}\\) \\(-2 \\sqrt{c}\\) Stable \\(x^{*}=-\\sqrt{c}\\) \\(2 \\sqrt{c}\\) Unstable (You should verify that the stability result we initially found when \\(c=1\\) matches the table.) If \\(c=0\\) there is only one steady state, summarized, in the following table: Equilibrium solution \\(f&#39;(x^{*})\\) Tendency of solution \\(x^{*}=0\\) 0 Inconclusive Even though in this case the stability test is inconclusive, based on the phase plane for \\(x&#39;=-x^{2}\\) (Figure 20.1), the equilibrium solution \\(x^{*}=0\\) is unstable. If the initial condition \\(x(0)\\) is greater than 0, the solution flows towards \\(x=0\\), but if the initial condition is less than zero, the solution flows away from \\(x=0\\). This type of behavior is similar to a one-dimensional analogue to a saddle node from Chapter 18. Finally, when \\(c&lt;0\\) then there are no steady states because the \\(\\sqrt{c}\\) will be an imaginary number.44 Notice how different values of \\(c\\) influence both the value \\((x^{*}=\\pm \\sqrt{c})\\) and the stability of the equilibrium solution (stable / unstable). Rather than making a series of tables, we can represent the dependence of the equilibrium solution and its stability in what is called a bifurcation diagram (Figure 20.2). FIGURE 20.2: A saddle node bifurcation for the differential equation \\(\\displaystyle x&#39;=c-x^{2}\\). Let’s talk about Figure 20.2. The graph represents the value of the equilibrium solution (\\(x^{*}\\), vertical axis) as a function of the parameter \\(c\\) (horizontal axis). Since equilibrium solutions are characterized by \\(x^{*}=\\pm \\sqrt{c}\\) we have the “sideways parabola,” traced in blue in Figure 20.2. When \\(c&lt;0\\), there is no equilibrium solution, (so nothing is plotted in the second and third quadrants of Figure 20.2). The difference between the solid and dashed lines in Figure 20.2 is used to distinguish between a stable equilibrium solution (\\(x^{*}=\\sqrt{c}\\) when \\(c&gt;0\\)) and an unstable equilibrium solution (\\(x^{*}=-\\sqrt{c}\\) when \\(c&gt;0\\)). It is so cool that all the information about the equilibrium solution and its stability is contained in Figure 20.2! The bifurcation structure of \\(\\displaystyle x&#39;=c-x^{2}\\) is called a saddle-node bifurcation. To give another context, it might be helpful to think of this \\(c\\) like a tuning knob. As \\(c&gt;0\\) we will always have two different equilibrium solutions that are symmetrical based on the value of \\(c\\). The positive equilibrium solution will be stable, the other unstable. As \\(c\\) approaches zero these equilibrium solutions will collapse into each other. If \\(c\\) is negative, the equilibrium solution disappears. FIGURE 20.3: Phase planes for Equation (20.1) for different values of \\(b\\). 20.2 Bifurcations with systems of equations We can also examine how bifurcation plays a role with systems of differential equations. As another example, let’s determine the behavior of solutions near the origin for the system of equations: \\[\\begin{equation} \\frac{\\vec{dx}}{dt} = \\begin{pmatrix} 3 &amp; b \\\\ 1 &amp; 1 \\end{pmatrix} \\vec{x}. \\tag{20.1} \\end{equation}\\] This equation has one free parameter \\(b\\) that we will analyze using the trace determinant conditions developed in Chapter 19. Let’s call the matrix \\(A\\), so the tr\\((A)=4\\) and \\(\\det(A)=3-b\\). Since the trace is always positive either the equilibrium solution will be a saddle if \\(\\det(A)&lt;0\\), or when \\(3&lt;b\\). We have a spiral source when \\(\\det(A)&gt;0\\) (this means \\(3 &gt; b\\)) and \\(\\det(A) &gt; (\\mbox{tr}(A))^{2}/4\\), or when \\(3-b &gt; 4\\), which leads to \\(b&lt;-1\\). Figure 20.3A-C displays the phase planes for different values of \\(b\\) along with sample solution curves. To summarize, Equation (20.1) has the following dynamics depending on the value of \\(b\\): When \\(b &lt; -1\\), the equilibrium solution will be a spiral source. When \\(-1 &lt; b &lt; 3\\), the equilibrium solution will be a source. When \\(3&lt;b\\), the equilibrium solution will a saddle. Another approach to analyzing Equation (20.1) is to compute the eigenvalues directly, which in this case are \\(\\displaystyle \\lambda_{1,2}(b)=2 \\pm \\sqrt{b+1}\\). Creating a plot of the eigenvalues (Figure 20.4) can also help explain the bifurcation structure. When \\(b&lt;-1\\), the eigenvalues are imaginary, with Re\\((\\lambda_{1,2}(b)=2)=2\\), so the equilibrium solution is a spiral source. When \\(-1 &lt; b &lt; 3\\), both eigenvalues are positive, so the equilibrium solution is a source. Finally, when \\(3 &lt; b\\), one eigenvalue is positive and the other is negative, confirming our analyses with the trace-determinant plane. FIGURE 20.4: Bifurcation diagram for Equation (20.1). The vertical axis shows the value of the eigenvalues \\(\\lambda\\) (red and blue curves) as a function of the parameter \\(b\\). The annotations represent the stability of the original equilibrium solution. The benefit of a bifurcation diagram is to provide a complete understanding of the dynamics of the system as a function of the parameters. In this chapter we examined one-parameter bifurcations (for example we looked the stability of the equilibrium solution as it depends on c or b), but bifurcations can also be extended further to two parameter bifurcation families, applying similar methods. In general the methods are similar to what we have done. 20.3 Functions as equilibrium solutions: limit cycles In the previous examples the stability of an equilibrium solution changed depending on the value of a parameter. Typically equilibrium solutions are a single point in the phase plane. Another way we can represent an equilibrium solution is with a function. As an example, consider the following highly nonlinear system in Equation (20.2): \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} =-y-x(x^2+y^2-1) \\\\ \\frac{dy}{dt}=x-y(x^2+y^2-1) \\end{split} \\tag{20.2} \\end{equation}\\] The phase plane for Equation (20.2) is shown in Figure 20.5. You can verify that Equation (20.2) has an equilibrium solution at the point \\(x=0\\), \\(y=0\\). However Figure 20.5 suggests there might be other equilibrium solutions when various solution curves are plotted with the phase plane. FIGURE 20.5: Phase plane for Equation (20.2) with different solution curves. Notice the equilibrium solution described by the equation \\(x^{2}+y^{2}=1\\). What is interesting in Figure 20.5 is that the solution tends towards a circle of radius 1 (or the equation \\(x^{2}+y^{2}=1\\)). This is an example of an equilibrium solution that is a curve rather than a specific point. We can describe the phase plane with a new variable \\(X\\) that represents the distance from the origin (a radius \\(r\\)) by transforming this system from \\(x\\) and \\(y\\) to a single new variable \\(X\\) (see Exercise 20.11). \\[\\begin{equation} \\frac{dX}{dt} = -X(X-1) \\mbox{ where } X=r^{2} \\tag{20.3} \\end{equation}\\] How Equation (20.2) transforms to Equation (20.3) is by applying a polar coordinate transformation to this system. With stability analysis for Equation (20.3) we can show that the equilibrium solution \\(X=0\\) is unstable (meaning the origin \\(x=0\\) and \\(y=0\\) is an unstable equilibrium solution) and the circle of radius 1 is a stable equilibrium solution (which is the equation \\(x^{2}+y^{2}=1\\)). In this case we would say \\(r=1\\) is a stable limit cycle. You will study a similar system in Exercises 20.11 and 20.12. Equation (20.3) is an example of next steps with studying the qualitative analysis of systems. We can extend out Equation (20.2) further to introduce a parameter \\(\\mu\\) that, as \\(\\mu\\) changes, undergoes a bifurcation as \\(\\mu\\) increases. This is an example of a Hopf bifurcation. 20.4 Bifurcations as analysis tools The most important part in studying bifurcations is analyzing examples. This chapter has several exercises where you will construct bifurcation diagrams for one- and two-dimensional systems of differential equations. As a reminder, constructing sample phase lines / phase planes before analyzing stability and the bifurcation structure is always helpful to build understanding. Bifurcation analysis is a fascinating field of study that combines knowledge of differential equations, geometry, and other types of advanced mathematics. For further information, please see the texts by Strogatz (2015), Perko (2001), and Kuznetsov (2004). 20.5 Exercises Exercise 20.1 Explain why \\(\\displaystyle x&#39;= 1+x^{2}\\) does not have any equilibrium solutions. Exercise 20.2 Use separation of variables to verify that the general solution to \\(\\displaystyle x&#39; = -x^{2}\\) is \\(\\displaystyle x(t)=\\frac{1}{t+C}\\). Exercise 20.3 Apply local linearization to classify stability of the following differential equations: \\(\\displaystyle \\frac{dx}{dt} = x-x^{2}\\) \\(\\displaystyle \\frac{dx}{dt} = -x^{2}\\) \\(\\displaystyle \\frac{dx}{dt} = -x-x^{2}\\) Exercise 20.4 Consider the differential equation \\(\\displaystyle x&#39; = cx-x^{2}\\). What are equations that describe the dependence of the equilibrium solution on the value of \\(c\\)? Once you have that figured out, plot the bifurcation diagram, with the parameter \\(c\\) along the horizontal axis. This bifurcation is called a transcritical bifurcation. Exercise 20.5 Consider the differential equation \\(\\displaystyle x&#39; = cx-x^{3}\\). What are equations that describe the dependence of the equilibrium solution on the value of \\(c\\)? Once you have that figured out plot the bifurcation diagram, with the parameter \\(c\\) along the horizontal axis. This bifurcation is called a pitchfork bifurcation. Exercise 20.6 Construct a bifurcation diagram for the differential equation \\(\\displaystyle x&#39;=c+x^{2}\\) Exercise 20.7 Consider the differential equation \\(\\displaystyle x&#39; = x(x-1)(b-x)\\). The differential equation has equilibrium solutions at \\(x^{*}=0\\), \\(x^{*}=1\\), and \\(x^{*}=b\\), where \\(b &gt; 0\\). Use desmos or some other plotting software to investigate the effect of the number of roots as \\(b\\) increases from a value of 0. Analyze the stability of each of these equilibrium solutions. (You may want to multiply out the right hand side of the differential equation.) Whether a given equilibrium solution is stable may depend on the value of \\(b\\). Construct a bifurcation diagram for all three solutions together, with \\(b\\) on the horizontal axis and the value of \\(x^{*}\\) on the vertical axis. Exercise 20.8 Consider the system of differential equations: \\[\\begin{equation} \\begin{pmatrix} x&#39; \\\\ y&#39; \\end{pmatrix} = \\begin{pmatrix} -x \\\\ cy - y^{2} \\end{pmatrix} \\end{equation}\\] What are the equilibrium solutions for this (uncoupled) system of equations? Evaluate stability of the equilibrium solutions as a function of the parameter \\(c\\). Construct a few representative phase planes to verify your analysis. Exercise 20.9 Consider the following linear system of differential equations: \\[\\begin{equation} \\frac{d}{dt}\\vec{x} = \\begin{pmatrix} 3 &amp; b \\\\ b &amp; 1 \\end{pmatrix} \\vec{x}. \\end{equation}\\] Verify that the characteristic polynomial is \\(f(\\lambda,b)=\\lambda^{2}-4\\lambda+(3-b^{2})\\). Solve \\(f(\\lambda,b)=0\\) with the quadratic formula to obtain an expression for the eigenvalues as a function of \\(b\\), that is \\(\\lambda_{1,2}(b)\\). Using the eigenvalues, classify the stability of the equilibrium solution as \\(b\\) changes. Generate a few representative phase planes to verify your analysis. Create a plot similar to Figure 20.4 showing the bifurcation structure. Exercise 20.10 Consider the linear system of differential equations: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt}&amp;=cx-y \\\\ \\frac{dy}{dt} &amp;= -x+cy \\end{split} \\end{equation}\\] Determine the characteristic polynomial (\\(f(\\lambda,c)\\)) for this system of equations. Solve \\(f(\\lambda,c)=0\\) with the quadratic formula to obtain an expression for the eigenvalues as a function of \\(c\\), that is \\(\\lambda_{1,2}(c)\\). Using the eigenvalues, classify the stability of the equilibrium solution as \\(c\\) changes. Generate a few representative phase planes to verify your analysis. Create a plot similar to Figure 20.4 showing the bifurcation structure. Exercise 20.11 Consider the following highly nonlinear system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} =-y-x(x^2+y^2-1) \\\\ \\frac{dy}{dt}=x-y(x^2+y^2-1) \\end{split} \\end{equation}\\] We are going to transform the system by defining new variables \\(x=r \\cos \\theta\\) and \\(y=r \\sin \\theta\\). Observe that \\(r^2=x^2+y^2\\). Consider the equation \\(r^2=x^2+y^2\\), where \\(r\\), \\(x\\), and \\(y\\) are all functions of time. Apply implicit differentiation to determine a differential equation for \\(\\displaystyle \\frac{d(r^{2})}{dt}\\), expressed in terms of \\(x\\), \\(y\\), \\(\\displaystyle \\frac{dx}{dt}\\) and \\(\\displaystyle \\frac{dy}{dt}\\). Multiply the above equations \\(\\displaystyle \\frac{dx}{dt}\\) by \\(2x\\) and \\(\\displaystyle \\frac{dy}{dt}\\) by \\(2y\\) on both sides of the equation. Then add the two equations together. You should get an expression for \\(\\displaystyle \\frac{d(r^{2})}{dt}\\) in terms of \\(x\\) and \\(y\\). Rewrite the equation for the right hand side of \\(\\displaystyle \\frac{d(r^{2})}{dt}\\) in terms of \\(r^{2}\\). Use your equation that you found to verify that \\[\\begin{equation} \\frac{dX}{dt} = -2X(X-1), \\mbox{ where } X=r^{2} \\end{equation}\\] Verify that \\(X=1\\) is a stable node and \\(X=0\\) is unstable. As discussed in this chapter, this system has a stable limit cycle. What quick and easy modification to our system could you do to the system to ensure that this is an unstable limit cycle? Justify your work. Exercise 20.12 Construct a bifurcation diagram for \\(\\displaystyle \\frac{dX}{dt} = - 2X(X-\\mu)\\),; \\(\\mu\\) is a parameter. Explain how you can apply that result to understanding the bifurcation diagram of the system: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} =-y- x(x^2+y^2-\\mu) \\\\ \\frac{dy}{dt}=x- y(x^2+y^2-\\mu) \\end{split} \\end{equation}\\] This system is an example of a . Exercise 20.13 (Inspired by Logan and Wolesensky (2009)) The immune response to HIV can be described with differential equations. In the early stages (before the body is swamped by the HIV virions) the dynamics of the virus can be described by the following system of equations, where \\(v\\) is the virus load and \\(x\\) the immune response: \\[\\begin{equation} \\begin{split} \\frac{dv}{dt}&amp;=rv - pxv \\\\ \\frac{dx}{dt} &amp;= cv-bx \\end{split} \\end{equation}\\] You may assume that all parameters are positive. Explain the various terms in this model and their biological meaning. Determine the equilibrium solutions. Evaluate the Jacobian for each of the equilibrium solutions. Construct a bifurcation diagram for each of the equilibrium solutions. References "],["stoch-sys-21.html", "Chapter 21 Stochastic Biological Systems 21.1 Introducing stochastic effects 21.2 A discrete dynamical system 21.3 Environmental stochasticity 21.4 Discrete systems of equations 21.5 Exercises", " Chapter 21 Stochastic Biological Systems 21.1 Introducing stochastic effects Up to this point we have studied deterministic differential equations. We use the word deterministic because given an initial condition and parameters, the solution trajectory is known. In this part we are going to study stochastic differential equations or SDEs for short. A stochastic differential equation means that the differential equation is subject to random effects - either in the parameters (which may cause a change in the stability for a time) or in the variables themselves. Stochastic differential equations can be studied using computational approaches. This part will give you an introduction to SDEs with some focus on solution techniques, which I hope you will be able to apply in other contexts relevant to you. Understanding how to model SDEs requires learning some new mathematics and approaches to numerical simulation. Let’s get started! 21.2 A discrete dynamical system Let’s focus on an example that involves discrete dynamical systems. Moose are large animals (part of the deer family), weighing 1000 pounds, that can be found in Northern Minnesota. The moose population was 8000 in the early 2000s, but recent surveys show the population is maybe stabilized at 3000. A starting model that describes their population dynamics is the discrete dynamical system in Equation (21.1): \\[\\begin{equation} M_{t+1} = M_{t} + b \\cdot M_{t} - d \\cdot M_{t}, \\tag{21.1} \\end{equation}\\] where \\(M_{t}\\) is the population of the moose in year \\(t\\), and \\(b\\) the birth rate and \\(d\\) the death rate. Equation (21.1) can be reduced down to \\(M_{t+1}=r M_{t}\\) where \\(r=1+b-d\\) is the net birth/death rate. This model states that the population of moose in the next year is proportional to the current population. Equation (21.1) is a little bit different from a continuous dynamical system, but can be simulated pretty easily by defining a function. M0 &lt;- 3000 # Initial population of moose N &lt;- 5 # Number of years we simulate moose &lt;- function(r) { out_moose &lt;- array(M0, dim = N+1) for (i in 1:N) { out_moose[i + 1] &lt;- r * out_moose[i] } return(out_moose) } Notice how the function moose returns the current population of moose after \\(N\\) years with the net birth rate \\(r\\). Let’s take a look at the results for different values of \\(r\\) (Figure 21.1). FIGURE 21.1: Simulation of the moose population with different birth rates. Notice how for some values of \\(r\\) the population starts to decline, stay the same, or increase. To analyze Equation (21.1), just like with a continuous differential equation we want to look for solutions that are in steady state, or ones where the population is staying the same. In other words this means that \\(M_{t+1}=M_{t}\\), or \\(M_{t}=rM_{t}\\). If we simplify this expression this means that \\(M_{t}-r M_{t}=0\\), or \\((1-r)M_{t}=0\\). Assuming that \\(M_{t}\\) is not equal to zero, then this equation is consistent only when \\(r=1\\). This makes sense: we know \\(r=1-b-d\\), so the only way this can be one is if \\(b=d\\), or the births balance out the deaths. Okay, so we found our equilibrium solution. The next goal is to determine the general solution to Equation (21.1). In Chapter 7 for continuous differential equations, a starting point for a general solution was an exponential function. For discrete dynamical systems we will also assume a general solution is exponential, but this time we represent the solution as \\(M_{t}=M_{0} \\cdot v^{t}\\), which is an exponential equation. The parameter \\(M_{0}\\) is the initial population of moose (here it equals 3000). Now let’s determine \\(v\\) in Equation (21.1): \\[\\begin{equation} M_{t+1} = r M_{t} \\rightarrow 3000 \\cdot v^{t+1} = r \\cdot 3000 \\cdot v^{t} \\end{equation}\\] Our goal is to figure out a value for \\(v\\) that is consistent with this expression. Just like we did with continuous differential equations we can arrange the following equation, using the fact that \\(v^{t+1}=v^{t}\\cdot v\\): \\[\\begin{equation} 3000 v^{t} (v-r) = 0 \\end{equation}\\] Since we assume \\(v\\neq 0\\), the only possibility is if \\(v=r\\). Equation (21.2) represents the general solution for Equation (21.1): \\[\\begin{equation} M_{t}=3000 r^{t} \\tag{21.2} \\end{equation}\\] We know that if \\(r&gt;1\\) we have exponential growth exponential decay when \\(r&lt;1\\) exponential decay, consistent with our results above. There is some comfort here: just like in continuous systems we find eigenvalues that determine the stability of the equilibrium solution. For discrete dynamical systems the stability is based on the value of an eigenvalue relative to 1 (not 0). Note: this is a good reminder to be aware if the model is based in continuous or discrete time! 21.3 Environmental stochasticity It may be the case that environmental effects drastically change the net birth rate from one year to the next. For example during snowy winters the net birth rate changes because it is difficult to find food (Carroll 2013). For our purposes, let’s say that in snowy winters \\(r\\) changes from \\(1.1\\) to \\(0.7\\). This would be a pretty drastic effect on the system - when \\(r=1.1\\) the moose population grows exponentially and when \\(r=0.7\\) the moose population decays exponentially. A snowy winter occurs randomly. One way to model this randomness is to create a conditional statement based on the probability of it being snowy, defined on a scale from 0 to 1. How we implement this is by writing a function that draws a uniform random number each year and adjust the net birth rate: # We use the snowfall_rate as an input variable moose_snow &lt;- function(snowfall_prob) { out_moose &lt;- array(M0, dim = N+1) for (i in 1:N) { r &lt;- 1.1 # Normal net birth rate if (runif(1) &lt; snowfall_prob) { # We are in a snowy winter r &lt;- 0.7 # Decreased birth rate } out_moose[i + 1] &lt;- r * out_moose[i] } return(out_moose) } Figure 21.2 displays different solution trajectories of the moose population over time for different probabilities of a deep snowpack. FIGURE 21.2: Moose populations with different probability of adjusting to deep snowpacks. If you tried generating Figure 21.2 on your own you would not obtain the same figure. We are drawing random numbers for each year, so you should have different trajectories. While this may seem like a problem, one key thing that we will learn later in Chapter 22 is there is a stronger underlying signal when we compute multiple simulations and then compute an ensemble average. As you can see when the probability of a snowy winter is very high (\\(p = 0.75\\)), the population decays exponentially. If that probability is lower, the moose population can still increase, but one bad year does knock the population down. 21.4 Discrete systems of equations Another way to extend Equation (21.1) is to account for both adult (\\(M\\)) and juvenile (\\(J\\)) moose populations with Equation (21.3): \\[\\begin{equation} \\begin{split} J_{t+1} &amp;=f \\cdot M_{t} \\\\ M_{t+1} &amp;= g \\cdot J_{t} + p \\cdot M_{t} \\end{split} \\tag{21.3} \\end{equation}\\] Equation (21.3) is a little different from (21.1) because it includes juvenile and adult moose populations, which have the following parameters: \\(f\\): represents the birth rate of new juvenile moose \\(g\\): represents the maturation rate of juvenile moose \\(p\\): represents the survival probability of adult moose We can code up this model using R in the following way: M0 &lt;- 900 # Initial population of adult moose J0 &lt;- 100 # Initial population of juvenile moose N &lt;- 10 # Number of years we run the simulation moose_two_stage &lt;- function(f, g, p) { # f: birth rate of new juvenile moose # g: maturation rate of juvenile moose # p: survival probability of adult moose # Create a data frame of moose to return out_moose &lt;- tibble( years = 0:N, adult = M0, juvenile = J0 ) # And now the dynamics for (i in 1:N) { out_moose$juvenile[i + 1] &lt;- f * out_moose$adult[i] out_moose$adult[i + 1] &lt;- g * out_moose$juvenile[i] + p * out_moose$adult[i] } return(out_moose) } To simulate the dynamics we just call the function moose_two_stage and plot in Figure 21.3: moose_two_stage_rates &lt;- moose_two_stage( f = 0.5, g = 0.6, p = 0.7 ) ggplot(data = moose_two_stage_rates) + geom_line(aes(x = years, y = adult), color = &quot;red&quot;) + geom_line(aes(x = years, y = juvenile), color = &quot;blue&quot;) + labs( x = &quot;Years&quot;, y = &quot;Moose&quot; ) FIGURE 21.3: Simulation of a two stage moose population model. Looking at Figure 21.3, it seems like both populations stabilize after a few years. We could further analyze this model for stable population states (in fact, it would be similar to determining eigenvalues as in Chapter 18). Additional extensions could also incorporate adjustments to the parameters \\(f\\), \\(g\\), and \\(p\\) in snow years (Exercise 21.5). As you can see, introducing stochastic or random effects to a model yields some interesting (and perhaps more realistic) results. Next we will examine how computing can further explore stochastic models and how to generate expected patterns from all this randomness. Onward! 21.5 Exercises Exercise 21.1 Re-run the moose population model with probabilities of adjusting to the deep snowpack at \\(p = 0, \\; 0.1, \\; 0.9, \\mbox{ and} \\;1\\). How does adjusting the probability affect the moose population after 10 years? Exercise 21.2 Modify the function moose_snow so that runif(1) &lt; snowfall_prob) is changed to runif(1) &gt; snowfall_prob). How does that code change the resulting solution trajectories in Figure 21.2? Why is this not the correct way to code changes in the net birth rate in deep snowpacks? Exercise 21.3 Modify the two stage moose population model (Equation (21.3)) with the following parameters and plot the resulting adult and juvenile populations: \\(f = 0.6\\), \\(g = 0.6\\), \\(p = 0.7\\) \\(f = 0.5\\), \\(g = 0.6\\), \\(p = 0.4\\) \\(f = 0.3\\), \\(g = 0.6\\), \\(p = 0.5\\) You may assume \\(M_{0} = 900\\) and \\(J_{0}=100\\). Exercise 21.4 You are playing a casino game. If you win the game you earn $10. If you lose the game you lose your bet of $10. The probability of winning or losing is 50-50 (0.50). You decide to play the game 20 times and then cash out your net earnings. Write code that is able to simulate this stochastic process. Plot your results. Run this code five different times. What do you think your long term net earnings would be? Now assume that you have a 40% chance of winning. Re-run your code to see how that affects your net earnings. Exercise 21.5 Modify the two stage moose population model (Equation 21.5) to account for years with large snowdepths. In normal years, \\(f=0.5\\), \\(g=0.6\\), \\(p=0.7\\). However for snowy years, \\(f=0.3\\), \\(g=0.6\\), \\(p=0.5\\). Generate code that can account for these variable rates (similar to the moose population model). You may assume \\(M_{0} = 900\\), \\(J_{0}=100\\), and \\(N\\) (the number of years) is 30. Plot simulations when the probability of snowy winters is \\(s=0.05\\) \\(s=0.10\\), or \\(s=0.20\\). Comment on the long-term dynamics of the moose for these simulations. Exercise 21.6 A population grows according the the growth law \\(x_{t+1}=r_{t}x_{t}\\). Determine the general solution to this discrete dynamical system. Plot a sample growth curve with \\(r_{t}=0.86\\) and \\(r_{t}=1.16\\), with \\(x_{0}=100\\). Show your solution for \\(t=50\\) generations. Now consider a model where \\(r_{t}=0.86\\) with probability 1/2 and \\(r_{t}=1.16\\) with probability 1/2. Write a function that will predict the population after \\(t=50\\). Show three or four different realizations of this stochastic process. Exercise 21.7 (Inspired by Logan and Wolesensky (2009)) A rectangular preserve has area \\(a\\). At one end of the boundary of the preserve (contained within the area), is a small band of land of area (\\(a_{b}\\)) from which animals disperse into the wilderness. Only animals at that eged disperse. Let \\(u_{t}\\) be the number of animals in \\(a\\) at any time \\(t\\). The growth rate of all the animals in \\(a\\) is \\(r\\). The rate at which animals disperse from the strip is proportional to the fraction of the animals in the edge band, with proportionality constant \\(\\epsilon\\). Draw a picture of the situation described above. Explain why the equation that describes the dynamics is \\(\\displaystyle u_{t+1}=r \\, u_{t} - \\epsilon \\frac{a_{b}}{a} u_{t}\\). Determine conditions on the parameter \\(r\\) as a function of the other parameters under which the population is growing. References "],["stoch-sim-22.html", "Chapter 22 Simulating and Visualizing Randomness 22.1 Ensemble averages 22.2 Repeated iteration 22.3 Exercises", " Chapter 22 Simulating and Visualizing Randomness In Chapter 21 we examined models for stochastic biological systems. These types of models are an introduction to the study of stochastic differential equations (SDEs). A common theme to SDEs is learning how to analyze and visualize randomness, broadly defined. In order to do that we will need to level up our skills to summarize a cohort of simulations over time. Let’s get started! 22.1 Ensemble averages Consider Figure 22.1, which shows the weather forecast for Kuopio, a city in Finland (Finnish Meteorological Institute 2021):45 FIGURE 22.1: Long term weather forecast for Kuopio, a city in Finland, from the Finnish Weather Institute. Accessed 16-Dec 2021. Figure 22.1 shows a great example of what is called an ensemble average. The horizontal axis lists the time of day and the vertical axis is the temperature (the bar graph represents precipitation). The forecast temperature at a given point in time can have a range of outcomes, with the median of the distribution as the “temperature probability forecast”46. The red shading states that 80% of the outcomes fall in a given range, so while the median temperature on Monday, December 20 (labeled as Mo 20.12 - dates are represented as DAY-MONTH-YEAR) is \\(-10^{\\circ}\\)C, it may range between \\(-16\\) and \\(-4^{\\circ}\\)C (3 to 24 \\(^{\\circ}\\)F, brrr!). Based on the legends given, we would say the 80% confidence interval is between \\(-10\\) to \\(-4^{\\circ}\\)C, or the models have 80% confidence for the temperature to be between that range of temperatures. Because there may be different factors that alter the weather in a particular spot (e.g. the timing of a low pressure front, clouds, etc.) there are different possibilities for an outcome of the weather forecast. While it may seem like forecasting weather is impossible to do, sometimes these changes lead to small fluctuations in the forecasted weather at a given point. The ensemble average in Figure 22.1 becomes more uncertain (wider shading), as unforeseen events may drastically change the weather in the long term. 22.1.1 Spaghetti plots Now let’s focus on how to construct an ensemble average, but first let’s start with a sample dataset. Consider the following data in Table 22.1. Notice how all the simulations (sim1, sim2, sim3) share the variable t in common, so it makes sense to plot them on the same axis in Figure 22.2. We call a plot of all of the simulations together a spaghetti plot, because, well, it can look like a bowl of spaghetti noodles was dumped all over the plotting space. TABLE 22.1: Simulations of a variable at different times. t sim1 sim2 sim3 1 3.39 2.16 1.93 2 0.16 3.99 4.53 3 2.92 4.85 3.15 4 4.05 3.68 4.65 5 1.65 0.23 3.82 FIGURE 22.2: Spaghetti plot of the three simulations from Table 22.1. While making the spaghetti plot isn’t bad when you have three simulations, with (a lot) more simulations this would be a pain! An ensemble average computes across the rows in Table 22.1 (could be an average or a quantile) to generate a new column in the data. Building an ensemble average is a step by step process that involves a series of processes that transform a dataset where you will need to first pivot the dataset and then group and summarize. 22.1.2 Pivot When you have multiple columns of a plot that you want to show together (such as a spaghetti plot) we can use a command called pivot_longer that gathers different columns together (also shown in Table 22.2). my_table_long &lt;- my_table %&gt;% pivot_longer(cols = c(&quot;sim1&quot;:&quot;sim3&quot;), names_to = &quot;name&quot;, values_to = &quot;value&quot;) TABLE 22.2: Simulations of a variable at different times, condensed into a long table. t name value 1 sim1 3.39 1 sim2 2.16 1 sim3 1.93 2 sim1 0.16 2 sim2 3.99 2 sim3 4.53 3 sim1 2.92 3 sim2 4.85 3 sim3 3.15 4 sim1 4.05 4 sim2 3.68 4 sim3 4.65 5 sim1 1.65 5 sim2 0.23 5 sim3 3.82 Notice how the command pivot_longer takes the different simulations (sim1, sim2, sim3) and reassigns the column names to a new column called name, with values in the different columns appropriately assigned to the column value. This process called pivoting creates a new data frame, which makes it easier to generate the spaghetti plot (Figure 22.2). Try the following code out on your own to confirm this: my_table_long %&gt;% ggplot(aes(x = t, y = value, group = name)) + geom_point() + geom_line() + labs(x = &quot;Time&quot;, y = &quot;Simulation Value&quot;) 22.1.3 Group and summarize The next step after pivoting is to collect the time points at \\(t=1\\) together, \\(t=2\\) together, and so on. In each of these groups we then compute the mean (average). This process is called grouping and then applying a summarizing function to all the members in a particular group (which in this case is the mean). The code to do this summarizing is shown below, with the results in Table 22.3: summarized_table &lt;- my_table_long %&gt;% group_by(t) %&gt;% summarize(mean_val = mean(value)) TABLE 22.3: Ensemble averages for the three simulations at each of the times \\(t\\). t 1.00 2.00 3.00 4.00 5.0 mean_val 2.49 2.89 3.64 4.13 1.9 Notice how we are using the pipe %&gt;% command to help organize the actions that we are doing. Think of the pipe as part of a multistep process - similar to function composition. Here is how you can read the previous code: To compute the variable summarized_table we will: First start with the data frame my_table_long, Second signal to R to group_by the variable t (collect similar values together) in order to, Third compute the mean. To explain this code a little more: The command group_by(t) means collect similar time points together. The next line computes the mean. The command summarize means that we are going to create a new data frame column (labeled mean_val that is the mean of all the grouped times, from the value column. We can add this mean value to our data (Figure 22.2), represented with a thick red line. Try the following code out on your own: my_table_long %&gt;% ggplot(aes(x = t, y = value, group = name)) + geom_point() + geom_line() + geom_line(data = summarized_table, aes(x = t, y = mean_val), color = &quot;red&quot;, size = 2, inherit.aes = FALSE) + geom_point(data = summarized_table, aes(x = t, y = mean_val), color = &quot;red&quot;, size = 2, inherit.aes = FALSE) + guides(color = &quot;none&quot;) + labs(x = &quot;Time&quot;, y = &quot;Simulation Value&quot;) Notice two things: We can use the pipe (%&gt;%) to the workflow before plotting with ggplot. This signals that my_table_long is the input data into ggplot. We included the option inherit.aes = FALSE (inherit.aes stands for “inherit aesthetics” ) when we plotted summarized_table with geom_point and geom_line. When you add a new data frame to a plot, the initial aesthetics (such as the color or the group) are passed on to subsequent commands. Setting inherit.aes = FALSE allows you to work with a clean slate. 22.2 Repeated iteration The previous example introduced the concept of pivoting data and computing an ensemble mean. Let’s put this into additional practice with examples we have studied previously. Let’s work with the logistic differential equation \\(\\displaystyle \\frac{dx}{dt} = rx\\left(1- \\frac{x}{K}\\right)\\). Our goal is to examine how different (random) initial conditions affect the modeled solution trajectories. The way we will approach this problem is with the following workflow: Do once \\(\\rightarrow\\) Do several times \\(\\rightarrow\\) Summarize \\(\\rightarrow\\) Visualize We will apply this workflow step by step with code and results provided. 22.2.1 Do once To investigate the effect of the initial condition on the solution we will choose the initial condition from a uniform distribution between 0 and 20, shown in the following code and plotted in Figure 22.3: # Define the rate equation logistic_eq &lt;- c(dx ~ r * x * (1 - x / K)) # Identify any parameters params &lt;- c(r = .8, K = 100) # Random initial condition number 1 init_cond_rand &lt;- c(x = runif(1, min = 0, max = 20)) soln_rand &lt;- euler( system_eq = logistic_eq, initial_condition = init_cond_rand, parameters = params, deltaT = .05, n_steps = 200 ) # Random initial condition number 2 init_cond_rand_two &lt;- c(x = runif(1, min = 0, max = 20)) soln_rand_two &lt;- euler( system_eq = logistic_eq, initial_condition = init_cond_rand_two, parameters = params, deltaT = .05, n_steps = 200 ) # Plot your solutions: ggplot() + geom_line(data = soln_rand, aes(x = t, y = x), color = &quot;black&quot;) + geom_line(data = soln_rand_two, aes(x = t, y = x), color = &quot;red&quot;) + labs( x = &quot;Time&quot;, y = &quot;x&quot; ) FIGURE 22.3: Two solutions to the logistic differential equation with a random initial condition. 22.2.2 Do several times Running several hundred iterations of this model could quickly grow time consuming. Fortunately we can use iteration here to compute and gather several different solutions. First the code, followed by a deconstruction: n_sims &lt;- 500 # The number of simulations # Compute solutions logistic_sim &lt;- rerun(n_sims, c(x = runif(1, min = 0, max = 20))) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ euler( system_eq = logistic_eq, initial_condition = .x, parameters = params, deltaT = .05, n_steps = 200 )) %&gt;% map_dfr(~.x, .id = &quot;simulation&quot;) # Plot these all together logistic_sim %&gt;% ggplot(aes(x = t, y = x)) + geom_line(aes(color = simulation)) + ggtitle(&quot;Random initial conditions&quot;) + guides(color = &quot;none&quot;) FIGURE 22.4: Spaghetti plot for logistic differential equation with 500 random initial conditions. Wow! This spaghetti plot is really interesting - it should show how even though the initial conditions vary between \\(x=0\\) to \\(x=20\\), eventually all solutions flow to the carrying capacity \\(K=100\\) (which is a stable equilbrium solution). Initial conditions that start closer to \\(x=0\\) take longer, mainly because they are so close to the other equilibrium solution at \\(x=0\\) (which is an unstable equilibrium solution). Ok, let’s deconstruct this code line by line: rerun(n_sims, c(x=runif(1,min=0,max=20))) This line does two things: x=runif(1,min=0,max=20) makes a random initial condition, and the command rerun runs this again for n_sims times. set_names(paste0(\"sim\", 1:n_sims)) This line distinguishes between all the different simulations. map(~ euler( ... ) You should be familiar with euler, but notice the pronoun .x that substitutes all the different initial conditions into Euler’s method. The map function iterates over each of the simulations. map_dfr(~ .x, .id = \"simulation\") This line binds everything up together. The resulting data frame should have three columns: - simulation: which one of the 500 simulations (sim1, sim2, etc …) this corresponds to. - t: the value of the time - x: the output value of the variable x. This code applies a new concept called functional programming. This is a powerful tool that allows you to perform the process of iteration (do the same thing repeatedly) with uncluttered code. We won’t delve more into this here, but I encourage you to read about more functional programming concepts in Wickham and Grolemund (2017). 22.2.3 Summarize Computing the ensemble average requires knowledge of how to use R to compute percentiles from a distribution of values. For our purposes here we will use the 95% confidence interval, so that means the 2.5 and 97.5 percentile (in which only 5% of the values will be outside of the specified interval), along with the median value (50th percentile). Let’s take a look at the code for how to do that: quantile_vals &lt;- c(0.025, 0.5, 0.975) logistic_quantile &lt;- logistic_sim %&gt;% group_by(t) %&gt;% summarize( q_val = quantile(x, # x is the column to compute the quantiles probs = quantile_vals ), q_name = quantile_vals ) %&gt;% pivot_wider(names_from = &quot;q_name&quot;, values_from = &quot;q_val&quot;, names_glue = &quot;q{q_name}&quot;) While this code is a little more involved, let’s break it down piece by piece: To make things easier the variable quantile_vals computes the different quantiles, expressed between 0 to 1 - so 2.5% is 0.025, 50% is 0.5, and 97.5% is .975. As above, we are still grouping by the variable t and summarizing our data frame. However rather than applying the mean, we are using the command quantile, whose value is computed with the new column q_val. Like the mean, we define to which columns we apply the quantile function in the column value. We use probs = quantile_vals to specify the quantiles that we wish to compute. We also create a new column called q_name the contains the names of the quantile probabilities. The command pivot_wider takes the values in q_val with the associated names in q_name and creates new columns associated with each quantile. This process of making a data frame wider is the opposite of making a skinny and tall data frame with pivot_longer. A key convention with column names is not to start them with a number, so we glue a q onto the names of the column using the option names_glue = \"q{q_name}\". Wow. This is getting involved. One thing to keep in mind is that the the code as written should be easily adaptable if you need to compute an ensemble average. If you take an introductory data science or data visualization course I bet you will learn more about the role of pivoting data - but for now you can just adapt the above code to your needs 22.2.4 Visualize To plot the 95% confidence interval we introduce the plot geom called geom_ribbon. Applying geom_ribbon requires a few more aesthetics (ymin and ymax, or the minimum and maximum \\(y\\) values to be plotted). The option alpha = 0.2 refers to the transparency of the plot. The fill aesthetic just provides the shading (in other words the fill) between ymin and ymax. logistic_quantile %&gt;% ggplot() + geom_line(aes(x = t, y = q0.5), color = &quot;red&quot;, size = 2, inherit.aes = FALSE ) + geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2, fill = &quot;red&quot;, inherit.aes = FALSE ) + guides(color = &quot;none&quot;) + labs(x = &quot;Time&quot;, y = &quot;Ensemble average&quot;) FIGURE 22.5: Ensemble average plot for the logistic differential equation with 500 random initial conditions. Making ensemble averages isn’t easy and requires strengthening your computational skills on several levels. Fortunately there are a lot of good tools when doing iteration and looping that easily allow you to adapt (shall I say iterate on?) existing examples to your needs. In future chapters we will move beyond random initial conditions to figuring out how the variables or parameters can be subject to random effects as time goes on. 22.3 Exercises Exercise 22.1 Using the code to produce Figure 22.5: Adjust the alpha level to a number between 0 and 1. What does that do to the plot? Adjust the fill level to a color of your choosing. A list of R Colors can be found at the R Color chart. Exercise 22.2 Read the Chapter 12 (tidy data) in Wickham and Grolemund (2017). In this chapter you will learn about tidy data. Explain what tidy data is and the potential uses for pivoting data wider or longer. Exercise 22.3 Look at the documentation for quantile (remember you can type ?quantile at the command line to see the associated help for this function). Change the ensemble average in quantile_vals to compute the 25%, 50%, and 75% percentile for logistic_sim. Finally, produce a ensemble average plot of this percentile. Exercise 22.4 Consider the logistic differential equation: \\(\\displaystyle \\frac{dx}{dt} = rx\\left(1- \\frac{x}{K}\\right)\\). The function logistic_mod below takes the initial value problem \\(x(0)=3\\) and solves the differential equation. Run logistic_mod(r=0.8,K=100) and plot its result. Run 500 simulations with varying \\(r\\) chosen from a uniform distribution with minimum value of 0.4 and maximum value of 1.0. Create a spaghetti and ensemble average plot. Set \\(K=100\\). Run 500 simulations with varying \\(K\\) chosen from a uniform distribution with minimum value of 50 and maximum value of 150. Create a spaghetti and ensemble average plot. Set \\(r=0.8\\). Compare your results along with Figures 22.4 and 22.5. How does randomizing the initial condition or the parameters affect the results? logistic_mod &lt;- function(r,K) { logistic_eq &lt;- c(dx ~ r * x * (1 - x / K)) # Define the rate equation params &lt;- c(r=r,K=K) # Identify any parameters init_cond &lt;- c(x = 3) # Initial condition soln &lt;- euler( system_eq = logistic_eq, initial_condition = init_cond, parameters = params, deltaT = .05, n_steps = 200 ) return(soln) } Exercise 22.5 Using the data frame my_table, compare the following code below. The data frame table1 is skinny and long, and the second data frame table2 is called short and wide. Why did we need to make this data frame short and wide for plotting? # First code chunk table1 &lt;- my_table %&gt;% rowwise(t) %&gt;% summarise(q_val = quantile(c_across(starts_with(&quot;sim&quot;)), probs = quantile_vals), q_name = quantile_vals) # Second code chunk table2 &lt;- my_table %&gt;% rowwise(t) %&gt;% summarise(q_val = quantile(c_across(starts_with(&quot;sim&quot;)), probs = quantile_vals), q_name = quantile_vals) %&gt;% pivot_wider(names_from = &quot;q_name&quot;,values_from=&quot;q_val&quot;, names_glue = &quot;q{q_name}&quot;) Exercise 22.6 Consider the following differential equation: \\[\\begin{equation} \\begin{split} \\frac{dx}{dt} =-y-x(x^2+y^2-1) \\\\ \\frac{dy}{dt}=x-y(x^2+y^2-1) \\end{split} \\end{equation}\\] Generate a phase plane for this differential equation. Store this phase plane in a variable called pp1. Set your x and y windows to be between \\(-1\\) and \\(1\\). The code below defines a function limit_cycle_mod that creates a solution trajectory of the differential equation. Super-impose a few different solution trajectories with random initial conditions onto your phase plane (pp1). Use initial conditions x0 and y0 between 0 and 1. Be sure to use the plot geom geom_path. Modify the code from this chapter to run 50 different simulations with random initial conditions x0 and y0 between 0 and 1. Note: It may be helpful to include the code map(~ limit_cycle_mod(runif(1),runif(1))). Plot the initial conditions from your simulation onto your phase plane. Isn’t the result pretty? limit_cycle_mod &lt;- function(x0,y0) { limit_cycle_eq &lt;- c(dx ~ -y-x*x*(x^2+y^2-1), dy ~ x-y*(x^2+y^2-1) ) # Define the rate equation init_cond = c(x=x0,y=y0) soln &lt;- rk4( system_eq = limit_cycle_eq, initial_condition = init_cond, deltaT = .05, n_steps = 200 ) return(soln) } References "],["random-walks-23.html", "Chapter 23 Random Walks 23.1 Random walk on a number line 23.2 Iteration and ensemble averages 23.3 Random walk mathematics 23.4 Continuous random walks and diffusion 23.5 Exercises", " Chapter 23 Random Walks Chapters 21 and 22 introduced the concept of stochastic dynamical systems and ways to compute ensemble averages. In this chapter we will begin to develop some tools to understand stochastic differential equations by studying the concept of random walks. While exploring random walks may seem like a diversion from understanding stochastic differential equations, there are deep connections between the two topics. We will do some interesting computational exercises that may lead to some non-intuitive results. Curious? Let’s get started! 23.1 Random walk on a number line The conceptual idea of a random walk begins on a number line. Let’s begin at the origin (so at \\(t=0\\) then \\(x=0\\)). Based on this number line we can only move to the left or the right, with equal probability. At a given time we decide to move in a direction based on a random number \\(r\\) drawn between 0 and 1 (in R we do this with the command runif(1)). Figure 23.1 conceptually illustrates this random walk FIGURE 23.1: Schematic diagram for one-dimensional random walk. For each iteration of this process we will draw a random number using runif(1). We can code this process using a for loop. Try the following code out on your own: # Number of steps our random walk takes number_steps &lt;- 100 # Set up vector of results x &lt;- array(0, dim = number_steps) for (i in 2:number_steps) { if (runif(1) &lt; 0.5) { x[i] &lt;- x[i - 1] - 1 } # Move right else { x[i] &lt;- x[i - 1] + 1 } # Move left } # Let&#39;s take a peek at our result: plot(x, type = &quot;l&quot;) print(mean(x)) # Average position over the time interval print(sd(x)) # Standard deviation over the time interval Let’s remind ourselves what this code does: number_steps &lt;- 100: The number of times we draw a random number, referred to steps. x &lt;- array(0,dim=number_steps): We are going to pre-allocate a vector (array) of our results. Values in this array are all set at 0 for convenience. The for loop starts at the second step and then either adds or subtracts one from the prevoius position x[i-1] and updates the result to x[i]. plot(x,type='l') makes a simple line plot of the results. Now that you have run this code, try running it again. Do you get the same result? I hope you didn’t - because this process is random! It is interesting to run it several times because there can be a wide variance in our results - for some realizations of the sample path, we end up being strictly positive, other times we go negative, and other times we just hover around the middle line (\\(x=0\\)) in the plot. 23.2 Iteration and ensemble averages We can apply the workflow (Do once \\(\\rightarrow\\) Do several times \\(\\rightarrow\\) Summarize \\(\\rightarrow\\) Visualize) from Chapter 22 on this random walk process to investigate what happens when we run additional simulations and compute the ensemble average. Let’s apply this workflow: For the “Do once” step, we will define a function called random_walk that has the number of steps as an input: random_walk &lt;- function(number_steps) { # Set up vector of results x &lt;- array(0, dim = number_steps) for (i in 2:number_steps) { if (runif(1) &lt; 0.5) { x[i] &lt;- x[i - 1] - 1 } # Move right else { x[i] &lt;- x[i - 1] + 1 } # Move left } out_x &lt;- tibble(t = 0:(number_steps - 1), x) return(out_x) } Next to “Do several times” we will run this random walk function for 100 steps and 500 simulations and display the spaghetti plot in Figure 23.2: number_steps &lt;- 100 # Then number of steps in random walk n_sims &lt;- 500 # The number of simulations # Compute solutions random_walk_sim &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ random_walk(number_steps)) %&gt;% map_dfr(~.x, .id = &quot;simulation&quot;) # Plot these all together ggplot(data = random_walk_sim, aes(x = t, y = x)) + geom_line(aes(color = simulation)) + ggtitle(&quot;Random Walk&quot;) + guides(color = &quot;none&quot;) + labs(x=&quot;Steps&quot;) FIGURE 23.2: Spaghetti plot of 500 simulations for the random walk. For the “Summarize” and “Visualize” steps we will compute the 95% confidence interval, structuring the code similar to Figure 22.5 from Chapter 22. Try writing this code out on your own, but the results are shown in Figure 23.3. FIGURE 23.3: Ensemble average of 500 simulations for the random walk. Two interesting things are occurring in Figures 23.2 and 23.3. First, Figure 23.3 suggests that on average you go nowhere (in other words, the average position is \\(x=0\\)), but as the number of steps increases, you are very likely to be somewhere (in other words, the confidence interval increases as the number of steps increases). Second, the 95% confidence interval appears to be a square root function \\(y=a\\sqrt{t}\\). One way that we could confirm this is by running more realizations and investigating the ensemble average (Exercise 23.3). 23.3 Random walk mathematics Another way to corroborate our observations in Figures 23.2 and 23.3 is with mathematics. First we define some terminology and notation. Call \\(x^{n}\\) the position \\(x\\) at step \\(n\\) in a random walk that increments with step size \\(\\Delta x\\) (Equation (23.1): \\[\\begin{equation} x^{n}=x^{n-1}+r_{s} \\; \\Delta x = \\sum_{s=1}^{n} \\Delta x \\; r_{s}, \\tag{23.1} \\end{equation}\\] with \\(\\Delta x\\) being the jump size (in our example above \\(\\Delta x=1\\)), and \\(s\\) the particular step in our random walk. We denote \\(r_{s}\\) as a random variable that takes the value of \\(-1\\) or \\(1\\). Another way to write \\(r_{s}\\) is with Equation (23.2): \\[\\begin{equation} r_{s}=\\begin{cases} -1 &amp; p(-1)=0.5 \\\\ 1 &amp; p(1)=0.5 \\end{cases} \\tag{23.2} \\end{equation}\\] The way to interpret Equation (23.1) is that the variable \\(r_{s}\\) equals \\(-1\\) (\\(r_{s}=-1\\)) with probability 0.5 (\\(p(-1)=0.5\\)). We can write Equation (23.1) as a single summation because the position depends on the different values of \\(-1\\) or 1 generated at each step, only as long as we keep track of the sequence of \\(-1\\) or 1 for each step \\(s\\). When we run multiple simulations we will use the notation \\(x_{j}^{n}\\), which is the position at step \\(n\\) for simulation \\(j\\). Let’s introduce some terminology to help us out here. The quantity \\(\\displaystyle \\big \\langle X \\big \\rangle = \\sum_{i=1}^{n} p(X) \\cdot X\\) is the expected value for a discrete random variable. What we do is weight the value of each possibility by its probability. For the random variable \\(r_{s}\\) we have: \\[\\begin{equation} \\big \\langle r_{s} \\big \\rangle = (1) \\cdot 0.5 + (- 1) \\cdot 0.5 = 0 \\end{equation}\\] Nice! The expected value is a linear operator47. The next step is to determine the expected value of \\(x^{n}\\). Here we have \\(J\\) different simulations; this will be computed as \\(\\displaystyle \\big \\langle x^{n} \\big \\rangle = \\frac{1}{J} \\sum_{j=1}^{J} p(x_{j}^{n}) \\cdot x_{j}^{n}\\). We can further compute the expected value using Equation (23.1): \\[\\begin{equation} \\big \\langle x^{n} \\big \\rangle = \\big \\langle \\sum_{j=1}^{J} \\left( \\Delta x \\; r_{s} \\right) \\big \\rangle = \\sum_{j=1}^{J} \\left( \\big \\langle \\Delta x \\; r_{s} \\big \\rangle \\right) = \\sum_{j=1}^{J} \\Delta x \\cdot \\big \\langle \\; r_{s} \\big \\rangle = 0 \\end{equation}\\] Imagine that! All of this mathematics leads to the conclusion that the expected value of \\(x^{n}\\) is zero! In other words, on average a random walk goes nowhere! 23.3.1 Random walk variance Now that we have characterized the expected value we will determine the variance48 of \\(x^{n}\\), or \\(\\langle (x^{n})^{2} \\rangle\\). This is still a lot of work with summations, but it is worth it! First, multiply out the square of Equation (23.1) using properties of summation: \\[\\begin{equation} (x^{n})^{2} = \\left( \\sum_{s=1}^{n} \\left( \\Delta x \\; r_{s} \\right) \\right)^{2} = \\left( \\Delta x \\right)^{2} \\sum_{s=1}^{n} \\sum_{t=1}^{n} (r_{s} \\cdot r_{t} ) \\end{equation}\\] Now when we compute the expected value for \\(\\displaystyle \\big \\langle (x^{n})^{2} \\big \\rangle\\) we need to consider the double summation - so this means there are two different indices \\(s\\) and \\(t\\). However because of summation properties we have: \\[\\begin{equation} \\big \\langle (x^{n})^{2} \\big \\rangle = \\left( \\Delta x \\right)^{2} \\sum_{s=1}^{n} \\sum_{t=1}^{n} \\big \\langle r_{s} \\cdot r_{t} \\big \\rangle \\end{equation}\\] So really we need to consider the term \\(\\displaystyle \\big \\langle r_{s} \\cdot r_{t} \\big \\rangle\\). While that may seem scary, let’s break it down into two cases: when \\(t=s\\) and \\(t \\neq s\\): - Case \\(t \\neq s\\): Here we need to multiply together all the possible combinations for the random variable \\(r_{s} \\cdot r_{t}\\). There are four possibilities: \\(r_{s} \\cdot r_{t}=(-1) \\cdot (1)\\), \\(r_{s} \\cdot r_{t}=(-1) \\cdot (-1)\\), \\(r_{s} \\cdot r_{t}=(1) \\cdot (1)\\), \\(r_{s} \\cdot r_{t}=(1) \\cdot (-1)\\). When we multiply these results and consider the different probabilities associated with them we get the following random variable for \\(r_{s} \\cdot r_{t}\\): \\[\\begin{equation} r_{s} \\cdot r_{t} =\\begin{cases} -1 &amp; p(-1)=0.5 \\\\ 1 &amp; p(1)=0.5 \\end{cases} \\tag{23.3} \\end{equation}\\] Then the expected value is the following: \\[\\begin{equation} \\big \\langle r_{s} \\cdot r_{t} \\big \\rangle = (1) \\cdot 0.5 + (-1) \\cdot 0.5 = 0 \\end{equation}\\] - Case \\(t = s\\): This case is a little easier. In both instances (when \\(r_{t}=1\\) or \\(r_{t}=-1\\)) the variable \\(r_{t} \\cdot r_{t}\\) equals 1, so the expected value \\(\\displaystyle \\big \\langle r_{s} \\cdot r_{t} \\big \\rangle\\) is simply 1! So as a result: \\[\\begin{equation} \\begin{split} \\big \\langle (x^{n})^{2} \\big \\rangle &amp;= \\left( \\Delta x \\right)^{2} \\sum_{s=1}^{n} \\big \\langle r_{s}^{2} \\big \\rangle \\\\ &amp; = \\left( \\Delta x \\right)^{2} \\sum_{s=1}^{n} 1\\\\ &amp;= n \\left( \\Delta x \\right)^{2} \\end{split} \\tag{23.4} \\end{equation}\\] Equation (23.4) tells us that the variance, or the mean square displacement, is proportional to \\(n\\). Another way to state this is that the standard deviation (the square root of the variance) is equal to \\(\\pm \\sqrt{n} \\; \\Delta x\\), where \\(n\\) is the current step. This matches up with our graphs from earlier since \\(\\Delta x =1\\)! Informally, the variance tells us that on average you go nowhere, but eventually you travel everywhere - how cool! 23.4 Continuous random walks and diffusion On a final note, we can extend the discrete random walk to continuous time. This process is perhaps similar to how you may have seen that the Riemann sum to (discretely) approximate the area underneath a curve and the horizontal axis becomes a definite integral. Define the variable \\(t\\) such that \\(t= n \\Delta t\\). Equivalently \\(\\displaystyle n = \\frac{t}{\\Delta t}\\). With this information we can use Equation (23.4) to do the following: \\[\\begin{equation} \\big \\langle (x^{n})^{2} \\big \\rangle = \\frac{t}{\\Delta t} ( \\Delta x)^{2}. \\tag{23.5} \\end{equation}\\] The quantity \\(\\displaystyle D = \\frac{( \\Delta x)^{2}}{2 \\Delta t}\\) is known as the diffusion coefficent. So then the mean square displacement can be arranged as \\(\\langle (x^{n})^{2} \\rangle = 2Dt\\), confirming again that the variance grows proportional to \\(t\\). To connect this back to our discussion of stochastic differential equations, understanding random walks helps us to understand how demographic and environmental stochasticity may affect a differential equation. An excellent, highly readable book on random walks in biology is Howard Berg (1993). Since there is a randomness to solution trajectories, we will repeatedly use ensemble averages (developed in Chapter 22) to understand the expected behaviors of a stochastic differential equation. The next chapters will apply the workflows studied here to investigate the connections between random walks and stochastic differential equations. 23.5 Exercises Exercise 23.1 When doing the random walk mathematics, we made the claim that \\(\\displaystyle x^{n} = \\sum_{s=1}^n \\Delta x \\, r_{s}\\), where \\(r_{s}\\) takes on the value of \\(-1\\) or \\(1\\). Set \\(\\Delta x = 1\\) and do a random walk for 5 steps, keeping track whether the value of \\(r\\) is \\(-1\\) or \\(1\\) at each step. Does the final position equal the sum of all the values of \\(-1\\) or \\(1\\)? Exercise 23.2 Let \\(r_{s}\\) be the random variable defined by Equation (23.2). Multiply out the following summation: \\(\\displaystyle \\sum_{s=1}^{2} \\sum_{t=1}^{2} (r_{s} r_{t} )\\). Use the previous result to compute the expected value \\(\\displaystyle \\big \\langle \\sum_{s=1}^{2} \\sum_{t=1}^{2} r_{s} \\cdot r_{t} \\big \\rangle\\) Exercise 23.3 Re-run the code used to generate Figure 23.3, but where the number of realizations is set to 1000 and 5000 (this may take some time to compute). Do your results conform to the observation that the expected position is zero, but the uncertainty grows as the number of steps increases? Can you determine the value of \\(a\\) such that \\(y=a\\cdot\\sqrt{n}\\) that parameterizes the 95% confidence interval as a function of \\(n\\)? Exercise 23.4 Use the fact that the diffusion coefficient is equal to \\(\\displaystyle D = \\frac{ (\\Delta x)^{2}}{2\\Delta t}\\) to answer the following questions. Solve \\(\\displaystyle D = \\frac{ (\\Delta x)^{2}}{2\\Delta t}\\) to isolate \\(\\Delta t\\) on one side of the expression. The diffusion coefficient for oxygen in water is approximately \\(10^{-5}\\) cm\\(^{2}\\) sec\\(^{-1}\\). Use that value to complete the following table: Distance (\\(\\Delta x\\)) 1 \\(\\mu\\)m = 10\\(^{-6}\\) m 10 \\(\\mu\\)m 1 mm 1 cm 1 m Diffusion time (\\(\\Delta t\\)) Report the diffusion time in an appropriate unit (seconds, minutes, hours, years) accordingly. Navigate to the following website, which lists sizes of different cells:. For what cells would diffusion be a reasonable process to transport materials? Exercise 23.5 Consider Equation (23.5). Evaluate separately the effect of \\(\\Delta x\\) and \\(\\Delta t\\) on the variance. How would you characterize the variance if either of them independently is small or large? Exercise 23.6 Compute \\(\\langle r \\rangle\\) for the following random variable: \\[\\begin{equation} r=\\begin{cases} -1 &amp; p(-1)=0.52 \\\\ 1 &amp; p(1)=0.48 \\end{cases} \\end{equation}\\] Exercise 23.7 Compute \\(\\langle r \\rangle\\) for the following random variable: \\[\\begin{equation} r=\\begin{cases} -1 &amp; p(-1)=q \\\\ 1 &amp; p(1)=(1-q) \\end{cases} \\end{equation}\\] Exercise 23.8 Consider the following random variable: \\[\\begin{equation} r_{q}= \\begin{cases} -1 &amp; p(-1) = 1/3 \\\\ 0 &amp; p(0)= 1/3\\\\ 1 &amp; p(1)=1/3 \\end{cases} \\end{equation}\\] Modify the code for the one-dimensional random walk to generate a simulation of this random walk and plot your result. You can do this by applying an if else statement as shown in the code chunk below. Compute \\(\\displaystyle \\langle r_{q} \\rangle\\) and \\(\\displaystyle \\langle r_{q}^{2} \\rangle\\). Based on your last answer, explain how this random variable introduces a different random walk than the one described in this chapter. In what ways would this random walk change the calculations for the mean and variance of the ensemble simulations? # Code for random variable r_q: p &lt;- runif(1) if (p &lt; 1 / 3) { x[i] &lt;- x[i - 1] - 1 } else if (1 / 3 &lt;= p &amp; p &lt; 2 / 3) { x[i] &lt;- x[i - 1] } else { x[i] &lt;- x[i - 1] + 1 } Exercise 23.9 In this exercise you will write code and simulate a two-dimensional random walk. In a given step you can either move (1) left, (2) right, (3) up, or (4) down. (You cannot move up and left for example). The random walk starts at \\((x,y)=(0,0)\\). With \\(\\Delta x = 1\\), the random walk at step \\(n\\) can be described by \\(\\displaystyle (x,y)^{n} = \\sum_{s=1}^{n} r_{d}\\), where \\(r_{d}\\) is one of the four motions, represented as a coordinate pair. (A movement up is \\(r_{d}=(0,1)\\) for example.) Define a variable \\(r_{d}\\) that models the motion from step to step. Modify the code for the one-dimensional random walk to incorporate this two-dimensional random walk. One way to do this is to create a variable \\(y\\) structured similar to \\(x\\), and to have multiple if statements in the for loop that moves y. Plot a few different realizations of your sample paths. If we were to compute the mean and variance of the ensemble simulations, what do you think they would be? References "],["diffusion-24.html", "Chapter 24 Diffusion and Brownian Motion 24.1 Random walk redux 24.2 Simulating Brownian motion 24.3 Exercises", " Chapter 24 Diffusion and Brownian Motion Studying random walks in Chapter 23 led to some surprising results, namely that for an unbiased random walk the mean displacement was zero but the variance increased proportional to the step number. In this chapter we will revisit the random walk problem from another perspective that further strengthens its connection to understanding diffusion. Let’s get started! 24.1 Random walk redux The random walk derivation in Chapter 23 focused on the position of a particle on the random walk, based upon prescribed rules of moving to the left and the right. To revisit this random walk we consider the probability (between 0 and 1) that a particle is at position \\(x\\) in time \\(t\\), denoted as \\(p(x,t)\\). In other words, rather than focusing on where the particle is, we focus on the chance that the particle will be at a given spot. A way to conceptualize a random walk is that any given position \\(x\\), a particle can arrive to that position from either the left or the right (Figure 24.1): FIGURE 24.1: Schematic diagram for the one-dimensional random walk. We can generalize Figure 24.1 further where the particle moves in increments \\(\\Delta x\\), as defined in Equation (24.1): \\[\\begin{equation} p(x,t+\\Delta t) = \\frac{1}{2} p(x-\\Delta x,t) + \\frac{1}{2} p(x+\\Delta x,t) \\tag{24.1} \\end{equation}\\] To analyze Equation (24.1) we apply Taylor approximations on each side of Equation (24.1). First let’s do a locally linear approximation for \\(p(x,t+\\Delta t)\\): \\[\\begin{equation} p(x,t+\\Delta t) \\approx p(x,t) + \\Delta t \\cdot p_{t}, \\end{equation}\\] where we have dropped the shorthand \\(p_{t}(x,t)\\) as \\(p_{t}\\). On the right hand side of Equation (24.1) we will compute the 2nd degree (quadratic) Taylor polynomial: \\[\\begin{align*} \\frac{1}{2} p(x-\\Delta x,t) &amp; \\approx \\frac{1}{2} p(x,t) - \\frac{1}{2} \\Delta x \\cdot p_{x} + \\frac{1}{4} (\\Delta x)^{2}\\cdot p_{xx} \\\\ \\frac{1}{2} p(x+\\Delta x,t) &amp; \\approx \\frac{1}{2} p(x,t) + \\frac{1}{2} \\Delta x \\cdot p_{x} + \\frac{1}{4} (\\Delta x)^{2} \\cdot p_{xx} \\end{align*}\\] With these approximations we can re-write Equation (24.1) as Equation (24.2): \\[\\begin{equation} \\Delta t \\cdot p_{t} = \\frac{1}{2} (\\Delta x)^{2} p_{xx} \\rightarrow p_{t} = \\frac{1}{2} \\frac{(\\Delta x)^{2}}{\\Delta t} \\cdot p_{xx} \\tag{24.2} \\end{equation}\\] Equation (24.2) is called a partial differential equation - what this means is that it is a differential equation with derivatives that depend on two variables (\\(x\\) and \\(t\\) (two derivatives). As studied in Chapter 23, Equation (24.2) is called the diffusion equation. In Equation (24.2) we can also define \\(\\displaystyle D = \\frac{1}{2} \\frac{(\\Delta x)^{2}}{\\Delta t}\\) so \\(p_{t}=D \\cdot p_{xx}\\). The solution to Equation (24.2) is given by Equation (24.3).49 \\[\\begin{equation} p(x,t) = \\frac{1}{\\sqrt{4 \\pi Dt} } e^{-x^{2}/(4 D t)} \\tag{24.3} \\end{equation}\\] What Equation (24.3) represents is the probability that the particle is at the position \\(x\\) at time \\(t\\). Figure 24.2 shows profiles for \\(p(x,t)\\) when \\(D=0.5\\) at different values of \\(t\\). FIGURE 24.2: Profiles of \\(p(x,t)\\) (Equation (24.3)) for different values of \\(t\\) with \\(D = 0.5\\). As you can see, as time increases the graph of \\(p(x,t)\\) gets flatter - or more uniform. What this tells you is that the longer \\(t\\) increases it is less likely to find the particle at the origin. 24.1.1 Verifying the solution to the diffusion equation Verifying that Equation (24.3) is the solution to Equation (24.2) is a good review of your multivariable calculus skills! As a first step to verifying this solution, let’s take the partial derivative with respect to \\(x\\) and \\(t\\). First we will compute the partial derivative of \\(p\\) with respect to the variable \\(x\\) (represented as \\(p_{x}\\)): \\[\\begin{align*} p_{x} &amp;= \\frac{\\partial }{\\partial x} \\left( \\frac{1}{\\sqrt{4 \\pi Dt} } e^{-x^{2}/(4 D t)} \\right) \\\\ &amp;= \\frac{1}{\\sqrt{4 \\pi Dt} } e^{-x^{2}/(4 D t)} \\cdot \\frac{-2x}{4Dt} \\end{align*}\\] Notice something interesting here: \\(\\displaystyle p_{x} = p(x,t) \\cdot \\left( \\frac{-x}{2Dt} \\right)\\). To compute the second derivative, we have the following expressions by applying the product rule: \\[\\begin{align*} p_{xx} &amp;= p_{x} \\cdot \\left( \\frac{-x}{2Dt} \\right) - p(x,t) \\cdot \\left( \\frac{1}{2Dt} \\right) \\\\ &amp;= p(x,t) \\cdot \\left( \\frac{-x}{2Dt} \\right) \\cdot \\left( \\frac{-x}{2Dt} \\right)- p(x,t) \\cdot \\left( \\frac{1}{2Dt} \\right) \\\\ &amp;= p(x,t) \\left( \\left( \\frac{-x}{2Dt} \\right)^{2} - \\left( \\frac{1}{2Dt} \\right) \\right) \\\\ &amp;= p(x,t) \\left( \\frac{x^{2}-2Dt}{(2Dt)^{2}}\\right). \\end{align*}\\] So far so good. Now computing \\(p_{t}\\) gets a little tricky because this derivative involves both the product rule with the chain rule in two places (the variable \\(t\\) appears twice in the formula for \\(p(x,t)\\)). To aid in computing the derivative we identify two functions \\(\\displaystyle f(t) = (4 \\pi D t)^{-1/2}\\) and \\(\\displaystyle g(t) = -x^{2} \\cdot (4Dt)^{-1}\\). This changes \\(p(x,t)\\) into \\(p(x,t) = f(t) \\cdot e^{g(t)}\\). In this way \\(p_{t} = f&#39;(t) \\cdot e^{g(t)} + f(t) \\cdot e^{g(t)} \\cdot g&#39;(t)\\). Now we can focus on computing the individual derivatives \\(f&#39;(t)\\) and \\(g&#39;(t)\\) (after simplification - be sure to verify these on your own!): \\[\\begin{align*} f&#39;(t) &amp;= -\\frac{1}{2} (4 \\pi D t)^{-3/2} \\cdot 4 \\pi D = -2\\pi D \\; (4 \\pi D t)^{-3/2} \\\\ g&#39;(t) &amp;= x^{2}\\; (4Dt)^{-2} 4D = \\frac{x^{2}}{4Dt^{2}} \\end{align*}\\] Assembling these results together, we have the following: \\[\\begin{align*} p_{t} &amp;= f&#39;(t) \\cdot e^{g(t)} + f(t) \\cdot e^{g(t)} \\cdot g&#39;(t) \\\\ &amp;= -2\\pi D \\; (4 \\pi D t)^{-3/2} \\cdot e^{-x^{2}/(4 D t)} + \\frac{1}{\\sqrt{4 \\pi Dt} } \\cdot e^{-x^{2}/(4 D t)} \\cdot \\frac{x^{2}}{4Dt^{2}} \\\\ &amp;= \\frac{1}{\\sqrt{4 \\pi Dt} } \\cdot e^{-x^{2}/(4 D t)} \\left( -2 \\pi D \\; (4 \\pi D t)^{-1} + \\frac{x^{2}}{4Dt^{2}} \\right) \\\\ &amp;= \\frac{1}{\\sqrt{4 \\pi Dt} } \\cdot e^{-x^{2}/(4 D t)} \\left( -\\frac{1}{2t} + \\frac{x^{2}}{4Dt^{2}} \\right) \\\\ &amp;= p(x,t) \\left( -\\frac{1}{2t} + \\frac{x^{2}}{4Dt^{2}} \\right) \\end{align*}\\] Wow. Verifying that Equation (24.3) is a solution to the diffusion equation is getting complicated, but also notice that through algebraic simplification, \\(\\displaystyle p_{t} = p(x,t) \\left(\\frac{x^{2}-2Dt}{4Dt^{2}} \\right)\\). When we compare \\(p_{t}\\) to \\(D p_{xx}\\), they are equal! The connections between diffusion and probability are so strong. Equation (24.3) is related to the formula for a normal probability density function (Equation (9.1) from Chapter 9)! In this case, the standard deviation in Equation (24.3) equals \\(\\sqrt{2Dt}\\) and is time dependent (see Exercise 24.2). Even though we approached the random walk differently here compared to Chapter 23, we also saw that the variance grew proportional to the time spent, so there is some consistency. 24.2 Simulating Brownian motion Another name for the process of a particle undergoing small random movements is Brownian Motion. We can simulate Brownian motion similar to the random walk as discussed in Chapter 23. Brownian motion is connected to the diffusion equation (Equation (24.2)) and its solution (Equation (24.3)). These connections are helpful when simulating stochastic differential equations. To simulate Brownian motion we will also apply the workflow from Chapter 22 (Do once \\(\\rightarrow\\) Do several times \\(\\rightarrow\\) Summarize \\(\\rightarrow\\) Visualize). 24.2.1 Do once First we define a function called brownian_motion that will compute a sample path given the: number of steps to run the stochastic process; diffusion coefficient \\(D\\); timestep \\(\\Delta t\\); a sample path will be computed (see Figure 24.3). brownian_motion &lt;- function(number_steps, D, deltaT) { # D: diffusion coefficient # deltaT: timestep length ### Set up vector of results x &lt;- array(0, dim = number_steps) for (i in 2:number_steps) { x[i] &lt;- x[i - 1] + sqrt(2 * D * deltaT) * rnorm(1) } out_x &lt;- tibble(t = 0:(number_steps - 1), x) return(out_x) } # Run a sample trajectory and plot try1 &lt;- brownian_motion(100, 0.5, 0.1) plot(try1, type = &quot;l&quot;) FIGURE 24.3: Sample trajectory for a realization of Brownian motion. The horizontal axis represents a step of the random walk. 24.2.2 Do several times Once we have the function for Brownian motion defined we can then run this process several times and plot the spaghetti plot (try the following code out on your own): number_steps &lt;- 200 # Then number of steps in random walk D &lt;- 0.5 # The value of the diffusion coefficient dt &lt;- 0.1 # The timestep length n_sims &lt;- 500 # The number of simulations # Compute solutions brownian_motion_sim &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ brownian_motion(number_steps, D, dt)) %&gt;% map_dfr(~.x, .id = &quot;simulation&quot;) # Plot these all together ggplot(data = brownian_motion_sim, aes(x = t, y = x)) + geom_line(aes(color = simulation)) + ggtitle(&quot;Random Walk&quot;) + guides(color = &quot;none&quot;) 24.2.3 Summarize and visualize Finally, the 95% confidence interval is computed and plotted in Figure 24.4, using similar code from Chapter 22 to compute the ensemble average. Note that the horizontal axis is time so each step is scaled by dt. FIGURE 24.4: Ensemble average of 500 simulations for the random walk. Each step on the horizontal axis is scaled by dt. I sure hope the results are very similar to ones generated in Chapter 23 (especially Figure 23.3) - this is no coincidence! With the ideas of a random walk developed here and in Chapter 23, we will now be able to understand and simulate how small changes in a variable or parameter affect the solutions to a differential equation. Looking ahead to Chapters 25 and 26, we will simulate stochastic processes using numerical methods (Euler’s method in Chapter 4) with Brownian motion. Onward! 24.3 Exercises Exercise 24.1 Through direct computation, verify the following calculations: When \\(\\displaystyle f(t)=\\frac{1}{\\sqrt{4 \\pi Dt} }\\), then \\(\\displaystyle f&#39;(t)=-2\\pi D (4 \\pi D t)^{-3/2}\\) When \\(\\displaystyle g(t)=\\frac{-x^{2}}{4Dt}\\), then \\(\\displaystyle g&#39;(t)=\\frac{x^{2}}{4Dt^{2}}\\) Verify that \\(\\displaystyle \\left( -\\frac{1}{2t} + \\frac{x^{2}}{4Dt^{2}} \\right)= \\left( \\frac{x^{2}-2Dt}{(2Dt)^{2}}\\right)\\) Exercise 24.2 The equation for the normal distribution is \\(\\displaystyle f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma } e^{-(x-\\mu)^{2}/(2 \\sigma^{2})}\\), with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). Examine the formula for the diffusion equation (Equation (24.3)) and compare it to the formula for the normal distribution. If Equation (24.3) represents a normal distribution, what do \\(\\mu\\) and \\(\\sigma^{2}\\) equal? Exercise 24.3 For this problem you will investigate \\(p(x,t)\\) (Equation (24.3)) with \\(\\displaystyle D=\\frac{1}{2}\\). Evaluate \\(\\displaystyle \\int_{-1}^{1} p(x,10) \\; dx\\). Write a one sentence description of what this quantity represents. Using desmos or some other numerical integrator, complete the following table: Equation Result \\(\\displaystyle \\int_{-1}^{1} p(x,10) \\; dx=\\) \\(\\displaystyle \\int_{-1}^{1} p(x,5) \\; dx=\\) \\(\\displaystyle \\int_{-1}^{1} p(x,2.5) \\; dx=\\) \\(\\displaystyle \\int_{-1}^{1} p(x,1) \\; dx=\\) \\(\\displaystyle \\int_{-1}^{1} p(x,0.1) \\; dx=\\) \\(\\displaystyle \\int_{-1}^{1} p(x,0.01) \\; dx=\\) \\(\\displaystyle \\int_{-1}^{1} p(x,0.001) \\; dx=\\) Based on the evidence from your table, what would you say is the value of \\(\\displaystyle \\lim_{t \\rightarrow 0^{+}} \\int_{-1}^{1} p(x,t) \\; dx\\)? Now make graphs of \\(p(x,t)\\) at each of the values of \\(t\\) in your table. What would you say is occuring in the graph as \\(\\displaystyle \\lim_{t \\rightarrow 0^{+}} p(x,t)\\)? Does anything surprise you? (The results you computed here lead to the foundation of what is called the Dirac delta function.) Exercise 24.4 Consider the function \\(\\displaystyle p(x,t) = \\frac{1}{\\sqrt{4 \\pi D t}} e^{-x^{2}/(4 D t)}\\). Let \\(x=1\\). Explain in your own words what the graph \\(p(1,t)\\) represents as a function of \\(t\\). Graph several profiles of \\(p(1,t)\\) when \\(D = 1\\), \\(2\\), and \\(0.1\\). How does the value of \\(D\\) affect the profile? Exercise 24.5 In statistics an approximation for the 95% confidence interval is twice the standard deviation. Confirm this by adding the curve \\(y=2\\sqrt{2Dt}\\) to the ensemble average plot in Figure 24.4. Recall that \\(D\\) was equal to \\(0.5\\) and \\(\\Delta t = 0.1\\), so the horizontal axis will need to be scaled appropriately. Exercise 24.6 Consider the function \\(\\displaystyle p(x,t) = \\frac{1}{\\sqrt{\\pi t}} e^{-x^{2}/t}\\): Using your differentiation skills compute the partial derivatives \\(p_{t}\\), \\(p_{x}\\), and \\(p_{xx}\\). Verify \\(p(x,t)\\) is consistent with the diffusion equation \\(\\displaystyle p_{t}=\\frac{1}{4} p_{xx}\\). Exercise 24.7 Modify the code used to generate Figure 24.4 with \\(D=10, \\; 1, \\; 0.1, \\; 0.01\\). Generally speaking, what happens to the resulting ensemble average when \\(D\\) is small or large? In which scenarios are stochastic effects more prevalent? Exercise 24.8 For the one-dimensional random walk we discussed where there was an equal chance of moving to the left or the right. Here is a variation on this problem. Let’s assume there is a chance \\(v\\) that it moves to the left (position \\(x - \\Delta x\\)), and therefore a chance is \\(1-v\\) that the particle remains at position \\(x\\). The basic equation that describes the particle’s position at position \\(x\\) and time \\(t + \\Delta t\\) is: \\[\\begin{equation} p(x,t + \\Delta t) = (1-v) \\cdot p(x,t) + v \\cdot p(x- \\Delta x,t) \\end{equation}\\] Apply the techniques of local linearization in \\(x\\) and \\(t\\) to show that this random walk is used to derive the following partial differential equation, called the advection equation: \\[\\begin{equation} p_{t} = - \\left( v \\cdot \\frac{ \\Delta x}{\\Delta t} \\right) \\cdot p_{x} \\end{equation}\\] Note: you only need to expand this equation to first order Exercise 24.9 Complete Exercise 23.9 if you haven’t already. If Equation (24.3) (a normal distribution) is the solution to the one-dimensional diffusion equation, what do you think the solution would be in the bivariate case? References "],["sdes-25.html", "Chapter 25 Simulating Stochastic Differential Equations 25.1 The stochastic logistic model 25.2 The Euler-Maruyama method 25.3 Adding stochasticity to parameters 25.4 Systems of stochastic differential equations 25.5 Concluding thoughts 25.6 Exercises", " Chapter 25 Simulating Stochastic Differential Equations In this chapter we will begin to combine our knowledge of random walks to numerically simulate stochastic differential equations, or SDEs for short. Here is the good news: our previous work comes into focus. This chapter returns to a specific model you are familiar with (the logistic differential equation) and examines it stochastically. Hopefully this specific example will allow you to see how the methods developed here work in other contexts. Let’s get started! 25.1 The stochastic logistic model Equation (25.1) begins with the logistic differential equation, but written a little differently by multiplying the differential \\(dt\\) to the right hand side: \\[\\begin{equation} dx = rx \\left(1 - \\frac{x}{K} \\right) \\; dt \\tag{25.1} \\end{equation}\\] One way to interpret Equation (25.1) is that a small change is the variable \\(x\\) (denoted is \\(dx\\)), which is equal to the rate \\(\\displaystyle rx \\left(1 - \\frac{x}{K} \\right)\\) multiplied by \\(dt\\). A direct way to incorporate stochastics is to modify Equation (25.1) by incorporating aspects of Brownian motion, as shown with Equation (25.2): \\[\\begin{equation} dx = \\underbrace{rx \\left(1 - \\frac{x}{K} \\right) \\; dt}_{\\text{Deterministic part}} + \\underbrace{\\sqrt{2D \\, dt} \\, \\mathcal{N}(0,1)}_{\\text{Stochastic part}} \\tag{25.2} \\end{equation}\\] In Equation (25.2), \\(D\\) represents the diffusion coefficient and \\(\\mathcal{N}(0,1)\\) signifies the normal distribution with mean zero and variance one.50 It may seem odd to express Equation (25.2) in this form (i.e. \\(dx = ...\\) versus \\(\\displaystyle \\frac{dx}{dt} = ...\\)). However a good way to think of this stochastic differential equation is that a small change in the variable \\(x\\) (represented by the term \\(dx\\)) is computed in two ways: \\[\\begin{equation} \\begin{split} \\mbox{Deterministic part: } &amp; rx \\left(1 - \\frac{x}{K} \\right) \\; dt \\\\ \\mbox{Stochastic part: } &amp; \\sqrt{2D \\, dt} \\, \\mathcal{N}(0,1) \\end{split} \\tag{25.3} \\end{equation}\\] To simplify things somewhat we will represent \\(\\sqrt{2D \\, dt} \\, \\mathcal{N}(0,1)\\) in Equation (25.2) with \\(dW(t)\\), so that \\(dW(t)=\\sqrt{2D \\, dt} \\, \\mathcal{N}(0,1)\\). The term \\(dW(t)\\) can be thought of as similar to a stochastic differential equation \\(\\displaystyle \\frac{dW}{dt} = \\sqrt{2D \\, dt}\\), with \\(W(0)=0\\). The solution \\(W(t)\\) is also representative of a Weiner process. See Logan and Wolesensky (2009) For more information. In most cases a Weiner process does not include the term \\(\\sqrt{2D}\\) (effectively \\(D=\\frac{1}{2}\\)). It is helpful to keep the term \\(D\\) as a control parameter for simulating the SDE (see Exercises 25.1 - 25.3). How does the stochastic part of this differential equation change the solution trajectory? It turns out that the “exact” solutions to problems like these are difficult (we will study a sample of exact solutions to SDEs in Chapter 27). Rather than focus on exact solution techniques we will apply the workflow developed in Chapter 22 (Do once \\(\\rightarrow\\) Do several times \\(\\rightarrow\\) Summarize \\(\\rightarrow\\) Visualize) by simulating several solution trajectories and then taking the ensemble average at each of the time points. 25.2 The Euler-Maruyama method One way to numerically solve a stochastic differential equation begins with a variation of Euler’s method. The Euler-Maruyama method accounts for stochasticity and implements the random walk (Brownian motion). We will build this method up step by step. Like Euler’s method, the Euler-Maruyama method begins by writing the differential \\(dx\\) as a difference: \\(dx = x_{n+1}-x_{n}\\), where \\(n\\) is the current step of the method. Likewise \\(dW(t) = W_{n+1} - W_{n}\\), which represents one step of the random walk, but we approximate this difference by \\(\\sqrt{2D \\Delta t} \\mathcal{N}(0,1)\\), where \\(\\Delta t\\) is the timestep length. Given \\(\\Delta t\\), diffusion coefficient \\(D\\), and starting value \\(x_{0}\\), we can define the following method. From the initial condition \\(x_{0}\\), compute the value at the next time step (\\(x_{0}\\)), which for Equation (25.2) is: \\[\\begin{equation*} x_{1} = x_{0} + rx_{0} \\left(1 - \\frac{x_{0}}{K} \\right) \\; \\Delta t + \\sqrt{2D \\, \\Delta t} \\, \\mathcal{N}(0,1) \\end{equation*}\\] Repeat this iteration to step \\(n\\), where \\(\\mathcal{N}(0,1)\\) is re-computed at each timestep: \\[\\begin{equation*} x_{n} = x_{n-1} + rx_{n-1} \\left(1 - \\frac{x_{n-1}}{K} \\right) \\; \\Delta t + \\sqrt{2D \\Delta t} \\, \\mathcal{N}(0,1) \\end{equation*}\\] That is it! We can apply this numerical method for as many steps as we want. In the demodelr package the function euler_stochastic will apply the Euler-Maruyama method to a stochastic differential equation. Just like the functions euler or rk4 there are some things that need to be set first: The size (\\(\\Delta t\\)) of your timestep. The value of the diffusion coefficient \\(D\\) (we will discuss this later). The number of timesteps you wish to run the method. More timesteps means more computational time. If \\(N\\) is the number of timesteps, \\(\\Delta t \\cdot N\\) is the total time. A function for our deterministic dynamics. For Equation (25.1) this equals \\(\\displaystyle rx \\left(1 - \\frac{x}{K} \\right)\\). A function for our stochastic dynamics. For Equation (25.1) this equals 1. The values of the vector of parameters \\(\\vec{\\alpha}\\). For the logistic differential equation we will take \\(r=0.8\\) and \\(K=100\\). Sample code for this stochastic differential equation is shown below, with the resulting trajectory of the solution in Figure 25.1. # Identify the deterministic and stochastic parts of the DE: deterministic_logistic &lt;- c(dx ~ r*x*(1-x/K)) stochastic_logistic &lt;- c(dx ~ 1) # Identify the initial condition and any parameters init_logistic &lt;- c(x=3) logistic_parameters &lt;- c(r=0.8, K=100) # parameters: a named vector # Identify how long we run the simulation deltaT_logistic &lt;- .05 # timestep length timesteps_logistic &lt;- 200 # must be a number greater than 1 # Identify the standard deviation of the stochastic noise D_logistic &lt;- 1 # Do one simulation of this differential equation logistic_out &lt;- euler_stochastic( deterministic_rate = deterministic_logistic, stochastic_rate = stochastic_logistic, initial_condition = init_logistic, parameters = logistic_parameters, deltaT = deltaT_logistic, n_steps = timesteps_logistic, D = D_logistic ) # Plot out the solution ggplot(data = logistic_out) + geom_line(aes(x=t,y=x)) FIGURE 25.1: One realization of Equation (25.2). Let’s break the code down to generate Figure 25.1 step by step: We identify the deterministic and stochastic parts to our differential equation with the variables deterministic_logistic and stochastic_logistic. The same structure is used for Euler’s method from Chapter 4. Similar to Euler’s method we need to identify the initial conditions (init_logistic), parameters (logistic_parameters), \\(\\Delta t\\) (deltaT_logistic), and number of timesteps (timesteps_logistic). The diffusion coefficient for the stochastic process (\\(D\\)) is represented with D_logistic. The command euler_stochastic does one realization of the Euler-Maruyama method. 25.2.1 Do several times Figure 25.1 shows one sample trajectory of our solution, but there is benefit to running several simulations and then plotting out all the solution trajectories together. The code presented below accomplishes that task and makes a plot of all the solution trajectories (Figure 25.2). This code is similar to code presented in Chapter 22. As you may recall, the main engine of the code is contained in the map( ~ euler_stochastic ... ) which re-runs the codes for the number of times specified in n_sims. # Many solutions n_sims &lt;- 100 # The number of simulations # Compute solutions logistic_run &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ euler_stochastic(deterministic_rate = deterministic_logistic, stochastic_rate = stochastic_logistic, initial_condition = init_logistic, parameters = logistic_parameters, deltaT = deltaT_logistic, n_steps = timesteps_logistic, D = D_log)) %&gt;% map_dfr(~ .x, .id = &quot;simulation&quot;) # Plot these all together ggplot(data = logistic_run) + geom_line(aes(x=t, y=x, color = simulation)) + ggtitle(&quot;Spaghetti plot for the logistic SDE&quot;) + guides(color=&quot;none&quot;) FIGURE 25.2: Several different realizations of Equation (25.2). 25.2.2 Summarize and Visualize The code used to generate the ensemble average for the different simulations is shown below (try running this out on your own). This code is similar to ones presented in Chapter 22. The variable summarized_logistic first groups the simulations by the variable t in order to compute the quantiles across each of the simulations. # Compute Quantiles and summarize quantile_vals &lt;- c(0.025, 0.5, 0.975) ### Summarize the variables summarized_logistic &lt;- logistic_run %&gt;% group_by(t) %&gt;% summarize( q_val = quantile(x, # x is the column to compute the quantiles probs = quantile_vals ), q_name = quantile_vals ) %&gt;% pivot_wider(names_from = &quot;q_name&quot;, values_from = &quot;q_val&quot;, names_glue = &quot;q{q_name}&quot;) ### Make the plot ggplot(data = summarized_logistic) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x=t,ymin=q0.025,ymax=q0.975),alpha=0.2) + ggtitle(&quot;Ensemble average plot for the logistic SDE&quot;) 25.3 Adding stochasticity to parameters A second approach for modeling SDEs is to assume that the parameters are stochastic. For example, let’s say that the growth rate \\(r\\) in the logistic differential equation is subject to stochastic effects. How we would implement this is by replacing \\(r\\) with \\(r + \\mbox{ Noise }\\): \\[\\begin{equation} dx = (r + \\mbox{ Noise} ) \\; x \\left(1 - \\frac{x}{K} \\right) \\; dt \\end{equation}\\] Now what we do is separate out the terms that are multiplied by “Noise” - they will form the stochastic part of the differential equation. The terms that aren’t multipled by “Noise” form the deterministic part of the differential equation: \\[\\begin{equation} dx = r x \\left(1 - \\frac{x}{K} \\right) \\; dt + x \\left(1 - \\frac{x}{K} \\right) \\mbox{ Noise } \\; dt \\tag{25.4} \\end{equation}\\] When we write \\(\\mbox{ Noise } \\; dt = dW(t)\\), then the deterministic and stochastic parts to Equation (25.4) are easily identified: \\[\\begin{equation} \\begin{split} \\mbox{Deterministic part: } &amp; rx \\left(1 - \\frac{x}{K} \\right) \\; dt \\\\ \\mbox{Stochastic part: } &amp; x \\left(1 - \\frac{x}{K} \\right) dW(t) \\end{split} \\tag{25.5} \\end{equation}\\] There are a few things to notice with Equation (25.5). First, the deterministic part of the differential equation is what we would expect without incorporating the “Noise” term. Second, notice how the stochastic part of Equation (25.5) changed compared to Equation (25.3). Given these differences, let’s see what happens when we simulate this SDE! 25.3.1 Do once The following code will produce one realization of Equation (25.4), denoting the deterministic and stochastic parts as deterministic_logistic_r and stochastic_logistic_r respectively. We will also change the value of the diffusion coefficient \\(D\\) to equal 0.1. I encourage you to try this code on your own. # Identify the deterministic and stochastic parts of the DE: deterministic_logistic_r &lt;- c(dx ~ r*x*(1-x/K)) stochastic_logistic_r &lt;- c(dx ~ x*(1-x/K)) # Identify the initial condition and any parameters init_logistic &lt;- c(x=3) logistic_parameters &lt;- c(K=100,r=0.8) # parameters: a named vector # Identify how long we run the simulation deltaT_logistic &lt;- .05 # timestep length timesteps_logistic &lt;- 200 # must be a number greater than 1 # Identify the standard deviation of the stochastic noise D_logistic &lt;- .1 # Do one simulation of this differential equation logistic_out_r &lt;- euler_stochastic( deterministic_rate = deterministic_logistic_r, stochastic_rate = stochastic_logistic_r, initial_condition = init_logistic, parameters = logistic_parameters, deltaT = deltaT_logistic, n_steps = timesteps_logistic, D = D_logistic ) # Plot out the solution ggplot(data = logistic_out_r) + geom_line(aes(x=t,y=x)) 25.3.2 Do several times As we did before, we can run multiple iterations of Equation (25.4), which you can also try on your own. When you do this, does the resulting spaghetti plot look the same as or different from Figure 25.2? What could be some possible reasons for any differences? # Many solutions n_sims &lt;- 100 # The number of simulations # Compute solutions logistic_run_r &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ euler_stochastic(deterministic_rate = deterministic_logistic_r, stochastic_rate = stochastic_logistic_r, initial_condition = init_logistic, parameters = logistic_parameters, deltaT = deltaT_logistic, n_steps = timesteps_logistic, D = D_logistic) ) %&gt;% map_dfr(~ .x, .id = &quot;simulation&quot;) # Plot these all together ggplot(data = logistic_run_r) + geom_line(aes(x=t, y=x, color = simulation)) + ggtitle(&quot;Spaghetti plot for the logistic SDE&quot;) + guides(color=&quot;none&quot;) 25.3.3 Summarize and Visualize Now after running several different simulations we can plot the ensemble average, shown in Figure 25.3: # Compute Quantiles and summarize quantile_vals &lt;- c(0.025, 0.5, 0.975) # Summarize the variables summarized_logistic &lt;- logistic_run %&gt;% group_by(t) %&gt;% summarize( q_val = quantile(x, # x is the column to compute the quantiles probs = quantile_vals ), q_name = quantile_vals ) %&gt;% pivot_wider(names_from = &quot;q_name&quot;, values_from = &quot;q_val&quot;, names_glue = &quot;q{q_name}&quot;) # Make the plot ggplot(data = summarized_logistic) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x=t,ymin=q0.025,ymax=q0.975),alpha=0.2) + ggtitle(&quot;Ensemble average plot for the logistic SDE&quot;) FIGURE 25.3: Ensemble average plot for Equation (25.4). The results you obtain in Figure 25.3 might look similar to the ensemble average from simulations of Equation (25.2), but perhaps with more variability (represented in the shading for the 95% confidence interval). Letting \\(r\\) be a stochastic parameter affects how quickly the solution \\(x\\) increases to the carrying capacity at \\(x=100\\). 25.3.4 When you may have odd results Let’s discuss failure and other oddities when simulating SDEs. Figure 25.4 shows one odd realization of Equation (25.4): FIGURE 25.4: One odd realization of Equation . Notice in Figure 25.4 how the variable \\(x\\) has values that are negative. Is this a problem? … Maybe. As with analyzing (non-stochastic) differential equations, unanticipated results may be a signal of the following: You may have an error in the coding of the SDE (always double-check your work!) You chose too large of a timestep \\(\\Delta t\\). For Brownian motion, the variance grows proportional to \\(\\Delta t\\), so larger timesteps mean more variability when you simulate a random variable, which leads to larger stochastic jumps. (To be fair, large values of \\(\\Delta t\\) are also a shortcoming of Euler’s method.) Similar to the last point, the value of \\(D\\) may be too large. Recall that \\(D\\) is the diffusion coefficient and affects the rate of spread. Try setting \\(D=0\\) and seeing if that produces a result in line with your expectation from the (non-stochastic) differential equation. The Euler-Maruyama may cause the variable to move across an equilibrium solution, thereby undergoing a change in the long-term behavior.51 For the logistic equation, we know that \\(x=0\\) is a unstable equilibrium solution. If for this instance \\(x\\) becomes negative in Figure 25.4, it will move (quickly) away from \\(x=0\\) in the negative direction. Sometimes you may get NaN values in your simulations. You may still be able to compute the ensemble average, but you will need add to the following na.rm = TRUE in your quantile command (using the variable logistic_run as an example): # Summarize the variables summarized_logistic &lt;- logistic_run %&gt;% group_by(t) %&gt;% summarize( q_val = quantile(x, # x is the column to compute the quantiles probs = quantile_vals na.rm = TRUE), # Note the include of na.rm = TRUE q_name = quantile_vals ) %&gt;% pivot_wider(names_from = &quot;q_name&quot;, values_from = &quot;q_val&quot;, names_glue = &quot;q{q_name}&quot;) The last point is not a fault of the numerical method, but rather a feature of the differential equation. It is always helpful to understand your underlying dynamics before you start to implement a stochastic process! 25.4 Systems of stochastic differential equations To incorporate stochastic effects with a system of differential equations the process is similar to above, with a few changes. First, we need to keep track of the deterministic and stochastic parts for each variable. Second, when we summarize our results in computing the ensemble averages we also need to group by each of the variables. Let’s take a look at an example. Let’s revisit the tourism model from Chapter 13 (Sinay and Sinay 2006). This model described in Equation (25.6) relies on two non-dimensonal scaled variables: \\(R\\), which is the amount of the resource (as a percentage), and \\(V\\), the percentage of visitors that could visit (also as a percentage). \\[\\begin{equation} \\begin{split} \\frac{dR}{dt}&amp;=R\\cdot (1-R)-aV \\\\ \\frac{dV}{dt}&amp;=b\\cdot V \\cdot (R-V) \\end{split} \\tag{25.6} \\end{equation}\\] Equation (25.6) has two parameters \\(a\\) and \\(b\\), which relate to how the resource is used up as visitors come (\\(a\\)) and how as the visitors increase, word of mouth leads to a negative effect of it being too crowded (\\(b\\)). Sinay and Sinay (2006) reported \\(a=0.15\\) and \\(b=0.3316\\). Let’s assume that \\(b\\) is stochastic, leading to the following deterministic and stochastic parts to Equation (25.6): Deterministic part for \\(\\displaystyle \\frac{dR}{dt}\\): \\(R\\cdot (1-R)-aV\\) Stochastic part for \\(\\displaystyle \\frac{dR}{dt}\\): 0 Deterministic part for \\(\\displaystyle \\frac{dV}{dt}\\): \\(b\\cdot V \\cdot (R-V)\\) Stochastic part for \\(\\displaystyle \\frac{dV}{dt}\\): \\(V \\cdot (R-V)\\) Now we will apply the established workflow. The code to generate one realization of this stochastic process (the “Do once” step) is shown below (try this out on your own): # Identify the deterministic and stochastic parts of the DE: deterministic_tourism&lt;- c(dr ~ R*(1-R)-a*V, dv ~ b*V*(R-V)) stochastic_tourism &lt;- c(dr ~ 0, dv ~ V*(R-V)) # Identify the initial condition and any parameters init_tourism &lt;- c(R = 0.995, V = 0.00167) tourism_parameters &lt;- c(a = 0.15, b = 0.3316) # deltaT_tourism &lt;- .5 # timestep length timeSteps_tourism &lt;- 200 # must be a number greater than 1 # Identify the diffusion coefficient D_tourism &lt;- .05 # Do one simulation of this differential equation tourism_out &lt;- euler_stochastic( deterministic_rate = deterministic_tourism, stochastic_rate = stochastic_tourism, initial_condition = init_tourism, parameters = tourism_parameters, deltaT = deltaT_tourism, n_steps = timeSteps_tourism, D = D_tourism ) # We will pivot the data to ease in plotting: tourism_revised &lt;- tourism_out %&gt;% pivot_longer(cols=c(&quot;R&quot;,&quot;V&quot;)) # Plot out the solution ggplot(data = tourism_revised) + geom_line(aes(x=t,y=value)) + facet_grid(.~name) Note the additional creation of the variable tourism_revised and the command facet_grid in the plotting. Let’s break this down: The data frame tourism_out has three variables: \\(t\\), \\(R\\), and \\(V\\). Because we have more than one variable, we need to do an additional step in pivoting longer. (pivot_longer(cols=c(\"r\",\"v\"))) The data frame tourism_revised has more rows, but the second column (called name) contains the name of each variable), and the third column (value) has the associated value of each variable at a given point in time. In order to plot these variables in a multipanel plot we use facet_grid. Think of facet_grid(.~name) as a row by column display. We want the columns to be the instances of the variable name, with one row (.). The “Do several times” step follows a similar process as previous examples. The code to generate and plot multiple realizations of this stochastic process is below and also displayed with Figure 25.5. # Many solutions n_sims &lt;- 100 # The number of simulations # Compute solutions tourism_run &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ euler_stochastic( deterministic_rate = deterministic_tourism, stochastic_rate = stochastic_tourism, initial_condition = init_tourism, parameters = tourism_parameters, deltaT = deltaT_tourism, n_steps = timeSteps_tourism, D = D_tourism) ) %&gt;% map_dfr(~ .x, .id = &quot;simulation&quot;) # We will pivot the data to ease in plotting and computing: tourism_run_revised &lt;- tourism_run %&gt;% pivot_longer(cols=c(&quot;R&quot;,&quot;V&quot;)) # Plot these all together ggplot(data = tourism_run_revised) + geom_line(aes(x=t, y=value, color = simulation)) + facet_grid(.~name) + guides(color=&quot;none&quot;) FIGURE 25.5: Spaghetti plot for many realizations of Equation (25.6). Notice how the different realizations in Figure 25.5 show eventually the variables \\(R\\) and \\(V\\) approaching a steady-state value, but when \\(b\\) is stochastic it affects how quickly the steady state is approached. The variability in \\(R\\) and \\(V\\) (especially during the intervals \\(0 \\leq t \\leq 50\\)) may be important for how quickly this resource gets utilized. The final steps (Summarize and Visualize) can be down together and as you may have suspected, have a similar process to the previous examples. However when computing the ensemble average plot, there are three changes because we pivoted our results in the variable tourism_run_revised: When we group by our variables to compute the ensemble average we use group_by(t,name) so that we organize by each time t and the variables in our system (gathered in the column called name). When computing the ensemble average, we will need to specify that we are computing the quantile of the variable value in our pivoted data frame (tourism_run_revised). Finally, as in Figure 25.5, we need to apply the facet_grid command. The code to generate this ensemble average plot is shown below. Try this code out on your own to generate an ensemble average plot. # Compute Quantiles and summarize quantile_vals &lt;- c(0.025, 0.5, 0.975) # Summarize the variables summarized_tourism &lt;- tourism_run_revised %&gt;% group_by(t,name) %&gt;% # We also include grouping by each variable. summarize( q_val = quantile(value, # the column to compute quantiles probs = quantile_vals, na.rm = TRUE # Remove NA vlaues ), q_name = quantile_vals ) %&gt;% pivot_wider(names_from = &quot;q_name&quot;, values_from = &quot;q_val&quot;, names_glue = &quot;q{q_name}&quot;) # Make the plot ggplot(data = summarized_tourism) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975),alpha = 0.2) + facet_grid(.~name) + ggtitle(&quot;Ensemble average plot&quot;) 25.5 Concluding thoughts Whew. This chapter provided some concrete examples to model and visualize results for stochastic differential equations. To summarize, when you begin with a given differential equation and if you want to add stochastic effects to a variable, include a term \\(\\sqrt{2D \\, \\Delta t} \\, \\mathcal{N}(0,1)\\) in the differential equation to model Brownian motion. if want to add stochastic effects to a parameter, here are the following steps: Replace instances of the parameter with a “parameter + Noise” term (i.e \\(a \\rightarrow a + \\mbox{ Noise }\\)). Collect terms that are multiplied by Noise - they will form the stochastic part of the differential equation. The deterministic part of the differential equation should be your original (deterministic) differential equation. The most general form of the stochastic differential equation is: \\(\\displaystyle d\\vec{y} = f(\\vec{y},\\vec{\\alpha},t) \\; dt + g(\\vec{y},\\vec{\\alpha},t) \\; dW(t)\\), where \\(\\vec{y}\\) is the vector of variables, \\(\\vec{\\alpha}\\) is your vector of parameters, and \\(dW(t)\\) is the stochastic noise from the random walk. The workflow (Do once \\(\\rightarrow\\) Do several times \\(\\rightarrow\\) Summarize \\(\\rightarrow\\) Visualize) should provide structure to break down the different parts of simulation of SDEs. The general framework presented here allows you to computationally explore several different types of SDEs. If you are interested in further study of SDEs, see Gardiner (2004), J. P. Keener (2021), or Logan and Wolesensky (2009) to learn more. In the next two chapters we will investigate additional properties of SDEs. Onward! 25.6 Exercises Exercise 25.1 Consider the logistic differential equation (Equation (25.1)). In this chapter we set \\(D = 0.1\\). Re-run the code to generate one simulation with \\(D = 0.01, \\; 2, \\; 10\\). In each case, how does changing \\(D\\) affect the resulting solution trajectories? Exercise 25.2 Consider the logistic differential equation (Equation (25.1)). For this example we set \\(D = 1\\). Re-run the code to generate 100 simulations with \\(D = 0.01, \\; 2, \\; 10\\), and then compute the ensemble average. In each case, how does changing \\(D\\) affect the ensemble average? Exercise 25.3 Return to Equation (25.4). Use the code provided in this chapter to generate a spaghetti and ensemble average plot where \\(D = 0.1\\). What happens to the resulting spaghetti and ensemble plots when \\(D = 0.01, \\; 1, \\; 10\\)? Exercise 25.4 When \\(a=0.15\\) and \\(b=0.3316\\), determine the equilibrium solutions for Equation (25.6) and using the Jacobian, evaluate the stability of the equilibrium solutions. Exercise 25.5 Consider Equation (25.6) to answer the following questions: Determine the equilibrium solutions and the Jacobian matrix for general \\(a\\) and \\(b\\). Set \\(a=0.15\\) but let \\(b\\) be a free parameter (\\(b&gt;0\\)). Evaluate the stability of the equilibrium solutions as a function of \\(b\\). Exercise 25.6 (Inspired by Logan and Wolesensky (2009)) Consider the logistic differential equation: \\(\\displaystyle \\frac{dx}{dt} = r x \\left( 1-\\frac{x}{K} \\right)\\). Assume there is stochasticity in the inverse carrying capacity \\(1/K\\) (so this means you will consider \\(1/K + \\mbox{ Noise }\\)). Identify the deterministic and stochastic parts of each of the differential equation. Assume that \\(x(0)=3\\), \\(r=0.8\\), \\(K=100\\), \\(\\Delta t = 0.05\\), and the number of timesteps is 200. Set \\(D=0\\). Do you get a value consistent with the deterministic solution to the logistic differential equation? Run several simulations where you slowly increase the value of \\(D\\). At what point does \\(D\\) get too large to produce meaningful results? With your chosen value of \\(D\\), do 500 simulations of this stochastic process and compute the ensemble average. Contrast your results to when we added stochasticity to the parameter \\(r\\) in the logistic model. FIGURE 25.6: The \\(SIS\\) model. Exercise 25.7 (Inspired by Logan and Wolesensky (2009)) An \\(SIS\\) model is one where susceptibles \\(S\\) become infected \\(I\\), and then after recovering from an illness, become susceptible again. The schematic representing this is shown in Figure 25.6. While you can write this as a system of differential equations, assuming the population size is constant \\(N\\), this simplifies to the following differential equation: \\[\\begin{equation} \\frac{dI}{dt} = b(N-I) I - r I \\end{equation}\\] Determine the equilibrium solutions for this model. As a bonus, analyze the stability of the equilibrium solutions. You will need to assume that all parameters are positive and \\(bN-r &gt; 0\\). Assuming \\(N=1000\\), \\(r=0.01\\), and \\(b=0.005\\), \\(I(0)=1\\), apply Euler’s method to simulate this differential equation over two weeks with \\(\\Delta t = 0.1\\) days. Show the plot of your result. Assume the transmission rate \\(b\\) is stochastic. Write down this stochastic differential equation. Do 500 simulations of this stochastic process with with \\(D = 1 \\cdot 10^{-6}\\). Generate a spaghetti plot of your results. Contrast this result to the deterministic solution. Assume the recovery rate \\(r\\) is stochastic. Write down this stochastic differential equation. Do 500 simulations of this stochastic process with \\(D = 1 \\cdot 10^{-3}\\). Generate a spaghetti plot of your results. Contrast this result to the deterministic solution. Exercise 25.8 Organisms that live in a saline environment biochemically maintain the amount of salt in their blood stream. An equation that represents the level of \\(S\\) (as a percent) in the blood is the following: \\[\\frac{dS}{dt} = I + p \\cdot (W - S), \\] where the parameter \\(I\\) represents the active uptake of salt (% / hour), \\(p\\) is the permeability of the skin (hour\\(^{-1}\\)), and \\(W\\) is the salinity in the water (as a percent). Use this information to answer the following questions: When \\(S(0)=S_{0}\\), apply techniques from Chapter 7 to determine an exact solution for this initial value problem. Set \\(I = 0.1\\), \\(p = 0.05\\), \\(W = 0.4\\), \\(S_{0}=0.6\\). Make a plot of your solution for \\(0 \\leq t \\leq 20\\). What is a corresponding stochastic differential equation when the parameter \\(p\\) is stochastic? Set \\(I = 0.1\\), \\(p = 0.05\\), \\(W = 0.4\\), \\(S_{0}=0.6\\). With \\(\\Delta t = 0.05\\) and for 400 timesteps, with \\(D = 0.1\\), simulate this stochastic process. With an ensemble of 200 realizations compare the generated ensemble average to your deterministic solution. Exercise 25.9 (Inspired by Munz et al. (2009)) Consider the following model for zombie population dynamics: \\[\\begin{equation} \\begin{split} \\frac{dS}{dt} &amp;=-\\beta S Z - \\delta S \\\\ \\frac{dZ}{dt} &amp;= \\beta S Z + \\xi R - \\alpha SZ \\\\ \\frac{dR}{dt} &amp;= \\delta S+ \\alpha SZ - \\xi R \\end{split} \\end{equation}\\] Let’s assume the transmission rate \\(\\beta\\) is a stochastic parameter. With this assumption, group each differential equation into two parts: terms not involving noise (the deterministic part) and terms that are multiplied by noise (the stochastic part) Deterministic part for \\(\\displaystyle \\frac{dS}{dt}\\): Stochastic part for \\(\\displaystyle \\frac{dS}{dt}\\): Deterministic part for \\(\\displaystyle \\frac{dZ}{dt}\\): Stochastic part for \\(\\displaystyle\\frac{dZ}{dt}\\): Deterministic part for \\(\\displaystyle \\frac{dR}{dt}\\): Stochastic part for \\(\\displaystyle \\frac{dR}{dt}\\): Apply the Euler-Maruyama method to generate an ensemble avearage plot with the following values: \\(D = 5 \\cdot 10^{-6}\\) \\(\\Delta t= 0.05\\). Timesteps: 1000. \\(\\beta = 0.0095\\), \\(\\delta = 0.0001\\) ,\\(\\xi = 0.1\\), \\(\\alpha = 0.005\\). Initial condition: \\(S(0)=499\\), \\(Z(0)=1\\), \\(R(0)=0\\). Set the number of simulations to be 100. How does making \\(\\beta\\) stochastic affect the disease transmission? Exercise 25.10 Evaluate results from stochastic simulation of Equation (25.6) when the parameter \\(a\\) is stochastic. You will need to determine an appropriate value of \\(D\\) with the values of the parameters and timestep given. Contrast your findings with the results presented in Figure 25.5. References "],["simul-stoch-26.html", "Chapter 26 Statistics of a Stochastic Differential Equation 26.1 Expected value of a stochastic process 26.2 Birth-death processes 26.3 Wrapping up 26.4 Exercises", " Chapter 26 Statistics of a Stochastic Differential Equation In Chapter 25 you explored ways to incorporate stochastic processes into a differential equation. The workflow (Do once \\(\\rightarrow\\) Do several times \\(\\rightarrow\\) Summarize \\(\\rightarrow\\) Visualize) generated ensemble averages of several different realizations of a stochastic process, which allowed you to visually characterize the solution. The ensemble average indicates that the solution to a stochastic process is a probability distribution (similar to what we studied in Chapter 9). More importantly, this probability distribution may evolve and change in time. This chapter examines how this distribution changes in time by computing statistics from these stochastic processes. Additionally we will also investigate stochastic processes with tools from Markov modeling (such as the random walk mathematics from Chapter 23. Let’s get started! 26.1 Expected value of a stochastic process Chapter 25 introduced a general form of a stochastic differential equation for a single variable \\(X\\): \\[\\begin{equation} dX = a(X,t) \\; dt + b(X,t) \\, dW(t) \\tag{26.1} \\end{equation}\\] We will now look at a few idealized examples of Equation (26.1) to understand the behavior of simulated solutions. 26.1.1 A purely stochastic process The first example is when \\(a(X,t)=0\\) and \\(b(X,t)=1\\) (a constant) in Equation (26.1), so \\(\\displaystyle dX = dW(t)\\). We call this example a purely stochastic process. Let’s code this up using the function sde to examine one realization (Figure 26.1) with \\(\\Delta t =0.2\\) and \\(X(0)=0\\). sde &lt;- function(number_steps, dt) { x0 &lt;- 0 # The initial condition # Set up vector of results x &lt;- array(x0, dim = number_steps) # Iterate through this random process. for (i in 2:number_steps) { x[i] &lt;- x[i - 1] + rnorm(1) * sqrt(dt) } # Create the time vector t &lt;- seq(0, length.out = number_steps, by = dt) out_x &lt;- tibble(t, x) return(out_x) } out &lt;- sde(1000, .2) ggplot(data = out) + geom_line(aes(x = t, y = x)) FIGURE 26.1: One realization of the stochastic differential equation \\(dX = 2 \\; dW(t)\\). Let’s highlight two parts from the function sde: Notice the line x[i] &lt;- x[i-1] + rnorm(1)*sqrt(dt) where we iterate through the random process with the term \\(\\sqrt{dt}\\), similar to what we did in Chapter 25. To produce the correct time intervals in Figure 26.1 we defined a time vector: t &lt;- seq(0,length.out=number_steps,by=dt). When we repeat this process several times and plot of the results, we have the following ensemble average in Figure 26.2: # Many solutions n_sims &lt;- 1000 # The number of simulations # Compute solutions sde_run &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ sde(1000, .2)) %&gt;% map_dfr(~.x, .id = &quot;simulation&quot;) # Compute Quantiles and summarize quantile_vals &lt;- c(0.025, 0.5, 0.975) # Summarize the variables summarized_sde &lt;- sde_run %&gt;% group_by(t) %&gt;% summarize( q_val = quantile(x, # x is the column to compute the quantiles probs = quantile_vals, na.rm = TRUE # remove NA values ), q_name = quantile_vals ) %&gt;% pivot_wider( names_from = &quot;q_name&quot;, values_from = &quot;q_val&quot;, names_glue = &quot;q{q_name}&quot; ) # Make the plot ggplot(data = summarized_sde) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2) FIGURE 26.2: Ensemble average of 1000 realizations of the stochastic differential equation \\(dX = dW(t)\\). I sure hope (again!) the results are very similar to ones generated in Chapters 23 and Chapter 24 (especially Figures 23.3 and 24.4) - this is no coincidence! Figure 24.4 is another simulation of Brownian motion with \\(D = 1/2\\). 26.1.2 Stochastics with drift The second example is a modification where \\(a(X,t)=2\\) and \\(b(X,t)=1\\) in Equation (26.1), yielding \\(\\displaystyle dX = 0.2 \\; dt + dW(t)\\). To simulate this stochastic process you can easily modify this approach by modifying the function sde, which I will call sde_v2. The code to simulate the stochastic process and visualize the ensemble average (Figure 26.3) is shown below: sde_v2 &lt;- function(number_steps, dt) { a &lt;- 2 b &lt;- 1 # The value of b x0 &lt;- 0 # The initial condition ### Set up vector of results x &lt;- array(x0, dim = number_steps) for (i in 2:number_steps) { x[i] &lt;- x[i - 1] + a * dt + b * rnorm(1) * sqrt(dt) } # Create the time vectror t &lt;- seq(0, length.out = number_steps, by = dt) out_x &lt;- tibble(t, x) return(out_x) } # Many solutions n_sims &lt;- 1000 # The number of simulations # Compute solutions sde_v2_run &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ sde_v2(1000, .2)) %&gt;% map_dfr(~.x, .id = &quot;simulation&quot;) # Compute Quantiles and summarize quantile_vals &lt;- c(0.025, 0.5, 0.975) ### Summarize the variables summarized_sde_v2 &lt;- sde_v2_run %&gt;% group_by(t) %&gt;% summarize( q_val = quantile(x, # x is the column to compute the quantiles probs = quantile_vals, na.rm = TRUE # Remove NA values ), q_name = quantile_vals ) %&gt;% pivot_wider( names_from = &quot;q_name&quot;, values_from = &quot;q_val&quot;, names_glue = &quot;q{q_name}&quot; ) ### Make the plot ggplot(data = summarized_sde_v2) + geom_line(aes(x = t, y = q0.5)) + geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2) FIGURE 26.3: Ensemble average of 1000 realizations of the stochastic process \\(dX = 2 \\; dt + dW(t)\\). In Figure 26.3, the median seems to follow a straight line. Approximating the final position of the median at \\(t=200\\) and \\(X=400\\), the slope of this line \\(\\displaystyle \\frac{\\Delta X}{\\Delta t} \\approx \\frac{400}{200} = 2\\). So we could say that the median of this distribution follows the line \\(X=2t\\). I recognize the ensemble average computes the median which is not the average. Because we are simulating this process with random variables drawn from a normal distribution we assume the median approximates the average. Let’s generalize the lessons learned from these examples. The solution to the SDE \\(\\displaystyle dX = a(X,t) \\; dt + b(X,t) \\, dW(t)\\) is probability distribution \\(p(X,t)\\). The expected value, \\(\\mu\\), is denoted as \\(E[p(X,t)]\\). The solution to the differential equation \\(\\displaystyle dX = a(X,t) \\; dt\\) equals the expected value \\(\\mu\\). This result should make some intuitive sense: stochastic differential equations should - on the average - converge to the deterministic solution. The variance is a little harder to compute as the definition of the variance of a random variable \\(Y\\) is \\(\\displaystyle \\sigma^{2} = E[(Y - \\mu)^{2}] = \\sigma^{2}= E[Y^{2}] - (E[Y] )^{2}\\). However, what we can say is that the variance should be proportional (in some way) to \\(b(X,t) \\; dt\\). In both our cases studied here \\(b(X,t)=1\\), so we can infer that the variance grows in time. 26.2 Birth-death processes In Chapter 21 we examined a model of moose population dynamics with stochastic population fluctuations. In many biological contexts this makes sense: rarely do biological processes follow a deterministic curve. On the other hand, models usually start with a differential equation. What are we to do? To reconcile this, another way to generate a stochastic process is through consideration of the differential equation itself. Let’s go back to the logistic population model but re-written in a specific way (Equation (26.2)). \\[\\begin{equation} \\frac{dx}{dt} = r x \\left( 1 - \\frac{x}{K} \\right) = r x - \\frac{rx^{2}}{K} \\tag{26.2} \\end{equation}\\] From Equation (26.2), we can obtain a change in the variable \\(x\\) (denoted as \\(\\Delta x\\)) over \\(\\Delta t\\) units by re-writing the differential equation in differential form: \\[\\begin{equation} \\Delta x = r x \\Delta t - \\frac{rx^{2}}{K} \\Delta t \\tag{26.3} \\end{equation}\\] Equation (26.3) is separated into two terms - one that increases the variable \\(x\\) (represented by \\(r x \\Delta t\\), the same units as \\(x\\)) and one that decreases the variable (represented by \\(\\displaystyle \\frac{rx^{2}}{K} \\Delta t\\), the same units as \\(x\\)). We will consider these changes in \\(x\\) with the associated Markov random variable \\(\\Delta X\\) organized in Table 26.1. (It is okay and understandable if you envison \\(\\Delta x\\) and \\(\\Delta X\\) as conceptually similar.) TABLE 26.1: Unit increment changes for the Markov variable \\(\\Delta x\\), based on Equation (26.3). Outcome Probability \\(\\Delta X = 1\\) (population change by 1) \\(r X \\; \\Delta t\\) \\(\\Delta X = -1\\) (population change by \\(-1\\)) \\(\\displaystyle \\frac{rX^{2}}{K} \\; \\Delta t\\) \\(\\Delta X = 0\\) (no population change) \\(\\displaystyle 1 - rX \\; \\Delta t - \\frac{rX^{2}}{K} \\; \\Delta t\\) It also may be helpful to think of these changes on the following number line (Figure 26.4), similar to how we examined the random walk number line (or Markov process) from Chapter 23. FIGURE 26.4: A random walk for Equation (26.3). It may seem odd to think of the different outcomes (\\(\\Delta X\\) equals 1, \\(-1\\), or 1) as probabilities. Assuming that the time interval \\(\\Delta t\\) is small enough, this won’t be a numerical difficulty. Let’s compute \\(E[\\Delta X]\\) and the variance \\(\\sigma^{2}\\) as we did in Chapter 23. The computation of \\(\\mu\\) is shown in Equation (26.4). \\[\\begin{equation} \\begin{split} \\mu = E[\\Delta X] &amp;= (1) \\cdot \\mbox{Pr}(\\Delta X = 1) + (- 1) \\cdot \\mbox{Pr}(\\Delta X = -1) + (0) \\cdot \\mbox{Pr}(\\Delta X = 0) \\\\ &amp;= (1) \\cdot \\left( r X \\; \\Delta t \\right) + (-1) \\frac{rX^{2}}{K} \\; \\Delta t \\\\ &amp;= r X \\; \\Delta t - \\frac{rX^{2}}{K} \\; \\Delta t \\end{split} \\tag{26.4} \\end{equation}\\] Observe that the right hand side of Equation (26.4) is similar to the right hand side of the original differential equation (Equation (26.3)))! Next let’s also calculate the variance of \\(\\Delta X\\) (Equation (26.5)). \\[\\begin{equation} \\begin{split} \\sigma^{2} &amp;= E[(\\Delta X)^{2}] - (E[\\Delta X] )^{2} \\\\ &amp;= (1)^{2} \\cdot \\mbox{Pr}(\\Delta X = 1) + (- 1)^{2} \\cdot \\mbox{Pr}(\\Delta X = -1) + (0)^{2} \\cdot \\mbox{Pr}(\\Delta X = 0) - (E[\\Delta X] )^{2} \\\\ &amp;= (1) \\cdot \\left( r X \\; \\Delta t \\right) + (1) \\frac{rX^{2}}{K} \\; \\Delta t - \\left( r X \\; \\Delta t - \\frac{rX^{2}}{K} \\; \\Delta t \\right)^{2} \\end{split} \\tag{26.5} \\end{equation}\\] Notice the term \\(\\displaystyle \\left( r X \\; \\Delta t - \\frac{rX^{2}}{K} \\; \\Delta t \\right)^{2}\\) in Equation (26.5). While this may seem complicated, we are going to simplify things by only computing the variance to the first order in \\(\\Delta t\\). Because of that, we are going to assume that in \\(\\sigma^{2}\\) any terms involving \\((\\Delta t)^{2}\\) are small, or in effect negligible. While this is a huge simplifying assumption for the variance, it is useful! The combination of the mean (Equation (26.4)) and variance (Equation (26.5)) yields the stochastic process in Equation (26.6). \\[\\begin{equation} \\begin{split} dX &amp;= \\mu + \\sigma \\; dW(t) \\\\ &amp;= \\left( r X - \\frac{rX^{2}}{K} \\right) \\; dt + \\sqrt{\\left( r X + \\frac{rX^{2}}{K} \\right)} \\; dW(t) \\end{split} \\tag{26.6} \\end{equation}\\] To simulate this stochastic process we will modify euler_stochastic from Chapter 25, appending _log to denote “logistic” for each of the parts and applying the workflow (Do once \\(\\rightarrow\\) Do several times \\(\\rightarrow\\) Summarize \\(\\rightarrow\\) Visualize). One realization of this stochastic process is shown in Figure 26.5. # Identify the birth and death parts of the DE: deterministic_rate_log &lt;- c(dx ~ r * x - r * x^2 / K) stochastic_rate_log &lt;- c(dx ~ sqrt(r * x + r * x^2 / K)) # Identify the initial condition and any parameters init_log &lt;- c(x = 3) parameters_log &lt;- c(r = 0.8, K = 100) # Identify how long we run the simulation deltaT_log &lt;- .05 # timestep length time_steps_log &lt;- 200 # must be a number greater than 1 # Identify the diffusion coefficient D_log &lt;- 1 # Do one simulation of this differential equation out_log &lt;- euler_stochastic( deterministic_rate = deterministic_rate_log, stochastic_rate = stochastic_rate_log, initial_condition = init_log, parameters = parameters_log, deltaT = deltaT_log, n_steps = time_steps_log, D = D_log ) # Plot out the solution ggplot(data = out_log) + geom_line(aes(x = t, y = x)) FIGURE 26.5: One realization of Equation (26.6). The following code will produce a spaghetti plot from 100 different simulations. Try this code out on your own. # Many solutions n_sims &lt;- 100 # The number of simulations # Compute solutions logistic_sim_r &lt;- rerun(n_sims) %&gt;% set_names(paste0(&quot;sim&quot;, 1:n_sims)) %&gt;% map(~ euler_stochastic( deterministic_rate = deterministic_rate_log, stochastic_rate = stochastic_rate_log, initial_condition = init_log, parameters = parameters_log, deltaT = deltaT_log, n_steps = time_steps_log, D = D_log )) %&gt;% map_dfr(~.x, .id = &quot;simulation&quot;) # Plot these all together ggplot(data = logistic_sim_r) + geom_line(aes(x = t, y = x, color = simulation)) + guides(color = &quot;none&quot;) Finally, Figure 26.6 displays the ensemble average plot from all the simulations. Figure 26.6 also includes the solution to the the logistic differential equation for comparison. # Compute Quantiles and summarize quantile_vals &lt;- c(0.025, 0.5, 0.975) # Summarize the variables summarized_logistic &lt;- logistic_sim_r %&gt;% group_by(t) %&gt;% summarize( q_val = quantile(x, # x is the column to compute the quantiles probs = quantile_vals, na.rm = TRUE ), q_name = quantile_vals ) %&gt;% pivot_wider( names_from = &quot;q_name&quot;, values_from = &quot;q_val&quot;, names_glue = &quot;q{q_name}&quot; ) logistic_solution &lt;- tibble( t = seq(0, 10, length.out = 100), x = 100 / (1 + 97 / 3 * exp(-0.8 * t)) ) # Make the plot ggplot(data = summarized_logistic) + geom_line(aes(x = t, y = q0.5), color = &quot;red&quot;, size = 1) + geom_ribbon(aes(x = t, ymin = q0.025, ymax = q0.975), alpha = 0.2, fill = &quot;red&quot;) + geom_line(data = logistic_solution, aes(x = t, y = x), size = 1, linetype = &quot;dashed&quot;) FIGURE 26.6: Ensemble average of 100 simulations of Equation (26.6) (in red). The red line represents the median with the shading the 95% confidence interval. For comparison, the deterministic solution to the logistic differential equation is the dashed black curve. Awesome! Notice how the median in the ensemble average plot in Figure 26.6 closely tracks the deterministic solution. The types of stochastic processes we are describing in this chapter are called “birth-death” processes. Here is another way to think about Equation (26.3): \\[\\begin{equation} \\begin{split} r x \\; \\Delta t &amp;= \\alpha(x) \\\\ \\frac{rx^{2}}{K} \\; \\Delta t &amp;= \\delta(x) \\end{split} \\tag{26.7} \\end{equation}\\] In this way, we think of \\(\\alpha(x)\\) in Equation (26.7) as a “birth process” and \\(\\delta(x)\\) as a “death process.” When we computed the mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) for Equation (26.3) we had the mean \\(\\mu = \\alpha(x)-\\delta(x)\\) and variance \\(\\sigma^{2}=\\alpha(x)+\\delta(x)\\). (You should double check to make sure this is the case!). Here is an interesting fact: \\(\\mu = \\alpha(x)-\\delta(x)\\) and \\(\\sigma^{2}=\\alpha(x)+\\delta(x)\\) hold up for any differential equation where we have identified a birth (\\(\\alpha(x)\\)) or death (\\(\\delta(x)\\)) process. Generalizing birth-death processes for a system of differential equations is a little more involved as there are more possibilities involved when computing \\(\\mu\\) and \\(\\sigma\\). We don’t tackle this complexity here, but some considerations are thinking through all possible combinations of increase or decrease in the variables. For example, one variable could increase where the other decreases, both could increase, or one variable decreases and the other remains the same. In addition, the standard deviation is computed by the square root of the variance, and for a linear system of equations the matrix square root involves matrix decompositions. If you are curious to know more, see Logan and Wolesensky (2009) for more information on implementing birth-death processes for a system of differential equations. 26.3 Wrapping up In the exercises for this chapter you will solve problems involving applications of stochastic processes in biological contexts. The nice part is that the framework presented here generalizes to several other contexts where a stochastic process can be identified. This chapter contained several key points about statistics for the SDE \\(dX = a(X,t) \\; dt + b(X,t) \\; dW(t)\\) and its solution \\(p(X,t)\\), summarized below: The solution of the deterministic equation \\(dX = a(X,t) \\; dt\\) equals \\(E[p(X,t)]\\). The variance is proportional (but not equal) to the solution of \\(dX = b(X,t) \\; dt\\). The SDE can be simulated by Brownian motion, so \\(dX \\approx \\Delta X = a(X,t) \\; \\Delta t + b(X,t)\\; \\sqrt{2D \\, \\Delta t} \\, Z\\), where \\(Z\\) is a random variable from a unit normal distribution (so in R we would use rnorm(1)). Since \\(\\Delta x = x_{n+1}-x_{n}\\), then we have \\(x_{n+1} = x_{n} + \\mu + a(X,t) \\; \\Delta t + b(X,t)\\; \\sqrt{2D \\, \\Delta t} \\, Z\\) (the Euler-Maruyama method). Several realizations of the SDE and subsequent computation of the ensemble average approximately characterize the distribution \\(p(X,t)\\). Chapter 27 will revisit some of the examples studied in this chapter, but apply tools to further characterize the distribution \\(p(X,t)\\) with partial differential equations. There are still more mathematics to investigate, so onward! 26.4 Exercises Exercise 26.1 Simulate the stochastic process \\(dX = 2 \\; dW(t)\\) with the following values: \\(D = 1\\) \\(\\Delta t= 0.05\\) Timesteps: 200 Initial condition: \\(X(0)=1\\). Set the number of simulations to be 100. Generate a spaghetti plot and ensemble average of your simluation results. Exercise 26.2 Return to the simulation of the logistic differential equation in this chapter. To generate Figure 26.6 we set \\(D = 1\\). What happens to the resulting ensemble average plots when \\(D =0, \\; 0.01, \\; 0.1, \\; 1, \\; 10\\)? You may use the following values: Set the number of simulations to be 100. Initial condition: \\(x(0)=3\\) Parameters: \\(r=0.8\\), \\(K=100\\). \\(\\Delta t = 0.05\\) for 200 time steps. Exercise 26.3 For the logistic differential equation consider the following splitting of \\(\\alpha(x)\\) and \\(\\delta(x)\\) as a birth-death process: \\[\\begin{equation} \\begin{split} \\alpha(x) &amp;= rx - \\frac{rx^{2}}{2K} \\\\ \\delta(x) &amp;= \\frac{rx^{2}}{2K} \\end{split} \\end{equation}\\] Simulate this SDE with the following values: Initial condition: \\(x(0)=3\\) Parameters: \\(r=0.8\\), \\(K=100\\). \\(\\Delta t = 0.05\\) for 200 time steps. \\(D=1\\). Generate an ensemble average plot. How does this SDE compare to Figure 26.6? Exercise 26.4 Let \\(S(t)\\) denote the cumulative snowfall at a location at time \\(t\\), which we will assume to be a random process. Assume that probability of the change in the cumulative amount of snow from day \\(t\\) to day \\(t+\\Delta t\\) is the following: change probability \\(\\Delta S = \\sigma\\) \\(\\lambda \\Delta t\\) \\(\\Delta S = 0\\) \\(1- \\lambda \\Delta t\\) The parameter \\(\\lambda\\) represents the frequency of snowfall and \\(\\sigma\\) the amount of the snowfall in inches. For example, during January in Minneapolis, Minnesota, the probability \\(\\lambda\\) of it snowing 4 inches or more is 0.016, with \\(\\sigma=4\\). (This assumes a Poisson process with rate = 0.5/31, according to the Minnesota DNR.). The stochastic differential equation generated by this process is \\(dS = \\lambda \\sigma \\; dt + \\sqrt{\\lambda \\sigma^{2}} \\; dW(t) = .064 \\; dt + .506 \\; dW(t)\\)$ . With this information, what is \\(E[\\Delta S]\\) and the variance of \\(\\Delta S\\)? Simulate and summarize this stochastic process. Use \\(S(0)=0\\) and run 500 simulations of this stochastic process. Simulate this process for a month, using \\(\\Delta t = 0.1\\) for 300 timesteps and with \\(D=1\\). Show the resulting spaghetti plot and interpret your results. Exercise 26.5 Consider the stochastic differential equation \\(\\displaystyle dS = \\left( 1 - S \\right) \\; dt + \\sigma \\; dW(t)\\), where \\(\\sigma\\) controls the amount of stochastic noise. For this stochastic differential equation what is \\(E[S]\\) and Var\\((S)\\)? Exercise 26.6 Consider the equation \\[\\begin{equation*} \\Delta x = \\alpha(x) \\; \\Delta t - \\delta(x) \\; \\Delta t \\end{equation*}\\] If we consider \\(\\Delta x\\) to be a random variable, show that the expected value \\(\\mu\\) equals \\(\\alpha(x) \\; \\Delta t - \\delta(x) \\; \\Delta t\\) and the variance \\(\\sigma^{2}\\), to first order, equals \\(\\alpha(x) \\; \\Delta t + \\delta(x) \\; \\Delta t\\). References "],["solvingSDEs-27.html", "Chapter 27 Solutions to Stochastic Differential Equations 27.1 Meet the Fokker-Planck equation 27.2 Deterministically the end 27.3 Exercises", " Chapter 27 Solutions to Stochastic Differential Equations Chapters 25 and 26 simulated SDEs. Visualizing the ensemble average of solution trajectories revealed a distribution of solutions that evolves in time. This final chapter examines methods to characterize exact solutions to stochastic differential equations by developing formulas to characterize the evolution of the distribution of solutions in time. There are a lot of mathematics here that can be a stepping stone for further study in the field of stochastics. Let’s (for one last time) get started! 27.1 Meet the Fokker-Planck equation Let’s start with a general way to express a stochastic differential equation: \\[\\begin{equation} dx = a(x,t) \\; dt + b(x,t) \\; dW(t) \\tag{27.1} \\end{equation}\\] The “solution” to this SDE will be a probability density function \\(p(x,t)\\), describing the evolution of \\(p(x,t)\\) in both time and space. Based on our work with Brownian motion in Chapters 25 and 26, the probability density function \\(p(x,t)\\) should have the following properties: \\(E[p(x,t)]\\) is the deterministic solution to \\(\\displaystyle \\frac{dx}{dt} = a(x,t)\\). The variance \\(\\sigma^{2}\\) incorporates the function \\(b(x,t)\\). A way to determine the probability density function is by solving the following partial differential equation, termed the Fokker-Planck Equation (Equation (27.2)). \\[\\begin{equation} \\frac{\\partial p}{\\partial t} = - \\frac{\\partial}{\\partial x} \\left(p(x,t) \\cdot a(x,t) \\right) + \\frac{1}{2}\\frac{\\partial^{2} }{\\partial x^{2}} \\left(\\; p(x,t) \\cdot (b(x,t))^{2} \\;\\right) \\tag{27.2} \\end{equation}\\] We can write Equation (27.2) in shorthand, dropping the dependence of \\(x\\) and \\(t\\) for \\(p(x,t)\\), \\(a(x,t)\\) and \\(b(x,t)\\): \\(\\displaystyle p_{t} = - (p \\cdot a)_{x} + \\frac{1}{2} (p \\cdot b^{2})_{xx}\\). We do not include the derivation of the Fokker-Planck Equation here, as the proof requires concepts from advanced calculus (see Logan and Wolesensky (2009) for more discussion and derivation of the Fokker-Planck equation). However, let’s build up understanding of Equation (27.2) through some examples. 27.1.1 Diffusion (again) Consider the SDE \\(dx = dW(t)\\) with \\(x(0)=0\\) and apply the Fokker-Planck equation to characterize the solution \\(p(x,t)\\). We know from Chapter 24 that SDE \\(dx = dW(t)\\) characterizes Brownian motion. When we compare this SDE to the Fokker-Planck equation and Equation (27.1), we have \\(a(x,t)=0\\) and \\(b(x,t)=1\\), yielding Equation (27.3): \\[\\begin{equation*} p_{t} = \\frac{1}{2} p_{xx}. \\tag{27.3} \\end{equation*}\\] This equation should look familiar - it is the partial differential equation for diffusion (Equation (24.2))!52 The solution to this SDE is given by Equation (27.4). Figure 27.1 shows the evolution of \\(p(x,t)\\) in time. \\[\\begin{equation} p(x,t) = \\frac{1}{\\sqrt{2 \\pi t}} e^{-x^{2}/(2 t)} \\tag{27.4} \\end{equation}\\] One way to describe Equation (27.4) is a normally distributed random variable, with \\(E[p(x,t)]=0\\) and \\(\\sigma^{2}\\) (the variance) equal to \\(t\\). Notice how the mean and variance for Equation (27.4) connect back to our previous work with random walks and diffusion in Chapters 23 and 24. Namely, simulations and random walk mathematics showed that the expected value of a random walk or Brownian motion was zero and the variance grew in time, which is the same for this SDE. FIGURE 27.1: Profiles for the solution to SDE \\(dx = dW(t)\\) (given by Equation (27.4)) for different values of \\(t\\). Let’s discuss the initial condition for Equation (27.4). Our SDE had the initial condition \\(x(0)=0\\). How this initial condition translates to Equation (27.4) is that \\(x(0)\\) is the same as \\(p(x,0)\\). For this example the initial condition \\(p(x,0)\\) is a special function called the Dirac delta function, written as \\(p(x,0)=\\delta(x)\\). The function \\(\\delta(x)\\) is a special type of probability density function, which you may study in a course that explores the theory of functions. Applications of the Dirac delta function include modeling an impulse (such as the release of a particle from a specific point) and watching its evolution in time. 27.1.2 Diffusion with drift Now that we have a handle on the SDE \\(dx = dW(t)\\), let’s extend this next example a little more. Consider the SDE \\(dx = r \\; dt + \\sigma \\; dW(t)\\), where \\(r\\) and \\(\\sigma\\) are constants. As a first step, let’s examine the deterministic equation: \\(dx = r \\; dt\\), which has a linear function \\(x(t) = rt + x_{0}\\) as its solution.53 As before, the initial condition for this SDE is \\(p(x,0)=\\delta(x)\\). Applying Equation (27.2) to this SDE we obtain Equation (27.5): \\[\\begin{equation} p_{t} = -r \\, p_{x}+ \\frac{\\sigma^{2}}{2} \\, p_{xx} \\tag{27.5} \\end{equation}\\] Equation (27.5) is an example of a diffusion-advection equation. Amazingly through a change of variables we will reduce Equation (27.5) to an example of Equation (27.4). Here’s how to do this: First, let \\(x=z+r \\, \\tau\\) and \\(t=\\tau\\). This change of variables may seem odd, but our goal here is to write \\(p(x,t)=p(z,\\tau)\\) and to develop expressions for \\(p_{\\tau}\\) \\(p_{z}\\), and \\(p_{zz}\\) from this change of variables. But in order to do that, we will need to apply the multivariable chain rule (see Figure 27.2). FIGURE 27.2: Multivariable chain rule. By the multivariable chain rule we can develop expressions for \\(p_{\\tau}\\) and \\(p_{z}\\): \\[\\begin{equation} \\begin{split} \\frac{\\partial p}{\\partial \\tau} &amp; = \\frac{\\partial p}{\\partial x} \\cdot \\frac{ \\partial x}{\\partial \\tau} + \\frac{\\partial p}{\\partial t} \\cdot \\frac{ \\partial t}{\\partial \\tau} \\\\ \\frac{\\partial p}{\\partial z} &amp; = \\frac{\\partial p}{\\partial x} \\cdot \\frac{ \\partial x}{\\partial z} \\end{split} \\end{equation}\\] Now let’s consider the partial derivatives \\(\\displaystyle \\frac{\\partial x}{ \\partial \\tau}\\), \\(\\displaystyle \\frac{\\partial x}{ \\partial z}\\), and \\(\\displaystyle \\frac{\\partial t}{ \\partial z}\\). Remember that \\(x=z+r \\, \\tau\\). By direct differentiation \\(x_{\\tau} = r\\) and \\(x_{z} = 1\\). Also since \\(t=\\tau\\) then \\(\\tau_{t}=1\\). With these substitutions, we can now re-write \\(p_{\\tau}\\) and \\(p_{z}\\): \\[\\begin{equation} \\begin{split} \\frac{\\partial p}{\\partial \\tau} &amp; = \\frac{\\partial p}{\\partial x} \\cdot \\frac{ \\partial x}{\\partial \\tau} + \\frac{\\partial p}{\\partial t} \\cdot \\frac{ \\partial t}{\\partial z} = \\frac{\\partial p}{\\partial x} \\cdot r + \\frac{\\partial p}{\\partial t} \\cdot 1 \\\\ \\frac{\\partial p}{\\partial z} &amp; = \\frac{\\partial p}{\\partial x} \\cdot \\frac{ \\partial x}{\\partial z} = \\frac{\\partial p}{\\partial x} \\cdot 1 \\rightarrow \\frac{\\partial^{2} p}{\\partial z^{2}} = \\frac{\\partial^{2} p}{\\partial x^{2}} \\end{split} \\tag{27.6} \\end{equation}\\] Finally if re-write Equation (27.5) with the variables \\(z\\), \\(\\tau\\), and the change of variables (Equation (27.6)) we obtain Equation (27.7). \\[\\begin{equation} \\begin{split} \\underbrace{p_{t} + r p_{x}} &amp;=\\frac{\\sigma^{2}}{2} p_{xx} \\\\ p_{\\tau} &amp;= \\frac{\\sigma^{2}}{2} p_{zz} \\end{split} \\tag{27.7} \\end{equation}\\] All this work and transformation of variables to obtain Equation (27.7) yielded a diffusion equation in the variables \\(z\\) and \\(\\tau\\)! Applying Equation (27.4) with the variables \\(z\\) and \\(\\tau\\), we have \\(\\displaystyle p(z, \\tau) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2} \\tau}} e^{-z^{2}/(2 \\sigma^{2} \\tau)}\\), which we then transform back into the original variables \\(x\\) and \\(t\\) (Equation (27.8)): \\[\\begin{equation} p(x, t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2} t}} e^{-(x-rt)^{2}/(2 \\sigma^{2} t)} \\tag{27.8} \\end{equation}\\] Now that we have an equation, next let’s visualize the solution. Let’s take a look at some representative plots in Figure 27.3: FIGURE 27.3: Profiles for the solution to SDE \\(dx = r \\; dt + \\sigma \\; dW(t)\\) (given with Equation (27.8)) for different values of \\(t\\). Based on what we know of this distribution, it should look like a normal distribution with mean (expected value) equal to \\(rt\\) and variance equal to \\(\\sigma^{2}t\\). What the mean and variance tell us is that the mean is shifting and growing more diffuse as time increases. Remember that our solution to the deterministic equation was linear, and the mean of our distribution grows linearly as well! Also notice in Figure 27.3 as \\(t\\) increases the solution drifts (“advects”) to the right. 27.2 Deterministically the end The examples in this chapter scratch the surface of developing a deeper understanding of stochastic processes and differential equations. Even though we looked at a few test cases, there is a lot of power in understanding them (and integration across much of the mathematics you may have learned). Stochastic differential equations and stochastic processes are a fascinating field of study with a lot of interesting mathematics - I hope what you learned here will make you want to study it further! 27.3 Exercises Exercise 27.1 Consider the SDE \\(dx = dW(t)\\) with initial condition \\(x(0)=2\\). Using the results from this chapter, what is the solution \\(p(x,t)\\) for this SDE? Exercise 27.2 What is the solution to the SDE \\(\\displaystyle dX = 0.2 \\; dt + dW(t)\\) with initial condition \\(X(0)=0\\)? Plot a few sample profiles (in \\(X\\)) of \\(p(X,t)\\) at different times. Using your solution, what is E\\([p(X,t)]\\) (expected value) and \\(\\sigma^{2}\\) (variance)? Exercise 27.3 Let \\(S(t)\\) denote the cumulative snowfall at a location at time \\(t\\), which we will assume to be a random process. Assume that probability of the change in the cumulative amount of snow from day \\(t\\) to day \\(t+\\Delta t\\) is the following: change probability \\(\\Delta S = \\sigma\\) \\(\\lambda \\Delta t\\) \\(\\Delta S = 0\\) \\(1- \\lambda \\Delta t\\) The parameter \\(\\lambda\\) represents the frequency of snowfall and \\(\\sigma\\) the amount of the snowfall in inches. For example, during January in Minneapolis, Minnesota, the probability \\(\\lambda\\) of it snowing 4 inches or more is 0.016, with \\(\\sigma=4\\). (This assumes a Poisson process with rate = 0.5/31, according to the Minnesota DNR.) The stochastic differential equation generated by this process is \\(dS = \\lambda \\sigma \\; dt + \\sqrt{\\lambda \\sigma^{2}} \\; dW(t) = .064 \\; dt + .506 \\; dW(t)\\). What is the Fokker-Planck equation for the probability distribution \\(p(S,t)\\)? What is the solution \\(p(S,t)\\) for the Fokker-Planck equation? What are \\(E[p(S,t)]\\) and the variance of \\(p(S,t)\\)? Generate representative plots of the solution as it evolves over time. Exercise 27.4 A particle is moving in a gravitational field but is still allowed to diffuse randomly. In this case the stochastic differential equation is \\(dx = -g \\; dt + \\sqrt{D} \\; dW(t)\\), where \\(g\\) and \\(D\\) are constants. What is the Fokker-Planck partial differential equation for the probability distribution \\(p(x,t)\\)? Based on the work done in this chapter, what is the equation for the probability distribution \\(p(x,t)\\)? Exercise 27.5 Consider the stochastic differential equation \\(\\displaystyle dS = \\left( 1 - S \\right) \\; dt + \\sigma \\; dW(t)\\), where \\(\\sigma\\) controls the amount of stochastic noise. First let \\(\\sigma = 0\\) so the equation is entirely deterministic. Classify the stability of the equilibrium solutions for this differential equation. Still let \\(\\sigma = 0\\). Apply separation of variables to solve this differential equation. Now let \\(\\sigma = 0.1\\). Do 100 realizations of this stochastic process, with initial condition \\(S(0)=0.5\\). What do you notice? Now try different values of \\(\\sigma\\) larger and smaller than 0.1. What do you notice? What is the Fokker-Planck partial differential equation for the probability distribution \\(p(S,t)\\)? References "],["references.html", "References", " References "]]
